REFACTORINGS (LABELS),FEATURE REQUEST,SMELLS
"   Rename Method,Extract Method,","Allowing SqlOperator to be overridden in validation Calcite allows function to be overridden at validation step. To be more specific, users can provide their SqlOperatorTable, and, at validation step, their SqlOperatorTable will be called (method: lookupOperatorOverloads) to get a overriding function.  However, so far, SqlOperator (e.g., +, - , *, etc.) does not have this mechanism yet. 

Since other systems (e.g., Apache Drill) would have more flexible type-checks for SqlOperator's operands, this mechanism is necessary for those systems to pass through the validation step.","Duplicated Code, Long Method, ,"
"   Rename Method,Move Method,Extract Method,Push Down Attribute,","Add support for timestampadd / timestampdiff functions When calling timestampadd / timestampdiff with first parameter:

(SQL_TSI_)MICROSECOND ((SQL_TSI_)FRAC_SECOND (deprecated but we can leave for older versions compatibility)), (SQL_TSI_)SECOND, (SQL_TSI_)MINUTE, (SQL_TSI_)HOUR, (SQL_TSI_)DAY, (SQL_TSI_)WEEK, (SQL_TSI_)MONTH, (SQL_TSI_)QUARTER, (SQL_TSI_)YEAR

(ex: timestampadd(second, 1, current_datetime), calcite throws an error:

Caused by: org.apache.calcite.sql.parser.SqlParseException: Encountered ""( SECOND"" at line 1, column 25.
Was expecting one of:
    ""("" ""*"" ...
    ""("" "")"" ...
    ""("" ""WITH"" ...","Duplicated Code, Long Method, , , , "
"   Rename Method,","Add unary operator support for IS_NULL and IS_NOT_NULL to RexImplicationChecker Currently we support only SQL Comparison operators (SqlKind.COMPARE) for checking if one predicate implies another using RexImplicationChecker.
We would like to extend it for couple of Unary operators based on user request ( CALCITE-1104): IS_NULL and IS_NOT_NULL",", "
"   Move Method,Extract Method,","Add ProjectRemoveRule to pre-processing program of materialization substitution In VolcanoPlanner, we apply a simple pre-processing hep program to normalize the ""target"" and ""query"" rels before materialization substitution. Currently this program runs with two rules: FilterProjectTransposeRule and ProjectMergeRule.
We need an extra rule ProjectRemoveRule for the Phoenix use case where a secondary index (modeled as materialized views) is defined on a view so the materialized view ""queryRel"" may have an identity projection introduced by this view.","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,Inline Method,","Improve two-level column structure handling Calcite now has support for nested column structure in parsing and validation, by representing the inner-level columns as a RexFieldAccess based on a RexInputRef. Meanwhile it does not flatten the inner level structure in wildcard expansion, which would then cause an UnsupportedOperationException in Avatica.
 
The idea is to take into account this nested structure in column resolving, but to flatten the structure when translating to RelNode/RexNode.
For example, if the table structure is defined as
{code}VARCHAR K0,
VARCHAR C1,
RecordType(INTEGER C0, INTEGER C1) F0,
RecordType(INTEGER C0, INTEGER C2) F1{code}
, it should be viewed as a flat type like
{code}VARCHAR K0,
VARCHAR C1,
INTEGER F0.C0,
INTEGER F0.C1,
INTEGER F1.C0,
INTEGER F1.C2{code}
, so that:
1) Column reference ""K0"" is translated as {{$0}}
2) Column reference ""F0.C1"" is translated as {{$3}}
3) Wildcard ""*"" is translated as: {{$0, $1, $2, $3, $4, $5}}
4) Complex-column wildcard ""F1.*"", which is translated as {{$2, $3}}
And we would like to resolve columns based on the following rules (here we only consider the ""suffix"" part of the qualified names, which means the table resolving is already done by this time):
a) A two-part column name is matched with its first-level column name and its second-level column name. For example, ""F1.C0"" corresponds to $4; ""F1,X"" will throw a column not found error.
b) A single-part column name is matched against non-nested columns first, and if no matches, it is then matched against those second-level column names. For example, ""C1"" will be matched as ""$1"" instead of ""$3"", since non-nested columns have a higher priority; ""C2"" will be matched as ""$5""; ""C0"" will lead to an ambiguous column error, since it exists under both ""F0"" and ""F1"".
c) We would also like to have a way for defining ""default first-level column"" so that it has a precedence in column resolving over other first-level columns. For example, if ""F0"" is defined as default, ""C0"" will not cause an ambiguous column error, but instead be matched as ""$2"".
d) Reference to first-level column only without wildcard is not allowed, e.g., ""F1"".","Duplicated Code, Long Method, , , "
"   Move Class,Move And Rename Class,Extract Interface,Rename Method,Move Method,Move Attribute,","Standardize code style for ""import package.*;"" Our house style does not specify whether/when imports are to be converted to stars. I propose that imports should be converted to stars if there are more than 3 from the same package. Thus:

{code}
import a.b.C1;
import a.b.C2;
import a.b.C3;
{code}

becomes

{code}
import a.b.*;
{code}

when {{a.b.C4}} is added. This is consistent with IntelliJ's default rule.

It is OK to use stars if there are 3 or fewer uses. Thus removing the use of {{a.b.C2}} would not require imports to be changed.

Checkstyle has a rule to ban star imports (excluding certain packages) but does not allow them to be limited to a particular number.",", , , Large Class, "
"   Move Method,Extract Method,Move Attribute,","Add statistics SPI for lattice optimization algorithm In OPTIQ-427 we added an an optimization algorithm to choose an initial set of tiles to materialize. The {{rowCountEstimate}} attribute is the number of rows in the lattice.

Add an SPI to generate estimates of (a) the number of rows in the lattice, (b) the number of rows in a given tile of the lattice (specified by its dimensions).

Also add a default implementation of the SPI that executes SQL queries, caches the results, and uses some kind of approximation for the cardinalities of sets of attributes e.g. if quarter has 4 distinct values and year has 10 distinct values then (year, quarter) has 40 distinct values (or perhaps an expectation of 39.79 distinct values in a table with 3,650 rows, per the formula {{n . (1 - ((n - 1) / n) ^ p)}}).

Implementations that read stats from external stats tables, or execute SQL that samples a small percentage of the rows, would also be possible.","Duplicated Code, Long Method, , , , "
"   Rename Method,Extract Method,Move Attribute,","Make parser accept configurable max length for SQL identifier Calcite currently uses 128 as the maximum length for sql identifier.  We would like to enhance the SQL parser, so that each system should be able to choose its own identifier max length. 

The following is the discussion copied from the dev mailing list:

===============================================================
[Jinfeng]

Calcite (formly Optiq) has set the maximum length for an SQL identifier to 128.  This sounds quite reasonable for an ordinary sql identifier.  However, for system like Drill which allows user to query a file directly, it's quite likely to run out of the allowed identifier length. 

Drill allows the use case of :
   
         select * from dfs.schema.`directory1/directory2/filename.json`;

The directory plus the filename is parsed as a SQL identifer. Since many file system would allow file name up to 255 bytes [1], it's likely user could use an identifier with more than 128 bytes when query the file directly.  

I would like to ask the Calcite community whether there is any performance impact if we bump up the maximum length for a SQL identifier.  If there is negligible performance impact, then, can we make the maximum length configurable in the SQL parser?  We could make slightly modification to CombinedParser.jj template, to allow each system to set its own max length, with default still being 128.

The approach to allow a configurable setting seems to be match what Postgres [2] is doing, which allows the length to be ""raised by changing the NAMEDATALEN constant in src/include/pg_config_manual.h."" [2]


[Julian]

Each system should be able to choose its own identifier max length.

Iâ€™d prefer to make it configurable at runtime. Then we can test it. Add an extra parameter to SqlParserâ€™s constructor, and have it call parser.setIdentifierMaxLength, similar to how it calls setQuotedCasing right now.

Or if you want to go further, create

interface SqlParser.Config {
 int identifierMaxLength();
 Casing quotedCasing();
 Casing unquotedCasing();
}

and replace all of the parameters of SqlParserâ€™s constructor with one Config parameter.
","Duplicated Code, Long Method, , , "
"   Rename Method,Move Method,Extract Method,",Add RelDistribution trait and Exchange relational expression Calcite is increasingly targeting to support distributed execution engines. Any distributed sql engine has Shuffle/Exchange operator to shuffle their data with other deamons in the runtime system. It will be useful to add a notion of {{Shuffle}} RelNode as a first class citizen in Calcite.,"Duplicated Code, Long Method, , , "
"   Extract Method,Move Attribute,",Add Avatica support for getTables Per the title.,"Duplicated Code, Long Method, , , "
"   Rename Method,Move Attribute,","Connection isolation for Avatica clients Avatica should manage independent connections to it's underlying data source for each client connection. Clients should be isolated from each other, so that one's statements don't collide with another's.",", , "
"   Rename Method,Extract Method,Pull Up Attribute,","Pass server-side exceptions back to the client Avatica RPC response objects should contain an exception field that can be deserialized and re-thrown on the client side. That way client sees stack traces that are more meaningful than ""500 error"".","Duplicated Code, Long Method, , Duplicated Code, "
"   Rename Method,Move Method,Extract Method,Move Attribute,","DML in Avatica, and split Execute out from Fetch request 0","Duplicated Code, Long Method, , , , "
"   Push Down Method,Push Down Attribute,","Provide a way for the Avatica client to query the server versions Currently there doesn't seem to be a way for Avatica clients to find out the Avatica RPC protocol version and versions of other server components.

The use case here is to allow the Apache Phoenix clients to request the Avatica version, Phoenix version and HBase version from the Phoenix Query Server.",", , , "
"   Rename Method,Extract Method,Move Attribute,","Enable client to recover from missing server-side state When deploying more than one instance of an avatica-server, we have the desire to treat the collection of servers as a single server. Ideally, we want to have the avatica-client operate in a manner that doesn't expect a server to have specific state For example, the avatica-client should be able to know that when a server doesn't have a statement with the ID the client thinks it should, the client can create a new statement.

This is desirable because it allows us to use a generic load-balancer between clients and servers without the need for clustering or sticky sessions. The downside is that in the face of failure, operations will take longer than normal. Even with the performance hit, as long as an avatica-server exists, the client can still retrieve the results for some query which is ideal (tl;dr it will take longer, but the client still gets the answer).

Two major areas that need to be addressed presently are:

1. Automatic recreation of Statements when they are not cached
2. Recreation of ResultSets to resume iteration (for fetch()). This depends on ""stable results"" by the underlying JDBC driver, otherwise external synchronization would be required. This is considered a prerequisite.","Duplicated Code, Long Method, , , "
"   Rename Method,","Support stream joins Stream joins are used to relate information from different streams or stream and relation combinations. Calcite lacks (proper) support for stream-to-relation joins and stream-to-stream joins.

stream-to-relation join like below fails at the SQL validation stage.

select stream orders.orderId, orders.productId, products.name from orders join products on orders.productId = products.id

But if 'products' is a stream, the query is valid according to Calcite, even though the stream-to-stream join in above query is not valid due to unbounded nature of streams.",", "
"   Push Down Method,Extract Method,Push Down Attribute,","Provide generic server metadata in responses Some follow on from CALCITE-903:

The assumption in that work was that the common case in running behind a load-balancer is that a given client would continue to be routed back to the same avatica server instance. Sadly, this is not necessarily reality.

If the only load balancer technology available is only capable of an round-robin algorithm (or similar), we need to provide the information for a client to make a decision to return to the same server upon subsequent requests (e.g. fetching the next page of results).

Thinking more generally, the server which processed a given request is just general metadata. We could include things like the Avatica version, the ""real"" JDBC version information, etc.","Duplicated Code, Long Method, , , , "
"   Rename Method,Extract Method,","Support INPUT_FILES Currently we support INPUT_SEGMENT_NUMBERS to reading from specified segments, but will reading all data for one segment, so add INPUT_FILES to reading from specified files.","Duplicated Code, Long Method, , "
"   Rename Method,Move Method,Extract Method,Move Attribute,","Change query related RDD to use TableInfo Currently query related RDD is using CarbonTable which is read from file system, which introduces unnecessary file read. We can pass this schema information from driver by serializing TableInfo.","Duplicated Code, Long Method, , , , "
"   Rename Class,Rename Method,Push Down Method,Push Down Attribute,","Secure Dictionary Server Port Secure Dictionary Server Port Implementation.

During single pass load  Dictionary key creation is done through driver and executor communication through external ports. The communication will happen through TCP communication which is not secure or encrypted.
But in case spark turn on Security parameters then carbon dictionary ports also has to follows same authentication and encryption i.e. communicate through SASL Authentication protocol and Digest-MD5 encryption.
This PR makes the dictionary server and client communication Secure and encrypted. In case spark turn ON security and authentication through the below parameters then Carbon communication also becomes secure. 
By default the communication is still non secure. 

Parameters 
*spark.authenticate true
spark.authenticate.enableSaslEncryption true
spark.authenticate.secret*",", , , "
"   Rename Method,Move Method,Extract Method,","Add the blocklet info to index file and make the datamap distributable with job Add the blocklet info to index file and make the datamap distributable with job

1. Add the blocklet info to the carbonindex file, so datamap not required to read each carbondata file footer to the blocklet information. This makes the datamap loading faster.

2. Make the data map distributable and add to the spark job. So datamap pruning could happen distributable and pruned blocklet list would be sent to driver.","Duplicated Code, Long Method, , , "
"   Pull Up Method,Extract Method,Pull Up Attribute,","Add a value based compression for decimal data type when decimal is stored as Int or Long Added a value based compression for decimal data type when decimal is stored as Int or Long

When decimal precision is <= 9, decimal values are stored in 4 bytes but they are not compressed further based on min and max values as compared with other primitive data type compression. Therefore now based on min and max value decimal data falling in Integer range will be further compressed as byte or short.
When decimal precision is <= 18, decimal values are stored in 8 bytes but they are not compressed further based on min and max values as compared with other primitive data type compression. Therefore now based on min and max value decimal data falling in Long range will be further compressed as byte, short or int.
Advantage: This will reduce the storage space thereby decreasing the IO time while reading and decompressing the data.","Duplicated Code, Long Method, , Duplicated Code, Duplicated Code, "
"   Rename Method,","CarbonData unsupport Boolean data type   Spark/Hive table support Boolean data type, the internal table also should support Boolean data type.

   Boolean data type Range: TRUE or FALSE. Do not use quotation marks around the TRUE and FALSE literal values. You can write the literal values in uppercase, lowercase, or mixed case. The values queried from a table are always returned in lowercase, true or false.
    In implementing this function, we employ:
#       endcoding: RLE
#       data expression: byte array
        
    CarbonData should support boolean data type in following aspects:

#       create table: support Boolean data type
#       insert into table values: support insert Boolean column 
#       insert overwrite
#       insert into table select from another table
#       select from a table
#       load data: from a local csv file
#       filter: including >=, >, =, <=, <, =, !=, in, not in
#       describle: should show boolean data type

We also add some test cases in booleantype directory of spark2",", "
"   Push Down Method,Extract Method,Push Down Attribute,","Get the detailed blocklet information using default BlockletDataMap for other datamaps All the detail information of blocklet which is need for exceuting query is present only BlockletDataMap. It is actually default datamap.
So if new datamap is added then it gives only information of blocklet and blockid, it is insuffucient information to exceute query.
Please add the functionality to retrieve detailed blocklet information from the BlockletDataMap based on block and blocklet id","Duplicated Code, Long Method, , , , "
"   Rename Method,Extract Method,Inline Method,","Support Database Location Configuration while Creating Database/ Support Creation of carbon Table in the database location Support Creation of carbon table at the database location
*Please refer to  for Design and discussion:*
http://apache-carbondata-dev-mailing-list-archive.1130556.n5.nabble.com/DISCUSSION-Support-Database-Location-Configuration-while-Creating-Database-td23492.html","Duplicated Code, Long Method, , , "
"   Rename Method,","Add Event Listener interface to Carbondata Add Event Listener interface to Carbondata. This will allow extending the current functionality of various commands to perform various other operations.
Example: After completion of load process, if any aggregate tables are created on that table, then data load operation need to be done for the aggregate table also. In this case we can create a listener such as AggregateLoadListener and register it as an event bus. Then this listener can be called once the load operation is completed which will take care of loading the aggregate table.",", "
"   Move Method,Extract Method,Move Attribute,","Add scale and decimal to DecimalType DecimalType should include scale and precision, and all scale and precision class member outside DecimalType should be removed","Duplicated Code, Long Method, , , , "
"   Move Method,Extract Method,","Merging carbonindex files for each segment. Hi,

Problem :
 The first-time query of carbon becomes very slow. It is because of reading many small carbonindex files and cache to the driver at the first time. 
 Many carbonindex files are created in below case
 Loading data in large cluster
   For example, if the cluster size is 100 nodes then for each load 100 index files are created per segment. So after 100 loads, the number of carbonindex files becomes 10000. .

It will be slower to read all the files from the driver since a lot of namenode calls and IO operations.

Solution :
Merge the carbonindex files in two levels.so that we can reduce the IO calls to namenode and improves the read performance.

Merge within a segment.
Merge the carbonindex files to single file immediately after load completes within the segment. It would be named as a .carbonindexmerge file. It is actually not a true data merging but a simple file merge. So that the current structure of carbonindex files does not change. While reading we just read one file instead of many carbonindex files within the segment.

","Duplicated Code, Long Method, , , "
"   Rename Method,Inline Method,","Filter Optimization If the include filter can give us more than 60% of the block data, then include filter can be converted to exclude filter",", , "
"   Rename Method,","Clean up store path interface There are many getStorePath API, it should be unified in one place",", "
"   Rename Class,Move Method,Extract Method,Inline Method,",GeneriVectorizedReader for Presto Write a Generic Vectorized Reader for Presto to remove the dependencies on spark,"Duplicated Code, Long Method, , , , "
"   Rename Method,",Refactor SortStepRowUtil to make it more readable Refactor and optimize `SortRowStepUtil` to make it efficient and more readable.,", "
"   Rename Method,Extract Method,",Support specify tablePath when creating table User should be able to specify table path when creating table,"Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,Inline Method,Move Attribute,","Optimization in reading/writing for sort temp row during data loading # SCENARIO

Currently in carbondata data loading, during sort process step, records will be sorted partially and spilled to the disk. And then carbondata will read these records and do merge sort.

Since sort step is CPU-tense, during writing/reading these records, we can optimize the serialization/deserialization for these rows and reduce CPU consumption in parsing the rows.

This should enhance the data loading performance.

# RESOLVE
We can pick up the un-sorted fields in the row and pack them as bytes array and skip paring them.

# RESULT

I've tested it in my cluster and seen about 8% performance gained (74MB/s/Node -> 81MB/s/Node).",", , , "
"   Rename Class,Rename Method,Extract Method,Move Attribute,","Optimization in reading/writing for sort temp row during data loading # SCENARIO

Currently in carbondata data loading, during sort process step, records will be sorted partially and spilled to the disk. And then carbondata will read these records and do merge sort.

Since sort step is CPU-tense, during writing/reading these records, we can optimize the serialization/deserialization for these rows and reduce CPU consumption in parsing the rows.

This should enhance the data loading performance.

# RESOLVE
We can pick up the un-sorted fields in the row and pack them as bytes array and skip paring them.

# RESULT

I've tested it in my cluster and seen about 8% performance gained (74MB/s/Node -> 81MB/s/Node).","Duplicated Code, Long Method, , , "
"   Rename Class,Rename Method,Extract Method,Move Attribute,","Optimization in reading/writing for sort temp row during data loading # SCENARIO

Currently in carbondata data loading, during sort process step, records will be sorted partially and spilled to the disk. And then carbondata will read these records and do merge sort.

Since sort step is CPU-tense, during writing/reading these records, we can optimize the serialization/deserialization for these rows and reduce CPU consumption in parsing the rows.

This should enhance the data loading performance.

# RESOLVE
We can pick up the un-sorted fields in the row and pack them as bytes array and skip paring them.

# RESULT

I've tested it in my cluster and seen about 8% performance gained (74MB/s/Node -> 81MB/s/Node).","Duplicated Code, Long Method, , , "
"   Rename Class,Rename Method,Extract Method,Move Attribute,","Optimization in reading/writing for sort temp row during data loading # SCENARIO

Currently in carbondata data loading, during sort process step, records will be sorted partially and spilled to the disk. And then carbondata will read these records and do merge sort.

Since sort step is CPU-tense, during writing/reading these records, we can optimize the serialization/deserialization for these rows and reduce CPU consumption in parsing the rows.

This should enhance the data loading performance.

# RESOLVE
We can pick up the un-sorted fields in the row and pack them as bytes array and skip paring them.

# RESULT

I've tested it in my cluster and seen about 8% performance gained (74MB/s/Node -> 81MB/s/Node).","Duplicated Code, Long Method, , , "
"   Rename Class,Rename Method,Extract Method,Move Attribute,","Optimization in reading/writing for sort temp row during data loading # SCENARIO

Currently in carbondata data loading, during sort process step, records will be sorted partially and spilled to the disk. And then carbondata will read these records and do merge sort.

Since sort step is CPU-tense, during writing/reading these records, we can optimize the serialization/deserialization for these rows and reduce CPU consumption in parsing the rows.

This should enhance the data loading performance.

# RESOLVE
We can pick up the un-sorted fields in the row and pack them as bytes array and skip paring them.

# RESULT

I've tested it in my cluster and seen about 8% performance gained (74MB/s/Node -> 81MB/s/Node).","Duplicated Code, Long Method, , , "
"   Rename Class,Rename Method,Extract Method,Move Attribute,","Optimization in reading/writing for sort temp row during data loading # SCENARIO

Currently in carbondata data loading, during sort process step, records will be sorted partially and spilled to the disk. And then carbondata will read these records and do merge sort.

Since sort step is CPU-tense, during writing/reading these records, we can optimize the serialization/deserialization for these rows and reduce CPU consumption in parsing the rows.

This should enhance the data loading performance.

# RESOLVE
We can pick up the un-sorted fields in the row and pack them as bytes array and skip paring them.

# RESULT

I've tested it in my cluster and seen about 8% performance gained (74MB/s/Node -> 81MB/s/Node).","Duplicated Code, Long Method, , , "
"   Rename Class,Rename Method,Extract Method,Move Attribute,","Optimization in reading/writing for sort temp row during data loading # SCENARIO

Currently in carbondata data loading, during sort process step, records will be sorted partially and spilled to the disk. And then carbondata will read these records and do merge sort.

Since sort step is CPU-tense, during writing/reading these records, we can optimize the serialization/deserialization for these rows and reduce CPU consumption in parsing the rows.

This should enhance the data loading performance.

# RESOLVE
We can pick up the un-sorted fields in the row and pack them as bytes array and skip paring them.

# RESULT

I've tested it in my cluster and seen about 8% performance gained (74MB/s/Node -> 81MB/s/Node).","Duplicated Code, Long Method, , , "
"   Rename Class,Rename Method,Inline Method,Move Attribute,","Optimization in reading/writing for sort temp row during data loading # SCENARIO

Currently in carbondata data loading, during sort process step, records will be sorted partially and spilled to the disk. And then carbondata will read these records and do merge sort.

Since sort step is CPU-tense, during writing/reading these records, we can optimize the serialization/deserialization for these rows and reduce CPU consumption in parsing the rows.

This should enhance the data loading performance.

# RESOLVE
We can pick up the un-sorted fields in the row and pack them as bytes array and skip paring them.

# RESULT

I've tested it in my cluster and seen about 8% performance gained (74MB/s/Node -> 81MB/s/Node).",", , , "
"   Rename Method,Extract Method,","Optimization in data loading for skewed data In one of my cases, carbondata has to load skewed data files. The size of data file ranges from 1KB to about 5GB.

In current implementation, carbondata will distribute the file blocks(splits) among the nodes to maximum the data locality and data evenly distributed, we call it `block-node-assignment` for short.

However, the current implementation has some problems.

The assignment is block number based. The goal is to make sure that all the nodes deal the same amount number of blocks. In the skewed data scenario described above, the block of a small file and the block of a big file are very different from its size (1KB v.s. 64MB). As a result, the difference of total data size assigned for each data node is very large.

In order to solve this problem, the size of block should be considered during the block-node-assignment. One node can deal more blocks than another as long as the total size of blocks are almost the same.","Duplicated Code, Long Method, , "
"   Rename Method,","Optimization in data loading for skewed data In one of my cases, carbondata has to load skewed data files. The size of data file ranges from 1KB to about 5GB.

In current implementation, carbondata will distribute the file blocks(splits) among the nodes to maximum the data locality and data evenly distributed, we call it `block-node-assignment` for short.

However, the current implementation has some problems.

The assignment is block number based. The goal is to make sure that all the nodes deal the same amount number of blocks. In the skewed data scenario described above, the block of a small file and the block of a big file are very different from its size (1KB v.s. 64MB). As a result, the difference of total data size assigned for each data node is very large.

In order to solve this problem, the size of block should be considered during the block-node-assignment. One node can deal more blocks than another as long as the total size of blocks are almost the same.",", "
"   Rename Method,","Optimization in data loading for skewed data In one of my cases, carbondata has to load skewed data files. The size of data file ranges from 1KB to about 5GB.

In current implementation, carbondata will distribute the file blocks(splits) among the nodes to maximum the data locality and data evenly distributed, we call it `block-node-assignment` for short.

However, the current implementation has some problems.

The assignment is block number based. The goal is to make sure that all the nodes deal the same amount number of blocks. In the skewed data scenario described above, the block of a small file and the block of a big file are very different from its size (1KB v.s. 64MB). As a result, the difference of total data size assigned for each data node is very large.

In order to solve this problem, the size of block should be considered during the block-node-assignment. One node can deal more blocks than another as long as the total size of blocks are almost the same.",", "
"   Rename Class,Rename Method,Inline Method,Move Attribute,","Optimization in data loading for skewed data In one of my cases, carbondata has to load skewed data files. The size of data file ranges from 1KB to about 5GB.

In current implementation, carbondata will distribute the file blocks(splits) among the nodes to maximum the data locality and data evenly distributed, we call it `block-node-assignment` for short.

However, the current implementation has some problems.

The assignment is block number based. The goal is to make sure that all the nodes deal the same amount number of blocks. In the skewed data scenario described above, the block of a small file and the block of a big file are very different from its size (1KB v.s. 64MB). As a result, the difference of total data size assigned for each data node is very large.

In order to solve this problem, the size of block should be considered during the block-node-assignment. One node can deal more blocks than another as long as the total size of blocks are almost the same.",", , , "
"   Rename Method,","Optimization in data loading for skewed data In one of my cases, carbondata has to load skewed data files. The size of data file ranges from 1KB to about 5GB.

In current implementation, carbondata will distribute the file blocks(splits) among the nodes to maximum the data locality and data evenly distributed, we call it `block-node-assignment` for short.

However, the current implementation has some problems.

The assignment is block number based. The goal is to make sure that all the nodes deal the same amount number of blocks. In the skewed data scenario described above, the block of a small file and the block of a big file are very different from its size (1KB v.s. 64MB). As a result, the difference of total data size assigned for each data node is very large.

In order to solve this problem, the size of block should be considered during the block-node-assignment. One node can deal more blocks than another as long as the total size of blocks are almost the same.",", "
"   Rename Method,","Optimization in data loading for skewed data In one of my cases, carbondata has to load skewed data files. The size of data file ranges from 1KB to about 5GB.

In current implementation, carbondata will distribute the file blocks(splits) among the nodes to maximum the data locality and data evenly distributed, we call it `block-node-assignment` for short.

However, the current implementation has some problems.

The assignment is block number based. The goal is to make sure that all the nodes deal the same amount number of blocks. In the skewed data scenario described above, the block of a small file and the block of a big file are very different from its size (1KB v.s. 64MB). As a result, the difference of total data size assigned for each data node is very large.

In order to solve this problem, the size of block should be considered during the block-node-assignment. One node can deal more blocks than another as long as the total size of blocks are almost the same.",", "
"   Rename Method,Extract Method,","Optimization in data loading for skewed data In one of my cases, carbondata has to load skewed data files. The size of data file ranges from 1KB to about 5GB.

In current implementation, carbondata will distribute the file blocks(splits) among the nodes to maximum the data locality and data evenly distributed, we call it `block-node-assignment` for short.

However, the current implementation has some problems.

The assignment is block number based. The goal is to make sure that all the nodes deal the same amount number of blocks. In the skewed data scenario described above, the block of a small file and the block of a big file are very different from its size (1KB v.s. 64MB). As a result, the difference of total data size assigned for each data node is very large.

In order to solve this problem, the size of block should be considered during the block-node-assignment. One node can deal more blocks than another as long as the total size of blocks are almost the same.","Duplicated Code, Long Method, , "
"   Rename Method,","Optimization in data loading for skewed data In one of my cases, carbondata has to load skewed data files. The size of data file ranges from 1KB to about 5GB.

In current implementation, carbondata will distribute the file blocks(splits) among the nodes to maximum the data locality and data evenly distributed, we call it `block-node-assignment` for short.

However, the current implementation has some problems.

The assignment is block number based. The goal is to make sure that all the nodes deal the same amount number of blocks. In the skewed data scenario described above, the block of a small file and the block of a big file are very different from its size (1KB v.s. 64MB). As a result, the difference of total data size assigned for each data node is very large.

In order to solve this problem, the size of block should be considered during the block-node-assignment. One node can deal more blocks than another as long as the total size of blocks are almost the same.",", "
"   Rename Class,Move And Rename Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,",Refactor on query scan process to improve readability ,"Duplicated Code, Long Method, , , , , "
"   Rename Class,Move And Rename Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,",Refactor on query scan process to improve readability ,"Duplicated Code, Long Method, , , , , "
"   Rename Class,Move And Rename Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,",Refactor on query scan process to improve readability ,"Duplicated Code, Long Method, , , , , "
"   Move Method,Extract Method,Move Attribute,",Presto Integration - Code Refactoring Presto Integration - Code Refactoring to remove unnecessary class and improve the performance.,"Duplicated Code, Long Method, , , , "
"   Move Class,Move Method,Move Attribute,",Remove carbon-spark dependency for sdk module store-sdk module should not depend on carbon-spark module,", , , "
"   Move Class,Move Method,Move Attribute,",Remove carbon-spark dependency for sdk module store-sdk module should not depend on carbon-spark module,", , , "
"   Move Class,Move Method,Move Attribute,",Remove carbon-spark dependency for sdk module store-sdk module should not depend on carbon-spark module,", , , "
"   Rename Method,Move Method,Extract Method,Move Attribute,",Restructure the partition folders as per the standard hive folders ,"Duplicated Code, Long Method, , , , "
"   Rename Method,Move Method,Extract Method,Move Attribute,",Restructure the partition folders as per the standard hive folders ,"Duplicated Code, Long Method, , , , "
"   Rename Method,Extract Method,","Optimization in unsafe sort during data loading Inspired by batch_sort, if we have enough memory, in local_sort with unsafe property, we can hold all the row pages in memory if possible and only spill the pages to disk as sort temp file if the memory is unavailable.

Before spilling the pages, we can do in-memory merge sort of the pages.

Each time we request an unsafe row page, if the memory is unavailable, we can trigger a merge sort for the in-memory pages and spill the result to disk as a sort temp file. So the incoming pages will be held into the memory instead of spilling to disk directly.

After this implementation, the data size during each spilling will be bigger than that of before and will benefit the disk IO.","Duplicated Code, Long Method, , "
"   Rename Class,Extract Method,","Support write JSON/Avro data to carbon files In SDK, user should be able to write JSON or Avro data to carbon files","Duplicated Code, Long Method, , "
"   Rename Method,",Collect SQL execution information to driver side ,", "
"   Move Method,Extract Method,",Support SDK API to read schema in data file and schema file ,"Duplicated Code, Long Method, , , "
"   Rename Class,Move Method,","Save the datamaps to system folder of warehouse *Problem*
Currently, datamap schema is saved inside the main table schema itself. This approach cannot satisfy if a datamap belongs to more than one table. For suppose if we need to create a datamap joining 2 tables then we cannot keep the datamap schema under any one table.
And also accessing the datamaps required to read the main table schema every time, it is not well optimized. And if we need to create multiple datamaps for a table then all datamaps need to store under the schema of that table so the size of main table schema grows and impacts the performance.

*Solution*
Make the datamap schema independent of main table schema. And store the schema underÂ {{_system}}folder location. This location is configurable by using carbon propertyÂ {{carbon.system.folder.location}}Â , by default, it stores under the store location.
Created datamap schema in JSON format for better readability. And has the interfaces to store it in database.
MadeÂ {{on table <tablename>}}Â for datamap DDL as optional , so now user can create/drop or show datamaps withoutÂ {{on table}}Â option.",", , "
"   Rename Method,Pull Up Method,Move Method,Extract Method,Pull Up Attribute,",Refactored code to improve Distributable interface ,"Duplicated Code, Long Method, , , Duplicated Code, Duplicated Code, "
"   Rename Method,Pull Up Method,Move Method,Extract Method,Pull Up Attribute,",Refactored code to improve Distributable interface ,"Duplicated Code, Long Method, , , Duplicated Code, Duplicated Code, "
"   Rename Method,Extract Method,","Distributed search mode using gRPC When user gives SQL statement that only includes projection and filter, we can use RPC calls to do distributed scan on the carbon files directly instead of using RDD to do the query. In this mode, RDD overhead like RDD construction and DAG scheduling is avoided.","Duplicated Code, Long Method, , "
"   Extract Superclass,Move Method,",Add QueryExecutor in SearchMode for row-based CarbonRecordReader ,", , Duplicated Code, Large Class, "
"   Rename Method,",Add Profiler output in EXPLAIN command More information should give in EXPLAIN command to show the effeteness of datamap,", "
"   Rename Method,","Support visible/invisible datamap for performance tuning Invisible datamap will not be used during the query, which can be used to verify whether to remove this datamap in the future.

Â 

This feature is similar to `Invisible indexed` in mysql (https://dev.mysql.com/doc/refman/8.0/en/invisible-indexes.html).",", "
"   Rename Method,","Search mode support lucene datamap Carbon doesn's support now
{code:java}
18/04/23 06:12:14 ERROR CarbonSession: Exception when executing search mode: Error while resolving filter expression, fallback to SparkSQL
18/04/23 06:12:14 ERROR CarbonSession: Exception when executing search mode: Error while resolving filter expression, fallback to SparkSQL
{code}

Carbon should support it.",", "
"   Rename Method,","Search mode support lucene datamap Carbon doesn's support now
{code:java}
18/04/23 06:12:14 ERROR CarbonSession: Exception when executing search mode: Error while resolving filter expression, fallback to SparkSQL
18/04/23 06:12:14 ERROR CarbonSession: Exception when executing search mode: Error while resolving filter expression, fallback to SparkSQL
{code}

Carbon should support it.",", "
"   Rename Class,Rename Method,Move Method,Extract Method,","All DataMap should support REFRESH command Currently, only LuceneDataMap supports REFRESH command. If user create a BloomFilter DataMap, and trigger REFRESH DATAMAP, it is ignoring the command. 
The makes the datamap usage a lot of confusion.","Duplicated Code, Long Method, , , "
"   Rename Class,Rename Method,Extract Method,","Index DataMap should support immediate load and deferred load when creating the DataMap For 'preaggregate' and 'timeseries' datamap, carbon is loading the datamap as soon as user creates it. But for 'lucene' and 'bloomfilter' it is not. This behavior should be aligned, otherwise user will be confused. 
A better option is that when creating datamap, let user to choose whether load the datamap immediately or deferred (manually refresh later)","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Support Flat folder structure in carbon. 1. Flat folder makes all carbondata files store flat under table path.
2. It is controlled through table property `flat_folder`. By default it is false.
3. It cannot be hybrid, so user cannot change the property once table created.
4. Segment file is created for each loading.And segment file is created under MetaData folder under table path.
5. Segment number is added as part of carbondata and index files.
6. All datamap files now create directly under table path with <tablepath>/<dmname>/<segment_number>/<task_name>/dm
 
IUD : It supports but list files during IUD may hit performance.
Compaction: Supports 
Delete Segment : No impact
Clean files : No impact
Alter table : No impact
Pre Agg : Property need to inherited to child, so it also supports flat folder structure.
Partition : No Impact on this feature as it already has flat folder structure.
Streaming : Only during handoff it supports flat folder structure. Streaming segment location is no change.

Â ","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,",Support Flat folder structure in carbon. ,"Duplicated Code, Long Method, , "
"   Rename Method,",The order is different between write and read data type of schema in SDK The order is different between write and read data type of schema in SDK,", "
"   Rename Method,",The order is different between write and read data type of schema in SDK The order is different between write and read data type of schema in SDK,", "
"   Rename Method,","Support StreamSQL for streaming job Currently carbon supports creating streaming job via Spark Streaming API, this requires user to use spark-submit to create the streaming job. 
To make it easier for SQL users, carbon should support StreamSQL to manage the streaming job.",", "
"   Rename Method,","Support StreamSQL for streaming job Currently carbon supports creating streaming job via Spark Streaming API, this requires user to use spark-submit to create the streaming job. 
To make it easier for SQL users, carbon should support StreamSQL to manage the streaming job.",", "
"   Move And Rename Class,Rename Method,Move Method,Extract Method,","Carbon to support spark 2.3 version 1) Column vector and Columnar Batch interface compatibility issue

2) Other compatibility issues related to spark 2.3 version with carbon as few methods parameters types has been changed and classes got renamed.","Duplicated Code, Long Method, , , "
"   Move Method,Move Attribute,","Carbon to support spark 2.3 version 1) Column vector and Columnar Batch interface compatibility issue

2) Other compatibility issues related to spark 2.3 version with carbon as few methods parameters types has been changed and classes got renamed.",", , , "
"   Move And Rename Class,Move Method,Inline Method,Move Attribute,","Implement LRU cache in Bloom filter based on Carbon LRU cache interface Currently bloom cache is implemented using guava cache, carbon has its own lru cache interfaces and complete sysytem it controls the cache intstead of controlling feature wise. So replace guava cache with carbonÂ lruÂ cacheÂ ",", , , , "
"   Move And Rename Class,Move Method,Inline Method,Move Attribute,","Implement LRU cache in Bloom filter based on Carbon LRU cache interface Currently bloom cache is implemented using guava cache, carbon has its own lru cache interfaces and complete sysytem it controls the cache intstead of controlling feature wise. So replace guava cache with carbonÂ lruÂ cacheÂ ",", , , , "
"   Move Method,Extract Method,Inline Method,",Optimize carbon schema reader interface of SDK Optimize carbon schema reader interface of SDK,"Duplicated Code, Long Method, , , , "
"   Move Method,Extract Method,Inline Method,",Optimize carbon schema reader interface of SDK Optimize carbon schema reader interface of SDK,"Duplicated Code, Long Method, , , , "
"   Rename Method,","Introduce configurable Lock Path Currently table level lock files are prepared in table path and system level locks are created inside store.

When supporting S3, locking cannot be done is using S3 files as S3 support onlyÂ  eventual consistency.

So user can chose different HDFS location to configure locks, so that data can be on S3 and locks can be on HDFS.

User also chose Zookeeper LockÂ if does not have HDFS location.

SoÂ require to support a configuration pathÂ ""carbon.lock.path""

Â ",", "
"   Rename Method,","Introduce configurable Lock Path Currently table level lock files are prepared in table path and system level locks are created inside store.

When supporting S3, locking cannot be done is using S3 files as S3 support onlyÂ  eventual consistency.

So user can chose different HDFS location to configure locks, so that data can be on S3 and locks can be on HDFS.

User also chose Zookeeper LockÂ if does not have HDFS location.

SoÂ require to support a configuration pathÂ ""carbon.lock.path""

Â ",", "
"   Rename Method,Move Method,","Presto Stream Readers performance Enhancement *Background*:

In the present system, we create carbonColumnVectorImpl object in carbonVectorbatch where carbon core fill up vector data (one by one) in column matched data type array, later which at the time presto block builder call, read by stream readers (based on data type) and iterated to fill up in block and returned to presto.

*Solution*:

We can eliminate the extra iteration over the carbonColumnVectorImpl object -> vectorArray, by extending it to create a directStreamReaders which will fill up carbon-core vector data (one by one) directly to the block(presto), and on the call of block builder it will return the block to the Presto.",", , "
"   Rename Method,Move Method,","Presto Stream Readers performance Enhancement *Background*:

In the present system, we create carbonColumnVectorImpl object in carbonVectorbatch where carbon core fill up vector data (one by one) in column matched data type array, later which at the time presto block builder call, read by stream readers (based on data type) and iterated to fill up in block and returned to presto.

*Solution*:

We can eliminate the extra iteration over the carbonColumnVectorImpl object -> vectorArray, by extending it to create a directStreamReaders which will fill up carbon-core vector data (one by one) directly to the block(presto), and on the call of block builder it will return the block to the Presto.",", , "
"   Rename Class,Push Down Method,","Remove dead code from carbonData Due to enhancements and functionality changes, many dead code are left in CarbonData leading to unnecessary maintenance effort.Remove such methods and code which are not required any more.",", , "
"   Rename Class,Push Down Method,","Remove dead code from carbonData Due to enhancements and functionality changes, many dead code are left in CarbonData leading to unnecessary maintenance effort.Remove such methods and code which are not required any more.",", , "
"   Rename Method,Move Method,","Fixed data loading performance issue Problem: Data Loading is taking more time when number of records are high(3.5 billion) records

Root Cause: In case of Final merge sort temp row conversion is done in main thread because of this final step processing became slower.

Solution: Mode conversion logic to pre-fetch thread for parallel processing",", , "
"   Rename Method,Move Method,","Fixed data loading performance issue Problem: Data Loading is taking more time when number of records are high(3.5 billion) records

Root Cause: In case of Final merge sort temp row conversion is done in main thread because of this final step processing became slower.

Solution: Mode conversion logic to pre-fetch thread for parallel processing",", , "
"   Rename Method,Move Method,Extract Method,Move Attribute,","Support standard spark's FIleFormat interface in carbon Current carbondata has deep integration with spark to provide optimizations in performance and also supports features like compaction, IUD, data maps and metadata management etc. This type of integration forces user to use CarbonSession instance to use carbon even for read and write operations.

For the users who wants a same spark datasource integration to support read and write data carbon should support FIleFormat interface exposed by spark.","Duplicated Code, Long Method, , , , "
"   Rename Method,","Support Avro datatype conversion to Carbon Format 1.Support Avro Complex Types: Enum, Union, Fixed with Carbon.

2.Support Avro Logical Types: TimeMillis, TimeMicros, Decimal with Carbon.

Â 

Please find the design document in the below link:

https://docs.google.com/document/d/1Jne8vNZ3OSYmJ_72hTIk_5I4EeIVtxGNE5mN_hBlnVE/edit?usp=sharing",", "
"   Extract Superclass,Rename Method,Move Method,Extract Method,Inline Method,Pull Up Attribute,","Adaptive encoding for primitive data types Currently Encoding and Decoding is present only for Dictionary, Measure Columns, but for no dictionary Primitive types encoding is *absent.*

Encoding is a technique used to reduce the storage size and Â after all these encoding, result will be compressed with snappy compression to further reduce the storage size.

With this feature, we support encoding on the no dictionary primitive data types also.","Duplicated Code, Long Method, , , Duplicated Code, Large Class, , Duplicated Code, "
"   Move Method,Move Attribute,","Support CarbonCli tool for data summary When I am tuning carbon performance, very often that I want to check the metadata in carbon files without launching spark shell or sql. In order to do that, I am writing a tool to print metadata information of a given data folder. 
Currently, I am planning to do like this:

usage: CarbonCli
 -a,--all                    print all information
 -b,--tblProperties          print table properties
 -c,--column <column name>   column to print statistics
 -cmd <command name>         command to execute, supported commands are:
                             summary
 -d,--detailSize             print each blocklet size
 -h,--help                   print this message
 -m,--showSegment            print segment information
 -p,--path <path>            the path which contains carbondata files,
                             nested folder is supported
 -s,--schema                 print the schema

In first phase, I think â€œsummaryâ€ command is high priority, and developers can add more command in the future.",", , , "
"   Rename Method,",Adaptive encoding support for timestamp no dictionary and Refactor ColumnPageWrapper ,", "
"   Rename Method,Extract Method,Inline Method,","Simplify SDK API interfaces CARBONDATA-2961 Simplify SDK API interfaces

problem: current SDK API interfaces are not simpler and don't follow builder pattern.
If new features are added, it will become more complex.

Solution: Simplify the SDK interfaces as per builder pattern.

*Refer the latest sdk-guide.*

*Added:*

*changes in Carbon Writer:*
public CarbonWriterBuilder withThreadSafe(short numOfThreads)
public CarbonWriterBuilder withHadoopConf(Configuration conf)

public CarbonWriterBuilder withCsvInput(Schema schema)
public CarbonWriterBuilder withAvroInput(org.apache.avro.Schema avroSchema)
public CarbonWriterBuilder withJsonInput(Schema carbonSchema)

public CarbonWriter build() throws IOException, InvalidLoadOptionException

*Changes in carbon Reader*
public CarbonReaderBuilder withHadoopConf(Configuration conf)
public CarbonWriter build() throws IOException, InvalidLoadOptionException

*Removed:*

*changes in Carbon Writer:*
public CarbonWriterBuilder isTransactionalTable(boolean isTransactionalTable)

{{public CarbonWriterBuilder persistSchemaFile(boolean persist);}}

setAccessKey
setAccessKey
setSecretKey
setSecretKey
setEndPoint
setEndPoint

public CarbonWriter buildWriterForCSVInput(Schema schema, Configuration configuration)
public CarbonWriter buildThreadSafeWriterForCSVInput(Schema schema, short numOfThreads,Configuration configuration)
public CarbonWriter buildWriterForAvroInput(org.apache.avro.Schema avroSchema,Configuration configuration)
public CarbonWriter buildThreadSafeWriterForAvroInput(org.apache.avro.Schema avroSchema,short numOfThreads, Configuration configuration)
public JsonCarbonWriter buildWriterForJsonInput(Schema carbonSchema, Configuration configuration)
public JsonCarbonWriter buildThreadSafeWriterForJsonInput(Schema carbonSchema, short numOfThreads,Configuration configuration)

*Changes in carbon Reader*
public CarbonReaderBuilder isTransactionalTable(boolean isTransactionalTable)
public CarbonWriter build(Configuration conf) throws IOException, InvalidLoadOptionException","Duplicated Code, Long Method, , , "
"   Rename Method,Move Method,Extract Method,Move Attribute,",Support scan performance benchmark tool ,"Duplicated Code, Long Method, , , , "
"   Rename Method,",Change bloom query model to proceed multiple filter values ,", "
"   Rename Method,","Data mismatch after compaction with measure sort columns problem: Data mismatch after compaction with measure sort columns

root cause : In compaction flow (DictionaryBasedResultCollector), in ColumnPageWrapper inverted index mapping is not handled. Because of this, row of no dictionary dimension columns gets data form other rows.

Hence the data mismatch

Â 

solution: Handle inverted index mapping forÂ  DictionaryBasedResultCollector flow in ColumnPageWrapper

Â ",", "
"   Rename Method,Extract Method,Inline Method,","Implement LRU cache for B-Tree  LRU Cache for B-Tree is proposed  to ensure to avoid out memory, when too many number of tables exits and all are not frequently used.

Problem:

CarbonData is maintaining two level of B-Tree cache, one at the driver level and another at executor level.  Currently CarbonData has the mechanism to invalidate the segments and blocks cache for the invalid table segments, but there is no eviction policy for the unused cached object. So the instance at which complete memory is utilized then the system will not be able to process any new requests.

Solution:

In the cache maintained at the driver level and at the executor there must be objects in cache currently not in use. Therefore system should have the mechanism to below mechanism.

1.       Set the max memory limit till which objects could be hold in the memory.

2.       When configured memory limit reached then identify the cached objects currently not in use so that the required memory could be freed without impacting the existing process.

3.       Eviction should be done only till the required memory is not meet.

For details please refer to attachments.","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,Pull Up Attribute,","Implement LRU cache for B-Tree  LRU Cache for B-Tree is proposed  to ensure to avoid out memory, when too many number of tables exits and all are not frequently used.

Problem:

CarbonData is maintaining two level of B-Tree cache, one at the driver level and another at executor level.  Currently CarbonData has the mechanism to invalidate the segments and blocks cache for the invalid table segments, but there is no eviction policy for the unused cached object. So the instance at which complete memory is utilized then the system will not be able to process any new requests.

Solution:

In the cache maintained at the driver level and at the executor there must be objects in cache currently not in use. Therefore system should have the mechanism to below mechanism.

1.       Set the max memory limit till which objects could be hold in the memory.

2.       When configured memory limit reached then identify the cached objects currently not in use so that the required memory could be freed without impacting the existing process.

3.       Eviction should be done only till the required memory is not meet.

For details please refer to attachments.","Duplicated Code, Long Method, , Duplicated Code, "
"   Rename Method,","make zookeeper lock as default if zookeeper url is configured. make the lock type as zookeeper if zookeeper URL is present in the spark conf.
if spark.deploy.zookeeper.url property is set in spark-default.conf then need to take the zookeeper locking.",", "
"   Rename Method,","Entity Tab switching on new entity creation When a new Db|Obj Entity is created, entity editor should switch to the ""Entity"" tab, with ""Name"" field acquiring focus. This thing while minor, causes me lots of irritation on new projects when lots of entities are created at once",", "
"   Rename Method,","Add confirmation dialog for delete actions. Apparently I'm foolish enough to constantly confuse the entity level and attribute/relationship level delete buttons.  As a result, I constantly delete the wrong thing.  With no undo support, this means closing the modeler, reopening, and redoing any work I hadn't save beforehand.  While I wholly acknowledge that a pop-up confirmation dialog may very well get ignored (a big chunk of my thesis revolved around this), adding that extra step may at least help prevent deletions due to inadvertent clicks.  If it became particularly annoying, we could add a switch to globally disable the confirmations.",", "
"   Rename Method,","Do something about to-many prefetch limitations http://cayenne.apache.org/doc/prefetching.html

""PREFETCH LIMITATION: To-many relationships should not be prefetched if a query qualifier can potentially reduce a number of related objects, resulting in incorrect relationship list.""

This can bite an unsuspecting user... So we either 

1. Address the core limitation by building a correct prefetch query (use subselect?)
2. Detect these cases and throw an exception
3. Detect these cases and silently drop a prefetch

I should note that debugging these problems is very hard, as they look totally random until you get to the cause.",", "
"   Rename Method,","Do something about to-many prefetch limitations http://cayenne.apache.org/doc/prefetching.html

""PREFETCH LIMITATION: To-many relationships should not be prefetched if a query qualifier can potentially reduce a number of related objects, resulting in incorrect relationship list.""

This can bite an unsuspecting user... So we either 

1. Address the core limitation by building a correct prefetch query (use subselect?)
2. Detect these cases and throw an exception
3. Detect these cases and silently drop a prefetch

I should note that debugging these problems is very hard, as they look totally random until you get to the cause.",", "
"   Rename Method,","Do something about to-many prefetch limitations http://cayenne.apache.org/doc/prefetching.html

""PREFETCH LIMITATION: To-many relationships should not be prefetched if a query qualifier can potentially reduce a number of related objects, resulting in incorrect relationship list.""

This can bite an unsuspecting user... So we either 

1. Address the core limitation by building a correct prefetch query (use subselect?)
2. Detect these cases and throw an exception
3. Detect these cases and silently drop a prefetch

I should note that debugging these problems is very hard, as they look totally random until you get to the cause.",", "
"   Rename Method,","Aligning query capabilities 1. EJBQLQuery should support DataRows, pagination, cache groups, just like SelectQuery does.
2. ProcedureQuery should support SQLResultSetMapping and 'setColumnNamesCapitalization', just like SQLTemplate",", "
"   Rename Method,Pull Up Method,Extract Method,","CM: Allow multiple item selections Allow multiple item selections on the left-hand project tree and right-hand attribute panels. Common operations (like ""delete"") should work on multiple objects.

This is GSoC 2008 task","Duplicated Code, Long Method, , Duplicated Code, "
"   Rename Method,Inline Method,","Add support for start index/offset of queries Cayenne already allows us to programatically set a query fetch limit.
It would be nice if we could also specify a fetch index/fetch offset into a query, to make it simple to fetch, eg, the 100th through the 150th results from the database.

",", , "
"   Rename Method,","Add support for start index/offset of queries Cayenne already allows us to programatically set a query fetch limit.
It would be nice if we could also specify a fetch index/fetch offset into a query, to make it simple to fetch, eg, the 100th through the 150th results from the database.

",", "
"   Extract Method,Inline Method,","ObjRelationship Mapping Dialog Improvements [This is a GSoC 2008 task]

The biggest complaint about the ObjRelationship mapping dialog is that it is often unclear how it operates, especially to the new users. I.e. empty list of DbRelationships is displayed as a white area, not giving any hints on what needs to be done to map a relationship. So that's confusing. Same thing when you add 1 path component, there is no hint that you can chain more path components for the flattened relationship.

At the minimum we may just add some hint text (""Select next DbRelationship"" in grey over the next available dropdown), but ideally we should implement a path browser, similar to how the SelectQuery prefetch and ordering browsers operate (and similar to how OS X Finder does).
","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","ObjRelationship Mapping Dialog Improvements [This is a GSoC 2008 task]

The biggest complaint about the ObjRelationship mapping dialog is that it is often unclear how it operates, especially to the new users. I.e. empty list of DbRelationships is displayed as a white area, not giving any hints on what needs to be done to map a relationship. So that's confusing. Same thing when you add 1 path component, there is no hint that you can chain more path components for the flattened relationship.

At the minimum we may just add some hint text (""Select next DbRelationship"" in grey over the next available dropdown), but ideally we should implement a path browser, similar to how the SelectQuery prefetch and ordering browsers operate (and similar to how OS X Finder does).
","Duplicated Code, Long Method, , "
"   Extract Interface,Extract Method,","Support for copy/paste of entities/attributes/relationships Support for copy/paste of entities/attributes/relationships.
This is GSoC 2008 task.

Some of my ideas follow. 
We can implement copy-paste in two ways.

First: the buffer is valid only within a sole project. So when you open another project, copied data is lost.
Second, the buffer is stored in whole modeler application, or even system buffer. This allows to copy data between different projects, but is more complex because we need to create shallow copies of entities, attrs etc. Personally I use only one cayenne.xml currently.","Duplicated Code, Long Method, , Large Class, "
"   Move Class,Move Method,Move Attribute,","CM DataNode panel reorg - pull the password encoding options into a tab Currently the DataNode editor displays too much stuff when ""DriverDataSourceFactory"" is selected, related to the password encoding mechanism. This clutters the view, so we can organize it better. The idea is the following:

1. Add a ""Password Encoding"" tab to the right of the ""Adapter"" tab.
2. Move ""Password Encoder"", ""Password Encoder Key"", ""Password Location"" and ""Password Source"" from the main DataNode tab to the new tab.
3. Make sure the new tab is only active when the DataSource Factory is a DriverDataSourceFactory and is not active for any other factory.",", , , "
"   Rename Method,","Nested contexts on ROP Nested contexts should be avaliable via CayenneContext. This also leads to moving up some methods from DataContext to BaseContext, or even ObjectContext",", "
"   Rename Method,","Nested contexts on ROP Nested contexts should be avaliable via CayenneContext. This also leads to moving up some methods from DataContext to BaseContext, or even ObjectContext",", "
"   Pull Up Method,Extract Method,","Nested contexts on ROP Nested contexts should be avaliable via CayenneContext. This also leads to moving up some methods from DataContext to BaseContext, or even ObjectContext","Duplicated Code, Long Method, , Duplicated Code, "
"   Pull Up Method,Move Method,Pull Up Attribute,","Move User properties API to ObjectContext & BaseContext Move User properties API to ObjectContext & BaseContext

(get)setUserProperty methods declaration should be moved to ObjectContext, implementation - to BaseContext",", , Duplicated Code, Duplicated Code, "
"   Rename Method,","Generated DataMap classes should contain public constants for all query names The new DataMap classes are great for accessing named queries in a type-safe manner.  An additional improvement would be to add the public static final strings for every named query (not just object select queries), similar to how DataObjects are generated with constants for references property names of the entity.",", "
"   Move Method,Extract Method,","Modeler search improvement The search dialog can be improved in a number of ways, such as:

* prevent search string disappearance when search dialog is opened, so that the same search could be rerun multiple times
* allow keyboard search result selection
* if only one result is found, do not open a dialog, but go to the result directly","Duplicated Code, Long Method, , , "
"   Rename Method,Inline Method,","Modeler search improvement The search dialog can be improved in a number of ways, such as:

* prevent search string disappearance when search dialog is opened, so that the same search could be rerun multiple times
* allow keyboard search result selection
* if only one result is found, do not open a dialog, but go to the result directly",", , "
"   Rename Method,Extract Method,","Expression API: boolean support in string representation We need an ability to specify boolean constants (true and false) in expression's string representation, e.g. something like
Expression.fromString(""abort=true"")
currently this is not valid","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,Inline Method,",Undo/Redo support in modeler Modeler should support Undo/Redo. History should include all add/remove/edit/sync operations and be reset after 'Major' operations like Reverse-Engineer.,"Duplicated Code, Long Method, , , "
"   Move And Rename Class,Move Method,Move Attribute,",Undo/Redo support in modeler Modeler should support Undo/Redo. History should include all add/remove/edit/sync operations and be reset after 'Major' operations like Reverse-Engineer.,", , , "
"   Rename Method,Extract Method,","Auto load schema on startup This feature would allow Cayenne to bootstrap the DB schema on startup if needed. I can see the following options for schema update:

1. Skip schema update. This is the current behavior.
2. Create if no existing tables are found. This is the most reliable but less powerful option. It will skip schema creation if at least one of the tables modeled in Cayenne already exist in DB. No exceptions will be thrown (maybe just an INFO log message). This will use DbGenerator.
3. Create if no existing tables are found ; throw if partial schema detected. This will check for schema changes compared to the model and throw an exception if differences are found in the tables mapped in the DataMap (i.e. if there are more tables than mapped in Cayenne, it will NOT throw). This will use the merge package.

(I wonder if ""4. Attempt to merge schema changes"" makes any sense at all)

This will have a modeler component - a dropdown at the DataNode level listing schema generation strategies. It will be saved in the XML as ""schema-update-strategy"" attribute for the ""node"" element. 

Runtime component will consist of strategy instantiation, and an intercept of DB operations run through the DataNode, passing it to an appropriate strategy. Still undecided whether the strategy should be a DataNode attribute or a DbAdapter attribute (prolly DataNode).","Duplicated Code, Long Method, , "
"   Rename Method,","Auto load schema on startup This feature would allow Cayenne to bootstrap the DB schema on startup if needed. I can see the following options for schema update:

1. Skip schema update. This is the current behavior.
2. Create if no existing tables are found. This is the most reliable but less powerful option. It will skip schema creation if at least one of the tables modeled in Cayenne already exist in DB. No exceptions will be thrown (maybe just an INFO log message). This will use DbGenerator.
3. Create if no existing tables are found ; throw if partial schema detected. This will check for schema changes compared to the model and throw an exception if differences are found in the tables mapped in the DataMap (i.e. if there are more tables than mapped in Cayenne, it will NOT throw). This will use the merge package.

(I wonder if ""4. Attempt to merge schema changes"" makes any sense at all)

This will have a modeler component - a dropdown at the DataNode level listing schema generation strategies. It will be saved in the XML as ""schema-update-strategy"" attribute for the ""node"" element. 

Runtime component will consist of strategy instantiation, and an intercept of DB operations run through the DataNode, passing it to an appropriate strategy. Still undecided whether the strategy should be a DataNode attribute or a DbAdapter attribute (prolly DataNode).",", "
"   Extract Method,Inline Method,","merge: a way to set value for null for madatory columns When adding a ""not null"" column to a table with rows, or when setting a column to ""not null"", there should be a way to tell DbMerger about the values that should be used instead of null for existing rows. 

With this information, the merger should create sql like the following before setting the column to ""not null"".
update table set col='the value' where col is null;","Duplicated Code, Long Method, , , "
"   Rename Method,","Add method to ExpressionFactory to match against the primary key of an object or list of objects It would be helpful to have a method in ExpressionFactory that build an expression using matchDbExp() to match an object's primary keys.

This can be helpful in excluding a single or list of known objects from the result of a query.",", "
"   Move Method,Extract Method,",merge primary key changes DbMerger should detect and create tokens for changes to the primary key definition.,"Duplicated Code, Long Method, , , "
"   Rename Method,","Use #result as optional directive for only few columns (not all) Here is few queries to show the problem:

SELECT ARTIST_ID, ARTIST_NAME FROM ARTIST  
- working properly

SELECT #result('ARTIST_ID' 'java.lang.Integer'), #result('ARTIST_NAME' 'java.lang.String') FROM ARTIST 
- also working properly

SELECT ARTIST_ID,  #result('ARTIST_NAME' 'java.lang.String') FROM ARTIST
- first column is returned as null!!! Not nice...",", "
"   Rename Method,","Update Ordering to take enums instead of boolean flags. The Ordering class currently takes booleans to indicate ascending/descending and case sensitive/insensitive.  When reading the code, it is impossible to know what ""true"" and ""false"" actually mean unless you are familiar with the API.  Update the class to use descriptive enums instead of boolean flags.
",", "
"   Rename Method,","Add deleteObjects() to ObjectContext DataContext has a deleteObjects() method, but ObjectContext doesn't.

The documentation shows using deleteObjects:

http://cayenne.apache.org/doc/deleting-objects.html

This is more of a hassle when using cayenneObject.getObjectContext().delete... instead of cayenneObject.getDataContext() because getDataContext() is deprecated in 3.0.
",", "
"   Rename Method,","Implement memorized sorting of modeler columns Per CAY-1251 columns in various tables in the Modeler can now be resized and reordered, with all user selections saved in local preferences. Here is another related improvement - allow users to click on the column headers and sort the items in the table. First click sorts ascending, second - descending. Only one column can be used for sorting at a time. Sort selection is memorized in prefrences for each table type.

Note that some tables are not sortable, as the displayed ordering is significant in Cayenne. Namely all callback / listener methods and stored procedure parameters. Those should be left unchanged.",", "
"   Rename Method,",QueryLogger to DI JdbcEventLogger migration Migration from deprecated QueryLogger to DI enabled JdbcEventLogger.,", "
"   Move Class,Move Method,Extract Method,","Implement constructor injection support for DefaultAdhocObjectFactory Currently DefaultAdhocObjectFactory doesn't support constructor injection, which might be very useful for us now (e.g. CAY-1603) and I suppose in future too.
So, let's implement it.","Duplicated Code, Long Method, , , "
"   Rename Method,","ObjectContext API to use varargs It is quite annoying to wrap objects in collections to invalidate objects... This is easily solved with varargs. For symmetry we can do the same with with deleteObjects. So we'll end up with:

void invalidateObjects(Collection<?> objects);
void invalidateObjects(Object... objects);

void deleteObjects(Collection<?> objects);
void deleteObjects(Object... objects);

@Deprecated // redundant
deleteObject(Object object);",", "
"   Rename Method,Move Method,","Transient objects do not work in ROP apps In addition the problem referenced in CAY-1624, transient objects don't work when using ROP because in the client template the relationship values (to-one and to-many) are   null and are only initialized to non-null values when they are added to an ObjectContext.  By modifying the template to initialize the relationship values to something non-null, transient objects can be worked with locally (but not still saved?).  I don't know if that is the best solution, but here's basically what I did.",", , "
"   Pull Up Method,Pull Up Attribute,","[PATCH] Can't disable validation for CayenneContext Validation can't be disabled for CayenneContext like it can for DataContext using setValidatingObjectsOnCommit(false).  This probably isn't generally a problem since the out-of-the-box entity templates for client objects don't implement the Validating interface so validation doesn't happen by default.  But CayenneContext will already do validation if you implement the interface.  So if you make you objects implement Validating, then it will do validation, but you have no way to turn it off if you don't want it all the time.

The solution is to:
1) pull up the validatingObjectsOnCommit property into BaseContext so it can be used in CayenneContext as well as DataContext
2) check the validatingObjectsOnCommit property before doing validation in CayenneContext",", Duplicated Code, Duplicated Code, "
"   Rename Class,Move And Rename Class,Rename Method,Move Method,Extract Method,Move Attribute,","cayenne-lifecycle: don't call String representation of ObjectId a UUID ""UUID"" term is used very loosely in cayenne-lifecycle. Per http://en.wikipedia.org/wiki/Universally_unique_identifier UUID is a 16 byte number. Our ""UUID"" is a String like ""Artist:1""... Need to call it something else and renamed the corresponding lifecycle classes before it made to a GA release. Not yet sure what a good short term might be, but essentially we are talking about an ObjectId serialized to String. ","Duplicated Code, Long Method, , , , "
"   Push Down Attribute,Move Attribute,","Consistent and concise property names aggregated in one place  One of the final things we need in 3.1 is to take all possible property names and DI collection names, with definitions scattered around various Java classes, and define them in a single place as String constants that follow some simple naming conventions. The constants location is org.apache.cayenne.configuration.Constants. The naming convention that I am trying here is this:

cayenne.<tier_name_or_logical_module_name>.<undescore_separated_lowercase_property_description>[.optional_project_or_data_node_qualifier]

Here is the list of  properties (""legacy"" property names are in parenthesis) :

[DI Collections]

cayenne.properties    (org.apache.cayenne.configuration.DefaultRuntimeProperties.properties)
cayenne.server.adapter_detectors   ( org.apache.cayenne.configuration.server.DefaultDbAdapterFactory.detectors)

cayenne.server.domain_filters    (org.apache.cayenne.configuration.server.DataDomainProvider.filters)
cayenne.server.project_locations  ( org.apache.cayenne.configuration.server.DataDomainProvider.locations)

cayenne.server.default_types   (org.apache.cayenne.dba.JdbcAdapter.defaultExtendedTypes)
cayenne.server.user_types (org.apache.cayenne.dba.JdbcAdapter.userExtendedTypes)
cayenne.server.type_factories (org.apache.cayenne.dba.JdbcAdapter.extendedTypeFactories)


cayenne.server.rop_event_bridge_properties (org.apache.cayenne.remote.hessian.service.HessianService.properties)


[JDBC Properties]

cayenne.jdbc.driver[.domain_name.node_name]
cayenne.jdbc.url[.domain_name.node_name] 
cayenne.jdbc.username[.domain_name.node_name]
cayenne.jdbc.password[.domain_name.node_name]
cayenne.jdbc.min_connections (cayenne.jdbc.min.connections[.domain_name.node_name])
cayenne.jdbc.max_connections (cayenne.jdbc.max.conections[.domain_name.node_name]) - notice typo (""n"" missing) in the old property name

[Cross-tier Properties]

cayenne.querycache.size    (cayenne.MapQueryCacheFactory.cacheSize)

[Server properties]

cayenne.server.contexts_sync_strategy           (org.apache.cayenne.sync_contexts)
cayenne.server.object_retain_strategy  (org.apache.cayenne.context_object_retain_strategy)
(cayenne.adapter[.domain_name.node_name]) - removed, was unused

[ROP Properties]

cayenne.rop.service_url     (cayenne.config.rop.service.url) 
cayenne.rop.service_username    (cayenne.config.rop.service.username)
cayenne.rop.service_password   (cayenne.config.rop.service.password) 
cayenne.rop.shared_session_name   (cayenne.config.rop.service.shared_session)
cayenne.rop.channel_events       (cayenne.config.rop.client.channel.events)
cayenne.rop.context_change_events   (cayenne.config.rop.client.context.change_events)
cayenne.rop.context_lifecycle_events (cayenne.config.rop.client.context.lifecycle_events)
cayenne.rop.service.timeout (cayenne.config.rop.service.timeout)

cayenne.server.rop_event_bridge_factory  (cayenne.RemoteService.EventBridge.factory)",", , , "
"   Rename Method,","Split long DISJOINT_BY_ID prefetch query on several smaller queries It is improvement for CAY-1681. From Andrus' comment:
And one more thing we will probably have to implement - breaking down OR query if it gets too long. This is a real problem which has been repeatedly mentioned in the context of the paginated queries, and in fact solved in IncrementalFaultList. see IncrementalFaultList.resolveInterval - it checks the number of clauses in the qualifier against 'maxFetchSize'. We may need to make ""maxFetchSize"" a container property used by IncrementalFaultList as well as our prefetch strategy, and take it into account in the later.",", "
"   Move And Rename Class,","cdbimport improvements So I finally started using a DB-first (and hopefully ORM-modeling free) approach on a project. In other words - a maven profile that executes cdbimport and cgen to refresh the XML and Java classes from the current DB state. 3.1 version of 'cdbimport' is rather basic and we need to extend it significantly to produce reliable and complete results. This is a cover task for all improvements. Will open subtasks for individual things.


",", "
"   Rename Method,Extract Method,","cdbimport improvements So I finally started using a DB-first (and hopefully ORM-modeling free) approach on a project. In other words - a maven profile that executes cdbimport and cgen to refresh the XML and Java classes from the current DB state. 3.1 version of 'cdbimport' is rather basic and we need to extend it significantly to produce reliable and complete results. This is a cover task for all improvements. Will open subtasks for individual things.


","Duplicated Code, Long Method, , "
"   Move Method,Extract Method,Inline Method,Move Attribute,","cdbimport improvements So I finally started using a DB-first (and hopefully ORM-modeling free) approach on a project. In other words - a maven profile that executes cdbimport and cgen to refresh the XML and Java classes from the current DB state. 3.1 version of 'cdbimport' is rather basic and we need to extend it significantly to produce reliable and complete results. This is a cover task for all improvements. Will open subtasks for individual things.


","Duplicated Code, Long Method, , , , , "
"   Move And Rename Class,","cdbimport improvements So I finally started using a DB-first (and hopefully ORM-modeling free) approach on a project. In other words - a maven profile that executes cdbimport and cgen to refresh the XML and Java classes from the current DB state. 3.1 version of 'cdbimport' is rather basic and we need to extend it significantly to produce reliable and complete results. This is a cover task for all improvements. Will open subtasks for individual things.


",", "
"   Move Class,Move And Rename Class,","cdbimport improvements So I finally started using a DB-first (and hopefully ORM-modeling free) approach on a project. In other words - a maven profile that executes cdbimport and cgen to refresh the XML and Java classes from the current DB state. 3.1 version of 'cdbimport' is rather basic and we need to extend it significantly to produce reliable and complete results. This is a cover task for all improvements. Will open subtasks for individual things.


",", "
"   Rename Method,",Flatten object entities for many to many relationships on reverse engineering Need to remove temporary object entities for many to many relationship on reverse engineering. And make flattened relationships direct to both tables that take part in relation.,", "
"   Rename Method,","Lock-free EntityResolver Need to improve EntityResolver to make it as lock-free as possible. There's no reason (other than its reuse in the Modeler) for keeping it synchronized. After the startup, it is essentially read-only.",", "
"   Push Down Method,Push Down Attribute,","Lock-free EntityResolver Need to improve EntityResolver to make it as lock-free as possible. There's no reason (other than its reuse in the Modeler) for keeping it synchronized. After the startup, it is essentially read-only.",", , , "
"   Rename Method,","Lock-free EntityResolver Need to improve EntityResolver to make it as lock-free as possible. There's no reason (other than its reuse in the Modeler) for keeping it synchronized. After the startup, it is essentially read-only.",", "
"   Rename Class,Move Method,Extract Method,Move Attribute,","Lock-free EntityResolver Need to improve EntityResolver to make it as lock-free as possible. There's no reason (other than its reuse in the Modeler) for keeping it synchronized. After the startup, it is essentially read-only.","Duplicated Code, Long Method, , , , "
"   Move Method,Extract Method,Move Attribute,","Lock-free EntityResolver Need to improve EntityResolver to make it as lock-free as possible. There's no reason (other than its reuse in the Modeler) for keeping it synchronized. After the startup, it is essentially read-only.","Duplicated Code, Long Method, , , , "
"   Rename Method,Extract Method,","Optimize Expression conversion to String and EJBQL Expression.toString() is pretty heavy:

 @Override
    public String toString() {
        StringWriter buffer = new StringWriter();
        PrintWriter pw = new PrintWriter(buffer);
        encodeAsString(pw);
        pw.close();
        buffer.flush();
        return buffer.toString();
    }

We didn't bother much about it, as it wasn't supposed to be called in runtime... Well it is sometimes:

SelectTranslator,java:

433 String labelPrefix = pathExp.toString().substring(""db:"".length());

And I am seeing this line occasionally in my app profiling reports. So we need to override ""toString"" at least for ASTObjPath and ASTDbPath with a lighter implementation","Duplicated Code, Long Method, , "
"   Rename Method,","Optimize Expression conversion to String and EJBQL Expression.toString() is pretty heavy:

 @Override
    public String toString() {
        StringWriter buffer = new StringWriter();
        PrintWriter pw = new PrintWriter(buffer);
        encodeAsString(pw);
        pw.close();
        buffer.flush();
        return buffer.toString();
    }

We didn't bother much about it, as it wasn't supposed to be called in runtime... Well it is sometimes:

SelectTranslator,java:

433 String labelPrefix = pathExp.toString().substring(""db:"".length());

And I am seeing this line occasionally in my app profiling reports. So we need to override ""toString"" at least for ASTObjPath and ASTDbPath with a lighter implementation",", "
"   Rename Method,","Optimize Expression conversion to String and EJBQL Expression.toString() is pretty heavy:

 @Override
    public String toString() {
        StringWriter buffer = new StringWriter();
        PrintWriter pw = new PrintWriter(buffer);
        encodeAsString(pw);
        pw.close();
        buffer.flush();
        return buffer.toString();
    }

We didn't bother much about it, as it wasn't supposed to be called in runtime... Well it is sometimes:

SelectTranslator,java:

433 String labelPrefix = pathExp.toString().substring(""db:"".length());

And I am seeing this line occasionally in my app profiling reports. So we need to override ""toString"" at least for ASTObjPath and ASTDbPath with a lighter implementation",", "
"   Rename Class,Rename Method,Extract Method,","Merge Entity Attributes and Relationships tabs together with one toolbar. For convenient display attributes and relationships, there were two tabs will integrate ObjEntityAttributeTab and ObjEntityRelationshipTab one, DbEntityAttributeTab and DbEntityRelationshipTab, using the new classes ObjEntityAttrRelationshipTab and DbEntityAttrRelationshipTab. Now the toolbar is formed in these new classes. That you can use one toolbar for attributes and relationships were made new classes ""mediators"" CopyAttrRelationshipsAction, CutAttrRelationshipsAction and RemoveAttrRelationshipsAction. Was also been modified button ""edit relationships"", which incorrectly became active. In the new patch renamed classes ObjEntityAttributeTab, ObjEntityRelationshipTab, DbEntityAttributeTab, DbEntityRelationshipTab in ObjEntityAttributePanel, ObjEntityRelationshipPanel, DbEntityAttributePanel, DbEntityRelationshipPanel, and in the ""mediators"" classes replaced static ""currentSelectedPanel"" field.","Duplicated Code, Long Method, , "
"   Rename Method,","Make ResultIterator implement Iterable<T>, create ObjectContext.iterate method 1. Make ResultIterator implement Iterable<T> to simplify its use in loops.

2. Create ObjectContext.iterator method that is available both in ROP and server stacks. CayenneContext can simply do something stupid, like iterating over a regular list. 

3. Create a callback flavor - ObjectContext,iterate(Select, ResultIteratorCallback)

4. Move ResultIterator to org.apache.cayenne - it should be available to all layers.

5. Stop throwing CayenneException from all methods.",", "
"   Extract Method,Move Attribute,","Conditionally log slow / long-running queries I wanted to add logging slow / long-running queries without having to log every single query, so I made a patch to do it.  But there are a lot of implementation questions and some general design questions about the jdbcLogger.

1) I added a property to control the logging threshold - seems like these should go in Constants, but there was already a property defined in JdbcAdapter where I was working, so I just put it there.  Also, I'm not sure what the property naming conventions are exactly.  I called it ""cayenne.server.query_execution_time_logging_threshold""

2) For the JdbcLogger, currently all the messages are at the INFO level.  I can't add this new logging with that level because then you wouldn't be able to see just the long-running queries, you would still see all or none.  So I added generic ""warn"" method that uses the WARN level.  But I wonder if a more semantic method would be better, like ""logLongQuery"" or something.  Also, I wonder if it would be better to push the existing SQL logging down to the debug level and leave the connection opening at the INFO level so you could just see those logs (which is something I have wanted).

3) I am logging only the SQL string and not the parameters because there wasn't any easy way to access the params from the logger.  Ideally the params would be logged also.

4) In Project Wonder some functionality like this exists, but it allows you to pair log levels with query running times, so you could log at the WARN level for queries longer than one second and log at the ERROR level for queries longer than five seconds.  I don't think this is very important as it complicates the property API, but I thought I would throw out the idea.","Duplicated Code, Long Method, , , "
"   Rename Class,Move Class,Move Method,Move Attribute,","Straighten thread model and synchronization in the Modeler Lots of synchronization in the Modeler code points to our earlier misunderstanding of Swing single-thread GUI update model [1]. We should remove synchronized blocks from the UI code, and at the same time ensure SwingUtilities.invokeLater is used from other threads when updating the GUI. Most notably - ProjectWatchdog thread.


[1] http://docs.oracle.com/javase/tutorial/uiswing/concurrency/dispatch.html",", , , "
"   Rename Method,Move Method,Extract Method,",Porting to OSGi environment Cayenne framework doesn't run under OSGi environments as class-loading problems arise in the dynamic loading of autogenerated classes by Cayenne Modeler.,"Duplicated Code, Long Method, , , "
"   Inline Method,Move Attribute,",Porting to OSGi environment Cayenne framework doesn't run under OSGi environments as class-loading problems arise in the dynamic loading of autogenerated classes by Cayenne Modeler.,", , , "
"   Rename Class,Extract Interface,Rename Method,Move Method,Move Attribute,",Porting to OSGi environment Cayenne framework doesn't run under OSGi environments as class-loading problems arise in the dynamic loading of autogenerated classes by Cayenne Modeler.,", , , Large Class, "
"   Rename Class,Rename Method,Extract Method,","Config-free ServerRuntime Often it is useful to have a Cayenne stack to do raw SQL operations via SQLTemplate and friends. There may be no ORM mapping present, and often an externally managed DataSource is provided. Currently ServerRuntime won't start without an XML descriptor.

So implement methods in ServerRuntimeBuilder that allow to assemble basic parts of such stack, as well as tweak ServerRuntime itself to allow starting with no XML configs. Here is a usage example:

ServerRuntime localRuntime = new ServerRuntimeBuilder()
     .jdbcDriver(""com.foo.Driver"")
     .url(""jdbc:foo://"")
     .password(""XXXX"")
     .user(""user"")
     .minConnections(1)
     .maxConnections(2)
     .build();","Duplicated Code, Long Method, , "
"   Rename Class,Move Method,Inline Method,Move Attribute,","Replace Oracle LOB hacks with JDBC 4.0 API JDBC 4.0 (included in Java 6) provides methods for BLOB/CLOB manipulation that work with latest Oracle drivers (ojdbc6.jar). So we can remove all the Oracle LOB hacks that among other things make interception of BatchActions a pain. 

Per Oracle docs, 11-series drivers work with DB versions back to 9.x: http://docs.oracle.com/cd/E11882_01/java.112/e10589/getsta.htm#JJDBC28046 So what can possibly go wrong here :)",", , , , "
"   Move Class,Rename Method,Move Method,Extract Method,Move Attribute,","CDbimport improvements Weâ€™ve experimented with automated db-first approach to Cayenne modeling for more than a year on a set of client projects. Roughly this approach means that DB evolution is managed via external tools (e.g. liquibase) and Cayenne artifacts are managed using the following POM configuration:

<plugin>
	<groupId>org.apache.cayenne.plugins</groupId>
	<artifactId>maven-cayenne-plugin</artifactId>
	<configuration>
		...
	</configuration>
	<executions>
		<execution>
			<id>default-cli</id>
			<goals>
				<goal>cdbimport</goal>
				<goal>cgen</goal>
			</goals>
		</execution>
	</executions>
</plugin>

â€œcdbimportâ€ ensures that Cayenne model is always in sync with DB, â€œcgenâ€ - that Java classes are in sync with the model. There are zero problems with â€œcgen"", not so with â€œcdbimport"". If you control the schema, you get a decently named Java classes/properties in 95% of the cases. Here we are trying to address the remaining 5% that make things ugly:

* Inability to generate meaningful relationship names in many cases.
* Inability to customize attribute/relationship names and data types.

To solve this here we are proposing a merge algorithm that would preserve customizations to the Obj* layer made by the user. And in addition to that a special descriptor that can be used for more advanced filtering and customization of cdbimport process. Both of these improvements will hopefully result in â€œcdbimportâ€ becoming a tool of choice for Cayenne work for many users.

https://docs.google.com/document/d/1DF5-_mMDCuH7iUFhEFDm2q-ebVeSPgvOaymho88ywJ0/edit?pli=1","Duplicated Code, Long Method, , , , "
"   Rename Method,Move Method,Extract Method,","Deprecate SQLTemplate parameter batches One of the hurdles in creating a cleaner API for SQLTemplate is its support for parameter batches:

SQLTemplate template = ...

Map<String, Object>[] params = new Map<>[2];
params[0] = ..
params[1] = ..

sqlTemplate.setParams(params);

There's a very small performance benefit to using batches, as PreparedStatement is not precompiled between the batch bindings. So switching this form instance to QueryChain is not going to have much effect on speed, but makes things cleaner:

SQLTemplate template = ...;
Query[] queries = new Query[2];
queries[0] = template.queryQueryWithParameters(..);
queries[1] = template.queryQueryWithParameters(..);

QueryChain chain = new QueryChain(queries);

So going to deprecate parameter batches. ","Duplicated Code, Long Method, , , "
"   Rename Method,","SQLSelect cleanup and omissions Will clean up and refactor SQLSelect API:

Shorter style of chainable API :

1. rename 'useLocalCache' to 'localCache' 
2. rename 'sharedCache' to 'sharedCache'

Misc:

3. useLocalCache is void. Must return SQLSelect<T>
4. 'cacheStrategy' should take cache groups vararg
5. 'cacheGroups' should have a collection variant",", "
"   Rename Method,","Variants of Property.like(..) : contains(..), startsWith(..), endsWith(..) http://markmail.org/message/cx7dzla46lcykkzq

An idea that I had while analyzing boilerplate code of the client Cayenne apps. An argument to Property.like(..) (or second argument to ExpressionFactory.likeExp(..)) requires a full pattern to match against. So people would often write their own utility code to wrap a String in ""%"" signs. Cayenne can easily take care of this via a few extra methods. In addition these new methods can do proper symbol escaping, making ""like"" much safer to use. 

Property.contains(string); // same as Property.like(""%"" + string + ""%"");
Property.icontains(string); // case insensitive version

Property.startsWith(string); // same as Property.like(string + ""%"");
Property.istartsWith(string); // case insensitive version

Property.endsWith(string); // same as Property.like(""%"" + string);
Property.iendsWith(string); // case insensitive version
",", "
"   Rename Method,","Variants of Property.like(..) : contains(..), startsWith(..), endsWith(..) http://markmail.org/message/cx7dzla46lcykkzq

An idea that I had while analyzing boilerplate code of the client Cayenne apps. An argument to Property.like(..) (or second argument to ExpressionFactory.likeExp(..)) requires a full pattern to match against. So people would often write their own utility code to wrap a String in ""%"" signs. Cayenne can easily take care of this via a few extra methods. In addition these new methods can do proper symbol escaping, making ""like"" much safer to use. 

Property.contains(string); // same as Property.like(""%"" + string + ""%"");
Property.icontains(string); // case insensitive version

Property.startsWith(string); // same as Property.like(string + ""%"");
Property.istartsWith(string); // case insensitive version

Property.endsWith(string); // same as Property.like(""%"" + string);
Property.iendsWith(string); // case insensitive version
",", "
"   Rename Method,","Variants of Property.like(..) : contains(..), startsWith(..), endsWith(..) http://markmail.org/message/cx7dzla46lcykkzq

An idea that I had while analyzing boilerplate code of the client Cayenne apps. An argument to Property.like(..) (or second argument to ExpressionFactory.likeExp(..)) requires a full pattern to match against. So people would often write their own utility code to wrap a String in ""%"" signs. Cayenne can easily take care of this via a few extra methods. In addition these new methods can do proper symbol escaping, making ""like"" much safer to use. 

Property.contains(string); // same as Property.like(""%"" + string + ""%"");
Property.icontains(string); // case insensitive version

Property.startsWith(string); // same as Property.like(string + ""%"");
Property.istartsWith(string); // case insensitive version

Property.endsWith(string); // same as Property.like(""%"" + string);
Property.iendsWith(string); // case insensitive version
",", "
"   Rename Class,Move Attribute,","A property to override DataSources of multi-module projects  Here is a situation:

Collection<String> configs = // configs in random, changing order
ServerRuntime runtime = new ServerRuntimeBuilder().addConfigs(configs).build();

The resulting runtime has multiple DataNodes. Now in development I need to provide connection data for each of the DataNodes using properties per [1]. Due to the random order of configs collection, domain name in the resulting stack also changes between the invocations. So I can't use 'cayenne.jdbc.driver.domain_name.node_name' property reliably. 

We need an easy way to fix the domain name in a multi-project config. Internally this will be achieved via a new DI property - ""cayenne.server.domain.name"". 

Public API will use an existing ServerRuntimeBuilder(String) constructor, but redefining its argument as a name of the domain, not a config.

UPGRADE NOTES:
* Users of multi-config projects may be used to ServerRuntime behavior that the name of the result domain is equal to the name of the last project config. We are trying to move away from this behavior, so ServerRuntimeBuilder will only use config name as the name of the project if there's one config and no override. Otherwise it will use the override, or if not set - ""cayenne"" as the default name.

[1] http://cayenne.apache.org/docs/3.1/cayenne-guide/configuration-properties.html

",", , "
"   Rename Method,",Add support for iterators to Select ,", "
"   Rename Method,Extract Method,",Add support for iterators to Select ,"Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Speeding up PropertyUtils PropertyUtils are very slow (especially in multi-threaded apps) when working with POJOs as every call to 'getProperty' results in java.beans.Introspector call , that is synchronized and slow. Hoping that replacing this with an internal cache of accessors per path would improve the situation.","Duplicated Code, Long Method, , "
"   Rename Method,Inline Method,","ObjectSelect, SelectById: eliminating methods that reset query state There was a discussion on how ObjectSelect and SelectById API is confusing in that it contains methods that append to the existing options, as well as methods that reset the option state and start fresh: http://markmail.org/message/f4dsghqzp6ges6ya

The consensus seems to be that we can get rid of resetting methods.

Unfortunately to preserve long term API sanity, we'll have to make changes that are not compatible with 4.0.M2 - namely get rid of all ""add"" methods, and redefine the existing reset methods to append to the query state. Namely:

* addOrderBy - gone
* orderBy - append instead of reset

* addPrefetch - gone
* prefetch - merge instead of reset

* where - behave as ""and"" instead of reset",", , "
"   Move Class,Move Method,Move Attribute,",Java 7 ,", , , "
"   Move Class,Rename Class,Push Down Method,Move Method,Extract Method,Push Down Attribute,Move Attribute,","Applying new Reverse Engineering to the Modeler We want to apply new Reverse Engineering features to the Modeler. Those features have already been applied to the cdbimport.

You could follow the development process via https://github.com/apache/cayenne/pull/81

We can reach reverse engineering by: 
1) Tools > Reengineer Database Schema 
2) Reverse Engineering tab now located in DataMapTabbedView. 

Perfoming first option on dataMap leads to required tab of reverse engineering connected to the current dataMap. Performing the first option on the project leads to creation of new dataMap and switching to the required tab of reverse Engineering. Then you should write reverse engineering file. 

You have a possibility to look through the current state of your Reverse Engineering clicking on sync button(it simplifies writing xml). And you have to perform reverse engineering clicking on execute button. After execute the reverse engineering will be performed and reverse engineering will be included into the current project state. Reverse engineering file will be created after clicking on the save button. 

If you want to navigate through different dataMaps your last changes in the window will be saved. So after returning you will have no problems to continue your work.","Duplicated Code, Long Method, , , , , , "
"   Rename Class,Rename Method,Move Method,","Applying new Reverse Engineering to the Modeler We want to apply new Reverse Engineering features to the Modeler. Those features have already been applied to the cdbimport.

You could follow the development process via https://github.com/apache/cayenne/pull/81

We can reach reverse engineering by: 
1) Tools > Reengineer Database Schema 
2) Reverse Engineering tab now located in DataMapTabbedView. 

Perfoming first option on dataMap leads to required tab of reverse engineering connected to the current dataMap. Performing the first option on the project leads to creation of new dataMap and switching to the required tab of reverse Engineering. Then you should write reverse engineering file. 

You have a possibility to look through the current state of your Reverse Engineering clicking on sync button(it simplifies writing xml). And you have to perform reverse engineering clicking on execute button. After execute the reverse engineering will be performed and reverse engineering will be included into the current project state. Reverse engineering file will be created after clicking on the save button. 

If you want to navigate through different dataMaps your last changes in the window will be saved. So after returning you will have no problems to continue your work.",", , "
"   Rename Method,Move Method,Extract Method,",Replace Query objects in DataMap with query descriptors This will help to build cleaner named query API and untie query metadata from the actual query object instance.,"Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,Pull Up Attribute,","ProcedureCall fluent query API Implement fluent API for executing mapped stored procedures.

Some examples of the new ProcedureCall query syntax:

{code:java}
// select
List<Artist> artists = ProcedureCall.query(""select_stored_procedure"", Artist.class)
                .param(""name"", ""Artist"")
                .param(""paintingPrice"", 3000)
                .limit(2).select(context);

// update
int updated = ProcedureCall.query(""update_stored_procedure"")
                .param(""paintingPrice"", 3000).update(context);

// call and get out parameters
int outParam = ProcedureCall.query(""out_stored_procedure"")
                .param(""name"", ""Artist"")
                .call(context).getOutParam(""artist_out"");
{code}","Duplicated Code, Long Method, , Duplicated Code, "
"   Rename Method,Extract Method,","Expose callback for ""performInTransaction"" TransactionManager API added per CAY-1778 is nice to batch a bunch of operations in a single transaction. It is missing the API to control transaction parameters though. So we are back to API similar to the old TransactionDelegate, except the callback can be applied to individual operations (the delegate was inconveniently a stack singleton). The new API will look like this:

ServerRuntime.performInTransaction(TransactionOperation, TransactionListener)

and will allow to customize connection isolation level, etc.","Duplicated Code, Long Method, , "
"   Rename Method,Inline Method,","DbLoader - allow loading DataMap without Obj layer When cdbimport loads a map to merge to an existing map, it makes no sense to load obj later. Refactor DbLoader API to support such optimization.",", , "
"   Move Class,Rename Method,","DbLoader - allow loading DataMap without Obj layer When cdbimport loads a map to merge to an existing map, it makes no sense to load obj later. Refactor DbLoader API to support such optimization.",", "
"   Rename Method,Inline Method,","DbLoader - allow loading DataMap without Obj layer When cdbimport loads a map to merge to an existing map, it makes no sense to load obj later. Refactor DbLoader API to support such optimization.",", , "
"   Rename Method,","DbLoader - allow loading DataMap without Obj layer When cdbimport loads a map to merge to an existing map, it makes no sense to load obj later. Refactor DbLoader API to support such optimization.",", "
"   Rename Method,","DbLoader - allow loading DataMap without Obj layer When cdbimport loads a map to merge to an existing map, it makes no sense to load obj later. Refactor DbLoader API to support such optimization.",", "
"   Move Class,Move Method,Extract Method,Inline Method,Move Attribute,","DbLoader - allow loading DataMap without Obj layer When cdbimport loads a map to merge to an existing map, it makes no sense to load obj later. Refactor DbLoader API to support such optimization.","Duplicated Code, Long Method, , , , , "
"   Rename Method,","ObjectNameGenerator refactoring - unifying relationship name generation Will need to refactor ObjectNameGenerator API, mainly focusing on relationship name generation. The new API will generate both obj and db relationship names in a single method. We will no longer distinguish between the two, as DbRelationship name does not correspond to any actual name found in the database, and we might as well use the object layer name. The main visible change in the algorithm is that ObjRelationship name will be based on DbRelationship semantics, not on DbRelationship name.

In the future if this proves to be too limiting, we might split a separate DbRelationship name generator. For now it will allow to avoid passing ExportedKey object to the strategy, and make it private.

As a part of this task we will remove LegacyObjectNameGenerator that uses different assumptions.",", "
"   Rename Method,Extract Method,","Explicit ""contribution"" API for easier expansion of DI collections and maps Currently extending Cayenne via contribution to DI collections and maps is not easy, as locating the corresponding map/collection is not transparent. It requires the knowledge of a String key for a given collection, and doesn't tell the user the type of the objects in the collection. E.g.:

{noformat}
binder
                .bindList(Constants.SERVER_DEFAULT_TYPES_LIST)
                .add(new LocalDateType())
                .add(new LocalTimeType())
                .add(new LocalDateTimeType());
{noformat}
Let's wrap this in a static contribution API similar to what was developed in bootique.io. E.g.:

{noformat}
BQCoreModule.contributeExtendedTypes(binder).add(..).add(..)
{noformat}

This way the users will have explicit API to access all module collections / maps and will know the type of objects they expect.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Auto-loading of Cayenne modules We need to borrow the central modularity feature from bootique.io - module auto-loading based on Java ServiceLoader:

1. Each module in Cayenne (or custom modules if a user so desires) will ship with ""META-INF/services/org.apache.cayenne.di.spi.ModuleProvider"" file that  will contain the name of the provider class.

2. Provider will contain a factory method for the module, as well as a method that returns a collection of modules that this module ""overrides""

3. There will be a loader class that will load all providers via ""java.util.ServiceLoader"", create modules for each provider and then sort them in the order of override dependencies.

4. ServerRuntimeBuilder will contain a method to turn auto-loading on and off. ""on"" will be the default. ","Duplicated Code, Long Method, , "
"   Rename Method,","Auto-loading of Cayenne modules We need to borrow the central modularity feature from bootique.io - module auto-loading based on Java ServiceLoader:

1. Each module in Cayenne (or custom modules if a user so desires) will ship with ""META-INF/services/org.apache.cayenne.di.spi.ModuleProvider"" file that  will contain the name of the provider class.

2. Provider will contain a factory method for the module, as well as a method that returns a collection of modules that this module ""overrides""

3. There will be a loader class that will load all providers via ""java.util.ServiceLoader"", create modules for each provider and then sort them in the order of override dependencies.

4. ServerRuntimeBuilder will contain a method to turn auto-loading on and off. ""on"" will be the default. ",", "
"   Rename Class,Rename Method,","Auto-loading of Cayenne modules We need to borrow the central modularity feature from bootique.io - module auto-loading based on Java ServiceLoader:

1. Each module in Cayenne (or custom modules if a user so desires) will ship with ""META-INF/services/org.apache.cayenne.di.spi.ModuleProvider"" file that  will contain the name of the provider class.

2. Provider will contain a factory method for the module, as well as a method that returns a collection of modules that this module ""overrides""

3. There will be a loader class that will load all providers via ""java.util.ServiceLoader"", create modules for each provider and then sort them in the order of override dependencies.

4. ServerRuntimeBuilder will contain a method to turn auto-loading on and off. ""on"" will be the default. ",", "
"   Move Method,Extract Method,","Auto-loading of Cayenne modules We need to borrow the central modularity feature from bootique.io - module auto-loading based on Java ServiceLoader:

1. Each module in Cayenne (or custom modules if a user so desires) will ship with ""META-INF/services/org.apache.cayenne.di.spi.ModuleProvider"" file that  will contain the name of the provider class.

2. Provider will contain a factory method for the module, as well as a method that returns a collection of modules that this module ""overrides""

3. There will be a loader class that will load all providers via ""java.util.ServiceLoader"", create modules for each provider and then sort them in the order of override dependencies.

4. ServerRuntimeBuilder will contain a method to turn auto-loading on and off. ""on"" will be the default. ","Duplicated Code, Long Method, , , "
"   Rename Method,Inline Method,","Auto-loading of Cayenne modules We need to borrow the central modularity feature from bootique.io - module auto-loading based on Java ServiceLoader:

1. Each module in Cayenne (or custom modules if a user so desires) will ship with ""META-INF/services/org.apache.cayenne.di.spi.ModuleProvider"" file that  will contain the name of the provider class.

2. Provider will contain a factory method for the module, as well as a method that returns a collection of modules that this module ""overrides""

3. There will be a loader class that will load all providers via ""java.util.ServiceLoader"", create modules for each provider and then sort them in the order of override dependencies.

4. ServerRuntimeBuilder will contain a method to turn auto-loading on and off. ""on"" will be the default. ",", , "
"   Rename Method,Pull Up Method,Move Method,Inline Method,","Deprecate multiple cache groups in caching and query API We should drop support for multiple cache groups as it have little practical sense.
Only available caching provider with proper support for it is OS Cache and it is obsolete and should be actually deprecated too. 

So here is required API changes:
- replace internally multiple cache groups with single group
- deprecate corresponding 3.1 API and warn when multiple cache groups provided
- remove corresponding 4.0 API completely (e.g. in fluent queries API)",", , , Duplicated Code, "
"   Extract Method,Move Attribute,","DI: typesafe binding of List and Map  The goal of this improvement is to add compile time type-safety to dependency injection of Lists and Maps.

*Proposed changes in DI API*:
Add following methods in {{Binder}} interface
{code}
    <T> MapBuilder<T> bindMap(Class<T> valueType);
    <T> MapBuilder<T> bindMap(Class<T> valueType, String bindingName);
    <T> ListBuilder<T> bindList(Class<T> valueType, String bindingName);
    <T> ListBuilder<T> bindList(Class<T> valueType);
{code}
And deprecate non type-safe methods:
{code}
    <T> MapBuilder<T> bindMap(String bindingName);
    <T> ListBuilder<T> bindList(String bindingName);
{code}

*Incompatibility*
If Cayenne DI is used in third-party code (custom modules or Cayenne modifications) the code should be updated if bindings other than {{List<Object>}} and {{Map<String, Object>}} were used, as this code will be broken.","Duplicated Code, Long Method, , , "
"   Rename Class,Extract Method,","Module auto-loading Make following modules auto-loadable:
- (+) Cache invalidation Module 
- (+) PostCommit Module 
- (+) ROP - ClientRuntimeBuilder (in question, requires ServerRuntime) 
- (-) Modeler 
- (-) Tools 
","Duplicated Code, Long Method, , "
"   Rename Method,Move Method,","JdbcEventLogger: replace deprecated method logQuery(String, List) with log(String) ",", , "
"   Rename Method,Extract Method,","DI: Refactor ListBuilder API ambiguities for before() / after() bindings There are some problems and ambiguities with API for adding ordered dependencies:
# you can call {{after()}} / {{before()}} without adding anything first, that will cause NPE
# right order of calls is not clear: {{add().after()}} or {{after().add()}}

Suggested API modification:
# remove {{UnorderedListBuilder}}, keep everything in {{ListBuilder}}
# add methods with explicit before\after parameters:
{code}
    ListBuilder<T> addAfter(Class<? extends T> interfaceType, Class<? extends T> afterType);

    ListBuilder<T> addAfter(T value, Class<? extends T> afterType);

    ListBuilder<T> addAllAfter(Collection<T> values, Class<? extends T> afterType);

    ListBuilder<T> insertBefore(Class<? extends T> interfaceType, Class<? extends T> beforeType);

    ListBuilder<T> insertBefore(T value, Class<? extends T> beforeType);

    ListBuilder<T> insertAllBefore(Collection<T> values, Class<? extends T> beforeType);
{code}","Duplicated Code, Long Method, , "
"   Rename Method,","Rename PostCommit module and its content to CommitLog As postcommit module's name is not clarifies its purpose and clashes with {{POST_COMMIT}} callback it is proposed to change its name to {{commitlog}}.
This requires some braking changes, namely: 
- introduce new annotation {{@CommitLog}} instead of {{@Auditable}}
- move all postcommit module's code into new packages ({{org.apache.cayenne.commitlog.*}})
- rename {{Postcommit*}} classes",", "
"   Move Class,Move And Rename Class,Extract Interface,Rename Method,","Rename PostCommit module and its content to CommitLog As postcommit module's name is not clarifies its purpose and clashes with {{POST_COMMIT}} callback it is proposed to change its name to {{commitlog}}.
This requires some braking changes, namely: 
- introduce new annotation {{@CommitLog}} instead of {{@Auditable}}
- move all postcommit module's code into new packages ({{org.apache.cayenne.commitlog.*}})
- rename {{Postcommit*}} classes",", Large Class, "
"   Rename Class,Rename Method,","Rename PostCommit module and its content to CommitLog As postcommit module's name is not clarifies its purpose and clashes with {{POST_COMMIT}} callback it is proposed to change its name to {{commitlog}}.
This requires some braking changes, namely: 
- introduce new annotation {{@CommitLog}} instead of {{@Auditable}}
- move all postcommit module's code into new packages ({{org.apache.cayenne.commitlog.*}})
- rename {{Postcommit*}} classes",", "
"   Rename Class,Move Method,Move Attribute,","Limit input into numeric fields to 10 digits This issue affects the following fields:
||Where||What||
|DbEntity > Properties tab|Max Length, Scale|
|Procedure > Parameters tab|Max Length, Precision|

Currently, if you enter more than 10 digits, a message appears that has an incorrect meaning for this situation. *See 1.png*",", , , "
"   Extract Superclass,Extract Method,","Field based data objects See [this thread |https://lists.apache.org/thread.html/75b19bd03a6849aea1d65b687e49e5dc1f56675fd10a4f9b0d9e37ec@%3Cdev.cayenne.apache.org%3E] on dev@list for original idea and discussion. 
In short: it is definitely good idea to at least experiment with storing data in plain fields instead of `Map` (current default implementation) as it will lower memory consumption and boost performance.","Duplicated Code, Long Method, , Duplicated Code, Large Class, "
"   Rename Method,","Remove commons-collections usage completely This task is final part of effort of removing all external dependencies from *cayenne-server*.
*Why to do so*:
* keeping cayenne free of outer dependencies will allow easier integration of Cayenne in projects, reducing issues in case of dependencies incompatibilities
* most of commons-collections code used by Cayenne can be replaced by plain java (this will require Java 8) so it will be easier to maintain
* commons-collections v3.2.1 used now have security vulnerabilities (see this [issue|https://issues.apache.org/jira/browse/COLLECTIONS-580])

*Negative impact*
Cayenne use some tricky collections from that lib now and will require to deal with their replacement
Those collections are:
- {{LRUMap}} this will be seamlessly replaced by already used {{ConcurrentLinkedHashMap}}
- {{CompositeCollection}} this will be copied into Cayenne code base as it has almost no dependencies and relatively small
- {{ReferenceMap}} this should be implemented by Cayenne as copying this will lead to copying significant part of commons-collections code base, luckily Cayenne actually use only two variants of this map: strong keys + weak values and strong keys + soft values.",", "
"   Rename Method,","Remove commons-collections usage completely This task is final part of effort of removing all external dependencies from *cayenne-server*.
*Why to do so*:
* keeping cayenne free of outer dependencies will allow easier integration of Cayenne in projects, reducing issues in case of dependencies incompatibilities
* most of commons-collections code used by Cayenne can be replaced by plain java (this will require Java 8) so it will be easier to maintain
* commons-collections v3.2.1 used now have security vulnerabilities (see this [issue|https://issues.apache.org/jira/browse/COLLECTIONS-580])

*Negative impact*
Cayenne use some tricky collections from that lib now and will require to deal with their replacement
Those collections are:
- {{LRUMap}} this will be seamlessly replaced by already used {{ConcurrentLinkedHashMap}}
- {{CompositeCollection}} this will be copied into Cayenne code base as it has almost no dependencies and relatively small
- {{ReferenceMap}} this should be implemented by Cayenne as copying this will lead to copying significant part of commons-collections code base, luckily Cayenne actually use only two variants of this map: strong keys + weak values and strong keys + soft values.",", "
"   Move Class,Move Method,","Clean up build scripts and code after support for Java 7 will be dropped # {{cayenne-java8}} module should be included into core
# remove conditional compilation of java8 modules (tutorials, rop?)
# setup java 8 support in all modules
# check deprecation (MacOS Modeler)",", , "
"   Rename Method,","cdbimport: add option to create project file *cdbimport* tools are already pretty advanced and stable, but you still need Modeler to create new project. It is really slows down start of new project, and moreover complicates new users transition into Cayenne world.

New option in *cbimport* config can be like this:
{code:xml}
<configuration>
    <cayenneProject>${project.basedir}/src/main/resources/cayenne/cayenne-project.xml</cayenneProject>
    <map>${project.basedir}/src/main/resources/cayenne/datamap.map.xml</map>
    <cdbimport>
        <!-- ... -->
    <cdbimport>
</configuration>
{code}
And the logic should be like this:
 * without {{cayenneProject}} option result will be same as now
 * if {{cayenneProject}} is set but no file exists it will be created and DataMap linked to it
 * it {{cayenneProject}} is and file already exists then DataMap should be linked to it (if it is new) or update existing one",", "
"   Rename Method,Extract Method,","cdbimport: add option to create project file *cdbimport* tools are already pretty advanced and stable, but you still need Modeler to create new project. It is really slows down start of new project, and moreover complicates new users transition into Cayenne world.

New option in *cbimport* config can be like this:
{code:xml}
<configuration>
    <cayenneProject>${project.basedir}/src/main/resources/cayenne/cayenne-project.xml</cayenneProject>
    <map>${project.basedir}/src/main/resources/cayenne/datamap.map.xml</map>
    <cdbimport>
        <!-- ... -->
    <cdbimport>
</configuration>
{code}
And the logic should be like this:
 * without {{cayenneProject}} option result will be same as now
 * if {{cayenneProject}} is set but no file exists it will be created and DataMap linked to it
 * it {{cayenneProject}} is and file already exists then DataMap should be linked to it (if it is new) or update existing one","Duplicated Code, Long Method, , "
"   Rename Method,","Add prefetch-related API to SQLSelect This one is self-descriptory. There is just no prefetch capabilities in {{SQLSelect}} query.Â 

ShouldÂ be pretty straightforwardÂ as underlying {{SQLTemplate}} already supports it.",", "
"   Move Method,Move Attribute,","Modeler: add prefetch support for the SQLTemplate query Recommended way of using queries defined in Modeler is {{MappedSelect}}Â and {{MappedExec}}, however you can't define prefetch when selecting objects if underlying query is {{SQLTemplate}}.

So one way of resolving this problem is to add prefetch settings to {{SQLTemplate}}Â editor in the Modeler.",", , , "
"   Rename Method,","Make SqlTemplate and SqlExec possible to return generated keys. Make SqlTemplate and SqlExec possible to return generated keys.

https://lists.apache.org/list.html?user@cayenne.apache.orgÂ ",", "
"   Rename Method,Move Method,","Run Disjoint By Id queries outside of synchronized block While researching lock contention in read-only mode I found that queries that are run for {{DisjoinById}} prefetches inside context-wide lock on {{ObjectStore}}. This can lead in really huge lock contention between threads concurrently reading from same context.

Â ",", , "
"   Rename Method,Inline Method,","Implement Quoting of identifiers Say a table ""t0"" has an attribute ""my attrib 0"".
You correct the obj-attribute to, for instance, ""my_attrib_0"", but the db-attribute is still ""my attrib 0"".
When you try to run a query on ""t0"", the generated query looks like:
SELECT ..., t0.my attrib 0, ... FROM dbo.t0 AS t0

This, obviously, can't possibly work.
The correct SQL would be:
SELECT ..., t0.[my attrib 0], ... FROM dbo.[t0] AS t0

Notice the square brackets arround the attributes and table names: that makes the string a valid attribute or table name valid.
A further improvement might be to add the database name:
SELECT ..., t0.[my attrib 0], ... FROM [dbname].dbo.[t0] AS t0

I mention it because I use the quantum plugin as a database access plugin and it complained about a table called ""dbo.tablename"" (as I believe it should) and all was well when I used mydbname.dbo.tablename.",", , "
"   Rename Method,Extract Method,","Add progress/log view 


From: 	Andrus Adamchik <andrus@objectstyle.org>
Reply-To: 	cayenne-user@objectstyle.org
To: 	cayenne-user@objectstyle.org
Subject: 	Re: Detailed log/progress view?
Date: 	Fri, 2 Sep 2005 10:17:38 -0400  (16:17 CEST)

Care to file an improvement request in Jira?

http://objectstyle.org/cayenne/bugs-features.html

Andrus


On Sep 2, 2005, at 10:01 AM, Ã˜yvind Harboe wrote:

> Writeup of some Cayenne experiences follows...
>
> I've been using the Cayenne modeler for a couple of projects now  
> and the
> thing I miss the most is a detailed log/progress view where I can see
> everything that was attempted and the g(l)ory details of all
> exceptions/error messages.
>
> - Connecting to a database gives very limited feedback when it fails.
>   E.g. I had a misconfigured MS SQL server and Cayenne gave me
>   a ""something went wrong"" error message, whereas the exception in
>   my JDBC hello world program contained much more information. When
>   translating error message to corrective action using Google, *any*
>   little piece of information, no matter how insignificant it may seem
>   at the time can be of vital importance :-)
>
> - Reengineering an MS SQL database is an atomic user interface  
> operation
>   where there are crucial configuration steps that must be in place on
>   the SQLServer end. E.g. I accidentally created a user without enough
>   access rights to access the tables I wanted to reverse engineer.  
> This
>   didn't give me an error message, but rather nothing was reverse
>   engineered. Zero feedback can be frustrating when trying to  
> figure out
>   whats going wrong.
>
> - The Cayenne modeler reports ambiguously ""Schema Generation Complete""
>   after I've clicked ""Generate"". It puts up this message even if it
>   has failed. E.g. when I try to generate
>   a schema which contains a field called ""position"" w/HSQLDB, it
>   causes HSQLDB to choke. The exceptions contain enough information to
>   clue me in on what's wrong, it is just the modeler which seems to
>   sensor this information.
>
> - Weird error messages when switching between databases is business
>   as usual. The tricky part is when these error messages are not
>   propagated to the user. There is valuable information tucked away in
>   ~/.cayenne/modeler.log
>
> - Generated columns is supported by Cayenne if the underlying JDBC
>   driver + database adapter supports it. However, if the system
>   is somehow misconfigured, the ""Generated"" flag is silently ignored.
>   Some sort of feedback here(possibly via the log/progress view),  
> would
>   be nice. (I'm currently investigating why this does not work for SQL
>   server + jDTS, Cayenne 1.2M5).
>
>
>
>
>
> -- 
> Ã˜yvind Harboe
> http://www.zylin.com
>
>
>","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,Move Attribute,","Split Expressions By default, the Cayenne qualifier translator removes ""duplicate"" joins.   Add syntax to the Cayenne expression parser and qualifier translator to indicate which expressions should be ""split expressions"" and not be removed as duplicates.",", , "
"   Rename Method,","Split Expressions By default, the Cayenne qualifier translator removes ""duplicate"" joins.   Add syntax to the Cayenne expression parser and qualifier translator to indicate which expressions should be ""split expressions"" and not be removed as duplicates.",", "
"   Rename Method,","Split Expressions By default, the Cayenne qualifier translator removes ""duplicate"" joins.   Add syntax to the Cayenne expression parser and qualifier translator to indicate which expressions should be ""split expressions"" and not be removed as duplicates.",", "
"   Move Class,Move And Rename Class,","DataContext and DataDomain must support lifecycle callbacks out of the box without wrapping Wrapping cayenne stack objects in interceptors to do lifecycle callbacks is counterintuitive, requires extra code, and actually causes changes in behavior (CAY-797). We will need to incorporate callback logic in the main stack classes  - DataContext, DataDomain.",", "
"   Rename Method,",EJBQL Delete Statement Support Implement support for EJBQL Delete Statement,", "
"   Rename Method,",EJBQL Update Statement Support Implement support for EJBQL Update Statements,", "
"   Rename Method,Extract Method,","Support combination of Persistent objects and scalars in query results Recently introduced SQLResultSetMapping would not allow a mix of objects and scalars in result Object[], so dumb queries like that are not possible:

""SELECT count(p), p FROM Painting p GROUP BY p""

Need to support that.","Duplicated Code, Long Method, , "
"   Rename Method,","EJBQL Subquery support Support for EJBQL Subqueries, ANY, ALL and EXISTS constructs",", "
"   Move Method,Move Attribute,","Remove arbitrary reverse relationship mapping limitations  From the mailing list post:
Remove 
We have two rules related to relationship mapping that we can really do well without:

1. A DbRelationship always requires a reverse DbRelationship.
2. A to-many ObjRelationship without a reverse to-one is effectively read only.

I've done some work on a project where we've used generic persistent classes, and it occurred to me that while the two things above are indeed a property of Cayenne runtime, users don't have to worry about such low level details. Cayenne can automagically add missing reverse relationships in runtime to the corresponding entities, without user ever noticing. That simple - don't know why nobody thought of that before :-)

BTW what makes (2) painless is CayenneDataObject that can store arbitrary data in it, so a back pointer from toOne side to the toMany site can be stored. This won't work in case of POJO's (without extra enhancement), but for normal Cayenne we get that functionality out of the box.

",", , , "
"   Rename Method,","Remove arbitrary reverse relationship mapping limitations  From the mailing list post:
Remove 
We have two rules related to relationship mapping that we can really do well without:

1. A DbRelationship always requires a reverse DbRelationship.
2. A to-many ObjRelationship without a reverse to-one is effectively read only.

I've done some work on a project where we've used generic persistent classes, and it occurred to me that while the two things above are indeed a property of Cayenne runtime, users don't have to worry about such low level details. Cayenne can automagically add missing reverse relationships in runtime to the corresponding entities, without user ever noticing. That simple - don't know why nobody thought of that before :-)

BTW what makes (2) painless is CayenneDataObject that can store arbitrary data in it, so a back pointer from toOne side to the toMany site can be stored. This won't work in case of POJO's (without extra enhancement), but for normal Cayenne we get that functionality out of the box.

",", "
"   Extract Superclass,Rename Method,","EJBQL Support for Functional Expressions Support String, Arithmetic and Datetime functions in EJBQLQuery.

Will be committing shortly all functions except a few that'll require more work:

* SIZE (requires a correlated subquery)
* TRIM char (requires cross-db testing; there's no JDBC standard syntax)",", Duplicated Code, Large Class, "
"   Rename Method,","Support for mapping to-many as Maps and Sets and Collections Per JPA spec we should support mapping to-many relationships as Lists, Collections, Sets and Maps (we currently only do Lists). Need to add that stuff to Cayenne classic and map to JPA.  I see the following subtasks:

* Support explicit to-many semantics mapping in ObjRelationship (collection class; map key for Maps)
* In the Modeler allow to specify the choices in ObjRelationship Inspector
* In class generation template use correct collection type (I guess for maps the add/remove semantics can be the same as for lists ... not sure if we need removeFrom(Object key)??)
* Runtime support, including reverse relationships
* Support for prefetching
* Testing
* Bridging JPA mapping 
",", "
"   Rename Method,","Support for mapping to-many as Maps and Sets and Collections Per JPA spec we should support mapping to-many relationships as Lists, Collections, Sets and Maps (we currently only do Lists). Need to add that stuff to Cayenne classic and map to JPA.  I see the following subtasks:

* Support explicit to-many semantics mapping in ObjRelationship (collection class; map key for Maps)
* In the Modeler allow to specify the choices in ObjRelationship Inspector
* In class generation template use correct collection type (I guess for maps the add/remove semantics can be the same as for lists ... not sure if we need removeFrom(Object key)??)
* Runtime support, including reverse relationships
* Support for prefetching
* Testing
* Bridging JPA mapping 
",", "
"   Move Method,Extract Method,","Support for mapping to-many as Maps and Sets and Collections Per JPA spec we should support mapping to-many relationships as Lists, Collections, Sets and Maps (we currently only do Lists). Need to add that stuff to Cayenne classic and map to JPA.  I see the following subtasks:

* Support explicit to-many semantics mapping in ObjRelationship (collection class; map key for Maps)
* In the Modeler allow to specify the choices in ObjRelationship Inspector
* In class generation template use correct collection type (I guess for maps the add/remove semantics can be the same as for lists ... not sure if we need removeFrom(Object key)??)
* Runtime support, including reverse relationships
* Support for prefetching
* Testing
* Bridging JPA mapping 
","Duplicated Code, Long Method, , , "
"   Move Class,Move Method,Inline Method,","Support for mapping to-many as Maps and Sets and Collections Per JPA spec we should support mapping to-many relationships as Lists, Collections, Sets and Maps (we currently only do Lists). Need to add that stuff to Cayenne classic and map to JPA.  I see the following subtasks:

* Support explicit to-many semantics mapping in ObjRelationship (collection class; map key for Maps)
* In the Modeler allow to specify the choices in ObjRelationship Inspector
* In class generation template use correct collection type (I guess for maps the add/remove semantics can be the same as for lists ... not sure if we need removeFrom(Object key)??)
* Runtime support, including reverse relationships
* Support for prefetching
* Testing
* Bridging JPA mapping 
",", , , "
"   Rename Method,",Deprecate EventManager.getDefaultManager() and stop using it EventManager.getDefaultManager()  is used by the map package to update mapping on dependent mapping object changes. The rest of Cayenne runtime uses EventManager that belongs to Configuration. We should deprecate 'getDefaultManager' and stop using the singleton in runtime.,", "
"   Rename Method,Extract Method,Inline Method,","CayenneModeler free-text search When working with huge models, finding needed entities/attributes can be challenging in the Modeler. It would be nice to have a search field (either in the bottom of the frame, FireFox style, or in the top right corner next to the menu bar) with Ctrl-F shortcut (Command-F on Mac). The search should present a list of selectable matched model objects, automatically jumping to the first one.  

Project scanning and navigation to the search results can probably be copied from the validation action (except that it would be nice to avoid a modal popup window).","Duplicated Code, Long Method, , , "
"   Rename Class,Rename Method,Move Method,Extract Method,","merge changes between model and db I want to be able to migrate schema changes between a DataMap and a database. Mainly for two reasons.
 1) Make it easier for DBAs and developers to keep track of DB related changes in a project.
 2) Make it simpler for developers to keep the db schema in sync with the model.

","Duplicated Code, Long Method, , , "
"   Rename Method,Pull Up Method,Extract Method,Pull Up Attribute,","merge changes between model and db I want to be able to migrate schema changes between a DataMap and a database. Mainly for two reasons.
 1) Make it easier for DBAs and developers to keep track of DB related changes in a project.
 2) Make it simpler for developers to keep the db schema in sync with the model.

","Duplicated Code, Long Method, , Duplicated Code, Duplicated Code, "
"   Rename Method,","Refactoring class generator classes I am going to do some renaming and code refactoring to make class generator code easier to extend. Here is a few points I am planning to work on:

1. Naming. CayenneGenerator, ClassGenerator, MapClassGenerator are all independent classes, none inheriting from each other. Just by looking at the name , it is impossible to tell that CayenneGenerator is an ant task, ClassGenerator is a template processor, and MapClassGenerator is a controller for multiple templates

2. Version 1.1 vs. version 1.2... Ideally we should get rid of the versions, but for now it would be nice to may be nice to just split different version handlers into different subclasses... Don't have a clear idea yet how to do it...",", "
"   Rename Method,Inline Method,","Refactoring class generator classes I am going to do some renaming and code refactoring to make class generator code easier to extend. Here is a few points I am planning to work on:

1. Naming. CayenneGenerator, ClassGenerator, MapClassGenerator are all independent classes, none inheriting from each other. Just by looking at the name , it is impossible to tell that CayenneGenerator is an ant task, ClassGenerator is a template processor, and MapClassGenerator is a controller for multiple templates

2. Version 1.1 vs. version 1.2... Ideally we should get rid of the versions, but for now it would be nice to may be nice to just split different version handlers into different subclasses... Don't have a clear idea yet how to do it...",", , "
"   Extract Interface,Rename Method,Extract Method,","Refactoring class generator classes I am going to do some renaming and code refactoring to make class generator code easier to extend. Here is a few points I am planning to work on:

1. Naming. CayenneGenerator, ClassGenerator, MapClassGenerator are all independent classes, none inheriting from each other. Just by looking at the name , it is impossible to tell that CayenneGenerator is an ant task, ClassGenerator is a template processor, and MapClassGenerator is a controller for multiple templates

2. Version 1.1 vs. version 1.2... Ideally we should get rid of the versions, but for now it would be nice to may be nice to just split different version handlers into different subclasses... Don't have a clear idea yet how to do it...","Duplicated Code, Long Method, , Large Class, "
"   Move Method,Inline Method,","Refactoring class generator classes I am going to do some renaming and code refactoring to make class generator code easier to extend. Here is a few points I am planning to work on:

1. Naming. CayenneGenerator, ClassGenerator, MapClassGenerator are all independent classes, none inheriting from each other. Just by looking at the name , it is impossible to tell that CayenneGenerator is an ant task, ClassGenerator is a template processor, and MapClassGenerator is a controller for multiple templates

2. Version 1.1 vs. version 1.2... Ideally we should get rid of the versions, but for now it would be nice to may be nice to just split different version handlers into different subclasses... Don't have a clear idea yet how to do it...",", , , "
"   Rename Class,Move Method,Move Attribute,","Refactoring class generator classes I am going to do some renaming and code refactoring to make class generator code easier to extend. Here is a few points I am planning to work on:

1. Naming. CayenneGenerator, ClassGenerator, MapClassGenerator are all independent classes, none inheriting from each other. Just by looking at the name , it is impossible to tell that CayenneGenerator is an ant task, ClassGenerator is a template processor, and MapClassGenerator is a controller for multiple templates

2. Version 1.1 vs. version 1.2... Ideally we should get rid of the versions, but for now it would be nice to may be nice to just split different version handlers into different subclasses... Don't have a clear idea yet how to do it...",", , , "
"   Rename Class,Extract Method,","Refactoring class generator classes I am going to do some renaming and code refactoring to make class generator code easier to extend. Here is a few points I am planning to work on:

1. Naming. CayenneGenerator, ClassGenerator, MapClassGenerator are all independent classes, none inheriting from each other. Just by looking at the name , it is impossible to tell that CayenneGenerator is an ant task, ClassGenerator is a template processor, and MapClassGenerator is a controller for multiple templates

2. Version 1.1 vs. version 1.2... Ideally we should get rid of the versions, but for now it would be nice to may be nice to just split different version handlers into different subclasses... Don't have a clear idea yet how to do it...","Duplicated Code, Long Method, , "
"   Rename Class,Extract Superclass,","Refactoring class generator classes I am going to do some renaming and code refactoring to make class generator code easier to extend. Here is a few points I am planning to work on:

1. Naming. CayenneGenerator, ClassGenerator, MapClassGenerator are all independent classes, none inheriting from each other. Just by looking at the name , it is impossible to tell that CayenneGenerator is an ant task, ClassGenerator is a template processor, and MapClassGenerator is a controller for multiple templates

2. Version 1.1 vs. version 1.2... Ideally we should get rid of the versions, but for now it would be nice to may be nice to just split different version handlers into different subclasses... Don't have a clear idea yet how to do it...",", Duplicated Code, Large Class, "
"   Rename Method,Extract Method,","Refactoring class generator classes I am going to do some renaming and code refactoring to make class generator code easier to extend. Here is a few points I am planning to work on:

1. Naming. CayenneGenerator, ClassGenerator, MapClassGenerator are all independent classes, none inheriting from each other. Just by looking at the name , it is impossible to tell that CayenneGenerator is an ant task, ClassGenerator is a template processor, and MapClassGenerator is a controller for multiple templates

2. Version 1.1 vs. version 1.2... Ideally we should get rid of the versions, but for now it would be nice to may be nice to just split different version handlers into different subclasses... Don't have a clear idea yet how to do it...","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","SelectTranslator support for standard SQL join syntax, including outer joins Currently select translator generates JOIN syntax in an old fashion way - adds all participating tables to the FROM clause of the query, adds join conditions to the WHERE clause. Among other things this limits us to only INNER JOINS on almost all DB's (except for maybe Oracle). Will need to change the translator to generate modern cross-db explicit join syntax that places all tables and conditions in the FROM clause. E.g.:

Old: SELECT ... FROM ARTIST t0, PAINTING t1 WHERE t0.ARTIST_ID = t1.ARTIST_ID 
New: SELECT ... FROM ARTIST t0 JOIN PAINTING t1 ON (t0.ARTIST_ID = t1.ARTIST_ID)

Things to consider:

* Check all DbAdapters to see if some override the join generation methods and therefore need to be updated
* This feature does not change the fact that SelectQuery itself still will not support  explicit outer joins in the qualifier
* Still the new API for joins should allow callers to specify what kind of join they want (so that we could use it in prefetches down the line)
 
","Duplicated Code, Long Method, , "
"   Rename Method,Inline Method,","Database Schema Migration does not create FK constraints. It'd be nifty if the database schema migration tool could detect if a FK relationship needs to be set up.  The DB generation function can set up FK relationships as is, so hopefully it's not too great a stretch to make it work with the migration tool as well.",", , "
"   Rename Class,Extract Method,","Scaling paginated list An idea for scaling IncrementalFaultList to store massive amount of objects, like hundreds of thousands.This pertains to the server-side IncrementalFaultList. The problems to solve are the speed of the initial list initialization and overall memory use.

1. Simplify ID representation:

Even unresolved lists can take significant amount of memory... Each unresolved object slot currently stores a DataRow with N number of entries, where N is the number of PK columns for the entity. I.e. most often than not - 1 entry. Here is a memory use calculation for various representations of an unresolved entry, based on a single int PK DbEntity.

a. DataRow - 120 bytes, 
b. HashMap - 104 bytes,
c. Object[] - 32 bytes,
d java.lang.Integer - 16 bytes
[primitive int is even better, but it complicates the implementation, as we'd need a parallel int[]  (long[], double[], etc.) , so all in all we may get no gain]
","Duplicated Code, Long Method, , "
"   Rename Method,","CuratorFramework.Builder should allow adding multiple auths Currently, one can add a single authentication scheme/bytes when building CuratorFramework. It would be handy to add multiple.",", "
"   Rename Method,",Make ZKPaths accept more than one child ZKPaths currently only accepts one parent and one child nodes. It would be useful to be able to create paths with more depth.,", "
"   Move And Rename Class,Move Method,Extract Method,","PathChildrenCache closes it's executor always PathChildrenCache.close() calls shutdownNow() on its executor, always.

I generally reuse executors and have been passing them in on the constructor.  I have to wrap my executors in something that ignores the shutdownNow() call in order to work around this.  It's not the end of the world, but it is a little annoying.","Duplicated Code, Long Method, , , "
"   Move Class,Extract Method,","PathChildrenCache (wastefully) creates a thread per monitored node. PathChildrenCache creates a single-threaded executor. In the aggregate, this means there is a thread for every monitored node.

In my company's use case, we use ServiceCache (which uses PathChildrenCache) to monitor service discovery nodes for the locations of many sharded immutable key/value stores used by our services. We saw in excess of 250 threads devoted to the PathChildrenCache's used by ServiceCache. These threads were all parked so there was negligible CPU impact, but there is still the memory/stack impact of having so many idle threads. We would like to avoid that impact.

I'd like to modify PathChildrenCache to take an ExecutorService as an alternate to the ThreadFactory it currently takes. This would allow me to pass in a thread pool for my company's use case.

Are there any issues with which I should be concerned concerning PathChildrenCache's use of separate threads? (Non-reentrancy from watcher-invoked code? Binary compatibility?)
","Duplicated Code, Long Method, , "
"   Rename Class,Extract Interface,","Recursive delete Currently there is the ability to recursive create parent znodes when you create a node. However, there is no ability to recursively delete a hierarchy. Zookeeper already provides this in their ZKUtil.java package, but it seems like a very curator-ish thing to perform as well. There is the potential difficulty involved with the guarantee() functionality, but it should be workable.",", Large Class, "
"   Move Class,Rename Method,Extract Method,","JAXWS: First pass at server side Java bean dispatcher. I'm working on a first pass of the code that will be used to dispatch requests to server side Java bean endpoints.

For this first iteration, only endpoints that are based on Doc/Lit Wrapped WSDLs will be supported.","Duplicated Code, Long Method, , "
"   Move And Rename Class,Move Attribute,","Dispatcher support for ServiceMode=MESSAGE annotation The Dispatcher currently assumes ServiceMode=PAYLOAD (e.g. the Body of the message).   MESSAGE mode support for Provider<T>  types of Source, SOAPMessage, and String needs to be added.

I am working on a patch which I will submit shortly",", , "
"   Move Method,Extract Method,","Add RPC/Lit MethodMarshaller support With Rich's RPC updates to the JAX-WS Message model, we now need the MethodMarshaller support to go along with that.","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,",Proper error message should be given and user should be returned to the options list when providing invalid option in ServiceLifeCycle client Currently system just returns an exception when user provides a wrong option in Library client of ServiceLifeCycle sample. Please return a proper error message and provide user with the options list again when he/she enters a wrong option.,"Duplicated Code, Long Method, , "
"   Pull Up Method,Move Method,Pull Up Attribute,","Add SPI to allow naviation to ServiceDelegate and associated EndpointDescription from BindingProvider Add the abilility to navigate from a BindingProvider to the ServiceDelegate and the EndpointDescription.  The BindingProvider is implemented by the Proxy handler and the Dispatch that are returned to the client.  Middleware (via this new SPI) can cast the Dispatch or the proxy handler returned from Proxy.getInvocationHandler to the spi BindingProvider inteface and then retrieve the ServiceDelegate or the EndpointDescription.

See the test modules/jaxws/test/org/apache/axis2/jaxws/description/GetDescFromBindingProvider.java for an example.

Also refactor PortInfoData into the EndpointDescription and added protocol to return a PortInfo object from the EndpointDescription.",", , Duplicated Code, Duplicated Code, "
"   Extract Method,Move Attribute,","implementation of some not-yet-implemented SOAPBinding methods Some of the methods in SOAPBinding were not yet implemented.  This patch provides those implementations, which required some changes to other parts of the code.  Please review prior to committing.  Patch was created at the modules/jaxws/src directory level.

Thanks!
","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,Inline Method,","JAX-WS XML/HTTP Support Add support for XML/HTTP (REST).   

The first step is to upgrade the JAX-WS Message subsystem to handle REST.  

The second step is to add the support for Dispatch and Provider modes. 

The final step is to add support for XML/HTTP proxies. 

This issue will remain open while we add the various layers of code.

","Duplicated Code, Long Method, , , "
"   Move And Rename Class,Rename Method,Extract Method,","JAX-WS Cache or improve the @XMLRootElement and related annotation lookups The JAX-WS doc/lit wrapped marshalling code needs to query annotations such as @XmlRootElement, @XmlList, @XmlType etc.

This defect is opened to save/cache the resulting information on the OperationDescription or possibly a static WeakHashtable to improve performance.","Duplicated Code, Long Method, , "
"   Move Class,Move Method,Move Attribute,","JAX-WS Cache or improve the @XMLRootElement and related annotation lookups The JAX-WS doc/lit wrapped marshalling code needs to query annotations such as @XmlRootElement, @XmlList, @XmlType etc.

This defect is opened to save/cache the resulting information on the OperationDescription or possibly a static WeakHashtable to improve performance.",", , , "
"   Rename Method,Move Method,Extract Method,","JAX-WS Cache or improve the @XMLRootElement and related annotation lookups The JAX-WS doc/lit wrapped marshalling code needs to query annotations such as @XmlRootElement, @XmlList, @XmlType etc.

This defect is opened to save/cache the resulting information on the OperationDescription or possibly a static WeakHashtable to improve performance.","Duplicated Code, Long Method, , , "
"   Rename Method,",fix and enable the test added in jira AXIS2-1830 fix and enable the test added in jira AXIS2-1830,", "
"   Extract Superclass,Pull Up Method,Extract Method,Pull Up Attribute,","Duplicate code in OperationClient Glen and I went through client side code base and found out that there are multiple places we have duplicate codes , and the solution we came up with listed below;
- We can convert OperationClient interface to an abstract class and move the constructor code the abstract class .
- Move duplicate code inside the execute method to a utility class or somewhere ","Duplicated Code, Long Method, , Duplicated Code, Large Class, Duplicated Code, Duplicated Code, "
"   Rename Class,Rename Method,Move Attribute,","Upgrade SimpleHttpServer to HttpCore 4.0-alpha3 Folks,

I have upgraded SimpleHttpServer to use the newest version of HttpCore (4.0-a3) and also refactored some of the Axis specific classes that got somewhat messy. I also would like to do a little more refactoring work on SimpleHttpServer in the coming days, but I do not know how much this could interfere with your plans.  

Please let me know what you think.

Oleg",", , "
"   Rename Method,",SAAJ 1.3 implementation Two patches attached (for modules saaj & saaj-api) include part of saaj 1.3 implementation. Integrated & tested this with latest source from trunk. ,", "
"   Rename Method,Inline Method,",SAAJ 1.3 implementation Two patches attached (for modules saaj & saaj-api) include part of saaj 1.3 implementation. Integrated & tested this with latest source from trunk. ,", , "
"   Rename Method,",SAAJ 1.3 implementation Two patches attached (for modules saaj & saaj-api) include part of saaj 1.3 implementation. Integrated & tested this with latest source from trunk. ,", "
"   Extract Method,Inline Method,","New JSONBadgerfishMessageFormatter This patch includes a JSONBaderfishMessageFormatter which formats the json message using a badgerfish json writer... User should map the correct message formatter with the content type, in the axis2.xml.

And also some improvements in the JSONIntegrationTest are included in this patch...","Duplicated Code, Long Method, , , "
"   Rename Method,","The class org.apache.axis2.uitl.Builder can be refactored to improve usability, improve object orientation and slightly improve performance The method ""public static OMBuilder getBuilder(InputStream inStream, String charSetEnc, String soapNamespaceURI)"" in the class org.apache.axis2.uitl.Builder takes three parameters, but when you look at the code last two parameters are optional. A more object oriented way of handling this would be to have this method overloaded to handle the difference scenarios.

Right now this method has two null checks which get executed every time due to this. When I navigate through the calling stack I realize every time the caller is well aware of whether the optional parameters are available or not and he is passing null in such scenarios since there is no overloaded method. At the external interfaces where a parameter is really required implementors are passing a default value making the null check redundant.

Also there is one overloaded method which takes a parameter of type Reader which provides the same functionality. IMO this method is again redundant since the only difference is how you handle the input, which should be the responsibility of the caller, not the utility class like this.

",", "
"   Rename Method,Extract Method,",Improvement in json support Some minor improvements in JSON module.,"Duplicated Code, Long Method, , "
"   Rename Method,Move Method,Extract Method,","Rename SOAPOverHTTPSender to HTTPSender and refractor it to support all http methods With the introduction of messageFormatters we can get rid of RESTSender and add all four http methods to HTTPSender (Which used to be SOAPOverHTTPSender). Introduce two messageFormatters ApplicationXMLFormatter (which serializes data as application/xml) and XFormURLEncodedFormatter (which serializes data as application/x-www-form-urlencoded).

Thanks,
Keith.","Duplicated Code, Long Method, , , "
"   Rename Class,Rename Method,","Adding configurable parameter for Metadata Exchange 
This JIRA is to add configrable parameter for Metadata Exchange function. Following are the two 
configuration requirements that currently identified:

a. Once ""metadataExchange"" module is engaged globally in axis2.xml, we need a way
   to disable GetMetadata request for a service. 

b. When a GetMetadata request is issued, the /mex:Metadata element returns multiple 
   /mex:MetadataSection units. A MetadatSection could be either the embedded XML (inline),
   an endpoint reference to a Metadata Resource i.e. MetadataReference, 
   or a URL i.e. Location element. The WS-MEX spec does not define what output forms: inline, Location
   or MetadataReference should be returned for a GetMetadata request. Currently, MexMessageReceiver
   just default to generating Metadata Sections for all the 3 possible output forms as stated 
   in the spec. We need a way to configure the Metadata Section content to return for a GetMetadata 
   request. For example, if there is large amount of data, large amount of inline xml might not be
   desirable. The <outputform> allows to configure only GetMetadata response with Metadata Sections
   of Metadata Reference and Location instead of actual information. 
   


Solution implemented:

 Adding ""metadataExchange"" element parameter configuration in axis2.xml and services.xml,
 allows to support the above configuration needs as well as future needs.

 Following are the configurables items:                                       
    enable attribute - possible values: ""false"". This is used to disable
                        MEX support for a service. When disabled,      
                        MexDisabledException will be returned to the    
                        sender of GetMetadata request.                  
    outputform element contains optional ""dialect"" attribute and    
                 required ""forms"" attribute. possible values for ""forms"":      
                 inline,location,reference.If not configured, default is       
                 inline,location, and reference.  
  
 Note that the outputform only avoided unnessary processing in creating Metadata sections
 for data format that is not needed.                             
     
  As an example, 
  <parameter name=""metadataExchange"" locked=""false"">   
       <outputform dialect=""http://schemas.xmlsoap.org/wsdl/""             
                 forms=""inline,location"" />                             
       <outputform forms=""reference,location"" /> 
  </parameter>                          
 
  If the above configuration is added to the axis2.xml, this means the GetMetadata response
  will contain Metadata Sections of actual WSDL data, and URL for the WSDL dialect, and
  for other dialects such as policy, schema, etc., only Metadata Sections with 
  MetadataReference and Location will be included in the response.

 The order of precedence for the output form  will be similar to the data locator look-up:
         a. dialect specific at service level i.e. configured in services.xml    
         b. service level  i.e. without dialect attribute specified
         c. dialect specific at global level i,e, configured in axis2.xml
         d. service level  i.e. without dialect attribute specified
         e. default output forms to all: inline, location, reference

Summary of code changes:

New class:  org.apache.axis2.mex.MexDisabledException
Modified classes:
  MexConstants - adding constants for configurable items  refere in  ""metadataExchange"" parameter
  MexMessageReceiver -  Access the parameter axis configuration and service configuration 
                                              Check if MEX is disabled before processing
                                               Call MexUtil.determineOutputForm(..) instead of default to 
                                                all 3 output forms. 
 MexUtil: Added APIs: determineOutputForm(..), check_MEX_disabled()
                                              
",", "
"   Rename Method,","Change JAXWS MessageContext to use EndpointDescription The JAXWS MessageContext should hold a reference to the EndpointDescription, rather than the ServiceDescription.  This JIRA will be used to change all of the places where that is referenced.

This is also needed by AXIS2-2218 for integration of the handlers.",", "
"   Move And Rename Class,Rename Method,","handler integration, round 2 I will be applying the next round of handler integration code.  Nick Gallardo will have to follow with a fix to JAXWSProxyHandler.  He is already aware of this.",", "
"   Rename Method,Extract Method,Move Attribute,","OMNode, OMDocument use OMOutputImpl - violating package encapsulation The following functions:

OMNode.serializeWithCache(OMOutputImpl omOutput)
OMNode.serialize(OMOutputImpl omOutput)

OMDocument.serialize(OMOutputImpl omOutput)
OMDocument.serialize(OMOutputImpl omOutput, boolean includeXMLDeclaration)
OMDocument.serializeWithCache(OMOutputImpl omOutput)
OMDocument.serializeWithCache(OMOutputImpl omOutput, boolean includeXMLDeclaration)

All reference OMOutputImpl - which is found in the ""om.impl"" package, rather than the ""om"" package.  It seems to me that perhaps there should be an interface called ""OMOutput"", or something like that.","Duplicated Code, Long Method, , , "
"   Rename Method,",Add implementation of LogicalMessageContext/LogicalMessage for use in handlers We need an implementation of the APIs that are used in a LogicalHandler flow.  I have some of this completed already and will be contributing.,", "
"   Rename Method,","handler integration, tests enabled Handlers are more fully integrated, some tests are enabled.",", "
"   Rename Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,",Improve Axis idea plugin wizard as resizable wizard (WSDL2JAVA wizard) this patch provide select wsdl and option wizard panels for wsdl2java wizard.,"Duplicated Code, Long Method, , , , , "
"   Extract Interface,Rename Method,Move Method,Extract Method,Move Attribute,","MessageContext Persistence Performance Improvement MessageContext Persistence Performance Improvement
---------------------------------------------------------------------------------

Background: 
-----------------
When a MessageContext is persisted (for reliable messaging), the MessageContext object and associated
objects are written out to the ObjectOutput.  When a MessageContext is hydrated it is read from an 
InputObject.  The utility class, ObjectStateUtils, provides static utility functions to provide safety
mechanisms to write and read the data.

Problem:
--------------
The IBM performance team has profiled this code.  They found that the writing and reading of these objects is time 
consuming.  Some of the performance penalties are due to the use of static methods (thus hindering the ability to reuse
byte buffers).  Other penalties are due to the way that we determine if an object can be ""safely written"".
This JIRA issue addresses a number of these concerns.

Scope of Changes (Important):
-------------------------------------------
These changes only amend the existing writeExternal and readExternal support.  There is no impact on any code that 
does not use these methods.  No additional logic api's are added or changed. 


Specific Concerns and Solutions:
----------------------------------------------
  A) The original logic writes objects into a buffer.  If a serialization error occurs, the algorithm safely 
     accommodates the error.  The downside is that it is very expensive to write each object to a temporary buffer.

     Solution:
     A new marker interface, SafeSerializable, is introduced.  If an object (i.e. MessageContext) has this marker
     interface or is a lang wrapper object (i.e. String) then the object is written directly to the ObjectOutput.
     Eliminating the extra buffer write increases throughput.
     A similar change is made to the read algorithm.  The new algorithm detects whether the object was written directly
     or whether it was written as a byte buffer.  In the case where it is written directly, no extra buffering is needed
     when reading.

  B) If a buffer is needed to write or read an object, the ObjectStateUtils class creates a new buffer.  This 
     excessive allocation of buffers and subsequent garbage collection can hinder performance.

     Solution:
     The code is re-factored to use two new classes: SafeObjectOutputStream and SafeObjectInputStream.  These classes
     wrap the ObjectOutput and ObjectInput objects and provide similar logic as ObjectStateUtils.
     The key difference is that these are not static utility classes.  Therefore any buffers used during writing or reading can
     are reused for the life of the *Stream object.  In one series of tests, this reduced the number of buffers from 40 to 2 for 
     persisting a MessageContext.

  C) When an outbound MessageContext is persisted, its associated inbound MessageContext (if present) is also persisted.
     The problem is that the inbound MessageContext may have a large message.  Writing out this message can impact performance
     and in some cases causes logic errors.
  
     Solution:
     Any code that hydrates an outbound MessageContext should never need the message (soapenvelope) associated with the 
     inbound MessageContext.  The solution is to not persist the inbound message.

  D) In the current code, ""marker"" strings are persisted along with the data.  These marker strings may contain a lengthy 
     correlation id.   This extra information can impact performance and file size.

     Solution:
     I reduced the number of ""marker"" strings.  The remaining marker strings are changed to the ""common name"" of the object
     being persisted.  In most cases, the log correlation id is no longer present in the marker string.  In addition, I made
     changes to only create a log correlation id ""on demand"".  The log correlation code uses the (synchronized) UUIDGenerator.  
     Creating the log correlation id ""on demand"" limits unnecessary locking.

  E) Miscellaneous.  I spent time fine tuning the algorithmic logic in SafeObjectInputStream and SafeObjectOutputStream
     to eliminate extra buffers (i.e. ByteArrayOutputStream optimizations).  These are all localized changes.

Other Non-performance Related Changes
------------------------------------------------------------

  i) The externalize related code is refactored so that all lives in the new org.apache.axis2.context.externalize package.
 
  ii) The ObjectStateUtils class is retained for legacy reasons.  I didn't want to remove any api's.  The implementation 
      of ObjectStatUtils is changed to delegate to the new classes.

  iii) New tests are added.

  iv) I added classes DebugOutputObjectStream and DebugObjectInputStream.  
      These classes are installed when log.isDebugEnabled() is true.  
      The classes log all method calls to and from the underlying ObjectOutput and ObjectInput; thus they are helpful 
      in debugging errors.

  v) Andy Gatford has provided code that uses the context classloader when reading persisted data.

  vi) The high level logic used to write and read the objects is generally the same.  The implementation of the algorithms is changed/improved.
     In some cases, this required changes to the format of the persisted data.  An example is that each object is preceded by
     a boolean that indicates whether the object was written directly or written into a byte buffer.  I increased the revision id because
     I changed the format.



Kudos
---------
Much thanks to the following people who contributed to this work, helped with brainstorming, helped with testing or provided performance profiles:
Ann Robinson, Andy Gatford, Dan Zhong, Doug Larson, and Richard Slade.

Next Steps
---------------
I am attaching the patch to this JIRA.  I will be committing the patch in the next day or two.  Please let me know if you have any questions or concerns.

Thanks
Rich Scheuerle



","Duplicated Code, Long Method, , , , Large Class, "
"   Move And Rename Class,Rename Method,","There is no reason for HTTPSListener to be inner class org.apache.axis2.transport.httpHTTPSListener is innerr class for org.apache.axis2.transport.httpListingAgent.
Therefore it must be instantiated diffrently using reflection in AxisConfigBuilder.processTransportReceivers in case we need
it as TransportReceiver....instead of changing the implementation of  AxisConfigBuilder.processTransportReceivers, the inner class just have to be removed from Listing Agent, defined as a normal java class in the same package...i can provide patch if this is acepted...i personally see no reason for the class to be inner",", "
"   Rename Method,","[PATCH] Upgrades 'Simple' HTTP and NIO HTTP transports to HttpCore 4.0-alpha6  There has been a number of improvements and bug fixes Axis2 'simple' and NIO HTTP transports should pick up [1]. I am attaching a patch that upgrades Axis2 to the latest HttpCore API.

All test cases pass for me.

Oleg

[1] http://www.apache.org/dist/jakarta/httpcomponents/httpcore/RELEASE_NOTES.txt
",", "
"   Move Class,Pull Up Method,Move Method,Extract Method,Pull Up Attribute,Move Attribute,","JAX-WS: JAXB Processing Improvement History:

The JAX-WS layer has a Message component that delegates to the Axiom data model.  The Message component simplifies the marshalling and unmarshalling steps.
When this layer was developed, the Axiom OMDataSource (and OMSourcedElement) abstractions were not fully implemented.  The result is brittle code in the Message component.

Goal:

The goal is to decompose the Message layer.  The code should be redesigned so that we have JAXBDataSource, XMLStringDataSource and SourceDataSource
abstractions.  The Message Block abstraction should then simply delegate to these *DataSource objects.

Breaking out the JAXBDataSource would allow JAXB processing outside of the JAXWS module.

First Step:

In this JIRA, I am introducing the JAXBDataSource, XMLStringDataSource and SourceDataSource abstractions.

","Duplicated Code, Long Method, , , , Duplicated Code, Duplicated Code, "
"   Rename Method,Extract Method,","Modify BlockImpl to avoid double unmarshalling BlockImpl.getBusinessObject() creates an buisiness object from AXIOM.

I am trying to create buisiness object before JAXWSReceiver and set the buisiness object into OMSoucedElement. But current BlockImpl.getBuisinessObject() can not aware that. The BlockImpl wraps the OMSourcedElement by OMStaXWrapper and create same buisiness object again.

This patch modifies BlockImpl to be able to aware such OMSoucedElement which has buisiness object and set the buisiness object into BlockImpl.
","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","AbstractContext should lazily create the PropertyDifferences map Problem Summary:

The AbstractContext maintains a PropertyDifferences map to record each addtion/removal of a property.  This extra processing is only necessary in a cluster environment
and degrades perfromance in non-cluster environments.

Solution:
The suggestion is to lazily create the PropertyDifferences map.  It should only be created when clustering is enabled.

Kudos to David Strite for suggesting this change.

I am working on a fix.","Duplicated Code, Long Method, , "
"   Extract Superclass,Rename Method,","JAX-WS 2.1: Support @MTOM and MTOMFeature JAX-WS 2.1 adds some new config options for enabling MTOM, along with adding some new capabilities to the MTOM function.  This will mostly leverage existing infrastructure, but will also require some additional functionality and coordination with the message model.  A quick summary of the work to be done:

1. [DONE] Update the metadata APIs to reflect whether or not the MTOM configuration was found on the endpoint.  

2. [DONE] Update the annotation processing code to pick off the @MTOM annotation when included on an endpoint.

3. [DONE] Update the client creation code off of the ServiceDelegate to set the appropriate values when the MTOMFeature is configured for client instances. 

4. Change the marshalling code to read the threshold and toggle MTOM when appropriate.  


Here are some examples of the new ways that MTOM can be configured.

Before:
@WebService
@BindingType(SOAPBinding.SOAP11_HTTP_MTOM)
public class MyServiceImpl {
    ...   
}

After:
@WebService
@MTOM
public class MyServiceImpl {
    ...
}

Additionally, the threshold is configured as such.  

@WebService
@MTOM(enabled=""true"", threshold=""2000"")
public class MyServiceImpl {
    ...
}





On the client side, the configuration changes a little bit as well.  Here are a few examples of how this will be done.

        
MTOMFeature mtom = new MTOMFeature();
mtom.setEnabled(true);
mtom.setThreshold(2000);

Service service = Service.create(myServiceWSDL, serviceName);

MyProxy proxy = service.getPort(portName, MyProxy.class, mtom);
",", Duplicated Code, Large Class, "
"   Rename Method,","JAX-WS 2.1: Support @RespectBinding and RespectBindingFeature RespectBinding is a feature added in JAX-WS 2.1 that allows an endpoint to ignore the binding defined in a wsdl:binding element.  In other words, an endpoint could support both SOAP 1.1 and SOAP 1.2 even though the WSDL may indicate only SOAP 1.1 support for the endpoint.  Here's a quick summary of the work to be done:

1. Update the metadata APIs to expose the RespectBinding data available

2. Update the annotation processing code in the DescriptionBuilder to process the @RespectBinding annotation.

3. Update the WebServiceFeature processing code to account for the RespectBindingFeature.  

4. Change the EndpointController to have a toggle point that checks against a RespectBinding property.  

5. Update Provider processing code (ProviderDispatcer) to handle the scenario where the return type is invalid according to the input.  This is described in the check that exists in the EndpointController.

",", "
"   Rename Method,","Fixing the interop tests in itest Most of the Interop Tests in Integration/itest fail due to various reasons and I've started fixing those.

These interop tests have used XMLComparatorInterop to compare the returned Soap Envelope with the expected response. Since this class contains errors, we decided not to use it anymore. Instead, the /compare /method in WhiteMesaInterop class was changed and a new method, /compareXML /is added. So, should we remove XMLComparatorInterop, since it is not used anymore?
",", "
"   Rename Method,","JAXWS: Support Binding Property to Access SOAPHeaders  Background:
------------------

The JAX-WS specification defines properties to set/get attachments on the dispatch/proxy.  For example:
        // Create Dispatch for the payload
        Dispatch<String> dispatch = 
                svc.createDispatch(portName, String.class, Service.Mode.PAYLOAD);

        // Get the request context
        Map<String, Object> requestContext = dispatch.getRequestContext();

        // Get the attachments (non-payload) that should also be sent.
        Map<String, DataHandler> attachmentMap = new HashMap();
        attachmentMap.put(""javax.xml.ws.binding.attachments.outbound"", myDataHandler);

        // Attach the attachments to the request context
        dispatch.getRequestContext().put(""javax.xml.ws.binding.attachments.outbound"", attachmentMap);

The ""javax.xml.ws.binding.attachments.*"" properties make it convenient to get/receive attachments.

Proposal:
----------

The proposal is to add the same kind of functionality to get/set SOAP Headers.  
For example:
        // Create Dispatch for the payload
        Dispatch<String> dispatch = 
                svc.createDispatch(portName, String.class, Service.Mode.PAYLOAD);

        // Get the request context
        Map<String, Object> requestContext = dispatch.getRequestContext();

        // Create a new outbound header
        Map<QName, List<String>> headerMap = new HashMap();

        List<String> myHeadersList = new ArrayList<String>();
        myHeadersList.add(""<pre:sample xmlns:pre=""http://sample"">hello</pre:sample>"";
        QName myHeaderQName = new QName(""http://sample"", ""sample"", ""pre"");

        headersMap.put(myHeaderQName, myHeadersList );

        // Attach the headers map to the request context
        dispatch.getRequestContext().put(""jaxws.binding.soap.headers.outbound"", headerMap);

Details:
--------

Proposed names:
  ""jaxws.binding.soap.headers.outbound"" and ""jaxws.binding.soap.headers.inbound"".  
This is similar naming convention as the existing attachment properties

Proposed value:
   Map<QName, List<String>>
     QName is the qname of the header(s)
     List<String> is a list of xml values (normally one) 
        String is the xml string for a single header.  
          (The object is a String.  Most JAXWS users will not be familiar with OM and may not want to build a SAAJ SOAPHeader).

Semantics for ""jaxws.binding.soap.headers.outbound"":
  Prior to the dispatch/proxy invocation:
       The customer sets the outbound map on the RequestContext.
  During the dispatch/proxy invocation:
       The outbound jaxws engine adds the headers to the message.

Semantics for ""jaxws.binding.soap.headers.inbound""
  During the dispatch/proxy invocation:
       The engine will provide a map<QName, List<String>> on the ResponseContext.
  After the dispatch/proxy invocation:
       The customer accesses the inbound headers map from the ResponseContext.

 ",", "
"   Rename Method,",Refactored the changes made when improving Faulty Services handling. 0,", "
"   Move Method,Extract Method,","Adding Function to support RespectBinding changes in JAX-WS2.2 on Client and Server I am adding code that will Add support for RespectBinding changes in JAX-WS2.2 on Client and Server side. I will make modifications to RespectBindingConfiguration on client and server and also add Unit test cases to verify the functionality.

I will provide patch for this change.","Duplicated Code, Long Method, , , "
"   Rename Method,","Implicit SEI restiction on Static and Final Method exposure as webservice JAX-WS 2.2 specification restricts exposure of static and Final method on Implicit SEI as a webservice. I am making a change that will allow us to be compliant with this requirement, I am also adding new test cases to prove restriction on Static and Final operations.

",", "
"   Rename Method,","Support new SOAP/JMS binding ID The W3C SOAP/JMS binding specification (located here: http://www.w3.org/2002/ws/soapjms/) is nearing its first approved version, and has defined a more up to date binding ID for use in the @BindingType annotation (for JAX-WS endpoint impl classes) and within the soap:binding transport attribute within a WSDL document.    Previously, the SOAP/JMS spec defined this binding ID:
   http://www.example.org/2006/06/soap/bindings/JMS/
but that was just a placeholder, and the spec has amended that to this new binding ID:
   http://www.w3.org/2010/soapjms/

I've made some minor changes to the jax-ws and axis2 code to reflect this new binding ID, and I've also run some tests and found a few problems and fixed those as well.    I'm attaching a patch file with the changes.

The purpose of the JIRA is to request that someone apply the changes since I'm not a committer.",", "
"   Move Method,Extract Method,","WSDL customization API for Axis2 and stabilize WSDL 2.0 features  This project idea consists of collection of Axis2 issues related to WSDL features and mainly focus on following two areas. 

1.) Introduce a API to customize default WSDL generation behavior of Axis2 and this need to be supported to both WSDL 1.1 and WSDL 2.0. 
2.) Stabilize WSDL 2.0 features and add missing features. 


Following are the set of issues that have been identified as compulsory tasks of this project and required to provide  detailed  technical design with proposals. 

AXIS2-3492   - WSDLSupplier configuration and/or check is not proper/obvious
AXIS2-5278     - WSDLSupplier should support for both WSDL 1.1 and WSDL 2.0 
AXIS2-3653   - customization of dynamic WSDL creation
AXIS2-5240   - Provide an API/mechanism for setting parameters for run-time Java2WSDL generation.
AXIS2-3114   - Control what wsdl bindings returned via services.xml
AXIS2-5191   - Axis2 should support to use ""useOriginalWSDL"" property for WSDL 2.0


Further following are some of the optional tasks identified and it's expected to complete some of those issues as well ( Not required to fix following complete list of issues).                

AXIS2-4976 - Axis2 wsdl2code code generation bug caused by ""localName""
AXIS2-4407 - Axis 2 does not pick up wsdl2.0 which is modified to include whttp:location and whttp:method for Restful services 
AXIS2-4193 - WSDL2JAVA no setters in ADBBean
AXIS2-3768 - WSDL20ToAxisServiceBuilder does not read policies in WSDL 2.0 docs
AXIS2-3108 - Broken WSDL for operations added by modules
AXIS2-4734 - Issue with schema import in the wsdl file
AXIS2-4985 - NullPointerException in axis2-aar-maven-plugin if no fileSet is specified
AXIS2-4747 - Possible bug when generating code for livebookings wsdl
AXIS2-4521 - WSDL504,Could not locate the schema document when tomcat starts up
AXIS2-4436 - Woden attempts to load http://www.w3.org/2001/XMLSchema.xsd every time it parses a WSDL document (or if Axis2 instructs Woden to do so)
AXIS2-4978 - Copying data from inputStream to OuputStream needs appropriate buffer size
AXIS2-4065 - Policy attached to an input operation in a WSDL does not get copied to the Stub operation by WSDL2Java","Duplicated Code, Long Method, , , "
"   Move Method,Move Attribute,","Improve SimpleHTTPServer code structure, configuraiton, cookie use, and remove unused classes We need to clean up the code for the new HttpCore based SimpleHTTPServer.

Per Oleg:

I still would like to clean things up a little here and there (at the very least the old classes from HttpClient testing framework should be removed). I would also like to refactor the session context management code and improve cookie handling. Presently Set-Cookie2 headers generated by the HTTPWorker are not spec compliant.

Per my commit message:

Still to do:
    1. Identify and remove any Commons classes that are no longer referenced
    2. Extend HttpFactory and config parameters for additional options
    3. Document in xdocs
",", , , "
"   Rename Class,Extract Method,","JAXWS: Fix the invokeOneWay and invokeAsync (callback) implementations in AxisInvocationController. I've implemented the invokeOneWay and invokeAsync-callback methods in the AxisInvocationController.  The sync had already been implemented in a previous defect.

Also, I've updated the BaseDispatch to use this rather than the original AxisController.  This brings us much closer to removing the AxisController API in favor of the AxisInvocationController.  The changes to the async impl requires updates to the callbacks that are used.  Specifically, the AsyncResponse/AsyncResponseProcessor have been renamed to be a little more intuitive.

The AsyncListener is the task that will wait for the async response to come back.  This will be wrapped in an AsyncListenerWrapper that controls the starting and stopping.  This AsyncListenerWrapper is what will be used by the JAX-WS client to determine if a response is available or not.","Duplicated Code, Long Method, , "
"   Rename Method,","JAXWS: Update AxisInvocationController to use the OperationClient instead of the ServiceClient For the purposes of greater flexibility, the AxisInvocationController should use the Axis2 OperationClient API rather than using the ServiceClient.  I've started a little bit of this work in a sandbox and will hopefully have patches to post later on today.

Also, this work is dependant upon the patch that I posted for JIRA issue 909.",", "
"   Rename Class,Rename Method,Pull Up Method,Pull Up Attribute,","JAXWS Dynamic Proxy new Updates and test cases Updates to dynamic proxy, added new functionality and refactored ProxyHandler code. Also created ProxyDescriptorFacotory and ProxyHandlerFactory. I have added test case for Proxy invocation and updated maven.xml so jaxb schemas are generated at build time.

More functionality in Proxy to come.....",", Duplicated Code, Duplicated Code, "
"   Rename Method,Inline Method,","Adding OneWay and Asynchronous Callback functionality for Dynamic Proxy This is a new feature for Dynamic Proxy. 
I have now extended the proxy Doc/Lit wrapped support to make Async method calls. The BaseProxyHandler now looks at the  signature and return type of the method to determine if its a Asynchronous call. It also check for method name to end with Async if the return type is Future or Response.  
I have also modified proxy to implemented new features in InvocationController to get JAXB Block directly rather than reading MessageAsOM.

Next I will be adding Holder functionality to Proxy.",", , "
"   Rename Method,","Adding Simple Content Restriction to ADB Here I submitted the patch regarding the implementation of simple content restriction.

Any comments are appreciated ...",", "
"   Move Class,Move And Rename Class,Move Method,Move Attribute,",Move IBM contrib code for War to Wab conversion into trunk to provide a (default) BundleConverter for War files ,", , , "
"   Move Method,Extract Method,",Update blueprint integration tests to pax exam 3.4.0 ,"Duplicated Code, Long Method, , , "
"   Rename Method,Pull Up Method,Extract Method,",Update jmx integration tests to pax exam 3.4.0 ,"Duplicated Code, Long Method, , Duplicated Code, "
"   Rename Method,Extract Method,",Update web integration tests to pax exam 3 ,"Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,Inline Method,",Upgrade subsystem tests to pax exam 3 ,"Duplicated Code, Long Method, , , "
"   Pull Up Method,Extract Method,",Upgrade jpa itests to pax exam 3 ,"Duplicated Code, Long Method, , Duplicated Code, "
"   Pull Up Method,Move Method,Extract Method,Inline Method,Move Attribute,",Switch samples to pax exam 3 ,"Duplicated Code, Long Method, , , , , Duplicated Code, "
"   Extract Method,Inline Method,",Switch samples to pax exam 3 ,"Duplicated Code, Long Method, , , "
"   Rename Class,Move Method,Extract Method,",JAAS and JEE annotation based authorization ,"Duplicated Code, Long Method, , , "
"   Move Method,Move Attribute,",Support jpa 2.0 and 2.1 with the same code base ,", , , "
"   Move Method,Extract Method,Move Attribute,",Support jpa 2.0 and 2.1 with the same code base ,"Duplicated Code, Long Method, , , , "
"   Rename Method,Extract Method,",Support custom content in subsystems/.esa files ,"Duplicated Code, Long Method, , "
"   Extract Method,Move Attribute,",Add monitoring capability via MBean to Aries Transaction JDBC ,"Duplicated Code, Long Method, , , "
"   Rename Method,Move Method,Extract Method,",Implement the JNDI url scheme according to the OSGi Enterprise Specification ,"Duplicated Code, Long Method, , , "
"   Rename Method,",Implement the JNDI url scheme according to the OSGi Enterprise Specification ,", "
"   Extract Superclass,Extract Method,",Improve blueprint-maven-plugin: inherited annotations and qualifiers ,"Duplicated Code, Long Method, , Duplicated Code, Large Class, "
"   Rename Method,Extract Method,Move Attribute,",Support JPA2 features ,"Duplicated Code, Long Method, , , "
"   Extract Superclass,Rename Method,Move Method,Extract Method,",Also scan parent classes for jpa annotations ,"Duplicated Code, Long Method, , , Duplicated Code, Large Class, "
"   Rename Class,Rename Method,Move Method,",Support multiple EntityManager injections per class ,", , "
"   Pull Up Method,Extract Method,Inline Method,","""Make equals and hashCode comparisons within the header ","Duplicated Code, Long Method, , , Duplicated Code, "
"   Rename Method,",Compute service requirements and capabilities once in BundleRevisionResource. ,", "
"   Rename Class,Move Method,",Refactor the blog sample to be able to swap the persistence layer and add a comment service ,", , "
"   Rename Class,Rename Method,Move Method,Inline Method,",Refactor the blog sample to be able to swap the persistence layer and add a comment service ,", , , "
"   Rename Method,",Maven plugin no longer includes non-bundle artifacts ,", "
"   Rename Method,Inline Method,",AriesApplicationResolver backed by OBR ,", , "
"   Rename Method,",Separate the blueprint integration from the container context management ,", "
"   Move Class,Rename Class,Move Attribute,",Separate the blueprint integration from the container context management ,", , "
"   Move And Rename Class,",Separate the blueprint integration from the container context management ,", "
"   Rename Method,",Update FrameworkMBean API method names ,", "
"   Move And Rename Class,Extract Method,",Various application API improvements ,"Duplicated Code, Long Method, , "
"   Move Class,Move Method,",Blog - ensure that java package names begin with the artifactId of the bundle ,", , "
"   Rename Class,Move Method,",BundleTrackerCustomizers will not recurse on bundles added to a CompositeBundle before the composite bundle is started ,", , "
"   Rename Method,",Implement Framework MBean ,", "
"   Rename Method,",Implement User Admin MBean ,", "
"   Rename Method,",Update tranaction strategy to transaction attribute in blueprint transaction project ,", "
"   Move Method,Inline Method,","""Make jndi proxy creation more flexible and ",", , , "
"   Rename Method,","""Make jndi proxy creation more flexible and ",", "
"   Move Class,Move And Rename Class,Extract Interface,Rename Method,Extract Method,",Provisioning changes required to support application isolation ,"Duplicated Code, Long Method, , Large Class, "
"   Rename Method,",Provisioning changes required to support application isolation ,", "
"   Move Method,Inline Method,",Provisioning changes required to support application isolation ,", , , "
"   Move Method,Move Attribute,",Provisioning changes required to support application isolation ,", , , "
"   Move Class,Extract Method,",Provisioning changes required to support application isolation ,"Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,Extract Method,",JPA Quiesce Participant support ,"Duplicated Code, Long Method, , "
"   Move Class,Rename Method,",JPA Quiesce Participant support ,", "
"   Rename Method,",Move Subsystems to use org.apache.felix.bundlerepository instead of org.osgi.service.obr ,", "
"   Rename Method,",Injecting an entity manager using factory method ,", "
"   Move Method,Extract Method,",Improve the behaviour of OSGi in keeping with the OSGi way ,"Duplicated Code, Long Method, , , "
"   Move Class,Move Method,Extract Method,Move Attribute,","""Create common Proxy creation service to share proxying between blueprint ","Duplicated Code, Long Method, , , , "
"   Rename Class,Move Method,",Support for multiple namespace handlers for the same schema and use a compatible one wrt class loaders ,", , "
"   Rename Method,",blueprint:comp/componentName jndi namespace handler ,", "
"   Rename Method,",blueprint:comp/componentName jndi namespace handler ,", "
"   Move Method,Inline Method,",Improved parsing of Application metadata. ,", , , "
"   Move Class,Inline Method,",Expose ModelledBundleResource ,", , "
"   Rename Method,",Provide hook point for different Blueprint transaction interceptor similar to JPA hook point for persistence units ,", "
"   Rename Method,Extract Method,",Add a test to ensure we process fragments correctly ,"Duplicated Code, Long Method, , "
"   Rename Method,",Improve proxy support for final classes and final methods ,", "
"   Rename Method,Pull Up Method,Move Method,Extract Method,",Improve proxy support for final classes and final methods ,"Duplicated Code, Long Method, , , Duplicated Code, "
"   Rename Method,",Have blueprint extender process bundles associated with composite bundle when detecting the CompositeBundleFactory service ,", "
"   Extract Interface,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,",Replace Scope Admin with Region Digraph. ,"Duplicated Code, Long Method, , , , , Large Class, "
"   Extract Method,Move Attribute,",Improvements to IFile API ,"Duplicated Code, Long Method, , , "
"   Move Class,Move Method,Extract Method,Move Attribute,",Proxy code memory usage is high for generated interface proxys ,"Duplicated Code, Long Method, , , , "
"   Move Class,Extract Interface,",Make Aries bundle modelling API consumable by non-OSGi clients ,", Large Class, "
"   Move And Rename Class,Rename Method,Move Method,Move Attribute,",Make Aries bundle modelling API consumable by non-OSGi clients ,", , , "
"   Move Class,Pull Up Method,Pull Up Attribute,",Make Aries bundle modelling API consumable by non-OSGi clients ,", Duplicated Code, Duplicated Code, "
"   Move Class,Move Method,Move Attribute,",Add tests for EclipseLink - possibly include fragment for EclipseLink support ,", , , "
"   Rename Class,Rename Method,Move Method,Extract Method,Move Attribute,",Dry up itests ,"Duplicated Code, Long Method, , , , "
"   Rename Method,Extract Method,",Support using a ScheduledExecutorService for blueprint that is in the service registry ,"Duplicated Code, Long Method, , "
"   Rename Method,",Allow plugins to extend the Aries Application Modeller ,", "
"   Extract Superclass,Move Method,Extract Method,Move Attribute,",EJB support in Apache Aries ,"Duplicated Code, Long Method, , , , Duplicated Code, Large Class, "
"   Move Method,Move Attribute,",EJB support in Apache Aries ,", , , "
"   Rename Method,",EJB support in Apache Aries ,", "
"   Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,",Provide initial support for embedded subsystems. ,"Duplicated Code, Long Method, , , , , "
"   Move Method,Move Attribute,",Implement extender for detecting persitence units. ,", , , "
"   Move Class,Rename Class,Extract Interface,",Provide persistence unit metadata parser ,", Large Class, "
"   Move Class,Move Method,Move Attribute,",Provide persistence unit metadata parser ,", , , "
"   Move Method,Move Attribute,",Upgrade to asm 4 for java 7 support ,", , , "
"   Move And Rename Class,Pull Up Method,Move Method,Extract Method,Move Attribute,",Allow mixtures of interfaces and classes in proxys ,"Duplicated Code, Long Method, , , , Duplicated Code, "
"   Pull Up Method,Extract Method,","""Update subsystems to latest Subsystem ","Duplicated Code, Long Method, , Duplicated Code, "
"   Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","""Update subsystems to latest Subsystem ","Duplicated Code, Long Method, , , , , "
"   Move Class,Move And Rename Class,Move Method,Extract Method,Move Attribute,","""Update subsystems to latest Subsystem ","Duplicated Code, Long Method, , , , "
"   Rename Method,Extract Method,","""Update subsystems to latest Subsystem ","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","""Update subsystems to latest Subsystem ","Duplicated Code, Long Method, , "
"   Extract Method,Inline Method,","""Update subsystems to latest Subsystem ","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","""Update subsystems to latest Subsystem ","Duplicated Code, Long Method, , "
"   Rename Method,","""Update subsystems to latest Subsystem ",", "
"   Rename Method,Pull Up Method,Move Method,Extract Method,Inline Method,Move Attribute,","""Update subsystems to latest Subsystem ","Duplicated Code, Long Method, , , , , Duplicated Code, "
"   Move Method,Extract Method,Inline Method,","""Update subsystems to latest Subsystem ","Duplicated Code, Long Method, , , , "
"  Move Class,Extract Method,","""Update subsystems to latest Subsystem ","Duplicated Code, Long Method, , "
"   Rename Method,Inline Method,","""Update subsystems to latest Subsystem ",", , "
"   Rename Method,Extract Method,","""Update subsystems to latest Subsystem ","Duplicated Code, Long Method, , "
"   Extract Method,Inline Method,Move Attribute,","""Update subsystems to latest Subsystem ","Duplicated Code, Long Method, , , , "
"   Extract Interface,Rename Method,Extract Method,Inline Method,","""Update subsystems to latest Subsystem ","Duplicated Code, Long Method, , , Large Class, "
"   Rename Method,Move Attribute,","""Update subsystems to latest Subsystem ",", , "
"   Rename Method,",Modify default of Export-EJB: header to be ALL when empty string ,", "
"   Rename Class,Rename Method,Push Down Method,","""Refactor to make proxying code common ",", , "
"   Rename Method,",Implement application support ,", "
"   Rename Method,Extract Method,",Implement application support ,"Duplicated Code, Long Method, , "
"   Rename Method,",Implement application support ,", "
"   Move Class,Move Method,",Implement application support ,", , "
"   Extract Method,Inline Method,",Add support for push sort in Update part of DATAMONGO-832,"Duplicated Code, Long Method, , , "
"   Rename Method,","Add DbObject to GeoJson Converter The Spring Data MongoDB project provides some Geo Converters and especially a Geo Json To Db Object Converter. But Db Object To Geo Json Converter is not provided. 

I have write a custom Spring Converter to manage this feature but i think it could be useful if Spring Data MongoDB provide it. Maybe it could be also a good idea to move the class in the Spring Data Commons project. Other Spring projects could use these types.

Thank you.
",", "
"   Rename Method,","Add DbObject to GeoJson Converter The Spring Data MongoDB project provides some Geo Converters and especially a Geo Json To Db Object Converter. But Db Object To Geo Json Converter is not provided. 

I have write a custom Spring Converter to manage this feature but i think it could be useful if Spring Data MongoDB provide it. Maybe it could be also a good idea to move the class in the Spring Data Commons project. Other Spring projects could use these types.

Thank you.
",", "
"   Extract Method,Move Attribute,",Support partial filter expressions for indexing introduced in MongoDB 3.2 MongoDb 3.2 introduced an new index option which allows to index only documents where a given expression matches.See It would be nice to have the {{@Indexed}} extended to this.,"Duplicated Code, Long Method, , , "
"   Rename Method,",Support partial filter expressions for indexing introduced in MongoDB 3.2 MongoDb 3.2 introduced an new index option which allows to index only documents where a given expression matches.See It would be nice to have the {{@Indexed}} extended to this.,", "
"   Rename Method,",Support partial filter expressions for indexing introduced in MongoDB 3.2 MongoDb 3.2 introduced an new index option which allows to index only documents where a given expression matches.See It would be nice to have the {{@Indexed}} extended to this.,", "
"   Extract Method,Move Attribute,",Support partial filter expressions for indexing introduced in MongoDB 3.2 MongoDb 3.2 introduced an new index option which allows to index only documents where a given expression matches.See It would be nice to have the {{@Indexed}} extended to this.,"Duplicated Code, Long Method, , , "
"   Push Down Method,Push Down Attribute,",Add support for filter to aggregation see,", , , "
"   Rename Method,Extract Method,",Add support for map to aggregation ,"Duplicated Code, Long Method, , "
"   Rename Method,","Add new MongoDB  aggregation operators. like index Of Array, reverse Array, reduce,",", "
"   Rename Method,",Add replace Root aggregation stage ,", "
"   Move Class,Extract Method,",Add $facet bucket and bucket Auto aggregation stages ,"Duplicated Code, Long Method, , "
"   Rename Method,Inline Method,",Add facet bucket and bucket Auto aggregation stages ,", , "
"   Rename Method,","Not able to set server Selection Time out on Mongo Client Options using Mongo Client Options Factory Bean Hello, I'm using spring-data-mongo XML namespace to configure my mongo client used in my application. I'd like to set the server Selection Timeout  attribute of the underlying Mongo Client Options. But there is no way to set this attribute using the xml namespace. So that we have to deal with the default value (30s) of the plain Mongo Client Options which is way too long in our use-case Could it be possible to add a {{server-selection-timeout}} attribute on XML client-options tag server Selection Timeout method on Mongo Client Options Factory Bean , so that this timeout can be set ?
Thanks
",", "
"   Rename Method,","Not able to set server Selection Timeout on Mongo Client Options using Mongo Client Options Factory Bean Hello, I'm using spring-data-mongo XML namespace to configure my mongo client used in my application. I'd like to set the server Selection Timeout  attribute of the underlying Mongo Client Options. But there is no way to set this attribute using the xml namespace. So that we have to deal with the default value (30s) of the plain Mongo Client Options which is way too long in our use-case Could it be possible to add a {{server-selection-timeout}} attribute on XML client-options tag server Selection Timeout method on Mongo Client Options Factory Bean , so that this timeout can be set ?
Thanks
",", "
"   Rename Method,",Allow usage of projection interfaces in Fluent Mongo Operations on read we currently only allow mapping Document back into DTO types. By using Projection Factory within Mongo Template can enable support for interfaces and dynamic projections.,", "
"   Rename Method,Extract Method,",Allow usage of projection interfaces in Fluent Mongo Operations on read we currently only allow mapping Document back into DTO types. By using Projection Factory within Mongo Template can enable support for interfaces and dynamic projections.,"Duplicated Code, Long Method, , "
"   Rename Method,Move Method,Extract Method,Inline Method,",Add support for aggregation operators date From String date From Parts and date To Parts ,"Duplicated Code, Long Method, , , , "
"   Rename Method,Move Method,Extract Method,Inline Method,",Add support for aggregation operators date From String date From Parts and date To Parts ,"Duplicated Code, Long Method, , , , "
"   Extract Method,Inline Method,","Exception when trying to instantiate an entity having a primitive constructor argument and no according document field If I have some data in mongo and I add one new primitive type parameter to the constructor of my java class. Spring data throws exception while reading the old data. In case of non primitive parameter, it passes the null value but in case of primitive type, it doesn't pass the default value of the type.","Duplicated Code, Long Method, , , "
"   Rename Method,","Username password authentication support for Mongo Log Appender java The SD Mongo DB Log Appender currently only supports unauthenticated DB links.
Let's change that.
",", "
"   Rename Method,","Refactor Entity Metadata access in Mongo Query Method Currently the Mongo Query Method uses an Entity Information Creator to create an Entity Information instance being used by the query execution engine top determine the collection to query. As the collection can be determined by the sole inspection of mapping metadata, the get Collection Name should rather be on an Entity Metadata extension which would result in a simpler lookup of the metadata and the Entity Information Creator API being obsolete entirely.",", "
"   Rename Method,",Polish Bean Definition Parsers to avoid warnings in STS ,", "
"   Rename Method,Extract Method,","Projections should follow mongodb conventions more precisely. At present a projection of with the SD MongoDB Aggregation Framework support defined by project Will be rendered as But, in order to comply with the mongo db conventions, it should rather be rendered like: Note that both projections produce the same result:

","Duplicated Code, Long Method, , "
"   Move Class,Extract Method,",Improve cycle detection for DbRef's ,"Duplicated Code, Long Method, , "
"   Rename Method,","Sort can not use the metamodel classes generated by Query DSL When working with QueryDSL I would like to be able to sort queries in a type-safe way, and not just when building queries. To be able to sort by the meta model classes generated when using Query DSL the method get Key For Path of class  must be improved in order to allow dealing with metadata names like  customer Ast Name, customer metadata creation Date and so on where metadata is an embedded document. These metadata names are generated by using the meta model classes as follows : meta model generated from sample project Support in sorting through embedded documents would also be welcomed, note that special care must be taken because creates intermediate useless classes that must be ignored when creating the real path of the desired field to be used.",", "
"   Rename Method,",Add support for min Distance to Near Query ,", "
"   Rename Class,Push Down Method,",Use geo Within instead of within for geo queries within has been deprecated in 2.4.,", , "
"   Extract Method,Inline Method,",Add support for push sort in Update part of DATAMONGO-832,"Duplicated Code, Long Method, , , "
"   Extract Superclass,Extract Interface,Extract Method,",Add exists method to QuerDsMongRepository which accepts a querdsl Predicate ,"Duplicated Code, Long Method, , Duplicated Code, Large Class, Large Class, "
"   Rename Method,Pull Up Method,","Add support for Java 8 Stream as return type in repositories Since we don't provide an abstraction to stream results of a MongoDB query, Clients have to implement this functionality via block fetching manually if they have to deal with really large results. We should provide a way to stream large results block-wise to a consumer.",", Duplicated Code, "
"   Rename Class,Rename Method,",Use org bson types instead of com mongo db. use org bson Document This means incomplete list for queries for queries for queries for Basic DB Object Collection insert for insert Collection replace for replacing update Collection update for updates Update Options for eg. specifying upsert,", "
"   Rename Class,Rename Method,",Use org bson types instead of com mongo db. use org bson Document This means incomplete list for queries for queries for queries for Basic DB Object Collection insert for insert Collection replace for replacing update Collection update for updates Update Options for eg. specifying upsert,", "
"   Rename Class,Rename Method,Extract Method,Inline Method,",Use org bson types instead of com mongo db. use org bson Document This means incomplete list for queries for queries for queries for Basic DB Object Collection insert for insert Collection replace for replacing update Collection update for updates Update Options for eg. specifying upsert,"Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Add support for Query By Example. provide means to allow using a partially filled domain object as pattern for the actual query List Person result repository find All By Example new Example Person sample code
Provide Example wrapper where one is able to define {{null}} value and String handling.
Offer find By Example as repository method
","Duplicated Code, Long Method, , "
"   Move Method,Extract Method,","Add support for Query By Example provide means to allow using a partially filled domain object as pattern for the actual query List Person result repository find All By Example new Example Person sample code
Provide Example wrapper where one is able to define {{null}} value and String handling.
Offer find By Example as repository method
","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,Inline Method,","Provide read lifecycle events when loading DBRefs By now lifecycle events are just fired when dealing with root documents (although, by now, when using Query DSL this support is missing, see  I think that it would be useful to trigger After Load and After Convert events when loading related documents, through 
I assume that one will use this kind of relation DBRef when the relation in itself derives from root entities documents As far as I have seen the change is minimal. I have provided a sample project on github with a custom Mapping Mongo Converter that fires these events. There are two junits, one that fails because of using the default {{Mapping Mongo Converter and one that uses my customized Mapping Mongo Converter The only relevant part in my Custom Mapping Mongo Converted starts here Just 3 methods, the rest is pure C&P I could make a PR if this is a desired feature


","Duplicated Code, Long Method, , , "
"   Rename Method,","Add an option to specify the cursor batch Size for repository methods returning streams It would be great if you provide an option to set the  In case of ETL where you process a lot of GB, streaming results is already heaven on earth compared to paging. In the Mongo DB Cursor default implementation is set to 0 which means the database chooses it. In my configuration the batch Size seems to be very small. I could observe that when I fetch data from a remote database. Java Mongo DB Driver Batch Size I couldn't verify that overriding the batch Size gives the expected performance boost.",", "
"   Rename Class,Rename Method,Move Method,Extract Method,","Add support for validator when creating collection. 

 NOTE investigate usage if JSR-303 for defining validation to be propagated to the collection.
","Duplicated Code, Long Method, , , "
"   Move Method,Extract Method,Move Attribute,",Add support for projections on repository query methods ,"Duplicated Code, Long Method, , , , "
"   Rename Method,",Reactive support in Spring Data Mongo DB Investigate on reactive paradigms support in Spring Data MongoDB using Reactive-Streams. Create a prototype that includes native support for the MongoDB Reactive-Streams driver.,", "
"   Rename Method,",Reactive support in Spring Data Mongo DB Investigate on reactive paradigms support in Spring Data MongoDB using Reactive-Streams. Create a prototype that includes native support for the MongoDB Reactive-Streams driver.,", "
"   Move And Rename Class,Move Method,Extract Method,Inline Method,",Reactive support in Spring Data Mongo DB Investigate on reactive paradigms support in Spring Data MongoDB using Reactive-Streams. Create a prototype that includes native support for the MongoDB Reactive-Streams driver.,"Duplicated Code, Long Method, , , , "
"   Rename Method,",Add Db Object to Geo Json Converter The Spring Data MongoDB project provides some Geo Converters and especially a Geo Json To Db Object Converter. But Db Object To Geo Json Converter is not provided. I have write a custom Spring Converter to manage this feature but i think it could be useful if Spring Data MongoDB provide it. Maybe it could be also a good idea to move the class in the Spring Data Commons project. Other Spring projects could use these types. Thank you.,", "
"   Rename Method,","Support partial filter expressions for indexing introduced in Mongo DB Mongo Db 3.2 introduced an new index option which allows to index only documents where a given expression matches. See It would be nice to have the Indexed extended to this.




",", "
"   Rename Method,","Add support for no Cursor Timeout in Query MongoDB provides the cursor option cursor no Cursor Timeout which allows a cursor to continue until exhausted or closed, beyond Mongo's default 10 minutes timeout option.
The option could be exposed to the Query and Meta classes and then added to the cursor's options in Query Cursor Preparer prepare
",", "
"   Push Down Method,Push Down Attribute,",Add support for filter to aggregation ,", , , "
"   Rename Class,Rename Method,",Add support for Collations ,", "
"   Rename Method,Extract Method,",Add support for map to aggregation ,"Duplicated Code, Long Method, , "
"   Rename Method,","Add new MongoDB 3.4 aggregation operators. like index Of Array, reverse Array, reduce,",", "
"   Rename Method,",Add replace Root aggregation stage ,", "
"   Rename Method,Inline Method,",Add facet bucket and bucketAuto aggregation stages ,", , "
"   Move Class,Extract Method,",Add facet bucket and bucket Auto aggregation stages ,"Duplicated Code, Long Method, , "
"   Rename Method,",Add Template Wrapper to reduce method overloads on Mongo Template. Use indirections to provide meaningful fluent API calls like,", "
"   Rename Method,","Not able to set serverSelectionTimeout on Mongo Client Options using Mongo Clien tOptions Factory Bean Hello, I'm using spring-data-mongo XML namespace to configure my mongo client used in my application. I'd like to set the server Selection Timeout  attribute of the underlying Mongo Client Options. But there is no way to set this attribute using the xml namespace. So that we have to deal with the default value (30s) of the plain Mongo Client Options which is way too long in our use-case Could it be possible to add a server selection timeout attribute on XML client-options tag server Selection Timeout method on Mongo Client Options Factory Bean , so that this timeout can be set ?
Thanks
",", "
"   Move Method,Extract Method,",Support reactive aggregation streaming Add support to stream aggregation results with Flux Reactive Mongo Operations aggregate,"Duplicated Code, Long Method, , , "
"   Rename Class,Extract Interface,","Add fluent alternative for Reactive Mongo Operations I am a big fan of the new fluent API provided as part of DATAMONGO-1563, and I think such API would be very useful on Reactive Mongo Operations as well. I can contribute the related Kotlin extensions That could allow me to demonstrate the new fluent API in application, since it would be perfect for such use case.",", Large Class, "
"   Move Class,Extract Superclass,Extract Interface,Extract Method,",Fix dependency cycles No cycles between packages Nothing but config should depend on repository,"Duplicated Code, Long Method, , Duplicated Code, Large Class, Large Class, "
"   Rename Method,","Move to fluent API for repository query execution Moving to the new fluent API on Mongo Operations we should be able to optimize a few things in the repository query execution Being able to define which type to read (for query mapping) and which type to produce should allow us to get rid of the additional step of DTO mapping after read The fluent API returning new instances for intermediate steps should allow us to set up the broad coordinates which type to read, e and keep those around rather than deciding about that very late in the execution lifecycle.",", "
"   Rename Method,","Move to fluent API for repository query execution Moving to the new fluent API on Mongo Operations we should be able to optimize a few things in the repository query execution Being able to define which type to read (for query mapping) and which type to produce should allow us to get rid of the additional step of DTO mapping after read The fluent API returning new instances for intermediate steps should allow us to set up the broad coordinates which type to read, e and keep those around rather than deciding about that very late in the execution lifecycle.",", "
"   Rename Method,",Query B yExample Find One probe type Mongo Example Mapper inspects the probe type and writes type restrictions according to known types in  Mapping Context. Types assignable to the probe get included in the  operator. My probe type is different than the documents in the mongo collection.,", "
"   Move Class,Rename Method,Move Method,Extract Method,Move Attribute,",Add support for Mongo DB change streams MongoDB 3.6 introduces a new feature called change stream that emits all changes to a particular collection Mongo Collection Create the change stream publisher Change Stream Publisher Get the resume token from the last document we saw in the previous change stream cursor Bson Document resume Token Change Stream Document get Resume Token Pass the resume token to the resume after function to continue the change stream cursor publisher Watch accepts an aggregation pipeline to filter or transform elements.,"Duplicated Code, Long Method, , , , "
"   Rename Method,Extract Method,","Allow document replacements via Mongo Collection fin One And Replace Mongo Collection has no find And Modify method, and it seems that find One And Update was used as a replacement. Find And Modify allows field value in the update, but find One And Update does not. This breaks the ability to do find And Modify with a replacement document.","Duplicated Code, Long Method, , "
"   Rename Method,","Allow document replacements via Mongo Collection findOne And Replace Mongo Collection has no find And Modify method, and it seems that find One And Update was used as a replacement. Find And Modify allows field value in the update, but find One And Update does not. This breaks the ability to do find And Modify with a replacement document.",", "
"   Rename Method,Move Method,Extract Method,Inline Method,",Add support for aggregation operators date From String date From Parts and date To Parts ,"Duplicated Code, Long Method, , , , "
"   Rename Class,Extract Method,","Migrate to Document API-based Querydsl implementation Querydsl support in Spring Data MongoDB uses the legacy DBObject API which is going EOL at some point in time. We should consider a move towards a Document API-based Querydsl-component for query creation/execution to not lose Querydsl capabilities once DBObject is no longer supported by the driver.

Moving to Document API allows some cleanups in our code to get rid of legacy API/bridge code.

At this time, Querydsl does not provide a Document-API based implementation.","Duplicated Code, Long Method, , "
"   Rename Method,",Add support for mapReduce to ReactiveMongoOperations ,", "
"   Rename Method,",Provide additional options for setting Write Concern on a per operation basis With default Write Concern  NORMAL - do not wait for server errors  update method will return lazy version of Write Result - i.e. we can get update result after another call to database.  Thus we can not do update result check in one call to database - performance problem If application works with Write Concern. NORMAL by default then most effective way to do update with result checking is to call it with Write Concern. SAFE,", "
"   Rename Class,Move Class,Rename Method,Move Method,Extract Method,Move Attribute,",Lazy Load for DbRef DbRef's appear to be loaded eagerly Would be nice if there was support for storing DbRef's on a document but being able to lazy (or manually) load them.,"Duplicated Code, Long Method, , , , "
"   Extract Method,Inline Method,","Exception when trying to instantiate an entity having a primitive constructor argument and no according document field If I have some data in mongo and I add one new primitive type parameter to the constructor of my java class. Spring data throws exception while reading the old data. In case of non primitive parameter, it passes the null value but in case of primitive type, it doesn't pass the default value of the type.","Duplicated Code, Long Method, , , "
"   Rename Method,",Support After and Before keywords for query creation ,", "
"   Rename Method,","Username password authentication support for Mongo Log Appender java The SD Mongo DB Log Appender currently only supports unauthenticated DB links 
Let's change that.
",", "
"   Rename Class,Move And Rename Class,Rename Method,",Support calling of Mongo DB stored javascripts Support calling of mongodb stored javascripts,", "
"   Rename Class,Rename Method,",Polish namspace implementation ,", "
"   Rename Method,Extract Method,",QueryMapper should correctly transform associations Currently the Query Mapper does not transform values into DBRef objects if the value actually represents an association.,"Duplicated Code, Long Method, , "
"   Rename Method,","Add background attribute to Indexed and Compound Index MongoDB accept since version 2.2, a background indexation. Please add this function to annotation Indexed and Compound Index.",", "
"   Rename Class,Inline Method,","Provide support for remove Bydelete By methods like for find By on repository interfaces On the repository interfaces I can define methods like If I want to have the same method for deleting, e.g. I have to implement a custom Repository for this. Please provide also automatic interface method support for  like you do already for ",", , "
"   Rename Method,","Refactor Entity Metadata access in Mongo Query Method Currently the Mongo Query Method uses an Entity Information Creator to create an Entity Information instance being used by the query execution engine to determine the collection to query. As the collection can be determined by the sole inspection of mapping metadata, the get Collection Name should rather be on an Entity Metadata extension which would result in a simpler lookup of the metadata and the Entity Information Creator API being obsolete entirely.",", "
"   Rename Method,",Polish Bean Definition Parsers to avoid warnings in STS ,", "
"   Rename Method,Move Method,Extract Method,",Add support for new Aggregation Framework Mongo 2.2 has some new aggregation framework features that we could add query criteria support for.,"Duplicated Code, Long Method, , , "
"   Rename Method,",Add support for new Aggregation Framework Mongo 2.2 has some new aggregation framework features that we could add query criteria support for.,", "
"   Rename Method,Move Method,Extract Method,",Add support for new Aggregation Framework Mongo 2.2 has some new aggregation framework features that we could add query criteria support for.,"Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,Inline Method,",Add support for new Aggregation Framework Mongo 2.2 has some new aggregation framework features that we could add query criteria support for.,"Duplicated Code, Long Method, , , "
"   Rename Class,Rename Method,Extract Method,",Add support for new Aggregation Framework Mongo 2.2 has some new aggregation framework features that we could add query criteria support for.,"Duplicated Code, Long Method, , "
"   Move Class,Move Method,Move Attribute,",Add support for new Aggregation Framework Mongo 2.2 has some new aggregation framework features that we could add query criteria support for.,", , , "
"   Rename Method,Extract Method,","Projections should follow mongodb conventions more precisely. At present a projection of with the SD MongoDB Aggregation Framework support defined by project Will be rendered as But, in order to comply with the mongo db conventions, it should rather be rendered like: Note that both projections produce the same result:","Duplicated Code, Long Method, , "
"   Rename Class,Move Method,Extract Method,",Support SpEL expressions to define arithmetical projection operations in the aggregation framework ,"Duplicated Code, Long Method, , , "
"   Rename Class,Move Method,",Support SpEL expressions to define arithmetical projection operations in the aggregation framework ,", , "
"   Move Class,Move Method,Move Attribute,",Adapt to changes in Spring Data Commons triggered by repository initialization changes ,", , , "
"   Move Class,Move And Rename Class,Rename Class,Rename Method,",Add support for common geospatial structures ,", "
"   Rename Method,Move Method,Extract Method,",Add support for common geospatial structures ,"Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Add support for cond and if Null operators in aggregation operation Add support for cond operator in group and project operations. 
Conditional operator is describe here: ","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,Extract Method,",Add new field naming strategy and make it configurable through XML Java config My company needs to insert records into Mongo from a Java app and read from a node.js app. In keeping with our existing data models in Mongo we'd like to keep field names in lower case with underscores snake case. I've added XML and Java config attributes for  Field Naming  Strategy and created the class Lower Case With Underscores Field Naming Strategy A pull request is forthcoming via GitHub. I'll be happy to make any changes you see fit.,"Duplicated Code, Long Method, , "
"   Move Class,Rename Method,",Add support for the bulk operations introduced in MongoDB 2.6. Latest version of spring data mongo db have no support for the bulk operations introduced in MongoDB 2.6 and mongo db recommends using the new write protocols and new bulk api for bulk operations.,", "
"   Move Class,Move Method,Extract Method,Inline Method,Move Attribute,",Add support for deriving full text queries. It is possible to combine full text search and other Criteria Definition in a Query. It should be possible to indentify full text parameters and create the accoring query.,"Duplicated Code, Long Method, , , , , "
"   Rename Method,Extract Method,",Add support for date time operators in aggregation framework Mongo DB Aggregation framework supports a range for date time operators.,"Duplicated Code, Long Method, , "
"   Extract Method,Move Attribute,","Add support for reading meta projection on textScore into document. Using text search allows to add {{$meta}} projections eg. for text Score These values must not be written when storing the document but shall be mapped when reading.


","Duplicated Code, Long Method, , , "
"   Pull Up Method,Inline Method,",Add support for SpEL expressions in Query We should adapt the SpEL support that we have in Spring Data JPA in Spring Data MongoDB as well.,", , Duplicated Code, "
"   Rename Method,",Add support for SpEL expressions in Query We should adapt the SpEL support that we have in Spring Data JPA in Spring Data MongoDB as well.,", "
"   Rename Method,","Enable directory scanning with java 7 feature - WatchService  I would like to offer my implementation to DefaultDirectoryScanner.

I extended this class, 
In ""listEligibleFiles"" I'm listening to os events with java 7 feature ""WatchService"" . In that way 
1.I can be sure that I'm handling file only this time.(there will be only one ""create event"" for each file.
2.A very reliable way to pick files from directories.
3.For existing files in directory, I implemented init-method that scan all directories with ""Files.walkFileTree"" (also a java 7 feature).
the first time i call ""listEligibleFiles"" I'm handling the existing files. From Next time on - I will treat only ""create events"" that came from WatchService.
4.In same init-method Iregistered all subdirs of root directory ( that has been supplied through ""directory"" property) with java 7 feature ""Files.walkFileTree"".
",", "
"   Rename Method,",Add Ability to Customize the SpEL Evaluation Contexts Used Throughout the Framework  For example custom {{PropertyAccessors}}. ,", "
"   Rename Method,","JMS: DSL/XML Inconsistent receiveTimeout  The XML parser sets the default to no-wait; the DSL leaves it at the default (infinite wait).

They should be consistent, but I don't think no-wait is correct for the reasons discussed at the end of the SO answer. Infinite wait is incorrect too. Maybe 1000?

On second thought - perhaps no-wait (-1) if we have a {{CachingConnectionFactory}}, with {{cacheConsumers}} otherwise 1000?
",", "
"   Rename Method,","filter defined in scanner gets overwritten by the one defined in FileReadingMessageSource  As described here http://stackoverflow.com/questions/28087753/cant-set-a-different-filter-in-fileinbound-channel-adapter-and-scanner , and confirmed by Artem, we can't define 2 different {{filters}} in a {{scanner}} and a {{<inbound-channel-adapter>}} .

when you look at the documentation, its misleading because you have the feeling you use 2 independent components, while actually {{FileReadingMessageSource}} overwrites the scanner's filter config. 
",", "
"   Rename Method,Push Down Method,","Add Buffer Pool to AbstractByteArraySerializer  The IP (de)serializers are used in XD in modules such as the shell processor.

The byte array is created and discarded for each message.

Add pooling support. 
",", , "
"   Move Method,Extract Method,","FileTransferringMessageHandler - Add support for deleting source files after succesful transfer  It would be nice if this handler can delete source files after they are written successfully to a remote file adapter (SFTP, FTP, etc).

The file outbound adapter and transformers already supports deleting source files.

I have created a customized FileTransferringMessageHandler to handle this case.
If you are interested in the changes I have made to the original class please let me know. 
","Duplicated Code, Long Method, , , "
"   Rename Method,Pull Up Method,Move Method,Extract Method,Pull Up Attribute,","Adding support for SecurityContext Propagation  There are a lot of cases when it is necessary to use the SecurityContext in async mesage flow inside Web application. 
The principal is appeared in HttpRequest from springSecurityFilterChain and stored in the ThreadLocal holder.
So, at first, I propose to add supoprt of some global ChannelInterceptor which can propagetes that securityContext to async mesasge flow.
This is my solution. Sorry for Groovy code.
As well as always in any place of his application. 
","Duplicated Code, Long Method, , , Duplicated Code, Duplicated Code, "
"   Rename Method,",Expose Message ID generation strategy  ,", "
"   Rename Method,","Add MetadataValueStrategy to allow to determine the value for MetadataStore, not only key  ",", "
"   Rename Class,Move And Rename Class,Rename Method,Move Method,","update Redis collection updating class names to be consistent with the XML element naming  The element is named ""store-outbound-channel-adapter"", but the classnames are RedisCollection*. Those should be consistent. ",", , "
"   Move And Rename Class,Rename Method,Inline Method,","Improve <event:inbound-channel-adapter> perfomance  According to comments from linked PR, there is some overhead in the  when it accepts all  and then filters them by provided types. ",", , "
"   Rename Method,Inline Method,",Provide annotation configuration for  ,", , "
"   Rename Method,","Add support for @CompileStatic (and for other compiler options) into Groovy Script components  The is supported already now as a method-level hint.
Instead of:
To make it a bit easier and less verbose we could utilize the  as an out-of-the-box feature for Groovy script components to attempt to compile them statically.

See more information in the Blog Post: 
With that our scripts can still be as a one line but based on the Java runtime, not Groovy already:
",", "
"   Rename Method,Push Down Method,",Support for Reactive WebClient  Currently only support and thus blocking IO. Suggest to add support for and thus unblocking IO (like via using when constructing for scalable IO when sending HTTP requests to many destinations (like 100). ,", , "
"   Rename Method,","The Jackson2JsonObjectMapper should be consistent with similar from Spring Core around ObjectMapper configuration  The JavaDocs from the As well as the code from the So, the should configure its default the same way. ",", "
"   Rename Method,Extract Method,",ExpressionEvaluatingAdvice Should Send an AdviceMessage (or ErrorMessage)  ,"Duplicated Code, Long Method, , "
"   Move Method,Extract Method,Inline Method,","Merge Messaging Meta-Annotation Attributes  When using messaging meta-annotations (e.g. annotated with the framework currently only considers attrbutes on the framework annotation.
To provide increased flexibility, we should consider that attributes with the same name on the user annotation should supplement/override those on the framework annotation.
This would allow the user annotation to specify common attributes with each individual use of that annotation supply attributes such as 
","Duplicated Code, Long Method, , , , "
"   Rename Method,Extract Method,","Support Timeout in AMQP PollableChannel  Since the now supports a timeout on receive, we can refactor the pollable channel:","Duplicated Code, Long Method, , "
"   Extract Interface,Move Method,Extract Method,",Rotate (S)FTP Servers/Directories During Polls  Add a smart poller advice to rotate across multiple directories/servers with several policies - standard - rotate only when no file found; fair - rotate on every poll; custom. ,"Duplicated Code, Long Method, , , Large Class, "
"   Pull Up Method,Extract Method,",Make ErrorMessageSendingRecoverer more generic and introduce ErrorMessageStrategy for customization  ,"Duplicated Code, Long Method, , Duplicated Code, "
"   Rename Method,Extract Method,",Add support for 'Lazy-Load' for 'MessageGroup.messages' from Persistent MessageStore  The proposed implementation: ,"Duplicated Code, Long Method, , "
"   Move Class,Rename Method,Move Method,Move Attribute,","FTP Direct-to-Contents Message Source  SCSt (S)FTP source has a option to read the file and load the contents into a or emit individual lines if the contents are text.

In fact, these are really the only viable option on CF since leaves the payload as a file reference.

Add a 

Also consider the line-based splitter option. 
",", , , "
"   Rename Method,",Add support for MessageGroupStore to Redis module  ,", "
"   Rename Method,Pull Up Method,Extract Method,Pull Up Attribute,",Add SpEL support to ContinuousQueryMessageProducer Add the ability to set a payload expression. Similar to what is already in CacheListeningMessageProducer,"Duplicated Code, Long Method, , Duplicated Code, Duplicated Code, "
"   Move Class,Extract Superclass,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,",Implement MongoDB PriorityCapableChannelMessageStore  ,"Duplicated Code, Long Method, , , , Duplicated Code, Large Class, , "
"   Rename Method,Extract Method,","IntegrationEvaluationContextAware.setIntegrationEvaluationContext() invoked after afterPropertiesSet()  Beans implementing `IntegrationEvaluationContextAware` will expect that the `IntegrationEvaluationContext` is set as part of the bean initialization process. Due to the current approach of setting the context only in `IntegrationEvaluationContextAwareBeanPostProcessor#afterSingletonsInstantiated`, `afterPropertiesSet()` is called before the initialization takes place.
Reconsider the current approach, to reconcile the need for preventing early `BeanFactory` access with the expectations of `InitializingBean` 
","Duplicated Code, Long Method, , "
"   Rename Method,",DSL: Consider Making a Final .log Terminal If a flow ends with perhaps it could be just a logging adapter rather than a wireTap (or automatically set the output channel to,", "
"   Rename Class,Extract Method,",Expose Message ID generation strategy  ,"Duplicated Code, Long Method, , "
"   Rename Method,Move Method,Extract Method,",Expose serializers for Redis Store Inbound adapters We kind of did it for outbound but the inbound stil needs to be done,"Duplicated Code, Long Method, , , "
"   Rename Class,Extract Superclass,",FTP outbound adapter APPEND command support It would be nice to be able to APPEND to existing remote files.,", Duplicated Code, Large Class, "
"   Rename Method,","In org.springframework.integration.ws.MarshallingWebServiceOutboundGateway the field 'uri' doesn't work the same way the 'deafultUri' works in org.springframework.ws.client.core.WebServiceTemplate  In WebServiceTemplate the field 'defaultUri' can understand URI for both HTTP and But in 
the field 'uri' only understands HTTP URI. If one needs to use JMS URI then he needs to implement his own But this is not properly documented. It would be great if we could make 'uri' field understands JMS (or any other) URIs update the documentation so that users can easily find out that they need to implement DestinationProvider 
",", "
"   Pull Up Method,Extract Method,Move Attribute,","Use ChannelResolvers Instead of BeanFactory  To support the Java DSL, several components were set up to allow late binding of channels by name. This code accesses the directly instead of using a 
This precludes the configuration of an alternative channel resolver.
For example, in XD, we might want to configure a to route discarded messages to a named channel In order to do that, XD could configure a into any bean found during module deployment, in the same way that it does for s.
The same thing applies to and possibly other components; however, we should generalize the use of a channel resolver everywhere that late channel binding was added.

We could even consider back-porting this to 4.1.x given that it's a low risk change. 
","Duplicated Code, Long Method, , , Duplicated Code, "
"   Rename Method,","JVM NPE in Selector.close()  Under some circumstance (perhaps an unregistered selector), the JVM throws an NPE when closing a selctor.We currently catch an Check the selector is registered, and change the catch to ",", "
"   Move Class,Rename Class,Rename Method,Move Method,Pull Up Attribute,Move Attribute,",Extend groovy:script tag to support a ScriptVariableGenerator strategy  See the forum topic for discussion and patch ,", , , Duplicated Code, "
"   Pull Up Method,Move Method,Extract Method,Move Attribute,","Coordinate JSON Transformers with AMQP JsonMessageConverter  The AMQP adaptes can use a JsonMessageConverter, but that converter creates/requires type information in the MessageProperties.
Add functionality to the adapters to make these MessageProperties first class amqp_ headers.
Add functionality to the json transformers to create/understand the type headers used/created by the AMQP JsonMessageConverters. 
","Duplicated Code, Long Method, , , , Duplicated Code, "
"   Rename Method,","Implement ConcurrentMetadataStore  Extend as by adding the and methods.
Change the to implement 
Change the to use If returns an existing object (file timestamp) check the timestamp against the remote file timestamp. If they match, ignore this file. If they don't match, use the method update the key if the value hasn't changed again.
This will enable (S)FTP inbound adapters to work in a clustered environment such that only one of the instances will process a file. 
",", "
"   Rename Method,",ChannelInterceptorAware - Support Removal of an Individual Interceptor  See XD Perhaps 2 methods:,", "
"   Rename Method,","Make user-defined prefix configurable in DefaultHttpHeaderMapper  When setting custom headers http:outbound-gateway is appending ""X-"" for all the headers before making the call. We should have a boolean variable which enables/disables this behaviour. ",", "
"   Rename Method,","Extend groovy:script tag to support a ScriptVariableGenerator strategy  When setting custom headers http:outbound-gateway is appending ""X-"" for all the headers before making the call. We should have a boolean variable which enables/disables this behaviour. ",", "
"   Rename Method,","Extend groovy:script tag to support a ScriptVariableGenerator strategy  When setting custom headers http:outbound-gateway is appending ""X-"" for all the headers before making the call. We should have a boolean variable which enables/disables this behaviour. ",", "
"   Rename Class,Rename Method,","Extend groovy:script tag to support a ScriptVariableGenerator strategy  When setting custom headers http:outbound-gateway is appending ""X-"" for all the headers before making the call. We should have a boolean variable which enables/disables this behaviour. ",", "
"   Rename Method,","Extend groovy:script tag to support a ScriptVariableGenerator strategy  When setting custom headers http:outbound-gateway is appending ""X-"" for all the headers before making the call. We should have a boolean variable which enables/disables this behaviour. ",", "
"   Rename Class,Move Method,Move Attribute,","Extend groovy:script tag to support a ScriptVariableGenerator strategy  When setting custom headers http:outbound-gateway is appending ""X-"" for all the headers before making the call. We should have a boolean variable which enables/disables this behaviour. ",", , , "
"   Rename Method,","Extend groovy:script tag to support a ScriptVariableGenerator strategy  When setting custom headers http:outbound-gateway is appending ""X-"" for all the headers before making the call. We should have a boolean variable which enables/disables this behaviour. ",", "
"   Rename Method,","Extend groovy:script tag to support a ScriptVariableGenerator strategy  When setting custom headers http:outbound-gateway is appending ""X-"" for all the headers before making the call. We should have a boolean variable which enables/disables this behaviour. ",", "
"   Rename Method,","JmsMessageDrivenEndpoint contains method getComponetType() rather than getComponentType()  The  class contains method a method named I believe that methodname contains a typo and should instead be (note the additional ""n"" before the final",", "
"   Rename Method,",TCP: Provide Option to Separate Connection Establishment from Message Flow  Summary was: Provide options to configure TCP client to receive data only (without sending any data to server) ,", "
"   Rename Method,",DefaultConfiguringBeanFactoryPostProcessor should not create default beans if they are provided by a parent application context  This would make it easier to provide resources such as a shared thread pool or an error channel in a parent context. Currently this can be achieved by declaring an alias in the child context causing to report true. ,", "
"   Rename Method,",DefaultConfiguringBeanFactoryPostProcessor should not create default beans if they are provided by a parent application context  This would make it easier to provide resources such as a shared thread pool or an error channel in a parent context. Currently this can be achieved by declaring an alias in the child context causing to report true. ,", "
"   Rename Class,Extract Method,","IMAP inbound channel adapters could allow move to folders for processed messages  Looking at some of my own requirements around IMAP, this is something that I'm intending to implement and contribute as an extension to the existing IMAP channel adapter.
Currently, for processed messages, we can either delete them or mark them as read.
What I'd like to do (and I suspect is a use case for many mail scenarios) is to file successfully processed messages in a specified folder, and a different folder for messages that were attempted to be processed and failed This would work for scenarios where validation or transformation occur before any queues are involved.
A luxury would be to provide a strategy interface for determining the destination folder based on the thrown exception. 
","Duplicated Code, Long Method, , "
"   Rename Method,",Expose Message ID generation strategy  ,", "
"   Rename Method,",Expose Message ID generation strategy  ,", "
"   Rename Method,",Expose Message ID generation strategy  ,", "
"   Rename Method,",Expose Message ID generation strategy  ,", "
"   Move And Rename Class,",Expose Message ID generation strategy  ,", "
"   Rename Method,",Expose Message ID generation strategy  ,", "
"   Extract Method,Inline Method,",Add wildcards support for header-filter  The header filter component can only be configured by providing an array of String Wildcards are currently not allowed in ,"Duplicated Code, Long Method, , , "
"   Pull Up Method,Extract Method,Pull Up Attribute,","HttpRequestExecutingMessageHandler and Content-Type headers  populates the Content-Type header on all HTTP requests, even when it is not applicable (e.g., GET). As Content-Type is meant to identify the message body for POST and PUT requests, it does not make sense to populate it for other HTTP methods (GET, HEAD, DELETE, TRACE, optional for OPTIONS). ","Duplicated Code, Long Method, , Duplicated Code, Duplicated Code, "
"   Rename Class,Rename Method,",Add support for MessageGroupStore to Redis module  ,", "
"   Rename Method,Move Method,Extract Method,",Add support for MessageGroupStore to Redis module  ,"Duplicated Code, Long Method, , , "
"   Rename Method,",Add namespace support for Gemfire Inbound/Outbound Channel Adapters  ,", "
"   Rename Method,Pull Up Method,Extract Method,",Clean Up IP Module Sonar Code Rule Violations  ,"Duplicated Code, Long Method, , Duplicated Code, "
"   Move Class,Move And Rename Class,",Add namespace support for Continuous Query Inbound Channel Adapter  ,", "
"   Rename Class,Rename Method,Extract Method,","Provide a way to Catch/Handle TCP Adapter SocketTimeoutException  I have been using tcp-connection-factory and tcp-inbound-channel-adapter to listen data arriving on a socket port. I have configured a so-timeout attribute for the tcp-connection-factory.
This times out the socket after the timeout interval and logs the following message in DEBUG message which looks something like this.
DEBUG [ - Read exception SocketTimeoutException:null:Read timed out
I am not able to get hold of handle where I can catch this exception and perform my application logic
Please provide a way to Catch and handle this exception in the TCPAdapter via an event or any other way.
","Duplicated Code, Long Method, , "
"   Extract Superclass,Rename Method,",Add python scripting support  Add a PythonScriptExecutor that provides special handling required for Pyhon to work with the scripting framework ,", Duplicated Code, Large Class, "
"   Rename Method,Extract Method,",MessageGroupQueue loads all Messages from the MessageStore into the memory upon the first call to poll()  ,"Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Add an ability to supply procedure name from Message headers and/or using SpEl  Would it be possible to supply a stored procedure name into one of the stored procedure adapters or into a stored procedure gateway dynamically instead of pre-defining it in the configuration?
For example, would it be possible to supply it as one of the message headers or extract it from the message using SpEl?
I need to be able to run different stored procedures that have exactly the same signature and have an integer as a return value.
Thanks. 
","Duplicated Code, Long Method, , "
"   Move Method,Extract Method,",Add support for full RESTFul request-mappings to the HTTP inbound adapters  For more info see Spring-MVC Reference Manual: ,"Duplicated Code, Long Method, , , "
"   Rename Method,","Add ""container-class"" option into message-driven-channel-adapter container definition (jms module)  ",", "
"   Rename Method,","Tcp Client ConnectionFactory Failover  Provide a mechanism, say a comma delimited list of to allow a client tcp-connection-factory to fail over to alternative hosts. If the option is missing, use the port from the port attribute. ",", "
"   Extract Method,Inline Method,","Allow message ids to be set when using a custom deserializer  
I have created a mailing Gateway for my application which is configured as follows:
This was working fine when the transformer was creating a which is Serializable. The were able to serialize and then deserialize the entire 
However, in order to send HTML e-mail (a common requirement) I need to replace with This is not serializable. The answer is to replace the (de)serializer with a custom implementation.
This custom implementation will populate a Serializable object with the properties we need from the  along with the This can then be serialized as before. When we deserialize we need to create a new set the payload to be a new and restore the headers.
This currently doesn't work because when we create a new specifying the a new id gets generated. This means that the existing message in the database doesn't get removed after being picked up by the poller and the outbound mail adaptor keeps sending it.
The fix must be to allow the id to be specified if we need to. The code says:
This suggests that we can set it when we do a complete deserialization of the message and headers together (as done by the therefore it seems reasonable that if custom deserializers can be supplied they should also be able to restore the id.
I have cloned the Github repo and made the following code change to 
However this causes a test failure in so clearly it is considered important that the id cannot be set. As I said in my last reply on the forum that functionality seems to be at odds with allowing flexible custom (de)serializers. 
","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Additional Orderly Shutdown Behavior  implemented orderly shutdown of many components. There are three (known) remaining issues...
1. Tcp Adapters are stopped hard - sockets closed. This means that any in-flight TCP request/response scenarios will fail (because the reply cannot be written).
2. UDP adapters may cause error messages (executor threads may be interrupted).
3. No attempt is made for orderly shutdown of messages running on external (e.g. container) threads, such as http inbound endpoints.
Suggest adding two methods to the OrderlyShutdownCapable interface
can then be used to inform endpoints such as those above (http, tcp) to not accept any more incoming requests, while leaving in-flight requests in process time to quiesce.
can be called after the shutdown delay to attempt to force stop the TCP endpoints and report if http requests are still in-flight. 
","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,",Improve Groovy Control Bus to avoid overhead during script variables initialization with managed beans  ,", "
"   Rename Method,","Improve the File Overwrite Handling for the File Outbound Channel Adapter  We recently added support for file appends with. In order to improve the behavior further, we will remove the ""append"" attribute and replace it with a new enumeration called mode providing the following options: Replace - The Default Setting
Append - Same functionality as 
Fail - Raise an exception when the file already exists (new)
Ignore - Silently ignore the message payload file if the destination file already exists (new)
",", "
"   Rename Method,Extract Method,",Reply destination as expression on JmsOutboundGateway  It should be possible to set the reply destination on the jms outbound gateway as an expression. (just like the request-destination-expression) ,"Duplicated Code, Long Method, , "
"   Rename Method,",Add support for Uri Templating to the <ws:outbound-gateway> for all transports supported by Spring WS  ,", "
"   Rename Class,Rename Method,",Redis Zset - Support Score Increment  ,", "
"   Rename Method,Extract Method,","make StringRedisTemplate the default for outbound channel adapters that update collections  The current default is RedisTemplate and that uses JDK serialization for keys, values, hash-keys, and hash-values. The StringRedisTemplate seems like a better default, especially for keys. Considering the end-user can always reference their own RedisTemplate OR update the various serializers on the default instance (i.e. if not providing one explicitly), using the StringRedisTemplate as a default should be more convenient. ","Duplicated Code, Long Method, , "
"   Pull Up Method,Extract Method,",Support Transaction Synchronization on Polled Channels  factory is currently ignored if configured on a  poller used to poll a channel. ,"Duplicated Code, Long Method, , Duplicated Code, "
"   Rename Method,Pull Up Method,Push Down Method,Move Method,Inline Method,Push Down Attribute,",Support Transaction Synchronization on Polled Channels  factory is currently ignored if configured on a  poller used to poll a channel. ,", , , , Duplicated Code, , "
"   Move Class,Move Method,",Fix Code Tangles  ,", , "
"   Rename Method,Pull Up Method,Move Method,","Add support for Jackson 2.1 (JsonToObjectTransformer, ObjectToJsonTransformer)  Add support for Jackson ",", , Duplicated Code, "
"   Rename Method,","Add support for adding/removing individual recipients to the RecipientListRouter  We need to support adding and removing recipients dynamically as well as possibly support RLR with no recipients at its initial state. A typical use case is ""registering an interest in something that is part of the Message"". For example a dynamic Recipient could be created with the following selector expression:
Also, copying some email content regarding this:
_Seems like the simplest thing we could do is provide an addRecipient(..) method. Although, we'd probably want to expose it so that it takes a *string* expression and a *string* channel name (as opposed to the current Recipient object which pairs a MessageChannel instance with a MessageSelector instance. Having just Strings would be better if we also add  and let it be driven via Control Bus (although maybe we'd even want to parse a single string for that). Or, maybe we should *extend* RecipientListRouter with a new DynamicRecipientListRouter class and provide that one with an explicit channel for the dynamic configuration commands (probably need to support *removal* from the list also)._
As far as RLR with 0 Recipients, I think all messages would have to be dropped the same way they are dropped in pub-sub channel if there is no subscribers. Essentially RLR plays a role of selector-based pub-sub channel. But because it has logic (selector) its not a channel. 
",", "
"   Rename Method,Extract Method,","<filter> shouldn't discarding within <request-handler-advice-chain>  If we configure something like this:
we will be surprised, that our Advice is waiting until the 'discard'-flow will be completed.
Gary, I assign it to you, because I don't see any right solution.
Thanks 
","Duplicated Code, Long Method, , "
"   Rename Method,",Make AbstractReplyProducingMessageHandler.onInit() final and have custom method like such as doCustomInit()  Make final and have custom method like such as (with a NoOp in the abstract class) to prevent mistakes created by user when extending ,", "
"   Move Method,Extract Method,",Add Rename (mv) to (S)FTP Gateway  ,"Duplicated Code, Long Method, , , "
"   Move Method,Inline Method,",Add out-of-the-box #jsonPath() SpEL function  Add SpEL function and provide tests to show how it can be used for: ,", , , "
"   Move Class,Extract Superclass,Pull Up Method,Move Method,Extract Method,Move Attribute,","MongoDbMessageStore should provide a setter for messageTemplate  It would be really usefull if the could provide a setter, since it's common to set up a singleton messageTemplate configured with custom convertors, and some other Mongo write-concern policies. ","Duplicated Code, Long Method, , , , Duplicated Code, Large Class, Duplicated Code, "
"   Rename Method,Extract Method,",Add support for the max-number-of-results-expression  ,"Duplicated Code, Long Method, , "
"   Rename Class,Extract Method,","Recursive LS/MGET with (S)FTP Gateway  Now that the remote file gateways support the use of the when retrieving a file (GET and MGET), it would be useful to be able to do a recursive MGET of the remote directory. Also support retrieval of the full directory tree with LS. Add (recursive) to the LS and MGET commands. ","Duplicated Code, Long Method, , "
"   Move Class,Move Method,",Provide the Integration Configuration Abstraction using SpringFactoriesLoader  ,", , "
"   Extract Superclass,Rename Method,Extract Method,","Add MessageBuilderFactory Strategy  Add a strategy, and add a to the context by default. will return the existing - it won't be renamed to provide backwards compatibility.
Provide an optional which returns a which creates/modified s.
Normal user applications should continue to use the default, but specialized use cases (such as high volume ingestion) can use a with care. 
","Duplicated Code, Long Method, , Duplicated Code, Large Class, "
"   Rename Method,",Provide @EnableIntegrationMBeanExport support  Similat to but for the registration of bean. ,", "
"   Pull Up Method,Extract Method,",Improve MongoDbMessageStore to properly store/restore existing Message<?> implementations  ,"Duplicated Code, Long Method, , Duplicated Code, "
"   Rename Class,Rename Method,",Add Gemfire MetadataStore  ,", "
"   Pull Up Method,Move Method,Move Attribute,","Rework JMS/XML Module XSD Enumerations  Requires a good deal of rework in parsers (JMS: acknowledge, cache mode). XML:result type.
Deferred until 4.1. 
",", , , Duplicated Code, "
"   Rename Method,","The PointToPointSubscribableAmqpChannel should not declare Queue, if ""queueName"" is provided  ",", "
"   Rename Method,Extract Method,",Orderly Shutdown Improvements  When using the orderly shutdown in the don't stop the schedulers or executors at the beginning - there might be a mid-flow polled channel. Just stop the inbound endpoints,"Duplicated Code, Long Method, , "
"   Extract Superclass,Pull Up Method,Inline Method,Pull Up Attribute,",Support output-channel when an AbstractCorrelatingMessageHandler is 'ref'd by a Service Activator  Original title: Refactor the aggregator/resequencer to be ARPMH ,", Duplicated Code, Large Class, , Duplicated Code, Duplicated Code, "
"   Move Method,Move Attribute,",Implement support for the async Paho client in the MQTT adapter  See SO question:,", , , "
"   Rename Method,",Add Reactor's Promise<?> Gateway's return type support  ,", "
"   Rename Method,","Adding support to PacketExtension for the XMPP outbound adapter  Currently the format of an XMPP message is hard coded in handleMessageInternal method of ChatMessageSendingMessageHandler. It would be useful to expose support for custom formatting of the message, currently Smack support this by PacketExtension.
A workaround is to exploit the behaviour of handleMessageInternal , which will do a simple pass through if the Message payload is of type org.jivesoftware.smack.packet.Message, however this is not explicitly cited in the documentation, and it may change in the future
",", "
"   Pull Up Method,Extract Method,Move Attribute,","Use ChannelResolvers Instead of BeanFactory  To support the Java DSL, several components were set up to allow late binding of channels by name. This code accesses the directly instead of using a 
This precludes the configuration of an alternative channel resolver.
For example, in XD, we might want to configure a to route discarded messages to a named channel In order to do that, XD could configure a into any bean found during module deployment, in the same way that it does for s.
The same thing applies to and possibly other components; however, we should generalize the use of a channel resolver everywhere that late channel binding was added.
We could even consider back-porting this to 4.1.x given that it's a low risk change. 
","Duplicated Code, Long Method, , , Duplicated Code, "
"   Rename Method,","Publish aTcpServerConnectionExceptionEvent From Server Socket Errors  Currently, s are published when exceptions occur on established sockets but there is no even published on server socket errors (such as bind errors).
Add a new event type and publish is from exceptions. 
",", "
"   Rename Method,","Publish aTcpServerConnectionExceptionEvent From Server Socket Errors  Currently, s are published when exceptions occur on established sockets but there is no even published on server socket errors (such as bind errors).
Add a new event type and publish is from exceptions. 
",", "
"   Rename Method,","Publish aTcpServerConnectionExceptionEvent From Server Socket Errors  Currently, s are published when exceptions occur on established sockets but there is no even published on server socket errors (such as bind errors).
Add a new event type and publish is from exceptions. 
",", "
"   Rename Method,",BridgeHandler - Don't Create a New Message  should override  returning false,", "
"   Rename Method,",BridgeHandler - Don't Create a New Message  should override  returning false,", "
"   Rename Method,Extract Method,","Transaction configuration for non-polling inbound adapters/gateways  In the reference documentation, appendix C., I read:
These 6 \[mechanisms to initiate a Message flow\] could be split in 2 general categories Message flows initiated by a USER process - Example scenarios in this category would be invoking a Gateway method or explicitly sending a Message to a MessageChannel. In other words, these message flows depend on a third party process (e.g., some code that we wrote) to be initiated. Message flows initiated by a DAEMON process - Example scenarios in this category would be a Poller polling a Message queue to initiate a new Message flow with the polled Message or a Scheduler scheduling the process by creating a new Message and initiating a message flow at a predefined time.
Clearly the Gateway and all belong to the 1st category and Inbound Adapters/Gateways, Scheduler and Poller belong to the 2nd. 
And then the documentation says that while for a user process initiated message flow a transaction context should reasonably be started by the process itself, for daemon process initiated ones a way to tell Spring Integration to start a new transaction context when the process is triggered is needed. This is why transactions can be configured for pollers.
This is perfectly reasonable. However, there are some inbound adapters/gateways, which as per the above statement are to be considered daemon-like process message flow initiators, that are ""push"" adapters/gateways, and not ""pull"" ones. Hence, they do not work with a poller. A concrete example is the which receives incoming requests from an exposed web service. Other examples might be a HTTP inbound gateway/adapter or a JMS one. However, while JMS has the concept of ""transacted acknowledgement"" at the protocol level, the other two do not have something like that.
So, to be concrete, if I have a chain like this:
SOAP message -> WS inbound-gateway -> direct-channel -> JDBC outbound gateway
(that is: I receive a SOAP message and store it in a database)
how can I make SI start a transaction as soon as the SOAP message is received?
I can think of two possibilities:
 insert a gateway in between the two adapters that does nothing, except for starting the transaction
write some good amount of XML configuration to set up an AOP advice to start a transaction upon the call of the receiving method of the WS inbound gateway
The first solution sounds like a workaround to me, the second one is quite complex (at least for me) and introduces a lot of noise for a so simple scenario.
What do you think? 
","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,",DMLC: Set SessionTransacted to True By Default  A DMLC without transacted sessions can cause message loss,"Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Stomp adapters improvements  See ""after-merge"" discussion in the ","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","UDP output unicast communication through server datagram socket  Given
my Spring application works as UDP server using UDP Inbound Channel Adapter And
client application works behind NAT
When
server sends unicast message to client behind NAT
Then:
client behind NAT receives the unicast message.
In order to achieve above scenario UDP Outbound Channel Adapter must use UDP Inbound Adapters server socket (outgoing packets will have source port same as incoming packets destination port). 
","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,",Add SimpleSkipPollAdvice  ,"Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,",Add SimpleSkipPollAdvice  ,"Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,","JMS: Spring-integration does not use Spring Boot's auto-configured ConnectionFactory  Actual behavior When Spring Boot detects a JMS implementation it auto-configures a bean named 
However, Spring Integration does not expect this bean. It searches for a bean named 
Expected behaviour:
Spring Integration should detect the created by Spring Boot's auto-config. 
",", "
"   Rename Method,Move Method,","Support Extract Payload (and Map Headers) in AMQP-Backed Channels  Currently, AMQP-backed channels don't provide an option to map the message, they serialize the entire message, which only works for `Serializable` payloads. ",", , "
"   Rename Method,",Get rid of explicit script class name and rely on the GroovyClassLoader logic when the class name isn't provided  ,", "
"   Rename Method,","The directory should not be repeatly defined in both WatchServiceDirectoryScanner and <int-file:inbound-channel-adapter />  The requires the be defined as constructor. If this scanner be injected in, the  attribute has to be defined again in once more. I think the checking rule should be relaxed such that developer only has to defined once. ",", "
"   Extract Interface,Rename Method,Extract Method,",Add ChainNode and PolledChainNode to Object Graph  Include a list of name type pairs for handlers to the chain properties. ,"Duplicated Code, Long Method, , Large Class, "
"   Rename Method,Pull Up Attribute,",Consider improvment for IdempotentReceiverInterceptor Java Config usage  Update to distinguish the from all other in the advice and apply it properly to the handleMessage,", Duplicated Code, "
"   Rename Method,","RmiOutboundGateway needs to facilitate the propagation of the security context  Spring Security Remoting provides the classes and The factory (or a similar custom implementation) can be injected into the so that the Spring Security Context is propagated from the RMI client to the remote RMI server thread.
should allow the configuration of its so that the security context is propagated. 
",", "
"   Rename Class,Inline Method,",Ease Customization of JDBC Message Store  Make it simpler to customize the queries. ,", , "
"   Move Class,Rename Method,","DSL: add IntegrationFlows.from(MyGateway.class)...  Allow a flow to begin with a simplifying configuration and avoiding the need for to find gateways.
Something like...
And have the BPP build a GPFB.
",", "
"   Rename Method,Extract Method,","Pagination for mongodb inbound adapter  Would like to have a new numeric property 'page-size' on, to set the limit on the find query. Default: -1 , no limit. Would you welcome such pull request? ","Duplicated Code, Long Method, , "
"   Rename Method,",Make ExpressionEvaluatingRequestHandlerAdvice DSL-Friendly  Add support for channel names. ,", "
"   Rename Method,","FTP Inbound Adapter/Outbound Gateway supports Spel for filename-regex  In my use case, the filename regular expression is constructed on the fly.
The existing attribute and could only allowed static pattern
Is it possible to support Spel expression attribute for FTP Inbound Adapter and FTP Outbound Gateway similar to the following?
",", "
"   Rename Method,Extract Method,Move Attribute,","MessagingMethodInvokerHelper Improvement  introduced the preference to use over SpEL whenever possible. If the invocation fails for certain known reasons, we fall back to using SpEL.
Add a failure counter to and give up after some threshold, similar to the SpEL compiler, which gives up after a hard-coded 100 attempts to compile. After the threshold is exceeded, set the flag, to permanently use SpEL from that point on.
When we fail over, log the failure - do not log on every failure like we do now.
The current failure log logs which does not have a method.
Since we invoke the in the same try/catch, it appears that there are certain exceptions that will cause the SpEL to be invoked twice - i.e. we should not fall back to SpEL if we used SpEL in the first place. 
","Duplicated Code, Long Method, , , "
"   Rename Method,",Add JSON Representation of FileInfo (File Streaming Inbound Adapters)  ,", "
"   Pull Up Method,Extract Method,",Make ErrorMessageSendingRecoverer more generic and introduce ErrorMessageStrategy for customization  ,"Duplicated Code, Long Method, , Duplicated Code, "
"   Rename Method,","Provide DSL version of <int:exception-type-router />  In XML one can define a to create an easy routing based on the type of exception.
This is currently not as easy when using the Spring Integration Java DSL (haven't checked the Groovy and Scala versions!). I expected to be able to use the default route but that was problematic.
This is problematic as the is wrapped in 1 or more {{MessagingException}} s depending on the level it occurred on. 
To fix one would have to manually define a and pass it to the function.
Although it isn't that hard to do it would be nicer if we could use the DSL for it with a, for instance function. 
",", "
"   Move Class,Extract Superclass,Rename Method,Pull Up Method,Extract Method,Pull Up Attribute,",Add support for WebFlux server side  ,"Duplicated Code, Long Method, , Duplicated Code, Large Class, Duplicated Code, Duplicated Code, "
"   Pull Up Method,Pull Up Attribute,","Revisit header propagation on service activators  After 2 years developing on spring integration, this is the main issue we came across multiple times. In summary the current behavior of a service activating handler does not make sense for any of our use cases. When a service activator returns just the payload, of course all headers from the incoming message should be propagated. But when a service activator returns the object, it should be propagated to the next handler without changes.
Proposed Changes:
Remove members, they are not necessary
Change to simply decide to propate input message headers if just the payload was returned by the handler. If the is returned, this message is propagated as is.
With these changes, amongst others these scenarios would be solved:
A service activator would be able to user which it should be able ot
It would be possible to split a Message collection without the headers of all contained messages to be synced with the headers of the triggering message
This topic has been discussed before but in our opinion needs to be revisited. 
",", Duplicated Code, Duplicated Code, "
"   Rename Method,Extract Method,",Add delivery attempt header  Add header. In adapters that have an embedded retry template add this header as an and increment it before sending the message. ,"Duplicated Code, Long Method, , "
"   Extract Interface,Move Method,Extract Method,",Rotate (S)FTP Servers/Directories During Polls  Add a smart poller advice to rotate across multiple directories servers with several policies - standard - rotate only when no file found; fair - rotate on every poll; custom. ,"Duplicated Code, Long Method, , , Large Class, "
"   Rename Method,","JMS: DSL/XML Inconsistent receiveTimeout  The XML parser sets the default to no-wait; the DSL leaves it at the default (infinite wait).
They should be consistent, but I don't think no-wait is correct for the reasons discussed at the end of the SO answer. Infinite wait is incorrect too. Maybe 1000?
On second thought - perhaps no-wait (-1) if we have a 

  
",", "
"   Rename Method,","Roo generated Spring MVC apps should be deployable on GAE Currently GAE raises a compilation exception of the Roo generated jspx files in a GAE environment. The problem is the use of jasper runtime in the current version of GAE alongside geronimo . This means that GAE have an API which allows JSP and an implementation of Jasper which only supports JSP 2.0. Spring Roo generated MVC apps use a number of JSPfeatures mostly EL related which are supported by all current Web containers but not GAE As such GAE deployments are currently not supported by Roo generated Spring MVC applications. However, we are in contact with Google to resolve this issue as soon as possible. As a workaround Roo could ship a custom tag library (potentially with reduced functionality to support JSP  Another issue which needs to be resolved is the full support of the default GAE data store. This will require changes outlined in to fully support references between domain objects.",", "
"   Move Method,Move Attribute,","Improve addon creation support for developers by introducing different addon generation options Improve addon creation support for developers by introducing different addon generation options.  The current simple addon template should be moved to a new addon as one of the options a developer can choose when starting a new Spring Roo addon Currently three addon types are envisioned simple addon  commands operations advanced addon commands operations metadata for ITD generation trigger annotation
addon specialized addon which adds new i18n bundles to existing MVC addons",", , , "
"   Rename Method,",Allow separate installation of new languages for a MVC/JSP scaffolded application Based on the comments in it is desirable to install additional languages into a MVC/JSP scaffolded application via a separate command The implementation should be flexible enough to support easy addition of support for new languages.,", "
"   Rename Method,","Request Factory naming convention tweaks After watching Bob's confusion figuring out RequestFactory, he's suggested some name changes. I would like to get these into, but only after RayC and Amit s big changes land The last is the informal name for the objects that hang right off the Request Factory, and mostly shows up in generated java doc and the source for Request Factory Generator. E.g. in requests employee find All Employees  the employee all returns what should be called a request builder.",", "
"   Rename Method,",Move OBR-related services into dedicated module Currently the SimpleParserComponent loads the OBR indexes and uses them for automatic resolution of available commands. It is desirable this functionality be moved into a dedicated module so it becomes available for other search-related use cases.,", "
"   Move Method,Extract Method,Move Attribute,",install web flow command requires prior generated Spring MVC artifacts The current version of the WebFlow add on in Roo builds on top of the Spring MVC framework and therefore requires Spring MVC artifacts to be installed first a controller with a form backing object At the very least Roo should have hidden the install web flow command until the Spring MVC artifacts are available in the project.,"Duplicated Code, Long Method, , , , "
"   Pull Up Method,Extract Method,Pull Up Attribute,","Allow flash messages to be handled without necessarily having loaded a Shell instance Currently classes that use the flash message API such as Jdk Url Input Stream Service require a Shell instance to be available before they will start. This prevents certain low-level infrastructure from using flash messages during startup. It is more appropriate to offer an abstract class that can provide flash messages if a Shell is available, but still operate gracefully if the Shell has not yet loaded.","Duplicated Code, Long Method, , Duplicated Code, Duplicated Code, "
"   Rename Method,Move Method,Extract Method,Move Attribute,","Replace Velocity template engine with Hapax for addon-gwt To reduce the size of the Roo distribution, Hapax will be used instead of Velocity for the template engine","Duplicated Code, Long Method, , , , "
"   Rename Method,Extract Method,Inline Method,","Support multi module Maven projects This is the feature I would like to see in the future Below are the layout I can think for now Initially, Roo will create parent, core, and webapp projects parent project This is pom project and the place where we call the Roo shell and use its command to create modules. Module's name format is something like  project name module- or  project name plugin  The modules list in this project pom is handled by Roo generator The list contains: core, all module, and webapp projects  All the modules inside will inherit the parent project The pom of this project also has dependencies of application frameworks like spring  jpa Maybe we can use this project to build and run the full tests. 
core project It contains main application contexts and reusable codes shared between module projects: helpers, utils, base classes, general auditing aspects like loggers module project can be one or many jar  The module project contains the main components like controllers, models, services daos, templates and resources which belong to the controllers of the module. Module project has the core project as its default dependency. The dependencies between module projects are managed by the developers webapp project war It contains web configurations It doesn't contain any component codes, templates But it contains the web resources that are shared between module projects. The dependencies in webapp are automatically handled by Roo generator during module project creation. The dependencies in webapp includes: core and all module projects. During the packaging, all module projects will be jar-ed and stored in WEB-INF lib of the war. 
","Duplicated Code, Long Method, , , "
"   Rename Method,Move Method,Inline Method,Move Attribute,","Support multi module Maven projects This is the feature I would like to see in the future Below are the layout I can think for now Initially, Roo will create parent, core, and webapp projects parent project This is pom project and the place where we call the Roo shell and use its command to create modules. Module's name format is something like  project name module- or  project name plugin  The modules list in this project pom is handled by Roo generator The list contains: core, all module, and webapp projects  All the modules inside will inherit the parent project The pom of this project also has dependencies of application frameworks like spring  jpa Maybe we can use this project to build and run the full tests. 
core project It contains main application contexts and reusable codes shared between module projects: helpers, utils, base classes, general auditing aspects like loggers module project can be one or many jar  The module project contains the main components like controllers, models, services daos, templates and resources which belong to the controllers of the module. Module project has the core project as its default dependency. The dependencies between module projects are managed by the developers webapp project war It contains web configurations It doesn't contain any component codes, templates But it contains the web resources that are shared between module projects. The dependencies in webapp are automatically handled by Roo generator during module project creation. The dependencies in webapp includes: core and all module projects. During the packaging, all module projects will be jar-ed and stored in WEB-INF lib of the war. 
",", , , , "
"   Rename Class,Rename Method,Extract Method,Inline Method,","Support multi module Maven projects This is the feature I would like to see in the future Below are the layout I can think for now Initially, Roo will create parent, core, and webapp projects parent project This is pom project and the place where we call the Roo shell and use its command to create modules. Module's name format is something like  project name module- or  project name plugin  The modules list in this project pom is handled by Roo generator The list contains: core, all module, and webapp projects  All the modules inside will inherit the parent project The pom of this project also has dependencies of application frameworks like spring  jpa Maybe we can use this project to build and run the full tests. 
core project It contains main application contexts and reusable codes shared between module projects: helpers, utils, base classes, general auditing aspects like loggers module project can be one or many jar  The module project contains the main components like controllers, models, services daos, templates and resources which belong to the controllers of the module. Module project has the core project as its default dependency. The dependencies between module projects are managed by the developers webapp project war It contains web configurations It doesn't contain any component codes, templates But it contains the web resources that are shared between module projects. The dependencies in webapp are automatically handled by Roo generator during module project creation. The dependencies in webapp includes: core and all module projects. During the packaging, all module projects will be jar-ed and stored in WEB-INF lib of the war. 
","Duplicated Code, Long Method, , , "
"   Rename Method,","Support multi module Maven projects This is the feature I would like to see in the future Below are the layout I can think for now Initially, Roo will create parent, core, and webapp projects parent project This is pom project and the place where we call the Roo shell and use its command to create modules. Module's name format is something like  project name module- or  project name plugin  The modules list in this project pom is handled by Roo generator The list contains: core, all module, and webapp projects  All the modules inside will inherit the parent project The pom of this project also has dependencies of application frameworks like spring  jpa Maybe we can use this project to build and run the full tests. 
core project It contains main application contexts and reusable codes shared between module projects: helpers, utils, base classes, general auditing aspects like loggers module project can be one or many jar  The module project contains the main components like controllers, models, services daos, templates and resources which belong to the controllers of the module. Module project has the core project as its default dependency. The dependencies between module projects are managed by the developers webapp project war It contains web configurations It doesn't contain any component codes, templates But it contains the web resources that are shared between module projects. The dependencies in webapp are automatically handled by Roo generator during module project creation. The dependencies in webapp includes: core and all module projects. During the packaging, all module projects will be jar-ed and stored in WEB-INF lib of the war. 
",", "
"   Rename Method,Extract Method,","Support multi module Maven projects This is the feature I would like to see in the future Below are the layout I can think for now Initially, Roo will create parent, core, and webapp projects parent project This is pom project and the place where we call the Roo shell and use its command to create modules. Module's name format is something like  project name module- or  project name plugin  The modules list in this project pom is handled by Roo generator The list contains: core, all module, and webapp projects  All the modules inside will inherit the parent project The pom of this project also has dependencies of application frameworks like spring  jpa Maybe we can use this project to build and run the full tests. 
core project It contains main application contexts and reusable codes shared between module projects: helpers, utils, base classes, general auditing aspects like loggers module project can be one or many jar  The module project contains the main components like controllers, models, services daos, templates and resources which belong to the controllers of the module. Module project has the core project as its default dependency. The dependencies between module projects are managed by the developers webapp project war It contains web configurations It doesn't contain any component codes, templates But it contains the web resources that are shared between module projects. The dependencies in webapp are automatically handled by Roo generator during module project creation. The dependencies in webapp includes: core and all module projects. During the packaging, all module projects will be jar-ed and stored in WEB-INF lib of the war. 
","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Support multi module Maven projects This is the feature I would like to see in the future Below are the layout I can think for now Initially, Roo will create parent, core, and webapp projects parent project This is pom project and the place where we call the Roo shell and use its command to create modules. Module's name format is something like  project name module- or  project name plugin  The modules list in this project pom is handled by Roo generator The list contains: core, all module, and webapp projects  All the modules inside will inherit the parent project The pom of this project also has dependencies of application frameworks like spring  jpa Maybe we can use this project to build and run the full tests. 
core project It contains main application contexts and reusable codes shared between module projects: helpers, utils, base classes, general auditing aspects like loggers module project can be one or many jar  The module project contains the main components like controllers, models, services daos, templates and resources which belong to the controllers of the module. Module project has the core project as its default dependency. The dependencies between module projects are managed by the developers webapp project war It contains web configurations It doesn't contain any component codes, templates But it contains the web resources that are shared between module projects. The dependencies in webapp are automatically handled by Roo generator during module project creation. The dependencies in webapp includes: core and all module projects. During the packaging, all module projects will be jar-ed and stored in WEB-INF lib of the war. 
","Duplicated Code, Long Method, , "
"   Rename Method,","Support multi module Maven projects This is the feature I would like to see in the future Below are the layout I can think for now Initially, Roo will create parent, core, and webapp projects parent project This is pom project and the place where we call the Roo shell and use its command to create modules. Module's name format is something like  project name module- or  project name plugin  The modules list in this project pom is handled by Roo generator The list contains: core, all module, and webapp projects  All the modules inside will inherit the parent project The pom of this project also has dependencies of application frameworks like spring  jpa Maybe we can use this project to build and run the full tests. 
core project It contains main application contexts and reusable codes shared between module projects: helpers, utils, base classes, general auditing aspects like loggers module project can be one or many jar  The module project contains the main components like controllers, models, services daos, templates and resources which belong to the controllers of the module. Module project has the core project as its default dependency. The dependencies between module projects are managed by the developers webapp project war It contains web configurations It doesn't contain any component codes, templates But it contains the web resources that are shared between module projects. The dependencies in webapp are automatically handled by Roo generator during module project creation. The dependencies in webapp includes: core and all module projects. During the packaging, all module projects will be jar-ed and stored in WEB-INF lib of the war. 
",", "
"   Rename Method,Push Down Method,Extract Method,","Support multi module Maven projects This is the feature I would like to see in the future Below are the layout I can think for now Initially, Roo will create parent, core, and webapp projects parent project This is pom project and the place where we call the Roo shell and use its command to create modules. Module's name format is something like  project name module- or  project name plugin  The modules list in this project pom is handled by Roo generator The list contains: core, all module, and webapp projects  All the modules inside will inherit the parent project The pom of this project also has dependencies of application frameworks like spring  jpa Maybe we can use this project to build and run the full tests. 
core project It contains main application contexts and reusable codes shared between module projects: helpers, utils, base classes, general auditing aspects like loggers module project can be one or many jar  The module project contains the main components like controllers, models, services daos, templates and resources which belong to the controllers of the module. Module project has the core project as its default dependency. The dependencies between module projects are managed by the developers webapp project war It contains web configurations It doesn't contain any component codes, templates But it contains the web resources that are shared between module projects. The dependencies in webapp are automatically handled by Roo generator during module project creation. The dependencies in webapp includes: core and all module projects. During the packaging, all module projects will be jar-ed and stored in WEB-INF lib of the war. 
","Duplicated Code, Long Method, , , "
"   Rename Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Support multi module Maven projects This is the feature I would like to see in the future Below are the layout I can think for now Initially, Roo will create parent, core, and webapp projects parent project This is pom project and the place where we call the Roo shell and use its command to create modules. Module's name format is something like  project name module- or  project name plugin  The modules list in this project pom is handled by Roo generator The list contains: core, all module, and webapp projects  All the modules inside will inherit the parent project The pom of this project also has dependencies of application frameworks like spring  jpa Maybe we can use this project to build and run the full tests. 
core project It contains main application contexts and reusable codes shared between module projects: helpers, utils, base classes, general auditing aspects like loggers module project can be one or many jar  The module project contains the main components like controllers, models, services daos, templates and resources which belong to the controllers of the module. Module project has the core project as its default dependency. The dependencies between module projects are managed by the developers webapp project war It contains web configurations It doesn't contain any component codes, templates But it contains the web resources that are shared between module projects. The dependencies in webapp are automatically handled by Roo generator during module project creation. The dependencies in webapp includes: core and all module projects. During the packaging, all module projects will be jar-ed and stored in WEB-INF lib of the war. 
","Duplicated Code, Long Method, , , , , "
"   Rename Method,","Support multi module Maven projects This is the feature I would like to see in the future Below are the layout I can think for now Initially, Roo will create parent, core, and webapp projects parent project This is pom project and the place where we call the Roo shell and use its command to create modules. Module's name format is something like  project name module- or  project name plugin  The modules list in this project pom is handled by Roo generator The list contains: core, all module, and webapp projects  All the modules inside will inherit the parent project The pom of this project also has dependencies of application frameworks like spring  jpa Maybe we can use this project to build and run the full tests. 
core project It contains main application contexts and reusable codes shared between module projects: helpers, utils, base classes, general auditing aspects like loggers module project can be one or many jar  The module project contains the main components like controllers, models, services daos, templates and resources which belong to the controllers of the module. Module project has the core project as its default dependency. The dependencies between module projects are managed by the developers webapp project war It contains web configurations It doesn't contain any component codes, templates But it contains the web resources that are shared between module projects. The dependencies in webapp are automatically handled by Roo generator during module project creation. The dependencies in webapp includes: core and all module projects. During the packaging, all module projects will be jar-ed and stored in WEB-INF lib of the war. 
",", "
"   Rename Method,","Support multi module Maven projects This is the feature I would like to see in the future Below are the layout I can think for now Initially, Roo will create parent, core, and webapp projects parent project This is pom project and the place where we call the Roo shell and use its command to create modules. Module's name format is something like  project name module- or  project name plugin  The modules list in this project pom is handled by Roo generator The list contains: core, all module, and webapp projects  All the modules inside will inherit the parent project The pom of this project also has dependencies of application frameworks like spring  jpa Maybe we can use this project to build and run the full tests. 
core project It contains main application contexts and reusable codes shared between module projects: helpers, utils, base classes, general auditing aspects like loggers module project can be one or many jar  The module project contains the main components like controllers, models, services daos, templates and resources which belong to the controllers of the module. Module project has the core project as its default dependency. The dependencies between module projects are managed by the developers webapp project war It contains web configurations It doesn't contain any component codes, templates But it contains the web resources that are shared between module projects. The dependencies in webapp are automatically handled by Roo generator during module project creation. The dependencies in webapp includes: core and all module projects. During the packaging, all module projects will be jar-ed and stored in WEB-INF lib of the war. 
",", "
"   Rename Method,Extract Method,","Support multi module Maven projects This is the feature I would like to see in the future Below are the layout I can think for now Initially, Roo will create parent, core, and webapp projects parent project This is pom project and the place where we call the Roo shell and use its command to create modules. Module's name format is something like  project name module- or  project name plugin  The modules list in this project pom is handled by Roo generator The list contains: core, all module, and webapp projects  All the modules inside will inherit the parent project The pom of this project also has dependencies of application frameworks like spring  jpa Maybe we can use this project to build and run the full tests. 
core project It contains main application contexts and reusable codes shared between module projects: helpers, utils, base classes, general auditing aspects like loggers module project can be one or many jar  The module project contains the main components like controllers, models, services daos, templates and resources which belong to the controllers of the module. Module project has the core project as its default dependency. The dependencies between module projects are managed by the developers webapp project war It contains web configurations It doesn't contain any component codes, templates But it contains the web resources that are shared between module projects. The dependencies in webapp are automatically handled by Roo generator during module project creation. The dependencies in webapp includes: core and all module projects. During the packaging, all module projects will be jar-ed and stored in WEB-INF lib of the war. 
","Duplicated Code, Long Method, , "
"   Rename Method,","Support multi module Maven projects This is the feature I would like to see in the future Below are the layout I can think for now Initially, Roo will create parent, core, and webapp projects parent project This is pom project and the place where we call the Roo shell and use its command to create modules. Module's name format is something like  project name module- or  project name plugin  The modules list in this project pom is handled by Roo generator The list contains: core, all module, and webapp projects  All the modules inside will inherit the parent project The pom of this project also has dependencies of application frameworks like spring  jpa Maybe we can use this project to build and run the full tests. 
core project It contains main application contexts and reusable codes shared between module projects: helpers, utils, base classes, general auditing aspects like loggers module project can be one or many jar  The module project contains the main components like controllers, models, services daos, templates and resources which belong to the controllers of the module. Module project has the core project as its default dependency. The dependencies between module projects are managed by the developers webapp project war It contains web configurations It doesn't contain any component codes, templates But it contains the web resources that are shared between module projects. The dependencies in webapp are automatically handled by Roo generator during module project creation. The dependencies in webapp includes: core and all module projects. During the packaging, all module projects will be jar-ed and stored in WEB-INF lib of the war. 
",", "
"   Rename Class,Move And Rename Class,Extract Method,","Hide Property from public api Closely related to Property at the very least should become an implementation detail, private API Ray and I talked about this bug, and it could be tractable in a day. Steps involved Move Property from requestfactory.shared to an impl class Delete all the Properties from the Proxy classes and have the RequestFactoryGenrator generate it in the Proxy Impl class Should be easy to delete the references in the UI.","Duplicated Code, Long Method, , "
"   Rename Method,",Implement support for reference fields in entities for addon-gwt ,", "
"   Rename Method,Inline Method,","Roo update is too destructive In particular, is it possible to move activities and views out of its way",", , "
"   Rename Method,","Create new Type Location Service and implementation to locate java types based on the annotations a type contains A new TypeLocationService interface and implementation will be created to locate types that contain supplied annotations. With this service, the DbreTypeResolutionService Impl the new Jsf Operations Impl and the Controller Operations Impl classes can be simplified to avoid repetition of code in all these classes.",", "
"   Rename Method,Extract Method,Inline Method,",DBRE multiple schema support At the moment we allow only one database schema per project in DBRE What is the problem to support multiple For instance At the moment second script command wipes out all generated artifacts created by first command.,"Duplicated Code, Long Method, , , "
"   Move Class,Rename Class,","DBRE to include exclude specified tables would be totally helpful to have something like and/or At the moment DBRE takes all tables. You can strip ones you do not need directly changing dbre xml, but it is not right, since every next call to database reverse engineer overwrites dbre.xml.


",", "
"   Rename Method,Extract Method,Inline Method,","Bad API for instance methods We have realized that the convention for firing instance methods is leaving a lot of corner cases for bugs we're going to have to fix. The fact that one creates new proxies off of RF rather than a request object, besides being confusing, is easy to misuse For example Which new instance will be returned The fix is actually very simple. It requires no change to the wire format, and very little change to RequestFactoryGenerator and the servlet. We move the create from Request Factory to Request, and introduce a new interface InterfaceRequest
This has the nice side effect of fixing  Might as well fix  at the same time.","Duplicated Code, Long Method, , , "
"   Rename Method,Move Method,Extract Method,",Add addon to deploy and run Roo projects on Google App Engine infrastructure ,"Duplicated Code, Long Method, , , "
"   Rename Method,Inline Method,",Add addon to deploy and run Roo projects on Google App Engine infrastructure ,", , "
"   Rename Method,","Make File Converter respect Shell get Home convention 

reports an issue caused because File Converter does not respect the Shell get Home conventions required by STS' embedded Roo feature. File Converter should be changed accordingly.",", "
"   Rename Method,Extract Method,",Significant performance improvements As per the current metadata infrastructure is too slow for extreme use cases. Need to improve it.,"Duplicated Code, Long Method, , "
"   Rename Method,",GWT's ValueBoxEditorDecorator isn't ready for primetime The EditViews shouldn't use it.,", "
"   Rename Method,Extract Method,","Allow specification of a parent POM The roo-generated pom is extremely verbose. While it makes for a solid build, it is somewhat difficult to read. As a compromise, it would be nice if you could specify a parent artefact when you run 'create project If this parent contains plugin Management and dependency Management sections, then there would no need to explicitly specify versions for all the plugins and dependencies in the project's pom, leading to a smaller pom You could use standard g:a:v notation, something like 'create project -parent org.example:superpom:1 ...' to make the association.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Project with Database Reverse Engineering locks Roo shell up after restart If you do a database reverse engineering and create controllers for them and then restart the Roo shell it is unresponsive. Apparently it is trying to create some classes again Deleted Deleted Deleted Welcome to Spring Roo. For assistance press TAB or type hint then hit ENTER.
Felix Dispatch Queue Deleted springsource reverse engineering many more message like this to follow. This process starts fast, then becomes very slow so that the shell is blocked. Apparently everything is fine if you do only the database reverse engineering i.e. no controller. Then the classes are also created again but that process is fast. I attached a dumb of the MySQL database I am using and the Spring Roo log file.","Duplicated Code, Long Method, , "
"   Rename Method,Inline Method,",Performance improvements for DBRE Refactor Dbre Database Listener Impl and other classes to process tables in a more efficient manner,", , "
"   Move Class,Rename Method,Move Method,Extract Method,Move Attribute,","Convert DBRE XML format to use Torque DTD As there is optional custom key-value elements in the Torque DTD, DBRE can now be changed to use this format instead.","Duplicated Code, Long Method, , , , "
"   Rename Class,Extract Method,",add includeTables option ,"Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,Inline Method,",Provide import management within generated ITDs Generated ITDs presently rely on fully qualified type names. This makes them feel unnatural when a human looks at them. It also makes for an unpleasant experience when using AJDT's push in refactoring. Generated ITDs should therefore automatically manage import statements correctly.,"Duplicated Code, Long Method, , , "
"   Rename Method,",Add 'addon search' command to allow flexible addon discovery The new 'addon search' command should have the following attributes to facilitate Roo add-on discovery requiresDescription comma separated list of search terms  required linesPerResult maximum number of lines per add-on optional max Results maximum number of results option trustedOnly display only trusted add-ons in search results optional compatibleOnly display only compatible add-ons in search results (optional requires Command display only add-ons which offer the specified command optional,", "
"   Rename Class,Rename Method,Inline Method,",Improve performance of DBRE ,", , "
"   Move And Rename Class,Rename Method,Pull Up Method,Extract Method,Pull Up Attribute,",Roo core changes to facilitate  There are a number of changes that have had to be made to Roo core in order to complete  These include but aren't limited to inner types initializers parameter varargs more complete import model parsing type directly from a String nested types.,"Duplicated Code, Long Method, , Duplicated Code, Duplicated Code, "
"   Rename Method,",Clean up GWT addon source The GWT addon source formatting is all over the place and should be reformatted.,", "
"   Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Refactor RooBot client This is needed to take the adjusted XML roobot xml schema into account. This allows for registration of individual versions of add-ons. Furthermore, the roobot xml file is now available as zip for increased bandwidth efficiency.","Duplicated Code, Long Method, , , , , "
"   Rename Method,",ItdType Details Providing Metadata Item and Physical Type Metadata to adopt a parameterized T extends Member Holding Type Details> superclass with get Member Holding Type Details():T method ,", "
"   Rename Method,",Add a MetadataCache.put(MetadataItem) method for use by Roo infrastructure types ,", "
"   Rename Method,Inline Method,",Conversion Service should observe metadata immutability and dependency injection conventions ,", , "
"   Rename Method,Extract Method,Inline Method,",Remove JoinTable class from DBRE The Join Table class is not necessary as a simple boolean flag can be added to the Table class to indicate whether or not the table is a many to many join table. This will reduce the amount of code in DBRE,"Duplicated Code, Long Method, , , "
"   Rename Class,Rename Method,","Introduce add-on / component upgrade functionalities to Roo shell Currently it is fairly inconvenient for a user to determine if a newer version of any of the installed add-ons components is available through RooBot. A set of newly introduced commands addon upgrade should address this as follows Upgrade a specific Spring Roo Add-on Component Upgrade a specific Spring Roo Add-on / Component from a search result ID  Upgrade all relevant Spring Roo Add-ons / Components
addon upgrade available  List available Spring Roo Add-on Component upgrades addon upgrade settings Settings for Add-on upgrade operations, this allows the user to set his preferred add-on stability level Furthermore, the Roo shell should list the number of upgradable add-ons upon startup if roobot xml zip was successfully downloaded.
",", "
"   Rename Method,Push Down Method,Move Method,",Remove Bean Info Metadata from addon-finder and replace with Member Details Scanner Member Details Scanner is the new pattern to be used in Roo. This will also allow finders to be created from methods introduced in ITDs,", , , "
"   Rename Method,Extract Method,","Decimal Max constraint not honored in Roo Data On Demand tests Define an entity with the following field and generate the DoD driven integration tests Next, run the integration tests. The test Persist method will fail with a constraint violation. The test uses Integer.MAX VALUE as the value for the index in getRandomEntityName The DoD class then uses the index to construct the BigDecimal, even though the annotation above set the max decimal amount to This does not happen with integer fields  the annotation is used to limit the maximum value to the value specified by the maximum value.
","Duplicated Code, Long Method, , "
"   Rename Method,Inline Method,",Make Data On Demand Metadata immutable by removing MetdataService and MetadataDependencyRegistry Lookups for collaborating metadata will be done in the provider after this change,", , "
"   Move Method,Extract Method,Move Attribute,","Refactor Classpath Operations methods into TypeLocationService and a new Type Management Service Currently ClasspathOperations is injected into DBRE and MVC services. Roo Operations classes should only be called by other Operations classes. This change will re-factor out all the methods from ClasspathOperations into the existing TypeLocationService and a new Type Management Service. Also, entity specific commands will be moved from Classpath Commands to a new Entity Commands class and corresponding Entity Operations interface and impl class located in addon-entity. Methods remaining in ClasspathCommands will be specific to classes and interfaces only. Classpath Operations and Classpath Operations will be removed after the MVC dependencies are changed to use Type Location Service and Type Management Service. Below is the background from Ben Alex about this change The general pattern is this Utils are statless public abstract classes with public static methods. They cannot be instantiated or injected with anything. It's OK to pass things into the methods if it helps, but it is fairly unusual to pass services or complex objects given utility methods are simple Commands contain shell annotations. Generally call Operations, but can call Services as well. Usually deal with formatting for user I/O. Also ensure all types are of the intended format for successful method invocation. Operations are stateless types which respond to UI commands and offer their methods to other Operations types. The point was they were different from Commands because they didn't deal with all the user I/O preparing method arguments etc) nor did they deal with complex lifecycles requiring state and listener models like most of the metadata infrastructure and services do. They just did something quite simple and returned, with no ongoing lifecycle obligations. They are allowed call services and other Operations objects. They aren't allowed call Commands Services can be called by anyone, including control flows unrelated to a UI command, such as control flows instigated due to a
MetadataProvider or event. Services should never call an Operations or Command object  Listeners are really a special type of Service. Most of the time a so-called Service actually implements one of more Listeners  MetadataProviders and metadata infrastructure can call other metadata infrastructure and Services. They're really just a very special type of
service (due to the ability to identify them and invoke and notify the from string-based MIDs). They follow the same rules as Services, namely
they cannot call Operations or Commands. As time has gone on, more complex add-ons have needed to do more complex
things and as such more and more logic has moved from Operations to Services to make them more sophisticated and generally available. This
is what has happened in your case. It is starting to get to the point where I question the value of keeping the Operations concept at all.
Sure, they're stateless and not listeners, but aside from that they seem of negligible distinction versus a service. Maybe we should just rename
all Operations to Services for consistency The critical thing is one cannot ""call up"" to Commands, as it's silly to
re-convert missing arguments and figure out defaults etc like the Commands and Converter infrastructure does.
","Duplicated Code, Long Method, , , , "
"   Rename Class,Rename Method,",Data-on-demand and integration tests to support composite primary keys This ticket is to combine existing related tickets into one,", "
"   Extract Method,Inline Method,",Data-on-demand and integration tests to support composite primary keys This ticket is to combine existing related tickets into one,"Duplicated Code, Long Method, , , "
"   Move Method,Inline Method,Move Attribute,","Change dependency listening approach in ConversionServiceMD and remove use of BeanInfoMetadata Change dependency listening approach in Conversion ServiceMD and remove use of Bean Info Metadata.
",", , , , "
"   Rename Method,Extract Method,","Remove all Reference Path Resolver declarations from classes and retrieve Path Resolver from Project Metadata instead PathResolver is supposed to abstract away the project specific paths
being used by the project build system. Having it as a Reference means
there can only be one possible implementation instantiated by OSGi and
injected ie similr to an autowire-by-type constraint in Spring. So if
we had to support say Ant and Maven concurrently, MavenPathResolver
(which implements PathResolver) would be prohibited from being an
Service. It really shouldn't be an Service for this reason, and those
with Reference private PathResolver shouldn't really have that
declaration. Instead they should get it from ProjectMetadata, or if
that's too tedious, at least from a ProjectOperations AKA
ProjectServices so we can centralised the ""switching logic"" that gives
them back the correct PathResolver instance.

This might also exemplify why we have AbstractProjectOperations. It was intended to allow multiple build systems more easily. But as time has
progressed, so many methods in there are so Maven-specific I don't see
it as very realistic. My recommendation is all the repetitious code going and retrieving
ProjectMetadata is replaced by a nice ProjectOperations method which
does it. All Reference private PathResolver fields should disappear at
the very least. I did a search and there are only  of them by my
count, so it's not a massive undertaking. Then the private helper
methods which retrieve ProjectMetadata just to get its PathResolver
should away.





","Duplicated Code, Long Method, , "
"   Rename Method,Move Method,Extract Method,Inline Method,","Remove all Reference Path Resolver declarations from classes and retrieve Path Resolver from Project Metadata instead PathResolver is supposed to abstract away the project specific paths
being used by the project build system. Having it as a Reference means
there can only be one possible implementation instantiated by OSGi and
injected ie similr to an autowire-by-type constraint in Spring. So if
we had to support say Ant and Maven concurrently, MavenPathResolver
(which implements PathResolver) would be prohibited from being an
Service. It really shouldn't be an Service for this reason, and those
with Reference private PathResolver shouldn't really have that
declaration. Instead they should get it from ProjectMetadata, or if
that's too tedious, at least from a ProjectOperations AKA
ProjectServices so we can centralised the ""switching logic"" that gives
them back the correct PathResolver instance.

This might also exemplify why we have AbstractProjectOperations. It was intended to allow multiple build systems more easily. But as time has
progressed, so many methods in there are so Maven-specific I don't see
it as very realistic. My recommendation is all the repetitious code going and retrieving
ProjectMetadata is replaced by a nice ProjectOperations method which
does it. All Reference private PathResolver fields should disappear at
the very least. I did a search and there are only  of them by my
count, so it's not a massive undertaking. Then the private helper
methods which retrieve ProjectMetadata just to get its PathResolver
should away.





","Duplicated Code, Long Method, , , , "
"   Move Class,Move And Rename Class,Move Method,Extract Method,Move Attribute,","Remove all Reference Path Resolver declarations from classes and retrieve Path Resolver from Project Metadata instead With the deprication of Bean Info Metadata major refactoring of the MVC add-on family is required. As part of this work, a new Web Metadata Utils type will be introduced to eventually take care of all Metadata scanning.","Duplicated Code, Long Method, , , , "
"   Move Class,Rename Method,Move Method,Move Attribute,","Merge Maven Add-On into the Project module A project is currently heavily tied to the Maven build system, as such it makes a reasonable amount of sense to combine Maven Add-On with Project.",", , , "
"   Move Class,Move Attribute,","Merge Maven Add-On into the Project module A project is currently heavily tied to the Maven build system, as such it makes a reasonable amount of sense to combine Maven Add-On with Project.",", , "
"   Rename Method,Move Method,Extract Method,",Post RELEASE code refactor and clean up ,"Duplicated Code, Long Method, , , "
"   Rename Method,Move Method,Extract Method,Inline Method,",Post 1.1.2.RELEASE code refactor and clean up ,"Duplicated Code, Long Method, , , , "
"   Rename Method,Extract Method,","DBRE breaks Java portability By default DBRE generates Column annotation setting the column Definition attribute. Having column Definition set is a problem in scenarios where the DB migration could be a possibility for example, when volume of data could grow 

Column JPA annotation defines attribute columnDefinition as override the sql DDL fragment for this particular column non portable, so, it could be ommited. If so, the property hibernate hbm2ddlautoin persistence.xml must be commented out in order to disable the schema validation in the application startup. The inclussion of an option for the command database reverse engineer setting when or not to include columnDefinition in the column mappings is interesting.","Duplicated Code, Long Method, , "
"   Move Method,Move Attribute,","Move test integration and dod commands into addon-test and addon-dod respectively Now that addon-test and addon-dod do not have dependencies on addon-entity, the test integration command can be moved to its rightful place in addon-test and the dod command to addon-dod.",", , , "
"   Rename Class,Rename Method,Move Method,","Custom Data tags get lost when ITD supplied fields or methods get pushed in to the corresponding java sources If a Metadata producing type tags a field or method and the member is already available in the corresponding governor, the tagged MD is lost in favour of the original MD most likely JavaParser...MD. The tagged MD is lost in Abstract Member Holding Type Details Builder

",", , "
"   Move Class,Rename Method,","Custom Data tags get lost when ITD supplied fields or methods get pushed in to the corresponding java sources If a Metadata producing type tags a field or method and the member is already available in the corresponding governor, the tagged MD is lost in favour of the original MD most likely JavaParser...MD. The tagged MD is lost in Abstract Member Holding Type Details Builder

",", "
"   Rename Method,Extract Method,","roo git addon is too verbose Roo git addon  works good but stores lots of verbose things like quit, help, hint. Only commands which emit code should be stored in git.","Duplicated Code, Long Method, , "
"   Move Class,Move And Rename Class,Move Method,Move Attribute,","Refactor MVC controller add-on Currently the MVC controller add-on produces a monolythic ITD which contains all Spring MVC controller functionality, all finders (if configured) and all JSON functionality (if configured). This leads to a large ITD which is rather hard to read for developers. So it is desirable to separate these three functionalities into separate ITDs (core MVC, finders, json). 

This change will also allow for internal refactoring of WebScaffoldMetadata and the creation of WebJsonMetadata and WebFinderMetadata and their respective packages.",", , , "
"   Rename Class,Rename Method,","Refactor WebMetadataUtils to a service This improvement would just change the Web Metadata Utils to Web Metadata Service, removing all the static methods and having the previously passed services injected.",", "
"   Rename Method,","Modification of Roo commands to enhance usability and consistency between add-ons Roo's current command names have evolved over a lengthy period. New add-ons have been created and existing add-ons have addressed additional requirements. In addition, a wider audience of people have tried Roo and reported their experience in learning commands and intuitively understanding what the present commands mean. Invariably this evolution has resulted in a better understanding of usage patterns and provides considerable scope for revisiting the existing command names and improving them for consistency, expressiveness and memorisation ease.

This issue will result in existing command names changing. Command options (ie those portions of a command prefixed by a double-hyphen), on the other hand, will not be reviewed as part of this task. This is because command options are not nearly as critical for learning Roo as are command names, as once a command name has been established the tab-completing shell interface guides the user through the mandatory and optional command options in an intelligent and easy-to-use manner anyway.

The main area of intended improvement is to identify the desired output artifact at the beginning of the command. For example, create controller"" would become controller create (or similar). In addition, where possible the differentiation between installation and post-installation setup is to be removed. As such, install jpa would become ""persistence setup"" and the ""update jpa"" would be removed.

This task has been deferred until now so that a detailed understanding of version 1.0.0 commands would be reached.",", "
"   Rename Method,",Turbo charge performance of persistence setup command ,", "
"   Rename Method,Extract Method,","Enhance File Manager to create and update XML files Currently, clients are required to create a MutableFile instance to read an XML file to convert to a DOM Document File Manager and XmlUtils will be enhanced to do the work itself reducing the need for so much code in callers.","Duplicated Code, Long Method, , "
"   Rename Method,",Post RELEASE code refactor and clean up ,", "
"   Rename Method,",Post RELEASE code refactor and clean up ,", "
"   Rename Method,","Change TypeLocationService.getSrcMainJavaTypes to getProjectJavaTypes(Path path) Pass a Path attribute to method to make more generic , eg allow test java types to be returned if required",", "
"   Rename Method,",Upgrade of SolrJ driver version to 1.4.1 Upgrade of SolrJ driver version to ,", "
"   Push Down Method,Push Down Attribute,",Post RELEASE code refactor and clean up ,", , , "
"   Move Method,Extract Method,Move Attribute,","Parameterise org.springframework.roo.shell.Converter This is about refactoring an internal interface and will not affect Roo users or addon developers  org spring framework roo shell Converter interface currently has this signature For improved type safety, we should parameterise this interface with a type T instead of returning Object and accepting Class","Duplicated Code, Long Method, , , , "
"   Extract Superclass,Extract Method,","Create AbstractOperations class containing common methods for add-on operations implementations An abstract class, Abstract Operations, will be created in the classpath module so that operations classes such as JsfOperations Impl and Jsp Operations Impl can inherit methods common to both.","Duplicated Code, Long Method, , Duplicated Code, Large Class, "
"   Rename Method,","Creating a java.util.Set of URLs is slow and unreliable  returns a Set of URLs. Sets rely upon the equal and hashCode methods of the contained objects to be performant and correct. The URL class fails on both counts, as documented here  Notably, these methods require a DNS lookup slow and can return different results depending upon whether an internet connection is available  inconsistent We should change this method to return a Set of either plain Strings or URIs (which do not have this problem), based on the needs of the calling code.",", "
"   Push Down Method,Push Down Attribute,","Create interfaces out of DataOnDemandProvider and IntegrationTestProvider To conform to the pattern for other add-ons, DoD and addon-test will have interfaces for the providers and the existing providers will be suffixed with Impl as well as extending their respective interfaces",", , , "
"   Rename Method,","Controllers shouldn't call findAll on each request As explained in the forum thread, and using the Pet entity as an example, the controller code generated by Roo calls {{findAllPets()}} upon every request, thanks to the presence of this method This is wasteful given that Some of the controller's request handling methods show and delete don't even use the returned list of Pets The list method obtains its own list of Pets usually via The create and update methods only need the list of all Pets if the Pet class has a Pet (or Pets) field. We can make the scaffolded controllers more performant by only obtaining the list of all Pets when it's actually required In the meantime, the workaround is as follows Push in the populate Pets method and have it return null (saves time in the ""list"" method) If the entity has a field of its own type (or a collection thereof), push in the create and update methods and modify them to explicitly put the pets list into the model, for example showing the create methods




",", "
"   Rename Method,",Post RELEASE code refactor and clean up ,", "
"   Rename Method,",Post RELEASE code refactor and clean up ,", "
"   Rename Method,",Post RELEASE code refactor and clean up ,", "
"   Rename Method,",Post RELEASE code refactor and clean up ,", "
"   Rename Method,Extract Method,",Post RELEASE code refactor and clean up ,"Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,Move Attribute,",Post RELEASE code refactor and clean up ,"Duplicated Code, Long Method, , , "
"   Rename Method,",Upgrade Database.com (VMFORCE) JPA provider Roo is using an old version of the Database om (VMFORCE) JPA provider. We should upgrade the version as well as the JPA configuration.,", "
"   Rename Method,",Upgrade Database.com (VMFORCE) JPA provider Roo is using an old version of the Database om (VMFORCE) JPA provider. We should upgrade the version as well as the JPA configuration.,", "
"   Rename Method,Extract Method,","New @RooJpaEntity annotation to manage core JPA entity concerns The existing  annotation is responsible for a number of JPA-related concerns the id field and its getter/setter  the version field if desired and its getter setter  the  annotation annotation if necessary CRUD methods that implement the Active Record pattern , core finders, dynamic finders The layering changes for  support for JPA repositories/DAOs) make it desirable for items  in the above list to be managed by a new RooJpa Entity annotation, with an associated Jpa Entity Metadata Provider that will create a new  ITD for each entity in the user's project. To maintain backward compatibility, this metadata provider will be triggered by the presence of either as in existing projects or on a domain type, with the latter's attribute values taking precedence The entity shell command will continue to apply the @RooEntity annotation, which will continue to introduce all the code listed above. Users wishing to remove the CRUD methods but retain core JPA entity support  for use with a JPA repository will be able to do so by replacing @RooEntity with @RooJpaEntity. At some point in the future out of scope here, we may rename  to something more accurate, ","Duplicated Code, Long Method, , "
"   Move Method,Inline Method,Move Attribute,","New @RooJpaEntity annotation to manage core JPA entity concerns The existing  annotation is responsible for a number of JPA-related concerns the id field and its getter/setter  the version field if desired and its getter setter  the  annotation annotation if necessary CRUD methods that implement the Active Record pattern , core finders, dynamic finders The layering changes for  support for JPA repositories/DAOs) make it desirable for items  in the above list to be managed by a new RooJpa Entity annotation, with an associated Jpa Entity Metadata Provider that will create a new  ITD for each entity in the user's project. To maintain backward compatibility, this metadata provider will be triggered by the presence of either as in existing projects or on a domain type, with the latter's attribute values taking precedence The entity shell command will continue to apply the @RooEntity annotation, which will continue to introduce all the code listed above. Users wishing to remove the CRUD methods but retain core JPA entity support  for use with a JPA repository will be able to do so by replacing @RooEntity with @RooJpaEntity. At some point in the future out of scope here, we may rename  to something more accurate, ",", , , , "
"   Rename Method,Extract Method,","New @RooJpaEntity annotation to manage core JPA entity concerns The existing  annotation is responsible for a number of JPA-related concerns the id field and its getter/setter  the version field if desired and its getter setter  the  annotation annotation if necessary CRUD methods that implement the Active Record pattern , core finders, dynamic finders The layering changes for  support for JPA repositories/DAOs) make it desirable for items  in the above list to be managed by a new RooJpa Entity annotation, with an associated Jpa Entity Metadata Provider that will create a new  ITD for each entity in the user's project. To maintain backward compatibility, this metadata provider will be triggered by the presence of either as in existing projects or on a domain type, with the latter's attribute values taking precedence The entity shell command will continue to apply the @RooEntity annotation, which will continue to introduce all the code listed above. Users wishing to remove the CRUD methods but retain core JPA entity support  for use with a JPA repository will be able to do so by replacing @RooEntity with @RooJpaEntity. At some point in the future out of scope here, we may rename  to something more accurate, ","Duplicated Code, Long Method, , "
"   Move Class,Move And Rename Class,Move Method,Move Attribute,","Merge JPA addons, addon-jpa and addon-entity JPA is baked into these add-ons and it makes sense now to merge them",", , , "
"   Extract Method,Inline Method,",Update DBRE to use PersistenceMemberLocator ,"Duplicated Code, Long Method, , , "
"   Move Class,Move Method,Extract Method,Move Attribute,",Introduce new trigger annotation for MVC JSON integration Introduce new trigger annotation for MVC JSON integration  This change will deprecate disable the Roo Web Scaffold exposeJson attribute which was used so far to trigger integration of JSON support for MVC applications.,"Duplicated Code, Long Method, , , , "
"   Rename Class,Rename Method,Move Method,Inline Method,Move Attribute,","The current type parsing implementation is too tightly coupled to Roo This issue encompasses a number of problems with the current parsing and type manipulation implementation in Roo. At the heart of this problem is that it would be difficult to replace our parsing and type manipulation implementation. Roo currently uses JavaParser to parse and manipulate types on disk, this is done via the classpath-javaparser module. The first issue is that parsing and type manipulation is tied up with our metadata model via PhysicalTypeMetadataProvider instead of as a re-implementable service interface. It would be nicer if there was a default PhysicalTypeMetadataProvider which lived in the standard classpath package which used a parsing service, instead of the current situation where classpath-javaparser provides the reference PhysicalTypeMetadataProvider via JavaParserMetadataProvider. This would also remove the JavaParser implementation of PhysicalTypeMetadata and leave the standard DefaultPhysicalTypeMetadata. The second issue is that classpath-javaparser module has its own representation of the standard type model which provides enough detail to build a model from the the parsed compilation unit and then turn said model back into a JavaParser compilation unit. The standard type model used by Roo can be found in the classpath module under the details package, this is the model used to represent types in memory. The problem with having two models is that: 
 there is needless duplication;
the implementations need to be kept in sync. Whilst this isn't so much of an issue with regards to the interface, as it rarely changes, keeping the additional model in sync with Roo patterns such is mutable-model-builder and immutable-model is tedious; and,  by having two concrete representations of essentially the same thing we aren't able to easily implement equals or compareTo and thus don't have a consistent strategy for comparing, for example, one FieldMetadata to another. 

Finally there is also duplication with MutableClassOrInterfaceTypeDetails which essentially provides a mutable Class Or InterfaceType Details which alters the type on disk. It would be better if there was a type manipulation service that would be used to manipulate types on disk explicitly by passing in a standard Class Or Interface Type Details.",", , , , "
"   Rename Method,",Reduce the need to re-parse and re-evaluate z attribute xml files XML round tripping works by comparing the elements and applying a z attribute hash to detect if something has changed. This process is somewhat resource intensive and the less this type of operation is performed the better.,", "
"   Rename Method,","Provide a mechanism to see if the ITDs associated with a specific type have changed In Roo a representation of the ITDs in the project is not maintained and can only be retrieved from the metadata item. This is troublesome when trying to maintain a cache which takes into account changes to ITDs on disk, such as PersistenceMemberLocatorImpl. Another potential improvemen an ITD store would have is in MemberDetailsScannerImpl as a reduction in mindless metadata retrieval could be achieved by using the already produced ItdTypeDetails from subsequent metadata production. An initial prototype was developed for Member Details Scanner Impl which reduced metadata requests by 80% but more work is needed before this is ready for prime time.",", "
"   Rename Method,","Don't update database upon switching JPA provider if any entities are DBRE-managed The internal  bike shop test project uses MySQL with Hibernate. If after creating the project you change the JPA provider, e.g. as follows:
persistence setup  database MYSQL provider ECLIPSELINK then in , the provider is configured to update the database schema This is not desirable if project has any DBRE-managed entities, as by definition the corresponding tables should never be updated. It would be better for the above property to be set so as not to update the schema none in the case of EclipseLink While this would mean not creating or updating the required tables for non-DBRE entities, this is the lesser of two evils.",", "
"   Rename Method,Pull Up Method,Extract Method,Inline Method,",Post code refactor and clean up ,"Duplicated Code, Long Method, , , Duplicated Code, "
"   Rename Class,Move Attribute,",Post code refactor and clean up ,", , "
"   Move Method,Extract Method,",Post code refactor and clean up ,"Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,Inline Method,",Post code refactor and clean up ,"Duplicated Code, Long Method, , , "
"   Rename Class,Move And Rename Class,Move Method,Extract Method,",Post code refactor and clean up ,"Duplicated Code, Long Method, , , "
"   Rename Method,",Post code refactor and clean up ,", "
"   Rename Method,Extract Method,Inline Method,",Post code refactor and clean up ,"Duplicated Code, Long Method, , , "
"   Rename Method,",Post code refactor and clean up ,", "
"   Extract Method,Inline Method,",Post code refactor and clean up ,"Duplicated Code, Long Method, , , "
"   Rename Method,",Post code refactor and clean up ,", "
"   Rename Method,",Post code refactor and clean up ,", "
"   Rename Method,",Post code refactor and clean up ,", "
"   Rename Class,Move Attribute,",Post code refactor and clean up ,", , "
"   Push Down Method,Push Down Attribute,",Post code refactor and clean up ,", , , "
"   Rename Method,",Post code refactor and clean up ,", "
"   Move Method,Move Attribute,",Post code refactor and clean up ,", , , "
"   Rename Method,Inline Method,",Post code refactor and clean up ,", , "
"   Move And Rename Class,Rename Method,","Create add-on for displaying a pretty-print representation of a class A new add-on, addon displayname will be created to provide a method returning a string representation of the owning class. This is similar to addon-tostring however the new add-on will produce a string that can be used for display in UIs as well as to aid in conversion. The method name and the exact fields can be customised via the new Roo Display Name annotation",", "
"   Rename Method,Extract Method,Move Attribute,","Implement new Feature interface to allow add-ons to advise of their installation in projects Currently, operations classes must detect artifacts generated from other add-ons to determine if they can be installed in a project. For example, the JSP add-on tries to locate a JSF faces-config.xml file in the project and if found, does not allow JSP to be installed. This change will allow add-ons themselves to determine if they are enabled in a project which will mean callers only have to make simple calls to a ProjectOperations method.","Duplicated Code, Long Method, , , "
"   Rename Method,","Implement new Feature interface to allow add-ons to advise of their installation in projects Currently, operations classes must detect artifacts generated from other add-ons to determine if they can be installed in a project. For example, the JSP add-on tries to locate a JSF faces-config.xml file in the project and if found, does not allow JSP to be installed. This change will allow add-ons themselves to determine if they are enabled in a project which will mean callers only have to make simple calls to a ProjectOperations method.",", "
"   Rename Method,Extract Method,","Implement new Feature interface to allow add-ons to advise of their installation in projects Currently, operations classes must detect artifacts generated from other add-ons to determine if they can be installed in a project. For example, the JSP add-on tries to locate a JSF faces-config.xml file in the project and if found, does not allow JSP to be installed. This change will allow add-ons themselves to determine if they are enabled in a project which will mean callers only have to make simple calls to a ProjectOperations method.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,",Post code refactor and clean up ,"Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,",Post code refactor and clean up ,"Duplicated Code, Long Method, , "
"   Rename Method,",Post code refactor and clean up ,", "
"   Rename Method,",Post code refactor and clean up ,", "
"   Rename Method,",Allow DBRE to make DB connection via JNDI ,", "
"   Rename Method,Extract Method,",Allow DBRE to make DB connection via JNDI ,"Duplicated Code, Long Method, , "
"   Rename Method,","Decouple PrimeFaces references in JsfOperationsImpl Currently, Prime Faces is hard coded in the Jsf Operations Impl class. This improvement will add a new optional library option to the web jsf setup command and treat Prime Faces similarly to the user-selectable implementation option.",", "
"   Extract Method,Inline Method,",Post RELEASE code refactor and clean up ,"Duplicated Code, Long Method, , , "
"   Rename Method,",Post RELEASE code refactor and clean up ,", "
"   Rename Method,Extract Method,Inline Method,Move Attribute,","Choose between data access patterns At this time, the Aspect J generated aspects and entity classes follow the Active Record pattern. It would be nice if we could choose between this pattern and Data Access Object, with or without Spring Dao Support. The way to configure would be persistence setup  database AccessPattern Active Record DAO","Duplicated Code, Long Method, , , , "
"   Rename Method,Move Attribute,","Choose between data access patterns At this time, the Aspect J generated aspects and entity classes follow the Active Record pattern. It would be nice if we could choose between this pattern and Data Access Object, with or without Spring Dao Support. The way to configure would be persistence setup  database AccessPattern Active Record DAO",", , "
"   Extract Method,Inline Method,","Choose between data access patterns At this time, the Aspect J generated aspects and entity classes follow the Active Record pattern. It would be nice if we could choose between this pattern and Data Access Object, with or without Spring Dao Support. The way to configure would be persistence setup  database AccessPattern Active Record DAO","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,Inline Method,","Choose between data access patterns At this time, the Aspect J generated aspects and entity classes follow the Active Record pattern. It would be nice if we could choose between this pattern and Data Access Object, with or without Spring Dao Support. The way to configure would be persistence setup  database AccessPattern Active Record DAO","Duplicated Code, Long Method, , , "
"   Rename Method,Move Attribute,","Choose between data access patterns At this time, the Aspect J generated aspects and entity classes follow the Active Record pattern. It would be nice if we could choose between this pattern and Data Access Object, with or without Spring Dao Support. The way to configure would be persistence setup  database AccessPattern Active Record DAO",", , "
"   Rename Method,","Choose between data access patterns At this time, the Aspect J generated aspects and entity classes follow the Active Record pattern. It would be nice if we could choose between this pattern and Data Access Object, with or without Spring Dao Support. The way to configure would be persistence setup  database AccessPattern Active Record DAO",", "
"   Rename Method,","Choose between data access patterns At this time, the Aspect J generated aspects and entity classes follow the Active Record pattern. It would be nice if we could choose between this pattern and Data Access Object, with or without Spring Dao Support. The way to configure would be persistence setup  database AccessPattern Active Record DAO",", "
"   Rename Method,Extract Method,Inline Method,Move Attribute,","Choose between data access patterns At this time, the Aspect J generated aspects and entity classes follow the Active Record pattern. It would be nice if we could choose between this pattern and Data Access Object, with or without Spring Dao Support. The way to configure would be persistence setup  database AccessPattern Active Record DAO","Duplicated Code, Long Method, , , , "
"   Rename Method,","Choose between data access patterns At this time, the Aspect J generated aspects and entity classes follow the Active Record pattern. It would be nice if we could choose between this pattern and Data Access Object, with or without Spring Dao Support. The way to configure would be persistence setup  database AccessPattern Active Record DAO",", "
"   Rename Method,","Choose between data access patterns At this time, the Aspect J generated aspects and entity classes follow the Active Record pattern. It would be nice if we could choose between this pattern and Data Access Object, with or without Spring Dao Support. The way to configure would be persistence setup  database AccessPattern Active Record DAO",", "
"   Rename Method,","Choose between data access patterns At this time, the Aspect J generated aspects and entity classes follow the Active Record pattern. It would be nice if we could choose between this pattern and Data Access Object, with or without Spring Dao Support. The way to configure would be persistence setup  database AccessPattern Active Record DAO",", "
"   Rename Method,Move Method,","Choose between data access patterns At this time, the Aspect J generated aspects and entity classes follow the Active Record pattern. It would be nice if we could choose between this pattern and Data Access Object, with or without Spring Dao Support. The way to configure would be persistence setup  database AccessPattern Active Record DAO",", , "
"   Rename Class,Rename Method,Move Method,Inline Method,","Use Apache Commons libraries in Roo Until now, Roo has internally used copies of classes that are available in the Apache commons lang, commons io, commons collections, and commons codec libraries. This ticket will add the commons dependencies to Roo and remove the Roo managed equivalents and their test classes, such as String Utils. The commons libraries are small, well tested, and are already OSGi compatible.",", , , "
"   Move Method,Inline Method,Move Attribute,","Use Apache Commons libraries in Roo Until now, Roo has internally used copies of classes that are available in the Apache commons lang, commons io, commons collections, and commons codec libraries. This ticket will add the commons dependencies to Roo and remove the Roo managed equivalents and their test classes, such as String Utils. The commons libraries are small, well tested, and are already OSGi compatible.",", , , , "
"   Move Method,Inline Method,","Use Apache Commons libraries in Roo Until now, Roo has internally used copies of classes that are available in the Apache commons lang, commons io, commons collections, and commons codec libraries. This ticket will add the commons dependencies to Roo and remove the Roo managed equivalents and their test classes, such as String Utils. The commons libraries are small, well tested, and are already OSGi compatible.",", , , "
"   Rename Method,",Post RELEASE code refactor and clean up ,", "
"   Rename Class,Rename Method,",Update addon-tailor with supplied patches ,", "
"   Rename Method,",Add Selenium Add-on Roo should generate Selenium test cases for MVC applications.,", "
"   Rename Method,Extract Method,","second ""web gwt setup"" produced wrong pom.xml If you execute web gwt setup secound time in an existing project for update to newer versionnumers for example) the resulting pom xml is wrong
gwt dependencies are deleted gwt projectnature are duplicated plugins are not updated gwt-maven-plugin twice To reproduce this behavior:
 run expenses
include comments in gwt-maven-plugin and exec maven plugin to see if updated
run web gwt setup","Duplicated Code, Long Method, , "
"   Rename Method,",Spring Roo doesn't start with JDK 1.8 I am using Spring Roo  and java version When I try to start roo.sh I get a ton of unresolved constraints (log file attached). After searching the web I found this  and tried to replace the felijar in the Spring Roo distribution and could start the shell successfully.,", "
"   Rename Method,",MenuOperations should make use of fileManager.findMatchingAntPath(..) to update message bundles Currently MenuOperations updates messages roperties files individually to insert label names for menu items. This requires us to register every new messages properties file there. It makes sense to use fileManager find Matching Ant Path antPath  where the ant path matches all messages properties.,", "
"   Rename Method,",MenuOperations should make use of fileManager.findMatchingAntPath(..) to update message bundles Currently MenuOperations updates messages roperties files individually to insert label names for menu items. This requires us to register every new messages properties file there. It makes sense to use fileManager find Matching Ant Path antPath  where the ant path matches all messages properties.,", "
"   Rename Method,","Update available commands and converters after Bundle changes If we want to work with subsystems and Roo Addon Suites, is necessary to update available commands and available converters after new bundle installation or after some bundle status change.",", "
"   Rename Method,","Update available commands and converters after Bundle changes If we want to work with subsystems and Roo Addon Suites, is necessary to update available commands and available converters after new bundle installation or after some bundle status change.",", "
"   Rename Method,","Roo Addon Suite Support Include Roo Addon Suite Support on Spring Roo Shell. Related with  A Roo Addon Suite is a great way to package and distribute a set of add-ons together, for example if you want to distribute Roo custom distributions. Roo Addon Suite is based on OSGi R5 Subsystems that provides a really convenient deployment model, without compromising the modularity of Roo.

",", "
"   Move And Rename Class,Move Method,","Roo Commands Refactoring After analyze Sping Roo components, is really necessary to apply the following changes:

addon create wrapper command must not appear after project setup.
equals command must not appear until project setup Remove flash test command and its implementation
focus command must not appear until project setup
maven command must not appear until project setup
metadata command must not appear until project setup
 Rename project command with project setup
Rename poll command with project scan
project scan command must not appear until project setup 
process manager command must appear only with development mode
reference guide command must appear only with development mode
web mvc json setup command must not appear until project setup
Rename osgi framework command with 
Remove Spring Roo osgi commands
addon install id must be deleted
addon info id must be deleted
Create new command addon install url* to install bundles/addons by URL.  Create new command addon repository introspect that lists installed addons. Adds repository parameter to list addons from an specific repository
",", , "
"   Rename Method,","Roo Commands Refactoring After analyze Sping Roo components, is really necessary to apply the following changes:

addon create wrapper command must not appear after project setup.
equals command must not appear until project setup Remove flash test command and its implementation
focus command must not appear until project setup
maven command must not appear until project setup
metadata command must not appear until project setup
 Rename project command with project setup
Rename poll command with project scan
project scan command must not appear until project setup 
process manager command must appear only with development mode
reference guide command must appear only with development mode
web mvc json setup command must not appear until project setup
Rename osgi framework command with 
Remove Spring Roo osgi commands
addon install id must be deleted
addon info id must be deleted
Create new command addon install url* to install bundles/addons by URL.  Create new command addon repository introspect that lists installed addons. Adds repository parameter to list addons from an specific repository
",", "
"   Rename Method,","Search wrappings on default OSGi repository Roobot is not implemented on  , now components are searched on OSGi R5 repositories Update all wrapping searchs (jdbc drivers, libraries, etc...) to use repository structure instead of roobot structure. Repository Structure to Spring Roo Addons:

",", "
"   Rename Method,","Create visual component to manage Spring Roo Repositories and addons Manage OSGi repositories and addons are an important feature of The goal of this task is to create a new visual application (using JavaFX) to manage Spring Roo Repositories and addons using graphical mode. With a simple Spring Roo command like addon repository manager, this graphical application will be launched and will allow developers to manage their installed repositories and addons more easily than using commands.",", "
"   Pull Up Method,Pull Up Attribute,","Add support to generate Generic Methods Extend Spring Roo Method Builder API to include functionality that generates Generic Methods


",", Duplicated Code, Duplicated Code, "
"   Pull Up Method,Pull Up Attribute,","Add support to generate Generic Methods Extend Spring Roo Method Builder API to include functionality that generates Generic Methods


",", Duplicated Code, Duplicated Code, "
"   Rename Method,Inline Method,","Switch to ServiceTracker utility On we detected services cycle references at initialization time. We solved it by moving from automatic service dependency resolution to manual way by calling the Service Registry Bundle Context get All Service References to get the service references when they are needed It solved the cycle references problem but this solution doesn't manage the dynamic nature of OSGi services This task consist in migrating to Service Tracker utility to hide the complexities of listening to and consuming dynamic services. By the moment we will migrate those service references that are used in the activate method.
",", , "
"   Rename Method,",Create new command that make push in of declared methods and fields from ITDs to Java files Create new command that make push-in of declared methods and fields from ITDs to Java files. Commands to create will be push in all package class method,", "
"   Rename Method,",Create new command that make push in of declared methods and fields from ITDs to Java files Create new command that make push-in of declared methods and fields from ITDs to Java files. Commands to create will be push in all package class method,", "
"   Rename Method,","Annotation based configuration Use Spring Boot Move to class configuration in spite of XML configuration. For those unfamiliar with Configuration classes, you can think of them as a pure Java equivalent to Spring XML files. Since Spring  the Configuration approach provides a truly first-class option for those who wish to configure their applications without XML. Spring Boot is well suited for web application development:
It makes it easy to create stand-alone, production-grade Spring based Applications.
It favors Java-based configuration. It generally recommends that your primary source is a Configuration class.
Spring Boot auto-configuration attempts to automatically configure your Spring application based on the jar dependencies that you have added.
Auto-configuration is noninvasive, at any point you can start to define your own configuration to replace specific parts of the auto-configuration.
You are free to use any of the standard Spring Framework techniques to define your beans and their injected dependencies.
It allows you to externalize your configuration so you can work with the same application code in different environments.
Spring Profiles provide a way to segregate parts of your application configuration and make it only available in certain environments.
Spring Boot uses Commons Logging for all internal logging, but leaves the underlying log implementation open.
If Spring Security is on the classpath then web applications will be secure by default with ‘basic’ authentication on all HTTP endpoints.
If you work in a company that develops shared libraries, or if you work on an open-source or commercial library, you might want to develop your own auto-configuration. Auto-configuration classes can be bundled in external jars and still be picked-up by Spring Boot.
Spring Boot includes a number of additional features to help you monitor and manage your application when it’s pushed to production.
",", "
"   Rename Method,","Annotation based configuration Use Spring Boot Move to class configuration in spite of XML configuration. For those unfamiliar with Configuration classes, you can think of them as a pure Java equivalent to Spring XML files. Since Spring  the Configuration approach provides a truly first-class option for those who wish to configure their applications without XML. Spring Boot is well suited for web application development:
It makes it easy to create stand-alone, production-grade Spring based Applications.
It favors Java-based configuration. It generally recommends that your primary source is a Configuration class.
Spring Boot auto-configuration attempts to automatically configure your Spring application based on the jar dependencies that you have added.
Auto-configuration is noninvasive, at any point you can start to define your own configuration to replace specific parts of the auto-configuration.
You are free to use any of the standard Spring Framework techniques to define your beans and their injected dependencies.
It allows you to externalize your configuration so you can work with the same application code in different environments.
Spring Profiles provide a way to segregate parts of your application configuration and make it only available in certain environments.
Spring Boot uses Commons Logging for all internal logging, but leaves the underlying log implementation open.
If Spring Security is on the classpath then web applications will be secure by default with ‘basic’ authentication on all HTTP endpoints.
If you work in a company that develops shared libraries, or if you work on an open-source or commercial library, you might want to develop your own auto-configuration. Auto-configuration classes can be bundled in external jars and still be picked-up by Spring Boot.
Spring Boot includes a number of additional features to help you monitor and manage your application when it’s pushed to production.
",", "
"   Rename Method,","Annotation based configuration Use Spring Boot Move to class configuration in spite of XML configuration. For those unfamiliar with Configuration classes, you can think of them as a pure Java equivalent to Spring XML files. Since Spring  the Configuration approach provides a truly first-class option for those who wish to configure their applications without XML. Spring Boot is well suited for web application development:
It makes it easy to create stand-alone, production-grade Spring based Applications.
It favors Java-based configuration. It generally recommends that your primary source is a Configuration class.
Spring Boot auto-configuration attempts to automatically configure your Spring application based on the jar dependencies that you have added.
Auto-configuration is noninvasive, at any point you can start to define your own configuration to replace specific parts of the auto-configuration.
You are free to use any of the standard Spring Framework techniques to define your beans and their injected dependencies.
It allows you to externalize your configuration so you can work with the same application code in different environments.
Spring Profiles provide a way to segregate parts of your application configuration and make it only available in certain environments.
Spring Boot uses Commons Logging for all internal logging, but leaves the underlying log implementation open.
If Spring Security is on the classpath then web applications will be secure by default with ‘basic’ authentication on all HTTP endpoints.
If you work in a company that develops shared libraries, or if you work on an open-source or commercial library, you might want to develop your own auto-configuration. Auto-configuration classes can be bundled in external jars and still be picked-up by Spring Boot.
Spring Boot includes a number of additional features to help you monitor and manage your application when it’s pushed to production.
",", "
"   Extract Interface,Move Method,Extract Method,Move Attribute,","Annotation based configuration Use Spring Boot Move to class configuration in spite of XML configuration. For those unfamiliar with Configuration classes, you can think of them as a pure Java equivalent to Spring XML files. Since Spring  the Configuration approach provides a truly first-class option for those who wish to configure their applications without XML. Spring Boot is well suited for web application development:
It makes it easy to create stand-alone, production-grade Spring based Applications.
It favors Java-based configuration. It generally recommends that your primary source is a Configuration class.
Spring Boot auto-configuration attempts to automatically configure your Spring application based on the jar dependencies that you have added.
Auto-configuration is noninvasive, at any point you can start to define your own configuration to replace specific parts of the auto-configuration.
You are free to use any of the standard Spring Framework techniques to define your beans and their injected dependencies.
It allows you to externalize your configuration so you can work with the same application code in different environments.
Spring Profiles provide a way to segregate parts of your application configuration and make it only available in certain environments.
Spring Boot uses Commons Logging for all internal logging, but leaves the underlying log implementation open.
If Spring Security is on the classpath then web applications will be secure by default with ‘basic’ authentication on all HTTP endpoints.
If you work in a company that develops shared libraries, or if you work on an open-source or commercial library, you might want to develop your own auto-configuration. Auto-configuration classes can be bundled in external jars and still be picked-up by Spring Boot.
Spring Boot includes a number of additional features to help you monitor and manage your application when it’s pushed to production.
","Duplicated Code, Long Method, , , , Large Class, "
"   Move Class,Move Method,Move Attribute,","Annotation based configuration Use Spring Boot Move to class configuration in spite of XML configuration. For those unfamiliar with Configuration classes, you can think of them as a pure Java equivalent to Spring XML files. Since Spring  the Configuration approach provides a truly first-class option for those who wish to configure their applications without XML. Spring Boot is well suited for web application development:
It makes it easy to create stand-alone, production-grade Spring based Applications.
It favors Java-based configuration. It generally recommends that your primary source is a Configuration class.
Spring Boot auto-configuration attempts to automatically configure your Spring application based on the jar dependencies that you have added.
Auto-configuration is noninvasive, at any point you can start to define your own configuration to replace specific parts of the auto-configuration.
You are free to use any of the standard Spring Framework techniques to define your beans and their injected dependencies.
It allows you to externalize your configuration so you can work with the same application code in different environments.
Spring Profiles provide a way to segregate parts of your application configuration and make it only available in certain environments.
Spring Boot uses Commons Logging for all internal logging, but leaves the underlying log implementation open.
If Spring Security is on the classpath then web applications will be secure by default with ‘basic’ authentication on all HTTP endpoints.
If you work in a company that develops shared libraries, or if you work on an open-source or commercial library, you might want to develop your own auto-configuration. Auto-configuration classes can be bundled in external jars and still be picked-up by Spring Boot.
Spring Boot includes a number of additional features to help you monitor and manage your application when it’s pushed to production.
",", , , "
"   Rename Method,Move Method,","Annotation based configuration Use Spring Boot Move to class configuration in spite of XML configuration. For those unfamiliar with Configuration classes, you can think of them as a pure Java equivalent to Spring XML files. Since Spring  the Configuration approach provides a truly first-class option for those who wish to configure their applications without XML. Spring Boot is well suited for web application development:
It makes it easy to create stand-alone, production-grade Spring based Applications.
It favors Java-based configuration. It generally recommends that your primary source is a Configuration class.
Spring Boot auto-configuration attempts to automatically configure your Spring application based on the jar dependencies that you have added.
Auto-configuration is noninvasive, at any point you can start to define your own configuration to replace specific parts of the auto-configuration.
You are free to use any of the standard Spring Framework techniques to define your beans and their injected dependencies.
It allows you to externalize your configuration so you can work with the same application code in different environments.
Spring Profiles provide a way to segregate parts of your application configuration and make it only available in certain environments.
Spring Boot uses Commons Logging for all internal logging, but leaves the underlying log implementation open.
If Spring Security is on the classpath then web applications will be secure by default with ‘basic’ authentication on all HTTP endpoints.
If you work in a company that develops shared libraries, or if you work on an open-source or commercial library, you might want to develop your own auto-configuration. Auto-configuration classes can be bundled in external jars and still be picked-up by Spring Boot.
Spring Boot includes a number of additional features to help you monitor and manage your application when it’s pushed to production.
",", , "
"   Move Method,Move Attribute,","Annotation based configuration Use Spring Boot Move to class configuration in spite of XML configuration. For those unfamiliar with Configuration classes, you can think of them as a pure Java equivalent to Spring XML files. Since Spring  the Configuration approach provides a truly first-class option for those who wish to configure their applications without XML. Spring Boot is well suited for web application development:
It makes it easy to create stand-alone, production-grade Spring based Applications.
It favors Java-based configuration. It generally recommends that your primary source is a Configuration class.
Spring Boot auto-configuration attempts to automatically configure your Spring application based on the jar dependencies that you have added.
Auto-configuration is noninvasive, at any point you can start to define your own configuration to replace specific parts of the auto-configuration.
You are free to use any of the standard Spring Framework techniques to define your beans and their injected dependencies.
It allows you to externalize your configuration so you can work with the same application code in different environments.
Spring Profiles provide a way to segregate parts of your application configuration and make it only available in certain environments.
Spring Boot uses Commons Logging for all internal logging, but leaves the underlying log implementation open.
If Spring Security is on the classpath then web applications will be secure by default with ‘basic’ authentication on all HTTP endpoints.
If you work in a company that develops shared libraries, or if you work on an open-source or commercial library, you might want to develop your own auto-configuration. Auto-configuration classes can be bundled in external jars and still be picked-up by Spring Boot.
Spring Boot includes a number of additional features to help you monitor and manage your application when it’s pushed to production.
",", , , "
"   Rename Method,","Generate commands to include properties on default configuration file Develop new commands to include Spring Boot or others properties on default application config file By default this configuration file will be located on  but it could be modified using Spring Roo Shell Configuration features These commands should delegate on ApplicationConfigService 
These commands should take in mind force parameter These commands should take in mind profile parameter",", "
"   Rename Method,",Update Repository JPA commands Update Repository JPA commands to support new Spring Boot project structure. Check starter-data-jpa instead of persistence Doesn't include configuration files,", "
"   Move Class,Move Method,Move Attribute,","Generate repositories for read Only and read And Write entities When developer tries to generate new repository using repository commands, he needs to define for which entity wants to generate the repository. Take in mind that this entity could be  read Only or read And Write ReadOnly entities:
Roo generates an interface Entity Custom Repository to include dynamic queries.
Roo creates an empty implementation of previous interface that extends of Query Dsl Repository Support.
Roo generates an interface called Read Only Repository  that extends Repository like the following
 Roo creates an interface that extends interface Entity Custom Repository and Read Only Repository Will be annotated with email protected
Read And Write entities Roo generates an interface EntityCustomRepository to include dynamic queries.
Roo creates an empty implementation of previous interface that extends of Query Dsl Repository Support.
Roo generates an interface that extends Entity Custom Repository and JpaRepository Will be annotated with email protected
",", , , "
"   Rename Method,","Generate repositories for read Only and read And Write entities When developer tries to generate new repository using repository commands, he needs to define for which entity wants to generate the repository. Take in mind that this entity could be  read Only or read And Write ReadOnly entities:
Roo generates an interface Entity Custom Repository to include dynamic queries.
Roo creates an empty implementation of previous interface that extends of Query Dsl Repository Support.
Roo generates an interface called Read Only Repository  that extends Repository like the following
 Roo creates an interface that extends interface Entity Custom Repository and Read Only Repository Will be annotated with email protected
Read And Write entities Roo generates an interface EntityCustomRepository to include dynamic queries.
Roo creates an empty implementation of previous interface that extends of Query Dsl Repository Support.
Roo generates an interface that extends Entity Custom Repository and JpaRepository Will be annotated with email protected
",", "
"   Move Class,Rename Method,","Remove Active Record support on Spring Roo finders When developer tries to generate new repository using repository commands, he needs to define for which entity wants to generate the repository. Take in mind that this entity could be  read Only or read And Write ReadOnly entities:
Roo generates an interface Entity Custom Repository to include dynamic queries.
Roo creates an empty implementation of previous interface that extends of Query Dsl Repository Support.
Roo generates an interface called Read Only Repository  that extends Repository like the following
 Roo creates an interface that extends interface Entity Custom Repository and Read Only Repository Will be annotated with email protected
Read And Write entities Roo generates an interface EntityCustomRepository to include dynamic queries.
Roo creates an empty implementation of previous interface that extends of Query Dsl Repository Support.
Roo generates an interface that extends Entity Custom Repository and JpaRepository Will be annotated with email protected
",", "
"   Rename Method,","Remove Active Record support on Spring Roo finders Active record support was removed on Update finders generation to new Spring Data repositories system Update finders names to use the following Spring Data nomenclature:
",", "
"   Rename Method,","Refactoring of ""service"" commands offers some commands to manage service layer Generates new services for all entities of current project Generates new service for an specific entity Update these command and its parameters to follow Spring Roo analysis New commands should be like the following Generates new services for all entities of current project


",", "
"   Move Class,Move Method,Move Attribute,","Improve multimodule project generation Now a days, multimodule project generation is a little bit limited. Update current process to be able to generate better multimodule projects using Spring Roo Shell.",", , , "
"   Move Class,Move Method,Move Attribute,",Generate DTOs using Spring Roo commands Add new feature to Spring Roo shell to provide necessary commands to generate data transfer objects inside generated project Also update field commands to be able to add new fields inside DTOs and update finder commands to be able to change return type of generated finders to use DTOs.,", , , "
"   Move And Rename Class,Rename Method,Move Method,Extract Method,Move Attribute,",Include Spring MVC using Spring Boot starters Include Spring MVC library using Spring Boot starters Update commands and related items to be able to install Spring MVC on generated project using Spring Boot autoconfiguration features.,"Duplicated Code, Long Method, , , , "
"   Rename Method,Move Method,",Generate Controllers with different responseTypes Generate new Controllers inside current project Add parameter on provided command to allow developers to select the responseType for that new controller,", , "
"   Rename Method,",Generate Controllers with different responseTypes Generate new Controllers inside current project Add parameter on provided command to allow developers to select the responseType for that new controller,", "
"   Rename Method,",Generate Controllers with different responseTypes Generate new Controllers inside current project Add parameter on provided command to allow developers to select the responseType for that new controller,", "
"   Rename Method,",Generate Controllers with different responseTypes Generate new Controllers inside current project Add parameter on provided command to allow developers to select the responseType for that new controller,", "
"   Rename Method,",Generate Formatters Generate formatters inside Spring Roo project These formatters will be used on presentation layer to format registered entities.,", "
"   Move Class,Move Method,Move Attribute,",Generate Formatters Generate formatters inside Spring Roo project These formatters will be used on presentation layer to format registered entities.,", , , "
"   Rename Method,",Generate Formatters Generate formatters inside Spring Roo project These formatters will be used on presentation layer to format registered entities.,", "
"   Rename Method,","Generate views files using FreeMarker template engine Now a days, Spring Roo generates views files with an static format and structure. This format and structure is not editable by developer, so developer doesn't take any decision about view generation should provide system based on template engine that allows developers to customize view generation with its own content, format and structure. Free Marker will be the first template engine included on Spring Roo shell. This template engine will read developers custom templates and it will process them to generate final views .",", "
"   Rename Method,Pull Up Method,Extract Method,","Generate views files using FreeMarker template engine Now a days, Spring Roo generates views files with an static format and structure. This format and structure is not editable by developer, so developer doesn't take any decision about view generation should provide system based on template engine that allows developers to customize view generation with its own content, format and structure. Free Marker will be the first template engine included on Spring Roo shell. This template engine will read developers custom templates and it will process them to generate final views .","Duplicated Code, Long Method, , Duplicated Code, "
"   Rename Method,","Use JQuery Datatables on list views Spring Roo shell will use FreeMarker template engine to generate views. Include all necessary elements and configuration to be able use JQuery Datatables components on list views instead of static tables.

",", "
"   Rename Method,","Include dependency with database driver on repository modules Spring Roo includes a maven dependency with the database driver on application modules during  execution. However, is necessary to include this driver in every modules that contains a repository when command is executed. This dependency will be included for test purposes, so it should be included using the scope

",", "
"   Rename Method,","Create FieldCreatorProvider implementation for Embeddable classes To allow the addition of new fields to Embeddable classes is necessary to create new implementation of the Field Creator Provider interface that works with Embeddable classes and cover Embeddable classes requirements.

",", "
"   Rename Method,","Update addon-dto to generate ""Projection"" classes Update the addon addon-dto included on Spring Roo  to register a new command that separates Projections generation and DTO generation.",", "
"   Rename Method,",Update finder commands for using DTO's and Projections Update finder command so finders can return Entities and Projections and/or receive DTO's and Entities.,", "
"   Rename Method,",Implement command to generates detail controllers Implement command to generates detail controllers independlty of  web mvc controller command,", "
"   Rename Method,Move Method,Extract Method,",Create views for 'web mvc finder' command Implement new views for using finders in web layer Each finder will have two views Form view to make the request List view to show the response.,"Duplicated Code, Long Method, , , "
"   Rename Method,",Improve relationship management model level Improve entity relationship management,", "
"   Rename Method,",Improve relationship management model level Improve entity relationship management,", "
"   Rename Method,Extract Method,",Improve relationship management model level Improve entity relationship management,"Duplicated Code, Long Method, , "
"   Move And Rename Class,Move Method,Move Attribute,",Improve relationship management model level Improve entity relationship management,", , , "
"   Rename Method,",Include Spring Security Include Spring Security on the generated projects using Spring Roo Shell Provide two different ways to use security Security using auto configuration provided by Spring Boot Security using domain model,", "
"   Rename Method,Extract Method,",Include Spring Security Include Spring Security on the generated projects using Spring Roo Shell Provide two different ways to use security Security using auto configuration provided by Spring Boot Security using domain model,"Duplicated Code, Long Method, , "
"   Move And Rename Class,Move Method,Move Attribute,",Include Spring Security Include Spring Security on the generated projects using Spring Roo Shell Provide two different ways to use security Security using auto configuration provided by Spring Boot Security using domain model,", , , "
"   Rename Method,Extract Method,",Improve relationship management repository level Improve entity relationship management,"Duplicated Code, Long Method, , "
"   Move Method,Extract Method,",Improve relationship management repository level Improve entity relationship management.,"Duplicated Code, Long Method, , , "
"   Rename Method,",Improve I18n commands Improve  commands to be able to select a default language for the current application.,", "
"   Rename Method,Move Method,Extract Method,Move Attribute,",Move Finder into JPA repository Move finders feature into Repository JPA add on as current implementation just support this type of repository. Also makes easier manage generation.,"Duplicated Code, Long Method, , , , "
"   Move Class,Move Method,Extract Method,Move Attribute,",Move Finder into JPA repository Move finders feature into Repository JPA add on as current implementation just support this type of repository. Also makes easier manage generation.,"Duplicated Code, Long Method, , , , "
"   Rename Method,",Move Finder into JPA repository Move finders feature into Repository JPA add on as current implementation just support this type of repository. Also makes easier manage generation.,", "
"   Rename Method,Extract Method,","Improve dependency and plugin management Now a days, when some developer includes a new dependency, is necessary to take in count if the project is multimodule or not If is multimodule, is necessary to include manually the dependency in the dependency Management pom element. The same problem with plugins, that need to be added in plugin Management After that, developers need to define the same dependency or plugin without version in the child modules. That process is really dangerous and could produce errors if some developer forgots to include the dependencies and plugins in both. Update operations add Dependency and add Build Plugin in Project Operations API","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,",Improve relationship management Controller level ,"Duplicated Code, Long Method, , "
"   Rename Method,Pull Up Method,Move Method,Extract Method,",Improve relationship management Controller level ,"Duplicated Code, Long Method, , , Duplicated Code, "
"   Rename Class,Rename Method,Move Method,Move Attribute,",Improve relationship management Controller level ,", , , "
"   Extract Method,Inline Method,",Improve relationship management Controller level ,"Duplicated Code, Long Method, , , "
"   Rename Method,",Add support to send and receive emails Add support to send and receive emails using the service Java Mail Sender and the project springlets,", "
"   Rename Class,Rename Method,",Improve relationship management View level ,", "
"   Rename Method,Move Method,Extract Method,Pull Up Attribute,",Improve relationship management View level ,"Duplicated Code, Long Method, , , Duplicated Code, "
"   Rename Method,Extract Method,",Improve relationship management View level ,"Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,Inline Method,",Improve relationship management View level ,"Duplicated Code, Long Method, , , "
"   Rename Method,Inline Method,",Improve relationship management View level ,", , "
"   Rename Method,",Add support to send and receive JMS messages Add support to send and receive JMS messages,", "
"   Rename Method,",New entity visualization support using a new format annotation Modify generated code to implement a new visualization system for entities in datatables and selectors.,", "
"   Rename Method,","All included fields should be private All included fields should be private and the methods should use accessor and mutator methods instead of use the field reference directly.
",", "
"   Rename Method,","Generated reports should only show projection fields when defined Report column builder is not building projection fields when selected, it shows all entity fields.",", "
"   Rename Method,Move Method,Extract Method,Move Attribute,","Change DoD and tests implementation to use EntityManager instead of Spring Data Repositories New DoD implementation, using a Factory for each entity to obtain instances Externalize configuration for DoD classes using a new configuration class Change unit test command operations to generate unit tests for entities which will test its relations Make Lazy loading for services in Deserializers to avoid loading of all services when testing controllers","Duplicated Code, Long Method, , , , "
"   Move Class,Move And Rename Class,Rename Method,Move Method,Move Attribute,","Change DoD and tests implementation to use EntityManager instead of Spring Data Repositories New DoD implementation, using a Factory for each entity to obtain instances Externalize configuration for DoD classes using a new configuration class Change unit test command operations to generate unit tests for entities which will test its relations Make Lazy loading for services in Deserializers to avoid loading of all services when testing controllers",", , , "
"   Move Class,Extract Interface,Move Method,Move Attribute,","Create new integration tests using latest Spring boot support Modify test integration operations to create new integration tests using latest changes in Spring Boot.

",", , , Large Class, "
"   Rename Method,Push Down Method,Extract Method,Push Down Attribute,",Unify javaBens aj files To improve compilation in big projects could be good if all addon javaBean metadata generates its methods in a unique file. The affected metadata are Roo To String Roo Equalsand Roo Java Bean,"Duplicated Code, Long Method, , , , "
"   Move Method,Extract Method,",Expose generated methods from the dynamic finder add-on to the controller and view artifacts Currently the dynamic finder add-on will integrate custom finder methods into domain objects but leave it to the developer to expose them through the controller to the view layer. This could be automated.,"Duplicated Code, Long Method, , , "
"   Rename Class,Rename Method,Inline Method,","Introspect an existing database to support simple automatic JPA entities Most real world enterprise applications are built in one of two ways Starting with the relational schema and then building the application on top of it Starting with the required web views and then building the application to support them It would be nice if Roo offered extended support for as many people have existing databases they wish to introspect and then ""web expose"". The usage scenario would be Roo commands like:
In this example database reverse would scan all tables in the localhost my db database and create an entity for each table. Unlike a normal entity, though, the fields would be added to a Entity Name Roo Reverse ITD The benefit of adding the fields into an ITD is ""database reverse can automatically remove or update the Entity Nam Roo Reverse with each subsequent execution This is very useful if the database is changing",", , "
"   Rename Method,","Introspect an existing database to support simple automatic JPA entities Most real world enterprise applications are built in one of two ways Starting with the relational schema and then building the application on top of it Starting with the required web views and then building the application to support them It would be nice if Roo offered extended support for as many people have existing databases they wish to introspect and then ""web expose"". The usage scenario would be Roo commands like:
In this example database reverse would scan all tables in the localhost my db database and create an entity for each table. Unlike a normal entity, though, the fields would be added to a Entity Name Roo Reverse ITD The benefit of adding the fields into an ITD is ""database reverse can automatically remove or update the Entity Nam Roo Reverse with each subsequent execution This is very useful if the database is changing",", "
"   Rename Method,Extract Method,","Introspect an existing database to support simple automatic JPA entities Most real world enterprise applications are built in one of two ways Starting with the relational schema and then building the application on top of it Starting with the required web views and then building the application to support them It would be nice if Roo offered extended support for as many people have existing databases they wish to introspect and then ""web expose"". The usage scenario would be Roo commands like:
In this example database reverse would scan all tables in the localhost my db database and create an entity for each table. Unlike a normal entity, though, the fields would be added to a Entity Name Roo Reverse ITD The benefit of adding the fields into an ITD is ""database reverse can automatically remove or update the Entity Nam Roo Reverse with each subsequent execution This is very useful if the database is changing","Duplicated Code, Long Method, , "
"   Move And Rename Class,","Introspect an existing database to support simple automatic JPA entities Most real world enterprise applications are built in one of two ways Starting with the relational schema and then building the application on top of it Starting with the required web views and then building the application to support them It would be nice if Roo offered extended support for as many people have existing databases they wish to introspect and then ""web expose"". The usage scenario would be Roo commands like:
In this example database reverse would scan all tables in the localhost my db database and create an entity for each table. Unlike a normal entity, though, the fields would be added to a Entity Name Roo Reverse ITD The benefit of adding the fields into an ITD is ""database reverse can automatically remove or update the Entity Nam Roo Reverse with each subsequent execution This is very useful if the database is changing",", "
"   Push Down Method,Move Method,Extract Method,Push Down Attribute,Move Attribute,","Introspect an existing database to support simple automatic JPA entities Most real world enterprise applications are built in one of two ways Starting with the relational schema and then building the application on top of it Starting with the required web views and then building the application to support them It would be nice if Roo offered extended support for as many people have existing databases they wish to introspect and then ""web expose"". The usage scenario would be Roo commands like:
In this example database reverse would scan all tables in the localhost my db database and create an entity for each table. Unlike a normal entity, though, the fields would be added to a Entity Name Roo Reverse ITD The benefit of adding the fields into an ITD is ""database reverse can automatically remove or update the Entity Nam Roo Reverse with each subsequent execution This is very useful if the database is changing","Duplicated Code, Long Method, , , , , , "
"   Rename Method,","Introspect an existing database to support simple automatic JPA entities Most real world enterprise applications are built in one of two ways Starting with the relational schema and then building the application on top of it Starting with the required web views and then building the application to support them It would be nice if Roo offered extended support for as many people have existing databases they wish to introspect and then ""web expose"". The usage scenario would be Roo commands like:
In this example database reverse would scan all tables in the localhost my db database and create an entity for each table. Unlike a normal entity, though, the fields would be added to a Entity Name Roo Reverse ITD The benefit of adding the fields into an ITD is ""database reverse can automatically remove or update the Entity Nam Roo Reverse with each subsequent execution This is very useful if the database is changing",", "
"   Rename Class,Rename Method,Move Method,Extract Method,Move Attribute,","Introspect an existing database to support simple automatic JPA entities Most real world enterprise applications are built in one of two ways Starting with the relational schema and then building the application on top of it Starting with the required web views and then building the application to support them It would be nice if Roo offered extended support for as many people have existing databases they wish to introspect and then ""web expose"". The usage scenario would be Roo commands like:
In this example database reverse would scan all tables in the localhost my db database and create an entity for each table. Unlike a normal entity, though, the fields would be added to a Entity Name Roo Reverse ITD The benefit of adding the fields into an ITD is ""database reverse can automatically remove or update the Entity Nam Roo Reverse with each subsequent execution This is very useful if the database is changing","Duplicated Code, Long Method, , , , "
"   Rename Method,","Introspect an existing database to support simple automatic JPA entities Most real world enterprise applications are built in one of two ways Starting with the relational schema and then building the application on top of it Starting with the required web views and then building the application to support them It would be nice if Roo offered extended support for as many people have existing databases they wish to introspect and then ""web expose"". The usage scenario would be Roo commands like:
In this example database reverse would scan all tables in the localhost my db database and create an entity for each table. Unlike a normal entity, though, the fields would be added to a Entity Name Roo Reverse ITD The benefit of adding the fields into an ITD is ""database reverse can automatically remove or update the Entity Nam Roo Reverse with each subsequent execution This is very useful if the database is changing",", "
"   Rename Method,Extract Method,","Introspect an existing database to support simple automatic JPA entities Most real world enterprise applications are built in one of two ways Starting with the relational schema and then building the application on top of it Starting with the required web views and then building the application to support them It would be nice if Roo offered extended support for as many people have existing databases they wish to introspect and then ""web expose"". The usage scenario would be Roo commands like:
In this example database reverse would scan all tables in the localhost my db database and create an entity for each table. Unlike a normal entity, though, the fields would be added to a Entity Name Roo Reverse ITD The benefit of adding the fields into an ITD is ""database reverse can automatically remove or update the Entity Nam Roo Reverse with each subsequent execution This is very useful if the database is changing","Duplicated Code, Long Method, , "
"   Move And Rename Class,Rename Method,Move Method,Extract Method,","Introspect an existing database to support simple automatic JPA entities Most real world enterprise applications are built in one of two ways Starting with the relational schema and then building the application on top of it Starting with the required web views and then building the application to support them It would be nice if Roo offered extended support for as many people have existing databases they wish to introspect and then ""web expose"". The usage scenario would be Roo commands like:
In this example database reverse would scan all tables in the localhost my db database and create an entity for each table. Unlike a normal entity, though, the fields would be added to a Entity Name Roo Reverse ITD The benefit of adding the fields into an ITD is ""database reverse can automatically remove or update the Entity Nam Roo Reverse with each subsequent execution This is very useful if the database is changing","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Introspect an existing database to support simple automatic JPA entities Most real world enterprise applications are built in one of two ways Starting with the relational schema and then building the application on top of it Starting with the required web views and then building the application to support them It would be nice if Roo offered extended support for as many people have existing databases they wish to introspect and then ""web expose"". The usage scenario would be Roo commands like:
In this example database reverse would scan all tables in the localhost my db database and create an entity for each table. Unlike a normal entity, though, the fields would be added to a Entity Name Roo Reverse ITD The benefit of adding the fields into an ITD is ""database reverse can automatically remove or update the Entity Nam Roo Reverse with each subsequent execution This is very useful if the database is changing","Duplicated Code, Long Method, , "
"   Move Class,Rename Method,Move Method,Move Attribute,","Introspect an existing database to support simple automatic JPA entities Most real world enterprise applications are built in one of two ways Starting with the relational schema and then building the application on top of it Starting with the required web views and then building the application to support them It would be nice if Roo offered extended support for as many people have existing databases they wish to introspect and then ""web expose"". The usage scenario would be Roo commands like:
In this example database reverse would scan all tables in the localhost my db database and create an entity for each table. Unlike a normal entity, though, the fields would be added to a Entity Name Roo Reverse ITD The benefit of adding the fields into an ITD is ""database reverse can automatically remove or update the Entity Nam Roo Reverse with each subsequent execution This is very useful if the database is changing",", , , "
"   Move Class,Extract Method,","Introspect an existing database to support simple automatic JPA entities Most real world enterprise applications are built in one of two ways Starting with the relational schema and then building the application on top of it Starting with the required web views and then building the application to support them It would be nice if Roo offered extended support for as many people have existing databases they wish to introspect and then ""web expose"". The usage scenario would be Roo commands like:
In this example database reverse would scan all tables in the localhost my db database and create an entity for each table. Unlike a normal entity, though, the fields would be added to a Entity Name Roo Reverse ITD The benefit of adding the fields into an ITD is ""database reverse can automatically remove or update the Entity Nam Roo Reverse with each subsequent execution This is very useful if the database is changing","Duplicated Code, Long Method, , "
"   Rename Class,Move And Rename Class,Rename Method,","Introspect an existing database to support simple automatic JPA entities Most real world enterprise applications are built in one of two ways Starting with the relational schema and then building the application on top of it Starting with the required web views and then building the application to support them It would be nice if Roo offered extended support for as many people have existing databases they wish to introspect and then ""web expose"". The usage scenario would be Roo commands like:
In this example database reverse would scan all tables in the localhost my db database and create an entity for each table. Unlike a normal entity, though, the fields would be added to a Entity Name Roo Reverse ITD The benefit of adding the fields into an ITD is ""database reverse can automatically remove or update the Entity Nam Roo Reverse with each subsequent execution This is very useful if the database is changing",", "
"   Rename Method,Extract Method,","Introspect an existing database to support simple automatic JPA entities Most real world enterprise applications are built in one of two ways Starting with the relational schema and then building the application on top of it Starting with the required web views and then building the application to support them It would be nice if Roo offered extended support for as many people have existing databases they wish to introspect and then ""web expose"". The usage scenario would be Roo commands like:
In this example database reverse would scan all tables in the localhost my db database and create an entity for each table. Unlike a normal entity, though, the fields would be added to a Entity Name Roo Reverse ITD The benefit of adding the fields into an ITD is ""database reverse can automatically remove or update the Entity Nam Roo Reverse with each subsequent execution This is very useful if the database is changing","Duplicated Code, Long Method, , "
"   Rename Method,Move Attribute,","Introspect an existing database to support simple automatic JPA entities Most real world enterprise applications are built in one of two ways Starting with the relational schema and then building the application on top of it Starting with the required web views and then building the application to support them It would be nice if Roo offered extended support for as many people have existing databases they wish to introspect and then ""web expose"". The usage scenario would be Roo commands like:
In this example database reverse would scan all tables in the localhost my db database and create an entity for each table. Unlike a normal entity, though, the fields would be added to a Entity Name Roo Reverse ITD The benefit of adding the fields into an ITD is ""database reverse can automatically remove or update the Entity Nam Roo Reverse with each subsequent execution This is very useful if the database is changing",", , "
"   Extract Method,Inline Method,","Introspect an existing database to support simple automatic JPA entities Most real world enterprise applications are built in one of two ways Starting with the relational schema and then building the application on top of it Starting with the required web views and then building the application to support them It would be nice if Roo offered extended support for as many people have existing databases they wish to introspect and then ""web expose"". The usage scenario would be Roo commands like:
In this example database reverse would scan all tables in the localhost my db database and create an entity for each table. Unlike a normal entity, though, the fields would be added to a Entity Name Roo Reverse ITD The benefit of adding the fields into an ITD is ""database reverse can automatically remove or update the Entity Nam Roo Reverse with each subsequent execution This is very useful if the database is changing","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Introspect an existing database to support simple automatic JPA entities Most real world enterprise applications are built in one of two ways Starting with the relational schema and then building the application on top of it Starting with the required web views and then building the application to support them It would be nice if Roo offered extended support for as many people have existing databases they wish to introspect and then ""web expose"". The usage scenario would be Roo commands like:
In this example database reverse would scan all tables in the localhost my db database and create an entity for each table. Unlike a normal entity, though, the fields would be added to a Entity Name Roo Reverse ITD The benefit of adding the fields into an ITD is ""database reverse can automatically remove or update the Entity Nam Roo Reverse with each subsequent execution This is very useful if the database is changing","Duplicated Code, Long Method, , "
"   Rename Method,Inline Method,",JSF  Addon Create a Roo addon for JSF views with Facelets templates instead of JSPs.,", , "
"   Rename Method,",JSF  Addon Create a Roo addon for JSF views with Facelets templates instead of JSPs.,", "
"   Rename Method,",JSF  Addon Create a Roo addon for JSF views with Facelets templates instead of JSPs.,", "
"   Rename Method,Extract Method,Inline Method,",JSF  Addon Create a Roo addon for JSF views with Facelets templates instead of JSPs.,"Duplicated Code, Long Method, , , "
"   Move Class,Inline Method,",JSF  Addon Create a Roo addon for JSF views with Facelets templates instead of JSPs.,", , "
"   Rename Method,",JSF  Addon Create a Roo addon for JSF views with Facelets templates instead of JSPs.,", "
"   Move Class,Move Method,Extract Method,",JSF  Addon Create a Roo addon for JSF views with Facelets templates instead of JSPs.,"Duplicated Code, Long Method, , , "
"   Rename Method,Inline Method,",JSF  Addon Create a Roo addon for JSF views with Facelets templates instead of JSPs.,", , "
"   Rename Method,",JSF  Addon Create a Roo addon for JSF views with Facelets templates instead of JSPs.,", "
"   Rename Method,",JSF  Addon Create a Roo addon for JSF views with Facelets templates instead of JSPs.,", "
"   Rename Method,",JSF  Addon Create a Roo addon for JSF views with Facelets templates instead of JSPs.,", "
"   Rename Method,",JSF  Addon Create a Roo addon for JSF views with Facelets templates instead of JSPs.,", "
"   Rename Method,",JSF  Addon Create a Roo addon for JSF views with Facelets templates instead of JSPs.,", "
"   Rename Method,Extract Method,",JSF  Addon Create a Roo addon for JSF views with Facelets templates instead of JSPs.,"Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,Inline Method,",JSF  Addon Create a Roo addon for JSF views with Facelets templates instead of JSPs.,"Duplicated Code, Long Method, , , "
"   Rename Method,Move Method,Extract Method,",JSF  Addon Create a Roo addon for JSF views with Facelets templates instead of JSPs.,"Duplicated Code, Long Method, , , "
"   Rename Method,",JSF  Addon Create a Roo addon for JSF views with Facelets templates instead of JSPs.,", "
"   Rename Method,Pull Up Method,Move Method,",JSF  Addon Create a Roo addon for JSF views with Facelets templates instead of JSPs.,", , Duplicated Code, "
"   Rename Method,",JSF  Addon Create a Roo addon for JSF views with Facelets templates instead of JSPs.,", "
"   Rename Method,Move Method,Move Attribute,",JSF  Addon Create a Roo addon for JSF views with Facelets templates instead of JSPs.,", , , "
"   Rename Method,",JSF  Addon Create a Roo addon for JSF views with Facelets templates instead of JSPs.,", "
"   Rename Method,",JSF  Addon Create a Roo addon for JSF views with Facelets templates instead of JSPs.,", "
"   Rename Method,",JSF  Addon Create a Roo addon for JSF views with Facelets templates instead of JSPs.,", "
"   Rename Method,",JSF  Addon Create a Roo addon for JSF views with Facelets templates instead of JSPs.,", "
"   Rename Method,",JSF  Addon Create a Roo addon for JSF views with Facelets templates instead of JSPs.,", "
"   Rename Class,Inline Method,",JSF  Addon Create a Roo addon for JSF views with Facelets templates instead of JSPs.,", , "
"   Rename Method,","Reduce duplicate reference data code in generated controllers Imagine a domain in which a Car has a Make, Model, and Colour In the generated Car Controller Roo Controller file, the createForm create updateForm and update methods all contain the same code for populating the form's reference data This violation of the means that if you want to tweak what reference data goes into the model for example only offer non-metallic colours you have to ""push in"" all four of those public methods. If Roo could instead generate a resuable private addReferenceData Model Map method that was called by those four methods, then a developer wanting to tweak the reference data need only push in that one private method. In the above example, it would look like this by default and of course the four public methods listed above would also need to be changed to for example

",", "
"   Rename Method,","Improvements for addReferenceData() method in genrated Controllers Instead of one add Reference Data method, that adds all reference data, there could by one mehtod for each reference data. Those methods don't have to be called by the other controller methods, but could just be annotated with Model Attribute, like in the PetClinic example This would allow for better customization of how to get reference data. For example, sometimes it is not appropriate to use ""findAll"" for some reference data, but to do something like this This kind of customization is not possible with the current add Reference Data method.Another issue is, that the adReferencData method seems to get too much data For example, for a Pet entity I always get Why does a Pet Controller need all Pets as reference data? Usually, there is no Pet select box in a pet form




",", "
"   Rename Method,","Add integration with enum types I would d love to be able do something like this:

 new java enum -name ~.domain.EntreeOption
 add value CHICKEN
 add value STEAK
 add value VEGETARIAN
 new persistent class jpa domain Guest
 add field enum jpa entreeOption -type domain.EntreeOption

This would create the enum EntreeOption, the JPA entity Guest and add a field in Guest of type EntreeOption:


",", "
"   Push Down Method,Push Down Attribute,",Convert EntityMetadataProvider to interface to allow exposure as OSGi service Convert Entity Metadata Provider to interface to allow exposure as OSGi service,", , , "
"   Move Method,Move Attribute,",Convert WebScaffoldMetadataProvider to interface to allow exposure as OSGi service Convert WebScaffoldMetadataProvider to interface to allow exposure as OSGi service,", , , "
"   Rename Method,Extract Method,",Google Web Toolkit GWT Integration Roo should have support for GWT,"Duplicated Code, Long Method, , "
"   Extract Method,Inline Method,",Google Web Toolkit GWT Integration Roo should have support for GWT,"Duplicated Code, Long Method, , , "
"   Rename Method,",Google Web Toolkit GWT Integration Roo should have support for GWT,", "
"   Move Class,Push Down Method,Push Down Attribute,","Table tags should allow for nested column tags to define desired column names and labels in scaffolded MVC JSP views Table tags should allow for nested column tags to define desired column names and labels in scaffolded MVC JSP views

Current use
Proposed use
",", , , "
"   Rename Method,","Review templating approach to be more flexible and allow better custom branding of generated applications The generated view artifacts do currently allow for customization of the generated application via CSS, the header jsp and the footer jsp but there are some opportunities to simplify end-user branding.",", "
"   Rename Method,Extract Method,","Review templating approach to be more flexible and allow better custom branding of generated applications The generated view artifacts do currently allow for customization of the generated application via CSS, the header jsp and the footer jsp but there are some opportunities to simplify end-user branding.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,",Roo addons which manipulate web.xml should use the new WebXmlUtils Roo addons which manipulate web.xml should use the new WebXmlUtils ,"Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","New view-controller command As a user, I'd like a view-controller command to generate mvc:view-controller elements that select views for rendering.

Proposed syntax:
viewName is required.
path is optional.

When run, a mvc:view-controller element should be added to . The view template should be added to 

Example:","Duplicated Code, Long Method, , "
"   Move Method,Extract Method,",Refactor addons dependencies xml files to be more generic so to allow maven plugins  repositories  plugin repositories and properties Addons with a dependencies xml file will be changed to configuration xml and other maven artifacts will be added such as repositories,"Duplicated Code, Long Method, , , "
"   Move Method,Move Attribute,","Add tab support for the ""hint"" command Please add tab support for the ""hint"" command.

I noticed that if I type ""hint con[tab]"", nothing happens. It would be nice to have tab expansion using hint.
",", , , "
"   Rename Method,Extract Method,","Add History support to the generated GWT scaffold app Add History support to the generated GWT scaffold app.

Absolute stop-ship for M2. We are not an ajax app without this, we are not crawlable without this, and our entire architecture is extremely suspect.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","setup git/svn when building new project When a user creates a new project then with an optional switch a local git repo could be created.

This git repo can be used to store the state of all the actions performed on a roo script. A user could do it by hand as well, but this option would make this best practice of storing things in repo easier. Then with each command if it changes files an git commit could be done with relevant comment

if when command is : field string blah (git can store adding field blah to entity Blah
All of this can be done manually with some effort. Doing this can make easy to implement an undo functionality, ie going a few steps back and branching from that step and getting rid of the changes done from there on.","Duplicated Code, Long Method, , "
"   Move Method,Move Attribute,","Refactor Support module to better support non-OSGi usage requirements Presently the ""support"" module depends on OSGi JARs. There are only two methods in the support module that actually require these JARs. Because all other Roo modules depend on the support module, it is desirable that these two methods be relocated into a dedicated ""support-osgi"" module. That way non-OSGi usage requirements are better supported.

The old ""support"" module will extend osgi-bundle, as it should still package as an OSGi bundle. The new ""support-osgi"" module will also extend osgi-bundle, as it does not require any special SCR/Roo Maven plugin services",", , , "
"   Rename Method,Push Down Method,Push Down Attribute,","Refactor shell modules to better support non-OSGi usage requirements Currently the ""shell"" and ""shell-jline"" modules depend on OSGi JARs. This limits their usability outside an OSGi container.

This task is to separate the OSGi usage into dedicated modules.

To assist STS integration, the OSGi usage will be broken into two different modules. ""shell-osgi"" will provide OSGi services for ""shell"" types such as SimpleParser and the various Converter implementations. The ""shell-jline-osgi"" module will provide OSGi services for the ""shell-jline"" project only. This makes it simpler for STS, as it will simply not load the ""shell-jline"" and ""shell-jline-osgi"" modules.",", , , "
"   Rename Method,","Require explicit enabling of targetUrlParameter parameter in AbstractAuthenticationTargetUrlRequestHandler It's possible that an AuthenticationSuccessHandler or other class extending from this class may be used in scenarios where it isn't desirable to have a parameter used to determine the redirect location, though when it is invoked during the login request, this shouldn't be a problem. 
A user might create a subclass or use SimpleUrlAuthenticationSuccessHandler without realising that the parameter-based functionality exists. It would therefore probably be preferable to require that the functionality is explicitly enabled (as with the use of the ""referer"" header). 
",", "
"   Rename Method,Extract Method,","Support naming of filter chains in the namespace to allow for easier integration with external services like OAuth Since the namespace now supports multiple filter chains, this will allow an id to be added for each list of filters in the bean registry, which will make it easier for other services such as OAuth to specify which filter chain they want to integrate with. ","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,Extract Method,","Add Crypto module Import Keith's pull request for the crypto module code that is being used in Greenhouse, OAuth etc. ","Duplicated Code, Long Method, , "
"   Move Class,Move Method,Move Attribute,","Package Crypto classes with core requires that core depends on crypto which is counterintuitive. It would make more sense to move the crypto classes into core, even if we continue to supply the latter as a separate library. ",", , , "
"   Move Class,Pull Up Method,Extract Method,Pull Up Attribute,","hasPermission method in the AuthorizeTag For the moment, to check the permission on an object in JSP page, you can use the.
But, I think it would be a good idea to call method from the: 
where the book variable is provided from the page context.
Now, when you call method from, this throw a because the is not defined in the 
","Duplicated Code, Long Method, , Duplicated Code, Duplicated Code, "
"   Rename Method,","Excessive (and misleading) logging in DelegatingMethodSecurityMetadataSource If you switch on global method security Spring Security adds a custom pointcut matcher and delegates to the. This code in that class logs *every* method in ebery bean in the context 9as far as I can tell) whether or not it is going to be intercepted:
So 99.99% of these logs have attributes=[] (empty) which according to the matcher means it does not match.
Could the log level be changed to TRACE and also the message changed to ""Analyzing"" or ""Matching"" instead of ""Adding""? 
",", "
"   Extract Interface,Push Down Attribute,","Cannot override (protected) DefaultMethodSecurityExpressionHandler.createSecurityExpressionRoot There is some inconsistent visibility of the internals: is protected but it returns an instance of a package private class This would be fine except that then makes an assumption that the root is of this precise type, and therefore actually cannot be overridden (which would be quite useful). ",", , Large Class, "
"   Rename Method,",Support for Servlet 3.0/3.1 asynchronous request processing ,", "
"   Rename Method,","BasicAuthenticationFilter should not invoke on ERROR dispatch I have configured the following security element: I have set up custom error pages in my web.xml:
If for example an authentication fails the 401 page should be displayed. Now, if a client sends an invalid Basic value the filter chain finds that and calls the 401 page through Tomcat. Unfortunately, the filter chain proxy does not know that this is one request with an error forward only. It fires the entire chain again Now the maintains an property which tells him to observe checks only once. The does not have such a property which treats a  whole new request. The response gets reset twice and no output it written to the client.
I could set the error pages to security=""none"" but I want to maintain the security context on all pages whether they are protected or not.     
",", "
"   Rename Method,Extract Method,",Implement Servlet 3 HttpServletRequest Authentication Methods I will need to do some investigation to how to deal with the fact that often times Spring Security may commit the response when doing some of these items. ,"Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,",Spring Cache abstraction based UserCache implementation Spring has a cache abstraction since 3.1 would be nice if Spring Security took advantage of this where appropriate. ,"Duplicated Code, Long Method, , "
"   Rename Method,",Update BCrypt to be best practice The tutorial and JavaDoc should be updated to reflect that using BCrypt is the recommended best practice ,", "
"   Extract Superclass,Rename Method,Move Method,","Support Servlet 3.1's HttpServletRequest#changeSessionId() as alternate session fixation protection strategy The Servlet 3.1 specification, set to release in the next month or two, adds a new method to that supports changing the ID of a session as an approach to session fixation protection. This is a more ideal approach than Spring Security's current approach of creating a new session, copying all (or part) of the contents of the current session to the new session, then invalidating the old session. Using the method over Spring Security's current strategies would likely achieve a performance improvement, especially in a high-load scenario.
As a developer using Spring Security, I need SS to support calling as alternate session fixation protection strategy. I would be glad to implement this and submit the pull request, but it's a big change so I'd like to have a discussion and get some buy-in on my planned design before I set to work.
First: Timeline Considerations I don't know what the product roadmap is for Spring Security, but based on previous release cycles, I'm guessing you're targeting 3.2.0.M2 by March 1, M3 by April 1 and GA by May 3, putting it about 5 months behind Spring 3.2.0.GA (not saying that's a bad thing, just laying out some observations). If that timeline holds true, I'm guessing we can expect Spring Security 4.0 about this time next year and maybe a little later. I'd love to see support for before then.
Second: Technical Considerations
My belief in how this would technically work is that a new option would be added to the namespace attribute (currently ""none,"" and For argument's sake, let's call this new option ""."" I believe if the documentation clearly stated that this option required Servlet 3.1, we could implement it in SS 3.x. If the option was specified in an older Servlet environment, a configuration exception would be thrown. Then we could use reflection to access so that we didn't have to compile against Servlet 3.1.
Third: Design Considerations
I think the largest roadblock to implementing this in SS 3.x is the idea of design. Looking through the code and sketching some things out, I'm having a hard time imagining changing it without subtracting from the public interface of 
The problem is and The code that decides between these is already a little confusing. If we were to add the ""changeSessionId"" option to {{session-fixation-protection}}, we would now have to have a third property and do a mutual exclusion among them all (if we wanted to avoid subtracting from the public interface). The logic becomes sketchy and confusing and would make configuration more difficult.
The simpler alternative would be to remove the and completely and replace them with an Enum and a single property for that Enum. The Enum's options would be NEW_SESSION, MIGRATE_SESSION and CHANGE_SESSION_ID and those would map to ""newSession,"" and respectively.
The second option is ""the right way,"" IMO, but that is ignoring the fact that this is an existing software that people are using. The first option is more friendly because it doesn't break backwards compatibility. _However, the impact of the second option could be lessened by the fact that most people use the {{session-fixation-protection}} namespace attribute to configure this and don't bother with the class directly._ How many people actually use directly? That's a question I don't have the answer to.
An alternative third option would be to pick a suitable default (perhaps ""migrateSession,"" which is currently the default anyway) and deprecate as well, then change the documentation for and to say that they are now ignored and the default will be chosen if you don't use the Enum. This has the advantage of not breaking consuming projects' builds, but likewise has the disadvantage of masking when someone is using a method that isn't supported anymore (assuming they ignore compile warnings).

I know that occasionally changes interfaces between ?.x releases (I had compile errors when upgrading from Spring Framework 3.0 to 3.1). So the question is, how big of a deal is it if the interface of this class changes in 3.2? We could always wait for 4.0, but like I said, I reaaaalllly don't want to wait that long. :-)

Thoughts? 
",", , Duplicated Code, Large Class, "
"   Rename Method,","Handling Multiple AuthenticationEntryPoint defaults For example, we don't handle the following very well:
We should also handle cases where OAuth is involved. 
",", "
"   Rename Method,",http.authorizeUrls() to http.authorizeRequests() This is more descriptive of what is happening (we don't necessarily need to match on the URL) ,", "
"   Rename Class,Rename Method,","Add CSRF-related tags to the tag library The tag library could use a couple of new tags to better support the new CSRF features:
should insert a hidden form field with the correct name and value for the CSRF token. This will be very helpful to support those cases where, for whatever reason, you can't use Spring Framework's form tag library.
should insert the HTML meta tags recommended in the tutorial for holding the CSRF header name, form field name, and token value, for use in JavaScript code.
Additionally, these tags should abide by the feature specified in SPR-10916, assuming it is approved. 
",", "
"   Rename Class,Rename Method,","Add CSRF-related tags to the tag library The tag library could use a couple of new tags to better support the new CSRF features:
should insert a hidden form field with the correct name and value for the CSRF token. This will be very helpful to support those cases where, for whatever reason, you can't use Spring Framework's form tag library.
should insert the HTML meta tags recommended in the tutorial for holding the CSRF header name, form field name, and token value, for use in JavaScript code.
Additionally, these tags should abide by the feature specified in SPR-10916, assuming it is approved. 
",", "
"   Move Method,Move Attribute,",Provide Logical (Or And  Negated) RequestMatchers ,", , , "
"   Rename Method,",Update Java Configuration Samples to use @Autowired AuthenticationManagerBuilder This mechanism uses the Global so that it easier to share authentication with other configurations (i.e. and other instances). ,", "
"   Rename Method,","CsrfAuthenticationStrategy to add valid token to HTTP request after clearing the one in HTTP session clears the token stored in HTTP session after successful authentication:
But if the configured in the application is not a client redirect (HTTP 301/302) the token still stored in the request is invalid.
should store a new in current request after clearing the on in the session. Something like:
",", "
"   Rename Method,","Convert Java Config samples to thymeleaf and tiles JSPs are easier for users to get started with, but prevents reuse in the samples. is an older technology that has not had updates in quite some time and the additional filter causes more complex setup for Spring Security (save this for a specific lesson) ",", "
"   Rename Method,Inline Method,",SavedRequestAwareWrapper should not override cookies Motivation is described in ,", , "
"   Rename Method,","Support authorization rules by SimpMessageType The initial security only allows protecting a destination without taking into account the message type. When using a like STOMP subscriptions and messages should be protected independently: a user may be allowed to subscribe to a destination in order to receive messages, but not allowed to send messages to that destination. ",", "
"   Rename Class,Extract Method,","Consistency with ROLE_ Updated Description
Spring Security does not treat roles consistently. For example, the Java Configuration allows you to perform something like:
we must restate the ROLE_ even though it has already been expressed as a role.
h1. How should we improve this?
Anytime we see role elsewhere, it should automatically add the prefix of ""ROLE_"" if it is not already there (this helps things remain passive). So that means we should be able to do the following (which is the equivalent of the above example):
Implementation Details Expressions
To support expressions we will update the to automatically prefix hasRole and hasAnyRole with ""ROLE_"" if it does not exist. For example: To support JSR 250 we will update how the is parsed by updating much in the same way as we did with the expressions. This ensures that the {{Jsr250SecurityConfig}} is populated with an authority that is prefixed with ""ROLE_"" by default.
If developers return an Authentication or UserDetails that contains authorities that do NOT contain ""ROLE_"" as the prefix, the above changes introduce a non-passive change. What might this look like and how do we fix it?
Fixing Passivity The first option to fix things is to change our application to return authorities that are prefixed with ""ROLE_"". If we do that, then nothing else is needed. For example, with the in memory based solution we might do the following:
Obviously in a production system this would likely involve a data migration. Alternatively, you could update your UserDetailsService to prefix the roles programatically.
Option B (Update Configuration)
Another option is to reconfigure Spring Security. 
Create a BeanPostProcessor that looks like:
 Samples Restoring Passivity
You can find samples attached to this JIRA that demonstrate fixing passivity.
For XML refer to the differences between SEC-2785-xml.zip and SEC-2785-xml-4.0.zip
For Java Configuration refer to the differences between SEC-2785-jc.zip and SEC-2785-jc-4.0.zip
Just an observation I have when working with Spring Security. You can take it or leave it.
Different APIs use roles differently. What I mean is that certain parts require ROLE_USER, while other parts only require USER. Frankly, I am never sure which one to use and rely upon examples to deduce what. Look at the following:
I think we need to somehow clean this up. Probably a major, breaking affect on things, so I can't imagine such an improvement happening before a major release.
If this was scala, I would retool the APIs to either accept raw strings, which would be ""ROLE_USER"", or accept a case class like Role(""USER""), both meaning the same thing in the end. And then down the road, perhaps deprecating the plain strings.
But walking back to good ole Java, I'm not sure what the best design choice would be. This idea comes to mind:
Then with some static imports, 
A nice little static import would make this work. Just one idea. I imagine that requires sticking to strings. If people want to create their own prefixes in lieu of ROLE_, then perhaps they can extend some Spring Security piece of the API and inject it into their configuration. 
","Duplicated Code, Long Method, , "
"   Move Method,Move Attribute,","Add HttpStatusEntryPoint There doesn't appear to be an entry point that sends a 401 with an arbitrary WWW-Authenticate header. is nearly good enough, but it always sends ""Basic ..."" and that might not be what we are asking for. Example: a JavaScript client wants to detect the 401 without popping up a Basic auth dialog (which the browser will do automatically). ",", , , "
"   Rename Method,Extract Method,","Security HTTP Response Headers Configuration Cleanup Currently the mechanism for customizing HTTP Response Headers has a number of limitations. For example, if one wants to include all default headers but modify the X-Frame-Options to be SAMEORIGIN (i.e. for SockJS support), then they must duplicate a lot of configuration. For example, the Java Config would look like this:
The other issue is that it is not obvious that adding any element will remove all of the default headers. We should strive to be ""secure by default"". This change will make it so that the following can be used to include all of the default headers but modify X-Frame-Options to be SAMEORIGIN:
If users really want to disable the default headers it can be done explicitly. For example, the following would only include the ""X-Frame-Options: SAMEORIGIN"" header:
One can also disable a single default header. For example, the following will remove the ""X-Frame-Options"" and keep all of the other defaults:
","Duplicated Code, Long Method, , "
"   Extract Superclass,Rename Method,Move Method,Extract Method,","Add different mutable-object modes to runtime Currently, the runtime works strictly with mutable objects. That means that as few objects as possible (typically one or two) are reused for the data records all the time. Objects are cloned/restored, though, at various places to ensure that the contents is fresh at every call.

The rational behind this was to reduce pressure on the garbage collector. In fact, you can run programs where no garbage collection happens (if the UDFs are written to reuse objects as well).

It can, however, lead to bugs in not-carefully written user code.

I propose to add two modes to the runtime:
  - No-object-reuse (default) mode. New objects for every record. Safe but potentially slower.
  - Object-reusing mode - All objects are reused, without backup copies.. The UDFs must be careful to not keep any objects as state or not to modify the objects,



","Duplicated Code, Long Method, , , Duplicated Code, Large Class, "
"   Rename Method,",Make type() call in projections optional (or remove it) I think the type() call should be optional. The compiler can also cast the data set directly and the result type is computed from the input types anyways.,", "
"   Move Class,Move And Rename Class,Move Method,Move Attribute,","Add a ""mapPartition"" operator. Based on a pull request by Kay Fleischmann (https://github.com/apache/incubator-flink/pull/42)",", , , "
"   Move And Rename Class,Move Class,Rename Method,Move Method,Extract Method,","Type Extraction for Lambdas Lambdas currently work only for {{filter}} and {{reduce(a,b)}}, because Lambda type extraction is not in place right now.

We need to extend the type extraction for lambdas to support the other functions.","Duplicated Code, Long Method, , , "
"   Move Method,Extract Method,Move Attribute,"," Support function-level compatibility for  Hadoop's wrappers functions While the Flink wrappers for Hadoop Map and Reduce tasks are implemented in https://github.com/apache/incubator-flink/pull/37 it is currently not possible to use the {{HadoopMapFunction}} and the {{HadoopReduceFunction}} without a {{JobConf}}. It woule be useful if we could specify a Hadoop Mapper, Reducer (or Combiner) and use them as seperate components in a Flink Job.","Duplicated Code, Long Method, , , , "
"   Rename Class,Rename Method,"," Support function-level compatibility for  Hadoop's wrappers functions While the Flink wrappers for Hadoop Map and Reduce tasks are implemented in https://github.com/apache/incubator-flink/pull/37 it is currently not possible to use the {{HadoopMapFunction}} and the {{HadoopReduceFunction}} without a {{JobConf}}. It woule be useful if we could specify a Hadoop Mapper, Reducer (or Combiner) and use them as seperate components in a Flink Job.",", "
"   Rename Class,Extract Method,Move Attribute,","Add aggregations for streaming Add support for summation, mimimum and maximum aggregations based on the reduce functionality.","Duplicated Code, Long Method, , , "
"   Move And Rename Class,","Update Streaming examples to become self-contained Streaming examples do not follow the standard set by the recent examples refactor of the batch API.
TestDataUtil should be removed and Object[][] used to contain the example data.
Comments are also lacking in comparison with the batch counterpart.",", "
"   Rename Method,Move Method,Move Attribute,","Add a serial collection-based execution mode Summary of mailing list thread that the issue is based upon

Since Flink is a layered system, programs written against the APIs can be executed in a variety of ways. In this case, we just run the functions single-threaded directly on the Java collections, instead of firing up a memory management, IPC, parallel workers, data movement, etc. That gives programs a minimal execution footprint (like in the Java8 streams API) for small data. The idea is to enable users to use the same program in all sorts of different contexts.

The collection execution should sit below the common API, so both Java and Scala API can use it.",", , , "
"   Move Class,Move Method,Move Attribute,","Add a serial collection-based execution mode Summary of mailing list thread that the issue is based upon

Since Flink is a layered system, programs written against the APIs can be executed in a variety of ways. In this case, we just run the functions single-threaded directly on the Java collections, instead of firing up a memory management, IPC, parallel workers, data movement, etc. That gives programs a minimal execution footprint (like in the Java8 streams API) for small data. The idea is to enable users to use the same program in all sorts of different contexts.

The collection execution should sit below the common API, so both Java and Scala API can use it.",", , , "
"   Rename Method,Extract Method,",Add execute(String jobName) to StreamExecutionEnvironment This is the way to provide a name for the jobs and is is currently lacking for streaming jobs.,"Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","TypeInference on POJOs On Tuples, we currently use type inference that figures out the types of output type variables relative to the input type variable.

We need a similar functionality for POJOs.","Duplicated Code, Long Method, , "
"   Rename Method,",No createCollectionsEnvironment in Java API In the Scala API the ExecutionEnvironment has the method createCollectionEnvironment but not in the Java API. We should stick to one approach in both APIs.,", "
"   Extract Superclass,Rename Method,","Store broadcast variables once per TaskManager Currently, a copy of the broadcast variable is sent to each parallel task, rather than once to the TaskManager where the tasks share them.

Sharing the braodcast variables would increase memory efficiency significantly.",", Duplicated Code, Large Class, "
"   Rename Method,","[GitHub] Rework Configuration Objects Currently, the configurations are implemented hacky. Everything is represented as a serialized string and there is no clean interface, such that different flavors of configurations (global-, delegatin-, default) are inconsistent.

I propose to rework the configuration as a map of objects, which are serialized on demand with either a serialization library, or default serialization mechanisms. Factoring out the interface of a Configuration allows to keep all flavors consistent.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/12
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: enhancement, 
Created at: Mon Apr 29 23:43:11 CEST 2013
State: open
",", "
"   Rename Class,Rename Method,Extract Method,Inline Method,","Introduce TypeHints for Java API operators Due to type extraction issues with Java 8 Lambdas and many users with type erasure issues, TypeHints need to be introduced.

The whole discussion can be found on the mailing list:

http://mail-archives.apache.org/mod_mbox/flink-dev/201410.mbox/%3C544FA7D6.1070708@twalthr.com%3E","Duplicated Code, Long Method, , , "
"   Rename Class,Rename Method,Extract Method,","Generalize Flink's DistributedFileSystemClass to a Hadoop FileSystem wrapper Currently, the Hadoop's DistributedFileSystem class is somewhat hardwired into the Flink DistributedFileSystem class.
That is actually not necessary because Flink's DistributedFileSystem class is only assuming Hadoop's FileSystem class.

By generalizing the Flink DFS class, we can easily support other file systems implementing the Hadoop FS class, such as the GoogleHadoopFileSystem, Tachyon and others.","Duplicated Code, Long Method, , "
"   Rename Class,Push Down Attribute,","Make execution mode configurable As discussed in a PR: https://github.com/apache/incubator-flink/pull/227#discussion_r20788430.

The goal would be to make the execution mode configurable in order to easily configure closure cleaning, custom serializers, object reuse etc.

Configuration could be done either via 1) setters or 2) a configuration object.

I vote for 2).",", , "
"   Rename Class,Extract Superclass,Rename Method,Extract Method,","Make execution mode configurable As discussed in a PR: https://github.com/apache/incubator-flink/pull/227#discussion_r20788430.

The goal would be to make the execution mode configurable in order to easily configure closure cleaning, custom serializers, object reuse etc.

Configuration could be done either via 1) setters or 2) a configuration object.

I vote for 2).","Duplicated Code, Long Method, , Duplicated Code, Large Class, "
"   Rename Method,Move Method,Extract Method,Move Attribute,","Add option to Flink client to start a YARN session per job Currently, Flink users can only launch Flink on YARN as a ""YARN session"" (meaning a long-running YARN application that can run multiple Flink jobs)

Users have requested to extend the Flink Client to allocate YARN containers only for executing a single job.

As part of this pull request, I would suggest to refactor the YARN Client to make it more modular and object oriented.","Duplicated Code, Long Method, , , , "
"   Rename Class,Rename Method,Extract Method,","Make quoted String parsing optional and configurable for CSVInputFormats With the current implementation of the CSVInputFormat, quoted string parsing kicks in, if the first non-whitespace character of a field is a double quote. 

I see two issues with this implementation:
1. Quoted String parsing cannot be disabled
2. The quoting character is fixed to double quotes ("")

I propose to add parameters to disable quoted String parsing and set the quote character.","Duplicated Code, Long Method, , "
"   Move Class,Rename Class,Move And Rename Class,Rename Method,Move Method,Extract Method,Move Attribute,","Add an off-heap variant of the managed memory For (nearly) all memory that Flink accumulates (in the form of sort buffers, hash tables, caching), we use a special way of representing data serialized across a set of memory pages. The big work lies in the way the algorithms are implemented to operate on pages, rather than on objects.

The core class for the memory is the {{MemorySegment}}, which has all methods to set and get primitives values efficiently. It is a somewhat simpler (and faster) variant of a HeapByteBuffer.

As such, it should be straightforward to create a version where the memory segment is not backed by a heap byte[], but by memory allocated outside the JVM, in a similar way as the NIO DirectByteBuffers, or the Netty direct buffers do it.

This may have multiple advantages:
  - We reduce the size of the JVM heap (garbage collected) and the number and size of long living alive objects. For large JVM sizes, this may improve performance quite a bit. Utilmately, we would in many cases reduce JVM size to 1/3 to 1/2 and keep the remaining memory outside the JVM.
  - We save copies when we move memory pages to disk (spilling) or through the network (shuffling / broadcasting / forward piping)

The changes required to implement this are
  - Add a {{UnmanagedMemorySegment}} that only stores the memory adress as a long, and the segment size. It is initialized from a DirectByteBuffer.
  - Allow the MemoryManager to allocate these MemorySegments, instead of the current ones.
  - Make sure that the startup script pick up the mode and configure the heap size and the max direct memory properly.

Since the MemorySegment is probably the most performance critical class in Flink, we must take care that we do this right. The following are critical considerations:
  - If we want both solutions (heap and off-heap) to exist side-by-side (configurable), we must make the base MemorySegment abstract and implement two versions (heap and off-heap).
  - To get the best performance, we need to make sure that only one class gets loaded (or at least ever used), to ensure optimal JIT de-virtualization and inlining.
  - We should carefully measure the performance of both variants. From previous micro benchmarks, I remember that individual byte accesses in DirectByteBuffers (off-heap) were slightly slower than on-heap, any larger accesses were equally good or slightly better.
","Duplicated Code, Long Method, , , , "
"   Rename Class,Move And Rename Class,Rename Method,Move Method,Extract Method,Inline Method,","Rework Constant Field Annotations Constant field annotations are used by the optimizer to determine whether physical data properties such as sorting or partitioning are retained by user defined functions.

The current implementation is limited and can be extended in several ways:
- Fields that are copied to other positions
- Field definitions for non-tuple data types (Pojos)

There is a pull request (#83) that goes into this direction and which can be extended.","Duplicated Code, Long Method, , , , "
"   Rename Method,","Rework Constant Field Annotations Constant field annotations are used by the optimizer to determine whether physical data properties such as sorting or partitioning are retained by user defined functions.

The current implementation is limited and can be extended in several ways:
- Fields that are copied to other positions
- Field definitions for non-tuple data types (Pojos)

There is a pull request (#83) that goes into this direction and which can be extended.",", "
"   Rename Method,Pull Up Method,Extract Method,",The Pojo Serializers/Comparators fail when using Subclasses or Interfaces 0,"Duplicated Code, Long Method, , Duplicated Code, "
"   Rename Class,Extract Method,","Akka cleanups Currently, Akka has many different timeout values. From a user perspective, it would be helpful to deduce all different timeouts from a single timeout value. Additionally, the user should still be able to define specific values for the different timeouts.

Akka uses the akka.jobmanager.url config parameter to override the jobmanager address and the port in case of a local setup. This mechanism is not safe since it is exposed to the user. Thus, the mechanism should be replaced.

The notifyExecutionStateChange method allows objects to access the internal state of the TaskManager actor. This causes NullPointerExceptions when shutting down the actor. This method should be removed to avoid accessing the internal state of an actor by another object.

With the latest Akka changes, the TaskManager watches the JobManager in order to detect when it died or lost the connection to the TaskManager. This behaviour should be tested.","Duplicated Code, Long Method, , "
"   Move Class,Rename Method,Move Method,Move Attribute,","Automatically register nested types at Kryo Currently, the {{GenericTypeInfo}} registers the class of the type at Kryo. In order to get the best performance, it should recursively walk the classes and make sure that it registered all contained subtypes.",", , , "
"   Rename Class,Rename Method,",Typo fixes Fix some typos. Also fix some inconsistent uses of *partition operator* and *partitioning operator* in the codebase.,", "
"   Rename Method,",Add option to switch between Avro and Kryo serialization for GenericTypes Allow users to switch the underlying serializer for GenericTypes.,", "
"   Extract Method,Inline Method,","Change the Gelly examples to be consistent with the other Flink examples The current Gelly examples just work on default input data. 

If we look at the other Flink examples, e.g. Connected Components, they also allow input data to be read from a text file passed as a parameter to the main method. 
 
It would be nice to follow the same approach in our examples. A first step in that direction is the SSSP example. ","Duplicated Code, Long Method, , , "
"   Rename Method,","Add option to pass Configuration to LocalExecutor Right now its not possible for users to pass custom configuration values to Flink when running it from within an IDE.

It would be very convenient to be able to create a local execution environment that allows passing configuration files.",", "
"   Rename Class,Extract Method,","Remove window merge before flatten as an optimization After a Window Reduce or Map transformation there is always a merge step when the transformation was parallel or grouped.

This merge step should be removed when the windowing operator is followed by flatten to avoid unnecessary bottlenecks in the program.

This feature should be added as an optimization step to the WindowingOptimizer class.","Duplicated Code, Long Method, , "
"   Move Class,Move And Rename Class,Move Method,Move Attribute,","Rename Expression API and Operation Representation Right now the package is called flink-expressions and we refer to the API as the ""Expression API"". The equivalent to DataSet and DataStream is the ExpressionOperation. I'm not very happy with these names, so we should find something that is more marketable before making any big announcements.",", , , "
"   Move Class,Rename Method,Move Method,Extract Method,Move Attribute,","Add option to start Flink on YARN in a detached mode Right now, we expect the YARN command line interface to be connected with the Application Master all the time to control the ""yarn session"" or the job.

For very long running sessions or jobs users want to just ""fire and forget"" a job/session to YARN.
Stopping the session will still be possible using YARN's tools.

Also, prior to ""detaching"" itself, the CLI frontend could print the required command to kill the session as a convenience.","Duplicated Code, Long Method, , , , "
"   Rename Method,Pull Up Method,Extract Method,","Improve error handling when partitions not found When a result partition is released concurrently with a remote partition request, the request might come in late and result in an exception at the receiving task saying:

{code}
16:04:22,499 INFO  org.apache.flink.runtime.taskmanager.Task                     - CHAIN Partition -> Map (Map at testRestartMultipleTimes(SimpleRecoveryITCase.java:200)) (1/4) switched to FAILED : java.io.IOException: org.apache.flink.runtime.io.network.partition.queue.IllegalQueueIteratorRequestException at remote input channel: Intermediate result partition has already been released.].
	at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.checkIoError(RemoteInputChannel.java:223)
	at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.getNextBuffer(RemoteInputChannel.java:103)
	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.getNextBufferOrEvent(SingleInputGate.java:310)
	at org.apache.flink.runtime.io.network.api.reader.AbstractRecordReader.getNextRecord(AbstractRecordReader.java:75)
	at org.apache.flink.runtime.io.network.api.reader.MutableRecordReader.next(MutableRecordReader.java:34)
	at org.apache.flink.runtime.operators.util.ReaderIterator.next(ReaderIterator.java:59)
	at org.apache.flink.runtime.operators.NoOpDriver.run(NoOpDriver.java:91)
	at org.apache.flink.runtime.operators.RegularPactTask.run(RegularPactTask.java:496)
	at org.apache.flink.runtime.operators.RegularPactTask.invoke(RegularPactTask.java:362)
	at org.apache.flink.runtime.execution.RuntimeEnvironment.run(RuntimeEnvironment.java:205)
	at java.lang.Thread.run(Thread.java:745)
{code}","Duplicated Code, Long Method, , Duplicated Code, "
"   Rename Method,Extract Method,",Detect tumbling policies where trigger and eviction match The windowing api should automatically detect matching trigger and eviction policies so it can apply optimizations for tumbling policies.,"Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Refactor task registration/unregistration h4. Current control flow for task registrations

# JM submits a TaskDeploymentDescriptor to a TM
## TM registers the required JAR files with the LibraryCacheManager and returns the user code class loader
## TM creates a Task instance and registers the task in the runningTasks map
## TM creates a TaskInputSplitProvider
## TM creates a RuntimeEnvironment and sets it as the environment for the task
## TM registers the task with the network environment
## TM sends async msg to profiler to monitor tasks
## TM creates temporary files in file cache
## TM tries to start the task

If any operation >= 1.2 fails:
* TM calls task.failExternally()
* TM removes temporary files from file cache
* TM unregisters the task from the network environment
* TM sends async msg to profiler to unmonitor tasks
* TM calls unregisterMemoryManager on task

If 1.1 fails, only unregister from LibraryCacheManager.

h4. RuntimeEnvironment, Task, TaskManager separation

The RuntimeEnvironment has references to certain components of the task manager like memory manager, which are accecssed from the task. Furthermore it implements Runnable, and creates the executing task Thread. The Task instance essentially wraps the RuntimeEnvironment and allows asynchronous state management of the task (RUNNING, FINISHED, etc.).

The way that the state updates affect the task is not that obvious: state changes trigger messages to the TM, which for final states further trigger a msg to unregister the task. The way that tasks are unregistered again depends on the state of the task.

----

I would propose to refactor this to make the way the state handling/registration/unregistration is handled is more transparent.","Duplicated Code, Long Method, , , , , "
"   Move Method,Extract Method,Move Attribute,","Change the split between create/run of a vertex-centric iteration Currently, the vertex-centric API in Gelly looks like this:

{code:java}
Graph inputGaph = ... //create graph
VertexCentricIteration iteration = inputGraph.createVertexCentricIteration();
... // configure the iteration
Graph newGraph = inputGaph.runVertexCentricIteration(iteration);
{code}

We have this create/run split, in order to expose the iteration object and be able to call the public methods of VertexCentricIteration.

However, this is not very nice and might lead to errors, if create and run are  mistakenly called on different graph objects.

One suggestion is to change this to the following:

{code:java}
VertexCentricIteration iteration = inputGraph.createVertexCentricIteration();
... // configure the iteration
Graph newGraph = iteration.result();
{code}

or to go with a single run call, where we add an IterationConfiguration object as a parameter and we don't expose the iteration object to the user at all:

{code:java}
IterationConfiguration parameters  = ...
Graph newGraph = inputGraph.runVertexCentricIteration(parameters);
{code}

and we can also have a simplified method where no configuration is passed.

What do you think?
Personally, I like the second option a bit more.

-Vasia.","Duplicated Code, Long Method, , , , "
"   Rename Class,Rename Method,Move Method,Inline Method,Move Attribute,",Add test to Kafka streaming connector Add a test for Flink Streaming's Kafka connector using an integrated Kafka and Zookeeper server.,", , , , "
"   Rename Class,Move And Rename Class,Extract Interface,Rename Method,Move Method,",Add test to Kafka streaming connector Add a test for Flink Streaming's Kafka connector using an integrated Kafka and Zookeeper server.,", , Large Class, "
"   Move And Rename Class,Rename Method,Extract Method,Inline Method,","Add more tests for Kafka Connectors The current {{KafkaITCase}} is only doing a single test.

We need to refactor that test so that it brings up a Kafka/Zookeeper server and than performs various tests:

Tests to include:
- A topology with non-string types MERGED IN 359b39c3
- A topology with a custom Kafka Partitioning class MERGED IN 359b39c3
- A topology testing the regular {{KafkaSource}}. MERGED IN 359b39c3
- Kafka broker failure MERGED IN cb34e976
- Test with large records (up to 30 MB) MERGED IN 354922be
- Flink TaskManager failure


","Duplicated Code, Long Method, , , "
"   Rename Method,","Add more tests for Kafka Connectors The current {{KafkaITCase}} is only doing a single test.

We need to refactor that test so that it brings up a Kafka/Zookeeper server and than performs various tests:

Tests to include:
- A topology with non-string types MERGED IN 359b39c3
- A topology with a custom Kafka Partitioning class MERGED IN 359b39c3
- A topology testing the regular {{KafkaSource}}. MERGED IN 359b39c3
- Kafka broker failure MERGED IN cb34e976
- Test with large records (up to 30 MB) MERGED IN 354922be
- Flink TaskManager failure


",", "
"   Move And Rename Class,Extract Method,","Add more tests for Kafka Connectors The current {{KafkaITCase}} is only doing a single test.

We need to refactor that test so that it brings up a Kafka/Zookeeper server and than performs various tests:

Tests to include:
- A topology with non-string types MERGED IN 359b39c3
- A topology with a custom Kafka Partitioning class MERGED IN 359b39c3
- A topology testing the regular {{KafkaSource}}. MERGED IN 359b39c3
- Kafka broker failure MERGED IN cb34e976
- Test with large records (up to 30 MB) MERGED IN 354922be
- Flink TaskManager failure


","Duplicated Code, Long Method, , "
"   Move Class,Move Method,Extract Method,Move Attribute,","Add support for submitting single jobs to a detached YARN session We need tests ensuring that the processing slots are set properly when starting Flink on YARN, in particular with the per job YARN session feature.

Also, the YARN tests for detached YARN sessions / per job yarn clusters are polluting the local home-directory.","Duplicated Code, Long Method, , , , "
"   Rename Method,","Rename the function name from ""getCurrentyActiveConnections"" to ""getCurrentActiveConnections"" in  org.apache.flink.runtime.blob I think the function name ""getCurrentyActiveConnections"" in ' org.apache.flink.runtime.blob' is a wrong spelling, it should be ""getCurrentActiveConnections"" is more better, and also I add some comments about the function and the Tests.",", "
"   Rename Method,",Refactor streaming scala api to use returns for adding typeinfo Currently the streaming scala api uses transform to pass the extracted type information instead of .returns. This leads to a lot of code duplication.,", "
"   Rename Method,","Split SubmitTask method up into two phases: Receive TDD and instantiation of TDD A user reported that a job times out while submitting tasks to the TaskManager. The reason is that the JobManager expects a TaskOperationResult response upon submitting a task to the TM. The TM downloads then the required jars from the JM which blocks the actor thread and can take a very long time if many TMs download from the JM. Due to this, the SubmitTask future throws a TimeOutException.

A possible solution could be that the TM eagerly acknowledges the reception of the SubmitTask message and executes the task initialization within a future. The future will upon completion send a UpdateTaskExecutionState message to the JM which switches the state of the task from deploying to running. This means that the handler of SubmitTask future in {{Execution}} won't change the state of the task.",", "
"   Rename Class,Extract Superclass,Extract Method,","Add configuration options to Gelly-GSA Currently, it is not possible to configure a GSA iteration. Similarly to vertex-centric, we should allow setting the iteration name and degree of parallelism, aggregators, broadcast variables and whether the solution set is kept in unmanaged memory.
The docs should be updated accordingly.","Duplicated Code, Long Method, , Duplicated Code, Large Class, "
"   Rename Method,","Make python tests less verbose Currently, the python tests print a lot of log messages to stdout. Furthermore there seems to be some println statements which clutter the console output. I think that these log messages are not required for the tests and thus should be suppressed. ",", "
"   Rename Method,Extract Method,",#N/A,"Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Task Failures and Error Handling This is an issue to keep track of subtasks for error handling of task failures.

The ""design doc"" for this can be found here: https://cwiki.apache.org/confluence/display/FLINK/Task+Failures+and+Error+Handling","Duplicated Code, Long Method, , "
"   Rename Class,Move Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Introduce (Event)time in Streaming This requires introducing a timestamp in streaming record and a change in the sources to add timestamps to records. This will also introduce punctuations (or low watermarks) to allow windows to work correctly on unordered, timestamped input data. In the process of this, the windowing subsystem also needs to be adapted to use the punctuations. Furthermore, all operators need to be made aware of punctuations and correctly forward them. Then, a new operator must be introduced to to allow modification of timestamps.","Duplicated Code, Long Method, , , , , "
"   Pull Up Method,Extract Method,","Allowing users to decorate input streams Users may have to do unforeseeable operations on file input streams before they can be used by the actual input format logic, e.g., exotic compression formats or preambles such as byte order marks. Therefore, it would be useful to provide the user with a hook to decorate input streams in order to handle such issues.","Duplicated Code, Long Method, , Duplicated Code, "
"   Rename Class,Rename Method,","Integrate Flink with Apache Mesos There are some users asking for an integration of Flink into Mesos.

-There also is a pending pull request for adding Mesos support for Flink-: https://github.com/apache/flink/pull/251

Update (May '16):  a new effort is now underway, building on the recent ResourceManager work.

Update (Oct '16): the core functionality is in the master branch.   New sub-tasks track remaining work for a first release.

Design document:  ([google doc|https://docs.google.com/document/d/1WItafBmGbjlaBbP8Of5PAFOH9GUJQxf5S4hjEuPchuU/edit?usp=sharing])",", "
"   Rename Method,Move Method,Inline Method,","Rework examples to use ParameterTool In FLINK-1525, we introduced the {{ParameterTool}}.
We should port the examples to use the tool.

The examples could look like this (we should maybe discuss it first on the mailing lists):

{code}
public static void main(String[] args) throws Exception {
    ParameterTool pt = ParameterTool.fromArgs(args);
    boolean fileOutput = pt.getNumberOfParameters() == 2;
    String textPath = null;
    String outputPath = null;
    if(fileOutput) {
        textPath = pt.getRequired(""input"");
        outputPath = pt.getRequired(""output"");
    }

    // set up the execution environment
    final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();
    env.getConfig().setUserConfig(pt);
{code}",", , , "
"   Rename Class,Rename Method,Extract Method,","Add OuterJoin strategy with HashTable on inner side Flink does not natively support outer joins at the moment.
This issue proposes to implement a hash outer join algorithm that can cover left and right outer joins.

The implementation can be based on the regular hash join iterators (for example `ReusingBuildFirstHashMatchIterator` and `NonReusingBuildFirstHashMatchIterator`, see also `MatchDriver` class)

The Reusing and NonReusing variants differ in whether object instances are reused or new objects are created. I would start with the NonReusing variant which is safer from a user's point of view and should also be easier to implement.
","Duplicated Code, Long Method, , "
"   Pull Up Method,Pull Up Attribute,","Access the number of vertices from within the GSA functions Similarly to the Vertex-centric approach we would like to allow the user to access the number of vertices from the Gather, Sum and Apply functions respectively. This property will become available by setting [setOptNumVertices()] the numVertices option to true. 

The number of vertices can then be accessed in the gather, sum and apply functions using the getNumberOfVertices() method. If the option is not set in the configuration, this method will return -1. ",", Duplicated Code, Duplicated Code, "
"   Rename Class,Rename Method,",Add stateful Streaming Sequence Source 0,", "
"   Rename Method,",Fail YARN application on failed single-job YARN cluster Users find it confusing that jobs submitted in single-job YARN cluster mode leave the Flink YARN application in state SUCCEEDED after the job fails.,", "
"   Move Method,Move Attribute,","Make grouped reduce/fold/aggregations stateful using Partitioned state Currently the inner state of the grouped aggregations are not persisted as an operator state. 

These operators should be reimplemented to use the newly introduced partitioned state abstractions which will make them fault tolerant and scalable for the future.

A suggested implementation would be to use a stateful mapper to implement the desired behaviour.",", , , "
"   Rename Method,Move Method,Extract Method,","Add support for named streams in Storm compatibility layer Currently, the layer only works on single stream and ignores stream names, ie, each stream is treated as ""default"" stream. The declaration of multiple output streams is ignored (all tuples are emitted to the same stream). If multiple input streams are consumed all tuples are merged into a single stream.

This feature allows operators to declare multiple (named) output streams and emit tuples to different stream. Furthermore, it enables Bolts to distinguish incoming tuples from different streams by stream name (Storm tuple meta information).","Duplicated Code, Long Method, , , "
"   Rename Class,Extract Method,","Make Streaming File Sources Persistent Streaming File sources should participate in the checkpointing. They should track the bytes they read from the file and checkpoint it.

One can look at the sequence generating source function for an example of a checkpointed source.","Duplicated Code, Long Method, , "
"   Rename Method,",Rename OperatorState methods to .value() and .update(..) We should rename OperatorState methods to .value() and .update(..) from getState and updateState to make it more clear.,", "
"   Rename Class,Extract Interface,Rename Method,Extract Method,","Rework partitioned state storage Partitioned states are currently stored per-key in statehandles. This is alright for in-memory storage but is very inefficient for HDFS. 

The logic behind the current mechanism is that this approach provides a way to repartition a state without fetching the data from the external storage and only manipulating handles.

We should come up with a solution that can achieve both.","Duplicated Code, Long Method, , Large Class, "
"   Rename Method,Move Method,Extract Method,Inline Method,","Rework iteration construction in StreamGraph Currently the nodes representing the extra sinks and sources are incrementally added to the streamgraph when the user creates the iterative parts of the program.

This makes it difficult to enforce different partitioning schemes on the feedback edges and also makes it virtually impossible to handle more iteration heads with different parallelism.

The actual nodes in the streamgraph for the iteration sinks/sources should only be created when the program is finalized  after the user calls execute and before we create the jobgraph.","Duplicated Code, Long Method, , , , "
"   Move And Rename Class,Rename Method,Pull Up Method,Move Method,Pull Up Attribute,Move Attribute,","Implement Kafka connector using the new Kafka Consumer API Once Kafka has released its new consumer API, we should provide a connector for that version.
The release will probably be called 0.9 or 0.8.3.

The connector will be mostly compatible with Kafka 0.8.2.x, except for committing offsets to the broker (the new connector expects a coordinator to be available on Kafka). To work around that, we can provide a configuration option to commit offsets to zookeeper (managed by flink code).
For 0.9/0.8.3 it will be fully compatible.

It will not be compatible with 0.8.1 because of mismatching Kafka messages.
",", , , Duplicated Code, Duplicated Code, "
"   Rename Class,Move And Rename Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Decouple StreamGraph Building from the API Currently, the building of the StreamGraph is very intertwined with the API methods. DataStream knows about the StreamGraph and keeps track of splitting, selected names, unions and so on. This leads to the problem that is is very hard to understand how the StreamGraph is built because the code that does it is all over the place. This also makes it hard to extend/change parts of the Streaming system.

I propose to introduce ""Transformations"". A transformation hold information about one operation: The input streams, types, names, operator and so on. An API method creates a transformation instead of fiddling with the StreamGraph directly. A new component, the StreamGraphGenerator creates a StreamGraph from the tree of transformations that result from program specification using the API methods. This would relieve DataStream from knowing about the StreamGraph and makes unions, splitting, selection visible transformations instead of being scattered across the different API classes as fields.","Duplicated Code, Long Method, , , , , "
"   Rename Method,Extract Method,Pull Up Attribute,",[py] refactor PlanBinder/OperationInfo These two classes deserve a restructuring to become more readable and consistent with PythonPlanBinder/PythonOperationInfo.,"Duplicated Code, Long Method, , Duplicated Code, "
"   Rename Class,Move Class,Move Method,","Cleanup Gelly examples As per discussion in the dev@ mailing list, this issue proposes the following changes to the Gelly examples and library:

1. Keep the following examples as they are:
EuclideanGraphWeighing, GraphMetrics, IncrementalSSSP, JaccardSimilarity,  MusicProfiles.
2. Keep only 1 example to show how to use library methods.
3. Add 1 example for vertex-centric iterations.
4. Keep 1 example for GSA iterations and move the redundant GSA implementations to the library.
5. Improve the examples documentation and refer to the functionality that each of them demonstrates.
6. Port and modify existing example tests accordingly.",", , "
"   Move Method,Inline Method,Move Attribute,","Expose attemptNumber in RuntimeContext It would be nice to expose the attemptNumber of a task in the {{RuntimeContext}}. 
This would allow user code to behave differently in restart scenarios.",", , , , "
"   Move Class,Rename Class,Move Method,Extract Method,Move Attribute,","[py] Remove the need to specify types for transformations Currently, users of the Python API have to provide type arguments when using a UDF, like so:

{code}
d1.map(Mapper(), (INT, STRING))
{code}

Instead, it would be really convenient to be able to do this:

{code}
d1.map(Mapper())
{code}

The intention behind this issue is convenience, and it's also not really pythonic to specify types.

Before I'll go into possible solutions, let me summarize the way these type arguments are currently used, and in general how types are handled:

The type argument passed is actually an object of the type it represents, as INT is a constant int value, whereas STRING is a constant string value. You could as well write the following and it would still work.
{code}
d1.map(Mapper(), (1, ""ImNotATypInfo""))
{code}
This object is transmitted to the java side during the plan binding (and is now an actual Tuple2<Integer, String>), then passed to the type extractor, and the resulting TypeInformation saved in the java counterpart of the udf, which all implement the ResultTypeQueryable interface. 
The TypeInformation object is only used by the Java API, python never touches it. Instead, at runtime, the serializers used between python and java check the classes of the values passed and are thus generated dynamically.
This means that, if a UDF does not pass the type it claims to pass, the Python API wont complain, but the underlying java API will when it's serializers fail.

Now let's talk solutions.

In discussions on the mailing list, pretty much 2 proposals were made:
# Add a way to disable/circumvent type checks during the plan phase in the Java API and generate serializers dynamically.
# Have objects always in serialized form on the java side, stored in a single bytearray or Tuple2 containing a key/value pair.

These proposals vary wildly in the changes necessary to the system:
# ""How can we change the Java API to support this?""
This proposal would hardly change the way the Python API works, or even touch the related source code. It mostly deals with the Java API. Since I'm not to familiar with the Plan processing life-cycle on the java side I can't assess which classes would have to be changed.
# ""How can we make this work within the limits of the Java API?""
is the exact opposite, it changes nothing in the Java API. Instead, the following issues would have to be solved:
* Alter the plan to extract keys before keyed operations, while hiding these keys from the UDF. This is exactly how KeySelectors (will) work, and as such is generally solved. In fact, this solution would make a few things easier in regards to KeySelectors.
* Rework all operations that currently rely on Java API functions, that need deserialized data, for example Projections or the upcoming Aggregations; 
This generally means implementing them in python, or with special java UDF's (they could de-/serialize data within the udf call, or work on serialized data).
* Change (De)Serializers accordingly
* implement a reliable, not all-memory-consuming sorting mechanism on the python side

Personally i prefer the second option, as it
# does not modify the Java API, it works within it's well-tested limits
# Plan changes are similar to issues that are already worked on (KeySelectors)
# Sorting implementation was necessary anyway (for chained reducers)
# having data in serialized form was a performance-related consideration already

While the first option could work, and most likely require less work, i feel like many of the things required for option 2 will be implemented eventually anyway.","Duplicated Code, Long Method, , , , "
"   Rename Method,Extract Method,Inline Method,","Add a retry for SocketClientSink I found the SocketClientSink doesn`t use a re-connect when disconnect from the socket server or get exception.
I`d like to add a re-connect like socket source for socket sink.","Duplicated Code, Long Method, , , "
"   Move Class,Rename Class,Move Method,Move Attribute,",FlinkTopologyContext not populated completely Currently FlinkTopologyContext is not populated completely. It only contains enough information to make WordCount example work.,", , , "
"   Rename Method,Pull Up Method,Extract Method,","Add abstract equals, hashCode and toString methods to TypeInformation Flink expects that implementations of {{TypeInformation}} have valid implementations of {{hashCode}} and {{equals}}. However, the API does not enforce to implement these methods. Hence, this is a common origin for bugs such as for example FLINK-2633.

This can be avoided by adding abstract {{hashCode}} and {{equals}} methods to TypeInformation. An abstract {{toString}} method could also be added.

This change will brake the API and require to fix a couple of broken {{TypeInformation}} implementations.","Duplicated Code, Long Method, , Duplicated Code, "
"   Rename Class,Pull Up Method,","Untangle CsvInputFormat into PojoTypeCsvInputFormat and TupleTypeCsvInputFormat  The {{CsvInputFormat}} currently allows to return values as a {{Tuple}} or a {{Pojo}} type. As a consequence, the processing logic, which has to work for both types, is overly complex. For example, the {{CsvInputFormat}} contains fields which are only used when a Pojo is returned. Moreover, the pojo field information are constructed by calling setter methods which have to be called in a very specific order, otherwise they fail. E.g. one first has to call {{setFieldTypes}} before calling {{setOrderOfPOJOFields}}, otherwise the number of fields might be different. Furthermore, some of the methods can only be called if the return type is a {{Pojo}} type, because they expect that a {{PojoTypeInfo}} is present.

I think the {{CsvInputFormat}} should be refactored to make the code more easily maintainable. I propose to split it up into a {{PojoTypeCsvInputFormat}} and a {{TupleTypeCsvInputFormat}} which take all the required information via their constructors instead of using the {{setFields}} and {{setOrderOfPOJOFields}} approach.",", Duplicated Code, "
"   Rename Class,Move Method,","Consolidate NetUtils We currently have two classes called {{NetUtils}} with different networking related helpers. In some cases you need both classes, which results in quite some clumsiness.",", , "
"   Move Method,Extract Method,Move Attribute,","Redirect to leading JobManager web fronted in non-standalone mode In case of a non-standalone recovery mode, the job manager frontend of non-leading job managers prints the job manager information of its associated job manager. Because the job manager is not leading, nothing shows up.

The web frontend cannot directly communicate with the leading job manager, because many job manager structures like the execution graph are not serializable.

A work around is to redirect to the web frontend of the leading job manager. This makes sure that all interesting information is presented.","Duplicated Code, Long Method, , , , "
"   Rename Class,Move Class,Rename Method,Move Method,Extract Method,Pull Up Attribute,","Rework / Extend the StatehandleProvider I would like to make some changes (mostly additions) to the {{StateHandleProvider}}. Ideally for the upcoming release, as it is somewhat part of the public API.

The rational behind this is to handle in a nice and extensible way the creation of key/value state backed by various implementations (FS, distributed KV store, local KV store with FS backup, ...) and various checkpointing ways (full dump, append, incremental keys, ...)

The changes would concretely be:

1.  There should be a default {{StateHandleProvider}} set on the execution environment. Functions can later specify the {{StateHandleProvider}} when grabbing the {{StreamOperatorState}} from the runtime context (plus optionally a {{Checkpointer}})

2.  The {{StreamOperatorState}} is created from the {{StateHandleProvider}}. That way, a KeyValueStore state backend can create a {{StreamOperatorState}} that directly updates data in the KV store on every access, if that is desired (and filter accesses by timestamps to only show committed data)

3.  The StateHandleProvider should have methods to get an output stream that writes to the state checkpoint directly (and returns a StateHandle upon closing). That way we can convert and dump large state into the checkpoint without crating a full copy in memory before.


Lastly, I would like to change some names
  - {{StateHandleProvider}} to either {{StateBackend}}, {{StateStore}}, or {{StateProvider}} (simpler name).
  - {{StreamOperatorState}} to either {{State}} or {{KVState}}.","Duplicated Code, Long Method, , , Duplicated Code, "
"   Rename Method,","Gelly API improvements During the Flink Forward Gelly School training, I got some really valuable feedback from participants on what they found hard to grasp or non-intuitive in the API. 

Based on that, I propose we make the following improvements:
-  rename the mapper in creation methods to {{VertexInitializer}}, so that its purpose is easier to understand.
- add a {{fromTuple2DataSet}} method to easily create graphs from {{Tuple2}} datasets, i.e. edges with no values.
- in {{joinWith*}} methods, it is hard to understand what are the parameters in the mapper and what will be the output. I suggest we flatten them, try to give intuitive names and improve the javadocs.
- in neighborhood methods, it is hard to understand what are the arguments of the {{EdgeFunction.iterateEdges}} and {{ReduceEdgesFunction.reduceEdges}}. Javadocs and parameter names could be improved here too.",", "
"   Rename Method,Extract Method,","Add OuterJoin strategy with HashTable on outer side Outer joins are currently supported with two local execution strategies:
- sort-merge join
- hash join where the hash table is built on the inner side. Hence, this strategy is only supported for left and right outer joins.

In order to support hash-tables on the outer side, we need a special hash table implementation that gives access to all records which have not been accessed during the probe phase.","Duplicated Code, Long Method, , "
"   Rename Method,","Create database state backend The goal is to create a database state backend that can be used with JDBC supporting databases.

The backend should support the storage of non-partitioned states, and also the storage of Key-value states with high throughput. As databases provide advanced querying functionality the key-value state can be implemented to be lazily fetched and should scale to ""arbitrary"" state sizes by not storing the non-active key-values on heap.

An adapter class will be provided that can help bridge the gap between different sql implementations.

",", "
"   Rename Method,Extract Method,","Create database state backend The goal is to create a database state backend that can be used with JDBC supporting databases.

The backend should support the storage of non-partitioned states, and also the storage of Key-value states with high throughput. As databases provide advanced querying functionality the key-value state can be implemented to be lazily fetched and should scale to ""arbitrary"" state sizes by not storing the non-active key-values on heap.

An adapter class will be provided that can help bridge the gap between different sql implementations.

","Duplicated Code, Long Method, , "
"   Rename Method,","Add periodic offset commit to Kafka Consumer if checkpointing is disabled Flink only writes the offsets from the consumer into ZK if checkpointing is enabled.

We should have a similar feature to Kafka's autocommit in our consumer.


Issue reported by user: http://stackoverflow.com/questions/33501574/flink-kafka-why-am-i-losing-messages",", "
"   Rename Class,Rename Method,","Extend Window Operators to Allow Efficient Fold Operation Right now, a window fold is implemented as a WindowFunction that gets all the elements as input. No pre-aggregation is performed. The window operator should be extended to also allow the fold to also be pre-aggregated.

This requires changing the signature of the {{WindowBuffer}} so that it can emit a type other than the input type. ",", "
"   Rename Method,Move Method,Extract Method,Move Attribute,","Define a maximum number of concurrent inflight checkpoints The checkpoint coordinator should define an option to limit the maximum number of current inflight checkpoints, as well as the checkpoint timeouts.","Duplicated Code, Long Method, , , , "
"   Rename Method,","Rename Either creation methods to avoid name clash with projection methods Currently the method signatures for creating Either values `Either.left(left)` and the projection methods `either.left()` only differ in the parameters. 

This makes it awkward to use with lambdas such as: 'eitherStream.filter(Either:isLeft).map(Either::left)'
The above code is currently impossible.

I suggest to change the creation methods to `Either.createLeft(left)` and `Either.createRight(right)` and also to directly expose the Left, Right classes.",", "
"   Rename Method,Extract Method,","Allow reading from multiple topics with one FlinkKafkaConsumer Currently, a Kafka consumer allows to read from only one topic.
For cases where multiple topics contain messages with the same schema, it is useful to allow to subscribe to many topics using one FlinkKafkaConsumer instance.","Duplicated Code, Long Method, , "
"   Rename Class,Extract Interface,Extract Method,","Add merging WindowAssigner We should add the possibility for WindowAssigners to merge windows. This will enable Session windowing support, similar to how Google Cloud Dataflow supports.

For session windows, each element would initially be assigned to its own window. When triggering we check the windows and see if any can be merged. This way, elements with overlapping session windows merge into one session.","Duplicated Code, Long Method, , Large Class, "
"   Rename Method,","Decouple restart strategy from ExecutionGraph Currently, the {{ExecutionGraph}} supports the following restart logic: Whenever a failure occurs and the number of restart attempts aren't depleted, wait for a fixed amount of time and then try to restart. This behaviour can be controlled by the configuration parameters {{execution-retries.default}} and {{execution-retries.delay}}.

I propose to decouple the restart logic from the {{ExecutionGraph}} a bit by introducing a strategy pattern. That way it would not only allow us to define a job specific restart behaviour but also to implement different restart strategies. Conceivable strategies could be: Fixed timeout restart, exponential backoff restart, partial topology restarts, etc.

This change is a preliminary step towards having a restart strategy which will scale the parallelism of a job down in case that not enough slots are available.",", "
"   Rename Method,Move Method,Move Attribute,","Use Partitioned State Abstraction in WindowOperator Right now, the WindowOperator uses snapshotState and restoreState to do custom serialization/deserialization of the windowing state. Moving this to partitioned state would allow us to re-scale in the future. Also, once the partitioned state has support to run on ManagedMemory/Out-of-core state the WindowOperator would also automatically benefit from this, allowing very large windows and large window state.

The necessary steps are these:
- Enhance state interface with new primitive states: ReducableState, ListState, etc.. Also make state scoped to a namespace. Namespaces are necessary because we need state that is scoped by key and by window (window would be the namespace in this case)
- Enhance timer interface to also make timers scoped to a key/namespace
- Change WindowOperator to use these new interfaces",", , , "
"   Rename Method,","Implement DataSet.count using a single operator {{DataSet.count}} is currently implemented using a {{FlatMapFunction}} followed by a {{DiscardingOutputFormat}}. As noted by [~StephanEwen] in [FLINK-2716] this can be done with only a {{RichOutputFormat}}.

This change is also applicable to {{DataSet.collect}} and {{Utils.CollectHelper}}.",", "
"   Rename Method,Extract Method,",Auto type registration at Kryo is buggy The auto type registration relies on a static hash set to deduplicate class types. This means in that repeated runs certain classes will not be registered.,"Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Fix Slot Sharing in Streaming API Right now, the slot sharing/resource group logic is a bit ""nebulous"". The slot sharing group that operators are put in depends on the order in which operations are created. For example, in this case:

{code}
Source a = env.source()
Source b = env.source()

a.map().startNewResourceGroup().sink() 
b.map().sink()
{code}

We end up with two resource groups:
- group 1: source a
- group 2: map(), sink(), source b, map(), sink()

The reason is that the slot sharing id is incremented when transforming the {{startNewResouceGroup()}} call and all operators that are transformed afterwards in graph traversal get that new slot sharing id.

(There is also {{isolateResources()}} which can be used to isolate an operator.)

What I propose is to remove {{startNewResourceGroup()}} and {{isolateResouces()}} and replace it with {{slotSharingGroup(String)}}. By default, operations would be in slot sharing group ""default"". This allows very fine grained control over what operators end up in which slot sharing group. For example, I could have this topology:

{code}
Source a = env.source().slotSharingGroup(""sources"")
Source b = env.source().slotSharingGroup(""sources"")

a.map().slotSharingGroup(""heavy a"").sink().slotSharingGroup(""sinks"") 
b.map().slotSharingGroup(""heavy b"").sink().slotSharingGroup(""sinks"")
{code}

Which would isolate the lightweight sources and sinks in a group and put heavy operations inside their own slot groups.

This is a bit more low level than the previous API and requires more calls than a simple {{startNewResourceGroup()}} but I think not many people would use this feature and this design makes it very clear what operations end up in the same group.","Duplicated Code, Long Method, , "
"   Move Class,Extract Superclass,Pull Up Method,Push Down Method,Pull Up Attribute,","Add timeout handler to CEP operator Currently, event sequences which exceed the defined pattern timeout will be discarded. However, in some cases the user might be interested in getting to know when such a timeout occurred to return a default value for these event sequences.

Thus, the pattern API should be extended to be able to define a timeout handler. Furthermore, the {{NFA}} has to be extended to also return the discarded event sequences. The {{CEPOperator}} would then call for every discarded event sequence the timeout handler.",", Duplicated Code, Large Class, Duplicated Code, , Duplicated Code, "
"   Rename Class,Rename Method,Extract Method,","Provide an exactly-once Cassandra connector With FLINK-3311, we are adding a Cassandra connector to Flink.

It would be good to also provide an ""exactly-once"" C* connector.

I would like to first discuss how we are going to implement this in Flink.","Duplicated Code, Long Method, , "
"   Extract Interface,Rename Method,","Refactor TimestampExtractor Based on a lot of user feedback, the current {{TimestampExtractor}} seems very confusing. It implements simultaneously two modes of generating watermarks:

  - Each record that passes through can decide to cause a watermark.

  - The timestamp extractor can define a certain watermark timestamp which is periodically picked up by the system and triggers a watermark (if larger than the previous watermark).

Figuring out how these modes interplay, and how to define the methods to only use one mode has been quite an obstacle for several users. We should break this class into two different classes, one per mode of generating watermarks, to make it easier to understand.",", Large Class, "
"   Rename Method,Extract Method,","Flink Kafka consumer should support auto-commit opt-outs Currently the Kafka source will commit consumer offsets to Zookeeper, either upon a checkpoint if checkpointing is enabled, otherwise periodically based on {{auto.commit.interval.ms}}

It should be possible to opt-out of committing consumer offsets to Zookeeper. Kafka has this config as {{auto.commit.enable}} (0.8) and {{enable.auto.commit}} (0.9).","Duplicated Code, Long Method, , "
"   Rename Method,","Scramble HashPartitioner hashes The {{HashPartitioner}} used by the streaming API does not apply any hash scrambling against bad user hash functions.

We should apply a murmor or jenkins hash on top of the hash code, similar as in the {{DataSet}} API.",", "
"   Move Class,Extract Superclass,","Add fixed time trailing timestamp/watermark extractor Flink currently provides only one build-in timestamp extractor, which assumes strictly ascending timestamps. In real world use cases, timestamps are almost never strictly ascending.

Therefore, I propose to provide an utility watermark extractor which is generating watermarks with a fixed-time trailing.

The implementation should keep track of the highest event-time seen so far and subtract a fixed amount of time from that event time.

This way, users can for example specify that the watermarks should always ""lag behind"" 10 minutes.",", Duplicated Code, Large Class, "
"   Rename Class,Move Method,Extract Method,","Change interplay of Ingestion Time and Event Time Currently, ""EventTime"" and ""IngestionTime"" are completely the same.

For both happens the following:
  - Sources generate ingestion time timestamps and watermarks
  - If a user adds a manual timestamp extractor / watermark generator, then those override the ingestion time timestamps and watermarks
  - That implies that event time on a certain input falls back to ingestion time, if one forgets (or incorrectly uses) the timestamp extractors
  - Also, Ingestion Time and Event Time simply mix if some inputs have timestamp assigners, and others have not.

This behavior is quite tricky to understand. After some discussions with [~aljoscha] and [~rmetzger], we suggest to change it the following way.

  1. On Ingestion Time, the timestamps and watermarks are generated as they are now.

  2. On event time, no default timestamps and watermarks are generated. If a user does not implement a timestamp extractor / watermark generator, then the event time operations will fail fast.

  3. If one wants to use ingestion time on event time settings (mix), one can use an explicit ""WallClockTimetampsAndWatermark"" generator.

  4. Later, the ""Ingestion Time"" settings should automatically disable and user-defined timestamp extractors / assigners.
","Duplicated Code, Long Method, , , "
"   Move Method,Move Attribute,","Add Incremental Fold for Non-Keyed Window Operator Right now, non-keyed fold does not have constant space requirements.",", , , "
"   Rename Method,","Replace random NIC selection heuristic by InetAddress.getLocalHost Currently, the {{ConnectionUtils.findAddressUsingStrategy}} method returns the first {{NetworkInterface}} whose address is not a loop back address, not a link local address and an {{Inet4Address}}. Before returning this address, it is retried to connect to the {{JobManager}} using the {{InetAddress.getLocalHost}} address a last time.

The heuristic, if not choosing the {{InetAddress.getLocalHost}}, often makes no sense because it returns a random {{NetworkInterface}} address. It would be better to simply return the {{InetAddress.getLocalHost()}} instead.",", "
"   Rename Method,",Re-enable Table API explain The Table API explain was temporarily disabled to port the Table API on top of Calcite. It should be re-enabled before merge the changes back to the master branch.,", "
"   Move Method,Extract Method,","Change RollingSink Writer interface to allow wider range of outputs Currently the RollingSink Writer interface only works with FSDataOutputStreams, which precludes it from being used with some existing libraries like Apache ORC and Parquet.

To fix this, a new Writer interface can be created, which receives FileSystem and Path objects, instead of FSDataOutputStream.

To ensure exactly-once semantics, the Writer interface must also be extended so that the current write-offset can be retrieved at checkpointing time. For formats like ORC this requires a footer to be written, before the offset is returned. Checkpointing already calls flush on the writer, but either flush needs to return the current length of the output file, or alternatively a new method has to be added for this.

The existing Writer interface can be recreated with a wrapper on top of the new Writer interface. The existing code that manages the FSDataOutputStream can then be moved into this new wrapper.","Duplicated Code, Long Method, , , "
"   Move Class,Move Method,","Rework Table API tests The {{flink-table}} component consists of 

several APIs 

	* Scala-embedded Table API
	* String-based Table API (for Java)
	* SQL 

and compiles to two execution backends:

	* DataStream API
	* DataSet API

There are many different translation paths involved until a query is executed:

	# Table API String -> Table API logical plan
	# Table API Scala-expressions -> Table API logical plan
	# Table API logical plan -> Calcite RelNode plans
	# SQL -> Calcite RelNode plans (done by exclusively via Calcite)
	# Calcite RelNodes -> DataSet RelNodes
	# DataSet RelNodes -> DataSet program
	# Calcite RelNodes -> DataStream RelNodes
	# DataStream RelNodes -> DataStream program
	# Calcite RexNode expressions -> generated code

which need to be thoroughly tested.

Initially, many tests were done as end-to-end integration tests with high overhead.
However, due to the combinations of APIs and execution back-ends, this approach causes many redundant tests and long build times.

Therefore, I propose the following testing scheme:

1. Table API String -> Table API expression: 
The String-based Table API is tested by comparing the resulting logical plan (Table.logicalPlan) to the logical plan of an equivalent Table program that uses the Scala-embedded syntax. The logical plan is the Table API internal representation which is later converted into a Calcite RelNode plan.
All existing integration tests that check the ""Java"" Table API should be ported to unit tests. There will also be duplicated tests because, the Java Table API is tested for batch and streaming which is not necessary anymore.

2. Table API Scala-expressions -> Table API logical plan -> Calcite RelNodes -> DataSet RelNodes / DataStream RelNodes
These tests cover the translation and optimization of Table API queries and verify the Calcite optimized plan. We need distinct tests for DataSet and DataStream environments since features and translation rules vary. These test will also identify if added or modified rules or cost functions result in different plans. These should be the main tests for the Table API and very extensive. 
These tests should be implemented by extending the {{TableTestBase}} which is a base class for unit tests and hence very lightweight.

3. SQL -> Calcite RelNodes -> DataSet RelNodes / DataStream RelNodes
These are the same tests as described for 2. (Table API Scala-expressions -> DataSet / DataStream RelNodes) but just for SQL.

4. DataSet RelNode -> DataSet program
Unfortunately, the DataSet API lacks a good mechanism to test generated programs, i.e., get a plan traversable of all operators with access to all user-defined functions. Until such a testing utility is available, I propose to test the translation to DataSet programs as end-to-end integration tests. However, I think we can run most tests on a Collection ExecutionEnvironment, which does not start a Flink cluster but runs all code on Java collections. This makes these tests much more lightweight than cluster-based ITCases. The goal of these tests should be to cover all translation paths from DataSetRel to DataSet program, i.e., all DataSetRel nodes and their translation logic. These tests should be implemented by extending the {{TableProgramsCollectionTestBase}} (see FLINK-5268).
Moreover, we should have very few cluster-based ITCases in place that check the execution path with the actual operators, serializers, and comparators. However, we should limit these tests to the minimum to keep build time low. These tests should be implemented by extending the {{TableProgramsClusterTestBase}} (FLINK-5268) and all be located in the same class to avoid repeated instantiation of the Flink MiniCluster.

5. DataStream RelNode -> DataStream program
Here basically the same applies as for the DataSet programs. I'm not aware of a good way to test generated DataStream programs without executing them. A testing utility would be great for all libraries that are built on top of the API. Until then, I propose to use end-to-end integration tests. Unfortunately, the DataStream API does not feature a collection execution mode, so all tests need to be run on a MiniCluster. Therefore, we should again keep these tests to the minimum. These tests should be implemented by extending the {{StreamingMultipleProgramsTestBase}} and be located in few classes to avoid repeated instantiations of the FLink MiniCluster.

6. (Scala expressions | String-parsed expressions | SQL expressions) -> RexNode expressions -> Generated Code
In order to avoid extensive optimization tests for each supported expression or built-in function, we have the {{ExpressionTestBase}} which compiles expressions into generated code and tests for the correctness of results. All supported expressions and built-in function should be tested by extending the {{ExpressionTestBase}} instead of running a full integration test.

I will add a few JIRAs to migrate existing tests to the new testing scheme.

",", , "
"   Rename Class,Move Class,Move Method,","Rework Table API tests The {{flink-table}} component consists of 

several APIs 

	* Scala-embedded Table API
	* String-based Table API (for Java)
	* SQL 

and compiles to two execution backends:

	* DataStream API
	* DataSet API

There are many different translation paths involved until a query is executed:

	# Table API String -> Table API logical plan
	# Table API Scala-expressions -> Table API logical plan
	# Table API logical plan -> Calcite RelNode plans
	# SQL -> Calcite RelNode plans (done by exclusively via Calcite)
	# Calcite RelNodes -> DataSet RelNodes
	# DataSet RelNodes -> DataSet program
	# Calcite RelNodes -> DataStream RelNodes
	# DataStream RelNodes -> DataStream program
	# Calcite RexNode expressions -> generated code

which need to be thoroughly tested.

Initially, many tests were done as end-to-end integration tests with high overhead.
However, due to the combinations of APIs and execution back-ends, this approach causes many redundant tests and long build times.

Therefore, I propose the following testing scheme:

1. Table API String -> Table API expression: 
The String-based Table API is tested by comparing the resulting logical plan (Table.logicalPlan) to the logical plan of an equivalent Table program that uses the Scala-embedded syntax. The logical plan is the Table API internal representation which is later converted into a Calcite RelNode plan.
All existing integration tests that check the ""Java"" Table API should be ported to unit tests. There will also be duplicated tests because, the Java Table API is tested for batch and streaming which is not necessary anymore.

2. Table API Scala-expressions -> Table API logical plan -> Calcite RelNodes -> DataSet RelNodes / DataStream RelNodes
These tests cover the translation and optimization of Table API queries and verify the Calcite optimized plan. We need distinct tests for DataSet and DataStream environments since features and translation rules vary. These test will also identify if added or modified rules or cost functions result in different plans. These should be the main tests for the Table API and very extensive. 
These tests should be implemented by extending the {{TableTestBase}} which is a base class for unit tests and hence very lightweight.

3. SQL -> Calcite RelNodes -> DataSet RelNodes / DataStream RelNodes
These are the same tests as described for 2. (Table API Scala-expressions -> DataSet / DataStream RelNodes) but just for SQL.

4. DataSet RelNode -> DataSet program
Unfortunately, the DataSet API lacks a good mechanism to test generated programs, i.e., get a plan traversable of all operators with access to all user-defined functions. Until such a testing utility is available, I propose to test the translation to DataSet programs as end-to-end integration tests. However, I think we can run most tests on a Collection ExecutionEnvironment, which does not start a Flink cluster but runs all code on Java collections. This makes these tests much more lightweight than cluster-based ITCases. The goal of these tests should be to cover all translation paths from DataSetRel to DataSet program, i.e., all DataSetRel nodes and their translation logic. These tests should be implemented by extending the {{TableProgramsCollectionTestBase}} (see FLINK-5268).
Moreover, we should have very few cluster-based ITCases in place that check the execution path with the actual operators, serializers, and comparators. However, we should limit these tests to the minimum to keep build time low. These tests should be implemented by extending the {{TableProgramsClusterTestBase}} (FLINK-5268) and all be located in the same class to avoid repeated instantiation of the Flink MiniCluster.

5. DataStream RelNode -> DataStream program
Here basically the same applies as for the DataSet programs. I'm not aware of a good way to test generated DataStream programs without executing them. A testing utility would be great for all libraries that are built on top of the API. Until then, I propose to use end-to-end integration tests. Unfortunately, the DataStream API does not feature a collection execution mode, so all tests need to be run on a MiniCluster. Therefore, we should again keep these tests to the minimum. These tests should be implemented by extending the {{StreamingMultipleProgramsTestBase}} and be located in few classes to avoid repeated instantiations of the FLink MiniCluster.

6. (Scala expressions | String-parsed expressions | SQL expressions) -> RexNode expressions -> Generated Code
In order to avoid extensive optimization tests for each supported expression or built-in function, we have the {{ExpressionTestBase}} which compiles expressions into generated code and tests for the correctness of results. All supported expressions and built-in function should be tested by extending the {{ExpressionTestBase}} instead of running a full integration test.

I will add a few JIRAs to migrate existing tests to the new testing scheme.

",", , "
"   Rename Method,","Change access of DataSetUtils.countElements() to 'public'  The access of DatasetUtils.countElements() is presently 'private', change that to be 'public'. We happened to be replicating the functionality in our project and realized the method already existed in Flink.",", "
"   Rename Class,Rename Method,","Allow the FlinkKafkaProducer to send data to multiple topics Currently, the FlinkKafkaProducer is sending all events to one topic defined when creating the producer.

We could allow users to send messages to multiple topics by extending the {{KeyedSerializationSchema}} by a method {{public String getTargetTopic(T element)}} which overrides the default topic if the return value is not null.",", "
"   Rename Method,","Remove Nephele references There still exist a few references to nephele which should be removed:

{code}
flink\docs\setup\local_setup.md:
   79  $ tail log/flink-*-jobmanager-*.log
   80  INFO ... - Initializing memory manager with 409 megabytes of memory
   81: INFO ... - Trying to load org.apache.flinknephele.jobmanager.scheduler.local.LocalScheduler as scheduler
   82  INFO ... - Setting up web info server, using web-root directory ...
   83: INFO ... - Web info server will display information about nephele job-manager on localhost, port 8081.
   84  INFO ... - Starting web info server for JobManager on port 8081
   85  ~~~
   ..
  118  $ cd flink
  119  $ bin/start-local.sh
  120: Starting Nephele job manager
  121  ~~~
{code}

{code}
flink\flink-runtime\src\main\java\org\apache\flink\runtime\operators\TaskContext.java:
   70: 	AbstractInvokable getOwningNepheleTask();
{code}

{code}
flink\flink-runtime\src\main\java\org\apache\flink\runtime\operators\BatchTask.java:
 1149  	 * @param message The main message for the log.
 1150  	 * @param taskName The name of the task.
 1151: 	 * @param parent The nephele task that contains the code producing the message.
 1152  	 *
 1153  	 * @return The string for logging.
 ....
 1254  	 */
 1255  	@SuppressWarnings(""unchecked"")
 1256: 	public static <T> Collector<T> initOutputs(AbstractInvokable nepheleTask, ClassLoader cl, TaskConfig config,
 1257  										List<ChainedDriver<?, ?>> chainedTasksTarget,
 1258  										List<RecordWriter<?>> eventualOutputs,
{code}",", "
"   Rename Method,Extract Method,","Generalize client<->cluster communication Here are some notes I took when inspecting the client<->cluster classes with regard to future integration of other resource management frameworks in addition to Yarn (e.g. Mesos).

{noformat}

1 Cluster Client Abstraction
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1.1 Status Quo
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1.1.1 FlinkYarnClient
â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œ

  â€¢ Holds the cluster configuration (Flink-specific and Yarn-specific)
  â€¢ Contains the deploy() method to deploy the cluster
  â€¢ Creates the Hadoop Yarn client
  â€¢ Receives the initial job manager address
  â€¢ Bootstraps the FlinkYarnCluster


1.1.2 FlinkYarnCluster
â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œ

  â€¢ Wrapper around the Hadoop Yarn client
  â€¢ Queries cluster for status updates
  â€¢ Life time methods to start and shutdown the cluster
  â€¢ Flink specific features like shutdown after job completion


1.1.3 ApplicationClient
â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œ

  â€¢ Acts as a middle-man for asynchronous cluster communication
  â€¢ Designed to communicate with Yarn, not used in Standalone mode


1.1.4 CliFrontend
â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œ

  â€¢ Deeply integrated with FlinkYarnClient and FlinkYarnCluster
  â€¢ Constantly distinguishes between Yarn and Standalone mode
  â€¢ Would be nice to have a general abstraction in place


1.1.5 Client
â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œ

  â€¢ Job submission and Job related actions, agnostic of resource framework


1.2 Proposal
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1.2.1 ClusterConfig (before: AbstractFlinkYarnClient)
â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œ

  â€¢ Extensible cluster-agnostic config
  â€¢ May be extended by specific cluster, e.g. YarnClusterConfig


1.2.2 ClusterClient (before: AbstractFlinkYarnClient)
â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œ

  â€¢ Deals with cluster (RM) specific communication
  â€¢ Exposes framework agnostic information
  â€¢ YarnClusterClient, MesosClusterClient, StandaloneClusterClient


1.2.3 FlinkCluster (before: AbstractFlinkYarnCluster)
â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œ

  â€¢ Basic interface to communicate with a running cluster
  â€¢ Receives the ClusterClient for cluster-specific communication
  â€¢ Should not have to care about the specific implementations of the
    client


1.2.4 ApplicationClient
â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œ

  â€¢ Can be changed to work cluster-agnostic (first steps already in
    FLINK-3543)


1.2.5 CliFrontend
â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œ

  â€¢ CliFrontend does never have to differentiate between different
    cluster types after it has determined which cluster class to load.
  â€¢ Base class handles framework agnostic command line arguments
  â€¢ Pluggables for Yarn, Mesos handle specific commands


{noformat}

I would like to create/refactor the affected classes to set us up for a more flexible client side resource management abstraction.
","Duplicated Code, Long Method, , "
"   Rename Class,Extract Interface,Rename Method,Move Method,Extract Method,Move Attribute,","Generalize client<->cluster communication Here are some notes I took when inspecting the client<->cluster classes with regard to future integration of other resource management frameworks in addition to Yarn (e.g. Mesos).

{noformat}

1 Cluster Client Abstraction
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1.1 Status Quo
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1.1.1 FlinkYarnClient
â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œ

  â€¢ Holds the cluster configuration (Flink-specific and Yarn-specific)
  â€¢ Contains the deploy() method to deploy the cluster
  â€¢ Creates the Hadoop Yarn client
  â€¢ Receives the initial job manager address
  â€¢ Bootstraps the FlinkYarnCluster


1.1.2 FlinkYarnCluster
â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œ

  â€¢ Wrapper around the Hadoop Yarn client
  â€¢ Queries cluster for status updates
  â€¢ Life time methods to start and shutdown the cluster
  â€¢ Flink specific features like shutdown after job completion


1.1.3 ApplicationClient
â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œ

  â€¢ Acts as a middle-man for asynchronous cluster communication
  â€¢ Designed to communicate with Yarn, not used in Standalone mode


1.1.4 CliFrontend
â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œ

  â€¢ Deeply integrated with FlinkYarnClient and FlinkYarnCluster
  â€¢ Constantly distinguishes between Yarn and Standalone mode
  â€¢ Would be nice to have a general abstraction in place


1.1.5 Client
â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œ

  â€¢ Job submission and Job related actions, agnostic of resource framework


1.2 Proposal
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1.2.1 ClusterConfig (before: AbstractFlinkYarnClient)
â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œ

  â€¢ Extensible cluster-agnostic config
  â€¢ May be extended by specific cluster, e.g. YarnClusterConfig


1.2.2 ClusterClient (before: AbstractFlinkYarnClient)
â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œ

  â€¢ Deals with cluster (RM) specific communication
  â€¢ Exposes framework agnostic information
  â€¢ YarnClusterClient, MesosClusterClient, StandaloneClusterClient


1.2.3 FlinkCluster (before: AbstractFlinkYarnCluster)
â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œ

  â€¢ Basic interface to communicate with a running cluster
  â€¢ Receives the ClusterClient for cluster-specific communication
  â€¢ Should not have to care about the specific implementations of the
    client


1.2.4 ApplicationClient
â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œ

  â€¢ Can be changed to work cluster-agnostic (first steps already in
    FLINK-3543)


1.2.5 CliFrontend
â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œ

  â€¢ CliFrontend does never have to differentiate between different
    cluster types after it has determined which cluster class to load.
  â€¢ Base class handles framework agnostic command line arguments
  â€¢ Pluggables for Yarn, Mesos handle specific commands


{noformat}

I would like to create/refactor the affected classes to set us up for a more flexible client side resource management abstraction.
","Duplicated Code, Long Method, , , , Large Class, "
"   Rename Class,Rename Method,Pull Up Method,Move Method,Extract Method,Pull Up Attribute,Move Attribute,","Add an interface for Time aware User Functions I suggest to add an interface that UDFs can implement, which will let them be notified upon watermark updates.

Example usage:
{code}
public interface EventTimeFunction {
    void onWatermark(Watermark watermark);
}

public class MyMapper implements MapFunction<String, String>, EventTimeFunction {

    private long currentEventTime = Long.MIN_VALUE;

    public String map(String value) {
        return value + "" @ "" + currentEventTime;
    }

    public void onWatermark(Watermark watermark) {
        currentEventTime = watermark.getTimestamp();
    }
}
{code}","Duplicated Code, Long Method, , , , Duplicated Code, Duplicated Code, "
"   Move Class,Move Method,Move Attribute,","Replace Guava Preconditions class with Flink Preconditions In order to reduce the dependency on Guava (which has cause us quite a bit of pain in the past with its version conflicts), I suggest to add a Flink {{Preconditions}} class.",", , , "
"   Move Class,Move And Rename Class,Move Method,Extract Method,Move Attribute,","DataStream API PojoFieldAccessor doesn't support nested POJOs The {{PojoFieldAccessor}} (which is used by {{.sum(String)}} and similar methods) doesn't support nested POJOs right now.

As part of FLINK-3697 I'll add a check for a nested POJO and fail with an exception.","Duplicated Code, Long Method, , , , "
"   Move Class,Move And Rename Class,Move Method,Move Attribute,","DataStream API PojoFieldAccessor doesn't support nested POJOs The {{PojoFieldAccessor}} (which is used by {{.sum(String)}} and similar methods) doesn't support nested POJOs right now.

As part of FLINK-3697 I'll add a check for a nested POJO and fail with an exception.",", , , "
"   Rename Method,","Consolidate TimestampAssigner Methods in Kafka Consumer On {{DataStream}} the methods for setting a TimestampAssigner/WatermarkEmitter are called {{assignTimestampsAndWatermarks()}} while on {{FlinkKafkaConsumer*}} they are called {{setPunctuatedWatermarkEmitter()}} and {{setPeriodicWatermarkEmitter()}}.

I think these names should be matched, also the name {{setWatermarkEmitter}} does not hint at the fact that the assigner primarily assigns timestamps.",", "
"   Rename Method,","Introduce key groups for key-value state to support dynamic scaling In order to support dynamic scaling, it is necessary to sub-partition the key-value states of each operator. This sub-partitioning, which produces a set of key groups, allows to easily scale in and out Flink jobs by simply reassigning the different key groups to the new set of sub tasks. The idea of key groups is described in this design document [1]. 

[1] https://docs.google.com/document/d/1G1OS1z3xEBOrYD4wSu-LuBCyPUWyFd9l3T9WyssQ63w/edit?usp=sharing",", "
"   Inline Method,Move Attribute,","RabbitMQ Source/Sink standardize connection parameters The RabbitMQ source and sink should have the same capabilities in terms of establishing a connection, currently the sink is lacking connection parameters that are available on the source. Additionally, VirtualHost should be an offered parameter for multi-tenant RabbitMQ clusters (if not specified it goes to the vhost '/').

Connection Parameters
===================
- Host - Offered on both
- Port - Source only
- Virtual Host - Neither
- User - Source only
- Password - Source only

Additionally, it might be worth offer the URI as a valid constructor because that would offer all 5 of the above parameters in a single String.
",", , , "
"   Rename Method,Extract Method,","GlobalConfiguration doesn't ensure config has been loaded By default, {{GlobalConfiguration}} returns an empty Configuration. Instead, a call to {{get()}} should fail if the config hasn't been loaded explicitly.","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,Extract Method,Move Attribute,",Directed Clustering Coefficient A directed clustering coefficient algorithm can be implemented using an efficient triangle listing implementation which emits not only the three vertex IDs forming the triangle but also a bitmask indicating which edges form the triangle. A triangle can be formed with a minimum of three or maximum of six directed edges. Directed clustering coefficient can then shatter the triangles and emit a score of either 1 or 2 for each vertex.,"Duplicated Code, Long Method, , , "
"   Rename Class,Rename Method,","Support for Kerberos Authentication with Keytab Credential _This issue is part of a series of improvements detailed in the [Secure Data Access|https://docs.google.com/document/d/1-GQB6uVOyoaXGwtqwqLV8BHDxWiMO2WnVzBoJ8oPaAs/edit?usp=sharing] design doc._

Add support for a keytab credential to be associated with the Flink cluster, to facilitate:
- Kerberos-authenticated data access for connectors
- Kerberos-authenticated ZooKeeper access

Support both the standalone and YARN deployment modes.
 ",", "
"   Extract Interface,Extract Method,","Support for Kerberos Authentication with Keytab Credential _This issue is part of a series of improvements detailed in the [Secure Data Access|https://docs.google.com/document/d/1-GQB6uVOyoaXGwtqwqLV8BHDxWiMO2WnVzBoJ8oPaAs/edit?usp=sharing] design doc._

Add support for a keytab credential to be associated with the Flink cluster, to facilitate:
- Kerberos-authenticated data access for connectors
- Kerberos-authenticated ZooKeeper access

Support both the standalone and YARN deployment modes.
 ","Duplicated Code, Long Method, , Large Class, "
"   Rename Method,Move Method,Extract Method,","Make flink cli list, savepoint, cancel and stop work on Flink-on-YARN clusters Currently, flink cli can't figure out JobManager RPC location for Flink-on-YARN clusters. Therefore, list, savepoint, cancel and stop subcommands are hard to invoke if you only know the YARN application ID. As an improvement, I suggest adding a -yid <yarnApplicationId> option to the mentioned subcommands that can be used together with -m yarn-cluster. Flink cli would then retrieve JobManager RPC location from YARN ResourceManager.","Duplicated Code, Long Method, , , "
"   Move Class,Rename Class,Move And Rename Class,Rename Method,Move Method,Move Attribute,","Add version header to savepoints Adding a header with version information to savepoints ensures that we can migrate savepoints between Flink versions in the future (for example when changing internal serialization formats between versions).

After talking with Till, we propose to add the following meta data:

- Magic number (int): identify data as savepoint
- Version (int): savepoint version (independent of Flink version)
- Data Offset (int): specifies at which point the actual savepoint data starts. With this, we can allow future Flink versions to add fields to the header without breaking stuff, e.g. Flink 1.1 could read savepoints of Flink 2.0.

For Flink 1.0 savepoint support, we have to try reading the savepoints without a header before failing if we don't find the magic number.
",", , , "
"   Rename Method,","Change the name of ternary condition operator  'eval' to  '?'   The ternary condition operator in Table API is named {{eval}}, for example: {{(42 > 5).eval(""A"", ""B"")}} leads to ""A"".  IMO, the eval function is not well understood. Instead the ""?"" is a better choice I think, which is used in Java for condition operator. 

It will be clearer and more literal understood, e.g.

{{(42 > 5).?(""A"", ""B"")}} or {{(42 > 5) ? (""A"", ""B"")}}

If it make sense, I will pull a request.",", "
"   Rename Method,Extract Method,","Configurable delimiter for metric identifier The metric identifier is currently hard-coded to separate components with a dot.

We should make this configurable.","Duplicated Code, Long Method, , "
"   Rename Class,Move And Rename Class,Rename Method,","Remove `CONFIG_` prefix from KinesisConfigConstants variables I find the static variable names verbose. I think it's clear from context that they refer to the Kinesis configuration since they are all gathered in that class.

Therefore would like to remove the {{CONFIG_}} prefix before the release, so that we have
{code}
conf.put(KinesisConfigConstants.AWS_REGION, """")
{code}
instead of 
{code}
conf.put(KinesisConfigConstants.CONFIG_AWS_REGION, """")
{code}

For longer variables it becomes even longer otherwise.

---

Some basic variable names that might be accessed frequently are also very long:

{code}
CONFIG_AWS_CREDENTIALS_PROVIDER_BASIC_SECRETKEY
CONFIG_AWS_CREDENTIALS_PROVIDER_BASIC_ACCESSKEYID
{code}

It might suffice to just have:
{code}
AWS_SECRET_KEY
AWS_ACCESS_KEY
{code}
",", "
"   Rename Method,Pull Up Attribute,","Expose Kafka metrics through Flink metrics Currently, we expose the Kafka metrics through Flink's accumulators.
We can now use the metrics system in Flink to report Kafka metrics.",", Duplicated Code, "
"   Move Class,Move And Rename Class,Extract Interface,Rename Method,Move Method,Move Attribute,","Move Metrics API to separate module All metrics code currently resides in flink-core. If a user implements a reporter and wants a fat jar it will now have to include the entire flink-core module.

Instead, we could move several interfaces into a separate module.

These interfaces to move include:
* Counter, Gauge, Histogram(Statistics)
* MetricGroup
* MetricReporter, Scheduled, AbstractReporter

In addition a new MetricRegistry interface will be required as well as a replacement for the Configuration.
",", , , Large Class, "
"   Move Class,Rename Method,Move Method,Move Attribute,","Allow Specifying Multiple Metrics Reporters We should allow specifying multiple reporters. A rough sketch of how the configuration should look like is this:

{code}
metrics.reporters = foo,bar
metrics.reporter.foo.class = JMXReporter.class
metrics.reporter.foo.port = 42-117
metrics.reporter.bar.class = GangliaReporter.class
metrics.reporter.bar.port = 512
metrics.reporter.bar.whatever = 2
{code}",", , , "
"   Rename Method,","Rename ""recovery.mode"" config key to ""high-availability"" Currently, HA is configured via the following configuration keys:

{code}
recovery.mode: STANDALONE // No high availability (HA)
recovery.mode: ZOOKEEPER // HA
{code}

This could be more straight forward by simply renaming the key to {{high-availability}}. Furthermore, the term {{STANDALONE}} is overloaded. We already have standalone cluster mode.

{code}
high-availability: NONE // No HA
high-availability: ZOOKEEPER // HA via ZooKeeper
{code}

The {{recovery.mode}} configuration keys would have to be deprecated before completely removing them.",", "
"   Rename Method,","New Flink-specific option to set starting position of Kafka consumer without respecting external offsets in ZK / Broker Currently, to start reading from the ""earliest"" and ""latest"" position in topics for the Flink Kafka consumer, users set the Kafka config {{auto.offset.reset}} in the provided properties configuration.

However, the way this config actually works might be a bit misleading if users were trying to find a way to ""read topics from a starting position"". The way the {{auto.offset.reset}} config works in the Flink Kafka consumer resembles Kafka's original intent for the setting: first, existing external offsets committed to the ZK / brokers will be checked; if none exists, then will {{auto.offset.reset}} be respected.

I propose to add Flink-specific ways to define the starting position, without taking into account the external offsets. The original behaviour (reference external offsets first) can be changed to be a user option, so that the behaviour can be retained for frequent Kafka users that may need some collaboration with existing non-Flink Kafka consumer applications.

How users will interact with the Flink Kafka consumer after this is added, with a newly introduced {{flink.starting-position}} config:

{code}
Properties props = new Properties();
props.setProperty(""flink.starting-position"", ""earliest/latest"");
props.setProperty(""auto.offset.reset"", ""...""); // this will be ignored (log a warning)
props.setProperty(""group.id"", ""..."") // this won't have effect on the starting position anymore (may still be used in external offset committing)
...
{code}

Or, reference external offsets in ZK / broker:

{code}
Properties props = new Properties();
props.setProperty(""flink.starting-position"", ""external-offsets"");
props.setProperty(""auto.offset.reset"", ""earliest/latest""); // default will be latest
props.setProperty(""group.id"", ""...""); // will be used to lookup external offsets in ZK / broker on startup
...
{code}

A thing we would need to decide on is what would the default value be for {{flink.starting-position}}.


Two merits I see in adding this:

1. This compensates the way users generally interpret ""read from a starting position"". As the Flink Kafka connector is somewhat essentially a ""high-level"" Kafka consumer for Flink users, I think it is reasonable to add Flink-specific functionality that users will find useful, although it wasn't supported in Kafka's original consumer designs.

2. By adding this, the idea that ""the Kafka offset store (ZK / brokers) is used only to expose progress to the outside world, and not used to manipulate how Kafka topics are read in Flink (unless users opt to do so)"" is even more definite and solid. There was some discussion in this PR (https://github.com/apache/flink/pull/1690, FLINK-3398) on this aspect. I think adding this ""decouples"" more Flink's internal offset checkpointing from the external Kafka's offset store.",", "
"   Rename Method,Extract Method,","Unify CheckpointCoordinator and SavepointCoordinator The Checkpoint coordinator should have the functionality of both handling checkpoints and savepoints.

The difference between checkpoints and savepoints is minimal:
  - savepoints always write the root metadata of the checkpoint
  - savepoints are always full (never incremental)

The commonalities are large
  - jobs should be able to resume from checkpoint or savepoints
  - jobs should fall back to the latest checkpoint or savepoint

This subsumes issue https://issues.apache.org/jira/browse/FLINK-3397","Duplicated Code, Long Method, , "
"   Rename Method,","Unify CheckpointCoordinator and SavepointCoordinator The Checkpoint coordinator should have the functionality of both handling checkpoints and savepoints.

The difference between checkpoints and savepoints is minimal:
  - savepoints always write the root metadata of the checkpoint
  - savepoints are always full (never incremental)

The commonalities are large
  - jobs should be able to resume from checkpoint or savepoints
  - jobs should fall back to the latest checkpoint or savepoint

This subsumes issue https://issues.apache.org/jira/browse/FLINK-3397",", "
"   Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Provide support for asynchronous operations over streams Many Flink users need to do asynchronous processing driven by data from a DataStream.  The classic example would be joining against an external database in order to enrich a stream with extra information.

It would be nice to add general support for this type of operation in the Flink API.  Ideally this could simply take the form of a new operator that manages async operations, keeps so many of them in flight, and then emits results to downstream operators as the async operations complete.","Duplicated Code, Long Method, , , , , "
"   Rename Method,","Provide support for asynchronous operations over streams Many Flink users need to do asynchronous processing driven by data from a DataStream.  The classic example would be joining against an external database in order to enrich a stream with extra information.

It would be nice to add general support for this type of operation in the Flink API.  Ideally this could simply take the form of a new operator that manages async operations, keeps so many of them in flight, and then emits results to downstream operators as the async operations complete.",", "
"   Move And Rename Class,Move Method,Inline Method,","Replace ActorGateways in NetworkEnvironment by interfaces The {{NetworkEnvironment}} communicates with the outside world ({{TaskManager}} and {{JobManager}}) via {{ActorGateways}}. This bakes in the dependency on actors. 

In terms of modularization and an improved abstraction (especially wrt Flip-6) I propose to replace the {{ActorGateways}} by interfaces which exposes the required methods. The current implementation would then simply wrap the method calls in messages and send them via the {{ActorGateway}} to the recipient.

In Flip-6 the {{JobMaster}} and the {{TaskExecutor}} could simply implement these interfaces as part of their RPC contract.",", , , "
"   Rename Class,Rename Method,","Replace ActorGateway in Task by interface The {{Task}} communicates with the outside world ({{JobManager}} and {{TaskManager}}) via {{ActorGateways}}. This bakes in the dependency on actors.
In terms of modularization and an improved abstraction (especially wrt Flip-6) I propose to replace the {{ActorGateways}} by interfaces which exposes the required methods. The current implementation would then simply wrap the method calls in messages and send them via the {{ActorGateway}} to the recipient.

In Flip-6 the {{JobMaster}} could simply implement these interfaces as part of their RPC contract.",", "
"   Rename Method,","Make the ExecutionGraph independent of Akka Currently, the {{ExecutionGraph}} strongly depends on Akka as it requires an {{ActorSystem}} to create the {{CheckpointCoordinatorDeActivator}}. Furthermore, it allows {{ActorGateways}} to register for job status and execution updates.

In order to improve modularization and abstraction I propose to introduce proper listener interfaces. This would also allow to get rid of the {{CheckpointCoordinatorDeActivator}} by simply implementing this interface. Furthermore it will pave the way for the upcoming Flip-6 refactoring as it offers a better abstraction.",", "
"   Rename Method,","Introduce SlotProvider for Scheduler Currently the {{Scheduler}} maintains a queue of available instances which it scans if it needs a new slot. If it finds a suitable instance (having free slots available) it will allocate a slot from it. 

This slot allocation logic can be factored out and be made available via a {{SlotProvider}} interface. The {{SlotProvider}} has methods to allocate a slot given a set of location preferences. Slots should be returned as {{Futures}}, because in the future the slot allocation might happen asynchronously (Flip-6). 

In the first version, the {{SlotProvider}} implementation will simply encapsulate the existing slot allocation logic extracted from the {{Scheduler}}. When a slot is requested it will return a completed or failed future since the allocation happens synchronously.

The refactoring will have the advantage to simplify the {{Scheduler}} class and to pave the way for upcoming refactorings (Flip-6).",", "
"   Pull Up Method,Pull Up Attribute,","Generalize TaskExecutorToResourceManagerConnection to be reusable The {{TaskExecutorToResourceManagerConnection}} can be more generalized to be reusable across components. For example, the {{JobMaster}} requires a similar connection if we assume that the {{JobMaster}} can be run independently of the {{ResourceManager}}.

Pulling out the strong dependency on the `TaskExecutor` and the `ResourceManagerGateway` should be enough to make the {{TaskExecutorToResourceManagerConnection}} reusable.",", Duplicated Code, Duplicated Code, "
"   Move Class,Move Method,Move Attribute,","Duplicate/inconsistent logic for physical memory size in classes ""Hardware"" and ""EnvironmentInformation"" Both {{Hardware}} and {{EnvironmentInformation}} have some logic to determine the size of the physical memory. The {{EnvironmentInformation}} class uses that in a heuristic for the maximum heap size.

We should consolidate the logic in the {{Hardware}} class and call it from the {{EnvironmentInformation}}.",", , , "
"   Move Class,Pull Up Method,Extract Method,","TaskManager should commit suicide after cancellation failure In case of a failed cancellation, e.g. the task cannot be cancelled after a given time, the {{TaskManager}} should kill itself. That way we guarantee that there is no resource leak. 

This behaviour acts as a safety-net against faulty user code.","Duplicated Code, Long Method, , Duplicated Code, "
"   Move And Rename Class,Extract Interface,Extract Method,","Implement an archived version of the execution graph In order to implement a job history server, as well as separate the JobManager from the WebInterface, we require an archived version of the ExecutionGraph that is Serializable.","Duplicated Code, Long Method, , Large Class, "
"   Rename Method,Extract Method,","Unify behaviour of committed offsets to Kafka / ZK for Kafka 0.8 and 0.9 consumer The proper ""behaviour"" of the offsets committed back to Kafka / ZK should be ""the next offset that consumers should read (in Kafka terms, the 'position')"".

This is already fixed for the 0.9 consumer by FLINK-4618, by incrementing the committed offsets back to Kafka by the 0.9 by 1, so that the internal {{KafkaConsumer}} picks up the correct start position when committed offsets are present. This fix was required because the start position from committed offsets was implicitly determined with Kafka 0.9 APIs.

However, since the 0.8 consumer handles offset committing and start position using Flink's own {{ZookeeperOffsetHandler}} and not Kafka's high-level APIs, the 0.8 consumer did not require a fix.

I propose to still unify the behaviour of committed offsets across 0.8 and 0.9 to the definition above.

Otherwise, if users in any case first uses the 0.8 consumer to read data and have Flink-committed offsets in ZK, and then uses a high-level 0.8 Kafka consumer to read the same topic in a non-Flink application, the first record will be duplicate (because, like described above, Kafka high-level consumers expect the committed offsets to be ""the next record to process"" and not ""the last processed record"").

This requires incrementing the committed ZK offsets in 0.8 to also be incremented by 1, and changing how Flink internal offsets are initialized with accordance to the acquired ZK offsets.","Duplicated Code, Long Method, , "
"   Rename Method,","Port WebFrontend to new metric system While the WebFrontend has access to the metric system it still relies on older code in some parts.

The TaskManager metrics are still gathered using the Codahale library and send with the heartbeats.

Task related metrics (numRecordsIn etc) are still gathered using accumulators, which are accessed through the execution graph.",", "
"   Rename Class,Rename Method,","Introduce an OperatorIOMetricGroup Task related IO metrics (numBytesIn/Out) are not instantiated directly by the task, but instead within the IOMetricGroup contained in the respective TaskMetricGroup. They are then later accessed by relevant components, instead of creating them themselves. This has the advantage that they can be accessed from several places, and that they are guaranteed to always be instantiated identically (without requiring static name constants).

I propose to do the same for operators.

This is also a prerequisite for FLINK-4733.",", "
"   Extract Method,Pull Up Attribute,","Simplify access to MetricStore A few utility methods can be added to the MetricStore, which would make working with it a lot easier.

This includes getXMetricStore(args) methods that, for example, return the appropriate TaskMetricStore, without requiring the accessor to traverse the structure(and do several null checks) themselves.

In addition, a getMetric(name, defaultValue) method would ease the issue of metrics not being updated yet.","Duplicated Code, Long Method, , Duplicated Code, "
"   Move Class,Move Method,Move Attribute,","Implement Mini Cluster This task is to implement the embedded mini cluster (similar to the {{LocalFlinkMiniCluster}} based on the new components developed in ""Flink Improvement Proposal 6""",", , , "
"   Rename Method,","Add MapState for keyed streams Many states in keyed streams are organized as key-value pairs. Currently, these states are implemented by storing the entire map into a ValueState or a ListState. The implementation however is very costly because all entries have to be serialized/deserialized when updating a single entry. To improve the efficiency of these states, MapStates are urgently needed. ",", "
"   Rename Method,","Don't block on buffer request after broadcastEvent  After broadcasting an event (like the checkpoint barrier), the record writer might block on a buffer request although that buffer will only be needed on the next write on that channel.

Instead of assuming that each serializer has a buffer set, we can change the logic in the writer to request the buffer when it requires one.",", "
"   Rename Method,","Don't block on buffer request after broadcastEvent  After broadcasting an event (like the checkpoint barrier), the record writer might block on a buffer request although that buffer will only be needed on the next write on that channel.

Instead of assuming that each serializer has a buffer set, we can change the logic in the writer to request the buffer when it requires one.",", "
"   Rename Method,","Don't block on buffer request after broadcastEvent  After broadcasting an event (like the checkpoint barrier), the record writer might block on a buffer request although that buffer will only be needed on the next write on that channel.

Instead of assuming that each serializer has a buffer set, we can change the logic in the writer to request the buffer when it requires one.",", "
"   Rename Method,","Introduce non async future methods Currently, Flink's {{Futures}} support only async methods. This means that each chained operation is executed potentially by a different thread. In some cases this is not necessary and the context switches inflict additional costs. Therefore, I propose to add non async methods to the {{Futures}.",", "
"   Move Method,Extract Method,Move Attribute,","Introduce safety net for closing file system streams Streams that are opened through {{FileSystem}} must be closed at the end of their life cycle. However, we found hints that some code forgets to close such streams.

We should introduce i) a mechanism that closes leaking unclosed streams after usage and ii) provides logging that helps us to track down and fi the sources of such leaks.","Duplicated Code, Long Method, , , , "
"   Rename Method,","Allow the AbstractStreamOperatorTestHarness to test scaling down Currently the AbstractStreamOperatorTestHarness allows for testing an 
operator when scaling up, through snapshot and restore. This is not enough 
as many interesting corner cases arise when scaling down or during 
arbitrary combinations of scaling up and down.

This issue targets to add this functionality so that an operator can snapshot 
its state, restore with different parallelism, and later scale down or further up.",", "
"   Rename Method,Extract Method,",Extending Window Function Metadata https://cwiki.apache.org/confluence/display/FLINK/FLIP-2+Extending+Window+Function+Metadata,"Duplicated Code, Long Method, , "
"   Rename Method,","Rename Methods in ManagedInitializationContext We should rename {{getManagedOperatorStateStore()}} to {{getOperatorStateStore()}} and  {{getManagedKeyedStateStore()}} to {{getKeyedStateStore()}}. There are no unmanaged stores and having that extra word there seems a bit confusing, plus it makes the names longer.",", "
"   Rename Method,","Provide Timestamp in TimelyFlatMapFunction Right now, {{TimelyFlatMapFunction}} does not give the timestamp of the element in {{flatMap()}}.

The signature is currently this:
{code}
void flatMap(I value, TimerService timerService, Collector<O> out) throws Exception;
{code}

if we add the timestamp it would become this:
{code}
void flatMap(I value, Long timestamp, TimerService timerService, Collector<O> out) throws Exception;
{code}

The reason why it's a {{Long}} and not a {{long}} is that an element might not have a timestamp, in that case we should hand in {{null}} here.

This is becoming quite look so we could add a {{Context}} parameter that provides access to the timestamp and timer service.

",", "
"   Rename Method,Extract Method,","Introduce StreamStatus stream element to allow for temporarily idle streaming sources A {{StreamStatus}} element informs receiving operators whether or not they should continue to expect watermarks from the sending operator. There are 2 kinds of status, namely {{IDLE}} and {{ACTIVE}}. Watermark status elements are generated at the sources, and may be propagated through the operators of the topology using {{Output#emitWatermarkStatus(WatermarkStatus)}}.
Sources and downstream operators should emit either of the status elements once it changes between ""watermark-idle"" and ""watermark-active"" states.

A source is considered ""watermark-idle"" if it will not emit records for an indefinite amount of time. This is the case, for example, for Flink's Kafka Consumer, where sources might initially have no assigned partitions to read from, or no records can be read from the assigned partitions. Once the source detects that it will resume emitting data, it is considered ""watermark-active"".

Downstream operators with multiple inputs (ex. head operators of a {{OneInputStreamTask}} or {{TwoInputStreamTask}}) should not wait for watermarks from an upstream operator that is ""watermark-idle"" when deciding whether or not to advance the operator's current watermark. When a downstream operator determines that all upstream operators are ""watermark-idle"" (i.e. when all input channels have received the watermark idle status element), then the operator is considered to also be ""watermark-idle"", as it will temporarily be unable to advance its own watermark. This is always the case for operators that only read from a single upstream operator. Once an operator is considered ""watermark-idle"", it should itself forward its idle status to inform downstream operators. The operator is considered to be back to ""watermark-active"" as soon as at least one of its upstream operators resume to be ""watermark-active"" (i.e. when at least one input channel receives the watermark active status element), and should also forward its active status to inform downstream operators.","Duplicated Code, Long Method, , "
"   Rename Class,Move And Rename Class,Rename Method,","Rename TimelyFlatMap to Process The method on {{KeyedDataStream}} would be called {{process()}} and the function itself would be called {{ProcessFunction}}.

The reason for this is that {{TimelyFlatMapFunction}} is a bit of a mouthful and with the additions to the timer API and state the {{ProcessFunction}} could become the basic, low-level, user-facing API for cases where users nowadays implement their own operator.",", "
"   Rename Method,Move Method,Inline Method,Move Attribute,","Avoid redundant serialization when creating the TaskDeploymentDescriptor When creating the {{TaskDeploymentDescriptor}} we extract information from the {{ExecutionGraph}} which is defined job-wide and from the {{ExecutionJobVertex}} which is defined operator-wide. The extracted information will be serialized for every subtask even though it stays the same. 

As an improvement, we can serialize this information once and give the serialized byte array to the {{TaskDeploymentDescriptor}}. This will reduce the serialization work Flink has to do when deploying sub tasks.",", , , , "
"   Rename Method,Extract Method,","Make the BucketingSink rescalable. Aims at integrating the BucketingSink with the rescalable state 
abstractions so that the parallelism can change when 
restoring from a savepoint without sacrificing the provided guarantees.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,",Make the RollingSink rescalable. Integrate the RollingSink with the new state abstractions so that its parallelism can change after restoring from a savepoint.,"Duplicated Code, Long Method, , "
"   Rename Method,","Improved resource cleanup in RocksDB keyed state backend Currently, the resources such as taken snapshots or iterators are not always cleaned up in the RocksDB state backend. In particular, not starting the runnable future will leave taken snapshots unreleased.

We should improve the releases of all resources allocated through the RocksDB JNI bridge.",", "
"   Rename Method,","Make the production functions rescalable (apart from the Rolling/Bucketing Sinks) This issue targets porting all the functions in the production code to the new state abstractions. These functions are:
1) StatefulSequenceSource
2) MessageAcknowledgingSourceBase
3) FromElementsFunction
4) ContinuousFileMonitoringFunction",", "
"   Rename Method,Move Method,","Make consumption of input channels fair The input channels on the receiver side of the network stack queue incoming data and notify the input gate about available data. These notifications currently determine the order in which input channels are consumed, which can lead to unfair consumption patterns where faster channels are favored over slower ones.",", , "
"   Rename Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Make consumption of input channels fair The input channels on the receiver side of the network stack queue incoming data and notify the input gate about available data. These notifications currently determine the order in which input channels are consumed, which can lead to unfair consumption patterns where faster channels are favored over slower ones.","Duplicated Code, Long Method, , , , , "
"   Rename Method,","Consolidate and harmonize Window Translation Tests The tests that check whether API calls on {{WindowedStream}} (both Java and Scala) result in the correct runtime operation are scattered across {{TimeWindowTranslationTest}} and {{WindowTranslationTest}} and the test coverage is not the same for Scala and Java.

We should ensure that we test all API calls and that we also test the Scala API with the same level of detail.",", "
"   Rename Method,","Consolidate and harmonize Window Translation Tests The tests that check whether API calls on {{WindowedStream}} (both Java and Scala) result in the correct runtime operation are scattered across {{TimeWindowTranslationTest}} and {{WindowTranslationTest}} and the test coverage is not the same for Scala and Java.

We should ensure that we test all API calls and that we also test the Scala API with the same level of detail.",", "
"   Rename Method,","Properly Close StateBackend in StreamTask when closing/canceling Right now, the {{StreamTask}} never calls {{close()}} on the state backend.",", "
"   Rename Method,","Introduce state handle replication mode for CheckpointCoordinator Currently, the {{CheckpointCoordinator}} only supports repartitioning of {{OperatorStateHandle}}s based on a split-and-distribute strategy. For future state types, such as broadcast or union state, we need a different repartitioning method that allows for replicating state handles to all subtasks.

This is the first step on the way to implementing broadcast and union states.",", "
"   Rename Method,Move Method,","Explicit restore method in Snapshotable We should introduce an explicit {{restore(...)}} method to match the {{snapshot(...)}} method in this interface.

Currently, restore happens implicit in backends, i.e. when state handles are provided, backends execute restore logic in their constructors. This behaviour makes it hard for backends to participate in the task's lifecycle through {{CloseableRegistry}}, because we can only register backend objects after they have been constructed. As a result, for example, all restore operations that happen in the constructor are not responsive to cancelation.

When we introduce an explicit restore, we can first create a backend object, then register it, and only then run restore.",", , "
"   Pull Up Method,Extract Method,Pull Up Attribute,","Explicit restore method in Snapshotable We should introduce an explicit {{restore(...)}} method to match the {{snapshot(...)}} method in this interface.

Currently, restore happens implicit in backends, i.e. when state handles are provided, backends execute restore logic in their constructors. This behaviour makes it hard for backends to participate in the task's lifecycle through {{CloseableRegistry}}, because we can only register backend objects after they have been constructed. As a result, for example, all restore operations that happen in the constructor are not responsive to cancelation.

When we introduce an explicit restore, we can first create a backend object, then register it, and only then run restore.","Duplicated Code, Long Method, , Duplicated Code, Duplicated Code, "
"   Rename Method,","User-provided hashes for operators We could allow users to provided (alternative) hashes for operators in a StreamGraph. This can make migration between Flink versions easier, in case the automatically produced hashes between versions are incompatible. For example, users could just copy the old hashes from the web ui to their job.",", "
"   Extract Superclass,Move Method,Move Attribute,","remove duplicated tests Now we have test which run the same code 4 times, every run 17+ seconds.

Need do small refactoring and remove duplicated code.",", , , Duplicated Code, Large Class, "
"   Move And Rename Class,Extract Method,","Create a proper internal state hierarchy Currently, the state interfaces (like {{ListState}}, {{ValueState}}, {{ReducingState}}) are very sparse and contain only methods exposed to the users. That makes sense to keep the public stable API minimal

At the same time, the runtime needs more methods for its internal interaction with state, such as:
  - setting namespaces
  - accessing raw values
  - merging namespaces

These are currently realized by re-creating or re-obtaining the state objects from the KeyedStateBackend. That method causes quite an overhead for each access to the state

The KeyedStateBackend tries to do some tricks to reduce that overhead, but does it only partially and induces other overhead in the course.

The root cause of all these issues is a problem in the design: There is no proper ""internal state abstraction"" in a similar way as there is an external state abstraction (the public state API).

We should add a similar hierarchy of states for the internal methods. It would look like in the example below:

{code}
 *             State
 *               |
 *               +-------------------InternalKvState
 *               |                         |
 *          MergingState                   |
 *               |                         |
 *               +-----------------InternalMergingState
 *               |                         |
 *      +--------+------+                  |
 *      |               |                  |
 * ReducingState    ListState        +-----+-----------------+
 *      |               |            |                       |
 *      +-----------+   +-----------   -----------------InternalListState
 *                  |                |
 *                  +---------InternalReducingState
{code}","Duplicated Code, Long Method, , "
"   Rename Class,Push Down Attribute,","queryable state: execute the QueryableStateITCase for all three state back-ends The QueryableStateITCase currently is only tested with the MemoryStateBackend but as has been seen in the past, some errors or inconsistent behaviour only appeared with different state back-ends. It should thus be extended to be tested with all three currently existing state back-ends.",", , "
"   Rename Method,","Add an Option to Deactivate Kryo Fallback for Serializers Some users want to avoid that Flink's serializers use Kryo, as it can easily become a hotspot in serialization.

For those users, it would help if there is a flag to ""deactive generic types"". Those users could then see where types are used that default to Kryo and change these types (make them PoJos, Value types, or write custom serializers).

There are two ways to approach that:

  1. (Simple) Make {{GenericTypeInfo}} threw an exception whenever it would create a Kryo Serializer (when the respective flag is set in the {{ExecutionConfig}})

  2. Have a static flag on the {{TypeExtractor}} to throw an exception whenever it would create a {{GenericTypeInfo}}. This approach has the downside of introducing some static configuration to the TypeExtractor, but may be more helpful because it throws exceptions in the programs at points where the types are used (not where the serializers are created, which may be much later).",", "
"   Move Method,Move Attribute,","Clean up FileSystem The {{FileSystem}} class is overloaded and has methods that are not well supported. I suggest to do the following cleanups:

  - Pull the safety net into a separate class

  - Use the {{WriteMode}} to indicate overwriting behavior. Right now, the {{FileSystem}} class defines that enum and never uses it. It feels weird.

  - Remove the {{create(path, overwrite, blocksize, reolication, ...)}} method, which is not really supported across file system implementations. For HDFS, behavior should be set via the configuration anyways.

All changes have to be made in a non-API-breaking fashion.",", , , "
"   Rename Method,Extract Method,",Move JSON generation code into static methods In order to implement the HistoryServer we need a way to generate the JSON responses independent of the REST API. As such i suggest to move the main parts of the generation code for job-specific handlers into static methods. ,"Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Allow Access to Per-Window State in ProcessWindowFunction Right now, the state that a {{WindowFunction}} or {{ProcessWindowFunction}} can access is scoped to the key of the window but not the window itself. That is, state is global across all windows for a given key.

For some use cases it is beneficial to keep state scoped to a window. For example, if you expect to have several {{Trigger}} firings (due to early and late firings) a user can keep state per window to keep some information between those firings.

The per-window state has to be cleaned up in some way. For this I see two options:
 - Keep track of all state that a user uses and clean up when we reach the window GC horizon.
 - Add a method {{cleanup()}} to {{ProcessWindowFunction}} which is called when we reach the window GC horizon that users can/should use to clean up their state.

On the API side, we can add a method {{windowState()}} on {{ProcessWindowFunction.Context}} that retrieves the per-window state and {{globalState()}} that would allow access to the (already available) global state. The {{Context}} would then look like this:
{code}
/**
 * The context holding window metadata
 */
public abstract class Context {
    /**
     * @return The window that is being evaluated.
     */
    public abstract W window();

    /**
     * State accessor for per-key and per-window state.
     */
    KeyedStateStore windowState();

    /**
     * State accessor for per-key global state.
     */
    KeyedStateStore globalState();
}
{code}","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Let handlers take part in job archiving The key idea behind the HistoryServer is to pre-compute all JSON responses which the WebFrontend could request and store them as files in a directory structure resembling the REST-API.

For this require a mechanism to generate the responses and their corresponding REST URL.

FLINK-5852 made it easier to re-use the JSON generation code, while FLINK-5870 made handlers aware of the REST URLs that they are registered one.

The aim of this JIRA is to extend job-related handlers, building on the above JIRAs, enabling them to generate a number of (Path, Json) pairs for a given ExecutionGraph, containing all responses that they could generate for the given graph and their respective REST URL..","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Change new Java API functions to SAMs In order to support a compact syntax with Java 8 Lambdas, we would need to change the types of the functions to Single Abstract Method types (SAMs). Only those can be implemented by Lambdas.

That means that DataSet.map(MapFunction) would accept an interface MapFunction, not the abstract class that we use now. Many UDFs would not inherit form `AbstractFunction` any more. The inheritance from AbstractFunction would be optional, if life cycle methods (open / close) and runtime contexts are needed.

This may have also implications on the type extraction, as the generic parameters are in generic superinterfaces, rather than in generic superclasses.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/701
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: enhancement, java api, user satisfaction, 
Milestone: Release 0.6 (unplanned)
Created at: Thu Apr 17 13:06:40 CEST 2014
State: open
","Duplicated Code, Long Method, , "
"   Rename Class,Extract Interface,Extract Method,","Change new Java API functions to SAMs In order to support a compact syntax with Java 8 Lambdas, we would need to change the types of the functions to Single Abstract Method types (SAMs). Only those can be implemented by Lambdas.

That means that DataSet.map(MapFunction) would accept an interface MapFunction, not the abstract class that we use now. Many UDFs would not inherit form `AbstractFunction` any more. The inheritance from AbstractFunction would be optional, if life cycle methods (open / close) and runtime contexts are needed.

This may have also implications on the type extraction, as the generic parameters are in generic superinterfaces, rather than in generic superclasses.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/701
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: enhancement, java api, user satisfaction, 
Milestone: Release 0.6 (unplanned)
Created at: Thu Apr 17 13:06:40 CEST 2014
State: open
","Duplicated Code, Long Method, , Large Class, "
"   Rename Method,Inline Method,","Change new Java API functions to SAMs In order to support a compact syntax with Java 8 Lambdas, we would need to change the types of the functions to Single Abstract Method types (SAMs). Only those can be implemented by Lambdas.

That means that DataSet.map(MapFunction) would accept an interface MapFunction, not the abstract class that we use now. Many UDFs would not inherit form `AbstractFunction` any more. The inheritance from AbstractFunction would be optional, if life cycle methods (open / close) and runtime contexts are needed.

This may have also implications on the type extraction, as the generic parameters are in generic superinterfaces, rather than in generic superclasses.

---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/issues/701
Created by: [StephanEwen|https://github.com/StephanEwen]
Labels: enhancement, java api, user satisfaction, 
Milestone: Release 0.6 (unplanned)
Created at: Thu Apr 17 13:06:40 CEST 2014
State: open
",", , "
"   Rename Method,","Throw exception if solution set is CoGrouped on the wrong key. Co Grouping with the solution set on a key other than the solution set is not possible. The program, however, does not cause an error, but simply causes the program to not correctly match elements.

I have a patch coming up...",", "
"   Move Class,Move Method,","Remove printing config to System.out When the JobManager is started, it prints the global config to system.out.

I suggest to remove that",", , "
"   Rename Class,Extract Method,","Replace DataInput and DataOutput of IOReadableWritable with DataInputView and DataOutputView The DataInput and DataOutput view are not well designed interfaces with a limited function set. Therefore, we should replace the DataInput and DataOutput by the Flink defined DataInputView and DataOutputView in the IOReadableWritable interface. This is a preparative step for the serializer and input/output abstraction rework.","Duplicated Code, Long Method, , "
"   Rename Method,Move Method,",NettyConnectionManager does not close connections Network connections created via NettyConnectionManager are not closed.,", , "
"   Move Class,Extract Method,","Data input sampling and testing improvements It would be really nice to help debug an application by limiting the input data (% of input splits, max vertices per input split).  Also, it would be nice for the workers to provide a little more debugging info on how far along they are with processing the input data.","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,Extract Method,Move Attribute,","Support better message config for Giraph 
There are various different options for sending messages in Giraph:
- whether there is going to be a combiner
- MessageEncodeAndStoreType to be used
- message factory
- etc

If you want different properties in different iterations, currently you can set some of them in master through SuperstepClasses, which are then sent to all the workers, which then use MessageStoreFactory (for example - InMemoryWithPrimitivesMessageStoreFactory)
Some of the properties are missing there, and so cannot be changed at all (like message factory/MessageEncodeAndStoreType)
","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Add Blocks Framework This will be a set of diffs adding Blocks Framework, summary on each diff will explain it's contents.

More context in:
http://mail-archives.apache.org/mod_mbox/giraph-dev/201506.mbox/%3CCABJ-n3v-24YLzgNmrT3TZT6R8t4Vw1hrBcWWTghG_XgaC%3DYqrg%40mail.gmail.com%3E","Duplicated Code, Long Method, , "
"   Move Method,Extract Method,Move Attribute,","Out-of-core mechanism for input superstep and graph data Currently the out-of-core mechanism has several issues:

It does not work properly in input superstep
It has several bugs (correctness, and performance) for graph data structure
It is not considering specialized message classes for out-of-core messages.
This diff addresses some of these problems, and provides a cohesive mechanism for input superstep and graph data. With this way of doing out-of-core, efficiently handling out-of-core messages is straightforward (will be provided in a separate diff).

This diff provides an adaptive mechanism to remove users out of the loop for using out-of-core. If the graph can be fit in memory, all the computation remains in memory. If memory is very limited and GC is killing the performance, or the graph cannot fit in memory, out-of-core mechanism kicks in and helps improve performance in many cases, and avoids application failure due to OutOfMemory exception or it's related exceptions","Duplicated Code, Long Method, , , , "
"   Extract Interface,Rename Method,Move Method,Extract Method,Pull Up Attribute,Move Attribute,","Adding out-of-core messages to the newly added adaptive out-of-core mechanism GIRAPH-1022 implements an adaptive mechanism for out-of-core capable of handling graph data structure (in input/output and compute superstep). This diff, adds capability of handling out-of-core messages as well. This is useful specially in the case where message combiner is not used and large amount of messages may cause the job to either fail due to OOM (or its related exceptions), or run with a very low performance due to extensive GC calls.","Duplicated Code, Long Method, , , , Large Class, Duplicated Code, "
"   Extract Interface,Rename Method,Extract Method,","Remove zookeeper from input splits handling Currently we use zookeeper for handling input splits, by having each worker checking each split, and when a lot of splits are used this becomes very slow. We should have master coordinate input splits allocation instead, making the complexity proportional to #splits instead of #workers*#splits.","Duplicated Code, Long Method, , Large Class, "
"   Push Down Method,Push Down Attribute,",Allow extending JobProgressTrackerService We might want to perform additional actions on events from JobProgressTrackerService. Allow overriding it and specifying another class to use.,", , , "
"   Rename Method,Extract Method,","In fixed out-of-core policy the definition of in-memory partition is not clear The fixed out-of-core policy relies on the definition for in-memory partitions. However, the definition of in-memory partition is not clear, and this leads to bugs in fixed out-of-core policy. Currently, a partition is called in-memory only if the partition itself (regardless of its messages) is in memory. We should change the policy so that only partitions that have their current messages as well as the partition data in memory be considered as in-memory partitions. ","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,Move Method,Extract Method,Inline Method,Pull Up Attribute,Move Attribute,","Improve the graph distribution of Giraph Currently, Giraph assumes that the data from the VertexInputFormat is sorted.  If the user data is not sorted by the vertex id, they must first run a MapReduce or Pig job to generate a sorted dataset.  This is often a bit inconvenient.

Giraph graph partitioning is currently range based and there are some advantages and disadvantages of this approach.  The proposal of this JIRA would be to allow for both range and hash based partitioning and provide more flexibility to the user.

Design goals for the graph distribution:

* Allow vertices to be unordered or unordered
* Ability to repartition
* Select the partitioning scheme based on user needs (i.e. hash or range based)
* Ability to provide user-specific hints about partitions

Hash-based partitioning

* Good vertex balancing across ranges for random data
* Bad at vertex id locality

Range-based partitioning

* Good at vertex id locality
* Ability to split ranges easily
* Can cause hotspots for hot ranges
","Duplicated Code, Long Method, , , , , Duplicated Code, "
"   Rename Method,Extract Method,","Add memory estimation mechanism to out-of-core The new out-of-core mechanism is designed with the adaptivity goal in mind, meaning that we wanted out-of-core mechanism to kick in only when it is necessary. In other words, when the amount of data (graph, messages, and mutations) all fit in memory, we want to take advantage of the entire memory. And, when in a stage the memory is short, only enough (minimal) amount of data goes out of core (to disk). This ensures a good performance for the out-of-core mechanism.

To satisfy the adaptiveness goal, we need to know how much memory is used at each point of time. The default out-of-core mechanism (ThresholdBasedOracle) get memory information based on JVM's internal methods (Runtime's freeMemory()). This method is inaccurate (and pessimistic), meaning that it does not account for garbage data that has not been purged by GC. Using JVM's default methods, OOC behaves pessimistically and move data out of core even if it is not necessary. For instance, consider the case where there are a lot of garbage on the heap, but GC has not happened for a while. In this case, the default OOC pushes data on disk and immediately after a major GC it brings back the data to memory. This causes inefficiency in the default out of core mechanism. If out-of-core is used but the data can entirely fit in memory, the job goes out of core even though going out of core is not necessary.

To address this issue, we need to have a mechanism to more accurately know how much of heap is filled with non-garbage data. Consequently, we need to change the Oracle (OOC policy) to take advantage of a more accurate memory usage estimation.","Duplicated Code, Long Method, , "
"   Rename Method,","Clarify messages behavior in BasicVertex initialize() can receive a null parameter for messages (at least that's what EdgeListVertex does). We should avoid that and pass an empty Iterable instead. That should be cheap for us inside of the InputFormat, just passing a static immutable empty list.

setMessages(Iterable<M>) should be changed to putMessages(Iterable<M>). the set prefix suggests an assignment, while setMessages is used to transfer the messages to the internal datastructure the user is responsible for. putMessages() should clarify this.",", "
"   Rename Method,","Changes to ExtendedByteArrayDataOutput AddingÂ a new stream type which extends fromÂ com.esotericsoftware.kryo.io.Input/Output, so thatÂ it can be used with kryo serializer.Â It improves the performance of kryo by faster read/writes (unsafe IO), and alsoÂ eliminatesÂ the need for interim buffers to convertÂ fromÂ DataInput and DataOutput.",", "
"   Rename Method,","Adding faster serialization classes. These changes add two new kryo serialization classes (KryoSimpleWritable and KryoSimpleWrapper)Â which disable reference tracking at the expense of not supporting recursive and nested structures. Disabling reference tracking significantly improves the serialization performance.Â One a sample pipeline, the running time reduced from 75 minutes to 5 minutes.",", "
"   Move Method,Extract Method,Move Attribute,","Investigate communication improvements Currently every worker will start up a thread to communicate with every other workers.  Hadoop RPC is used for communication.  For instance if there are 400 workers, each worker will create 400 threads.  This ends up using a lot of memory, even with the option  

-Dmapred.child.java.opts=""-Xss64k"".  

It would be good to investigate using frameworks like Netty or custom roll our own to improve this situation.  By moving away from Hadoop RPC, we would also make compatibility of different Hadoop versions easier.","Duplicated Code, Long Method, , , , "
"   Move And Rename Class,Move Method,","Change PageRankBenchmark to be accessible via bin/giraph Currently the PageRankBenchmark has its own main and tool implementation and is difficult to access from the bin/giraph script.  It would be better if everything were accessible via bin/giraph.  The benchmark is particularly problematic because it uses inner classes for its two actual Vertex implementations, which have to be specified on the command line as their .class name(ie org.apache.giraph.benchmark.PageRankBenchmark$PageRankHashMapVertex) rather than just with dots, as one would expect.",", , "
"   Move Class,Rename Method,Pull Up Method,Move Method,Extract Method,Pull Up Attribute,","multigraph support in giraph The current vertex API only supports simple graphs, meaning that there can only ever be one edge between two vertices. Many graphs like the road network are in fact multigraphs, where many edges can connect two vertices at the same time.

Support for this could be added by introducing an Iterator<EdgeWritable> getEdgeValue() or a similar construct. Maybe introducing a slim object like a Connector between the edge and the vertex is also a good idea, so that you could do something like:

{code} 
for (final Connector<EdgeWritable, VertexWritable> conn: getEdgeValues(){
     final EdgeWritable edge = conn.getEdge();
     final VertexWritable otherVertex = conn.getOther();
     doInterestingStuff(otherVertex);
     doMoreInterestingStuff(edge);
}
{code} 
","Duplicated Code, Long Method, , , Duplicated Code, Duplicated Code, "
"   Extract Interface,Rename Method,Extract Method,Push Down Attribute,","Allow creation of graph by adding edges that span multiple workers Currently a graph is created only be adding vertices. The typical way is to read input text files line-by-line with each line describing a vertex (its value, its edges etc). The current API allows for the creation of a vertex only if all the information for the vertex is available in a single line.

However, it's common to have graphs described in the form of edges. Edges might span multiple lines in an input file or even span multiple workers. The current API doesn't allow this. In the input superstep, a vertex must be created by a single worker.

Instead, it should be possible for multiple workers to mutate the graph during the input superstep.

This has the following implications:
1) Instead of just instantiating a vertex, a vertex reader should be able to do vertex addition and edge addition requests.
2) Multiple workers might try to create the same vertex. Any conflicts should be handled with a VertexResolver. So the resolver has to be instantiated before load time.

","Duplicated Code, Long Method, , , Large Class, "
"   Move And Rename Class,","Move aggregators to a separate sub-package Since aggregators will be re-used throughout many projects and algorithms, it makes sense to implement the most common ones in a separate sub-package. This will reduce the time required for users when they implement their projects based on Giraph, because the required aggregators are already in place. I implemented the following ones:
for int/long/float/double: min, max, product, sum, overwrite
for boolean: and, or, overwrite

Most of them speak for themselves, except for the overwrite one. This aggregator simply overwrites the stored value when a new value is aggregated. This is useful when one node is in some way a master node (for example a source node in an routing algorithm), and this node wants to broadcast a value to all other nodes.

Attached is a patch against trunk implementing the aggregators and patching some existing files so they use the .aggregators package instead of the .examples one.",", "
"   Move Class,Rename Method,Move Method,Extract Method,Inline Method,",Move temporary test files from the project directory We shouldn't use the project directory as the location for temporary files generated by the tests.,"Duplicated Code, Long Method, , , , "
"   Rename Class,Rename Method,","remove hadoop RPC and keep just netty Given the netty communication behaves faster than rpc, we should just keep netty-based and drop the older one.",", "
"   Rename Method,","Add secure authentication to Netty IPC Gianmarco De Francisci Morales asked on the user list:

bq. I am getting the exception in the subject when running my giraph program
bq. on a cluster with Kerberos authentication.

This leads to the idea of having Kerberos authentication supported within GIRAPH. Hopefully it would use our fast GIRAPH-37 IPC, but could also interoperate with Hadoop security.

",", "
"   Rename Method,","Make iteration over edges more explicit Is there any particular reason why BasicVertex implements Iterable?

It seems to me that doing
{code:java}
for (I neighbor : vertex)
{code}
is not that explicit, and
{code:java}
for (I neighbor : this)
{code}
gets even obscure (which may be why all examples in the codebase explicitly instantiate an iterator and call next()).

What I propose is a more explicit
{code:java}
Iterator<I> outEdgesIterator()
{code}
and also a convenient
{code:java}
Iterable<I> outEdges()
{code}
so, for example, an algorithm can use
{code:java}
for (IntWritable neighbor : outEdges())
{code}",", "
"   Rename Class,Rename Method,Pull Up Method,Move Method,Extract Method,Inline Method,Move Attribute,","Vertex API redesign This is an effort to rationalize the Giraph API. I've put together a few issues that we've talked about lately. I'm focusing on making Giraph development even more intuitive and less error-prone, and fixing a few potential sources of bugs.

I'm sorry this is a big patch, but most of those issues are intertwined and I think this might be easier to review and integrate.

Here's an account of the changes:

Vertex API:
- Renamed BasicVertex to Vertex (as I understand, we used to have both and then Vertex was removed).
- Switched to Iterables instead of Iterators for both edges and messages. This makes code more concise for both implementors (no need to call .iterator() on collections) and users (can use foreach syntax). See also GIRAPH-221.
- Added SimpleVertex and SimpleMutableVertex classes, where there are no edge values and the iterable to be implemented is getNeighbors(). We donâ€™t have multiple inheritance, so the only way I could think of was to have SimpleVertex extend Vertex, SimpleMutableVertex extend MutableVertex, and duplicate the code for the edges iterables.
Also, due to type erasure, one still has to deal with Edge objects in SimpleMutableVertex#initialize. Overall I think this is still an improvement over the current situation.
- Added id and value field to the base Vertex class. All other classes were either writing the same boilerplate again and again, or using primitive fields and then creating Writables on the fly (inefficient; there was even a TODO about that). If there are any actually useful customizations here, Iâ€™ve yet to see them.
Also removed redundant â€œVertexâ€ from getters/setters (compare vertex.getId() with vertex.getVertexId()).
- Made halt a private field, and added a wakeUp() method to re-activate a vertex. isHalted()/voteToHalt()/wakeUp() are just more semantically-charged getter/setters.
- Renamed number of vertices/edges in graph to getTotalNum*. The previous naming (getNumEdges) was arguably confusing. If this one sucks too, please suggest a better one.
- Default implementations of hasEdge(), getEdgeValue(), getNumEdges(), readFields(), write(), toString(): the implementor can still optimize when there is a good opportunity. Currently we are duplicating a lot of code (see GIRAPH-238) and potentially introducing bugs (see GIRAPH-239).

HashMapVertex:
- Switched representation from Map<I, Edge<I, E>> to Map<I, E> (GIRAPH-242)
- Only override methods that can be optimized.

EdgeListVertex:
- Switched representation from two sorted lists to one list of Edge<I, E> (see GIRAPH-243). Mainly this makes iteration over edges (target id and value) linear instead of O(n log n). Mutations are still slow and should generally be discouraged.
- Only override methods that can be optimized.

Small nits:
- Our code conventions say we should try to avoid abbreviations, so I eliminated a few (req -> request, msg -> message).
- Unilaterally refer to the endpoint of an edge as targetVertex (before we had a mix of destVertex and targetVertex).
- You will notice some rearranged imports. Thatâ€™s just my IDE trying to be helpful (see GIRAPH-230).
","Duplicated Code, Long Method, , , , , Duplicated Code, "
"   Rename Class,Extract Method,","Move part of the graph out-of-core when memory is low There has been some talk about Giraph's scaling limitations due to keeping the whole graph and messages in RAM.
We need to investigate methods to fall back to disk when running out of memory, while gracefully degrading performance.

This issue is for graph storage. Messages should probably be a separate issue, although the interplay between the two is crucial.

We should also discuss what are our primary goals here: completing a job (albeit slowly) instead of failing when the graph is too big, while still encouraging memory optimizations and high-memory clusters; or restructuring Giraph to be as efficient as possible in disk mode, making it almost a standard way of operating.","Duplicated Code, Long Method, , "
"   Pull Up Method,Move Method,Move Attribute,","Mutable static global state in Vertex.java should be refactored Vertex.java has a bunch of static methods for getting/setting global graph state (total number of vertices, edges, a reference to the GraphMapper, etc).  Refactoring this into a GraphState object, which every Vertex can hold onto a reference to (yes, a tiny bit more memory per Vertex, but in comparison to what's already in there...)",", , , Duplicated Code, "
"   Rename Method,Move Method,Extract Method,Inline Method,","Aggregators shouldn't use Zookeeper We use Zookeeper znodes to transfer aggregated values from workers to master and back. Zookeeper is supposed to be used for coordination, and it also has a memory limit which prevents users from having aggregators with large value objects. These are the reasons why we should implement aggregators gathering and distribution in a different way.","Duplicated Code, Long Method, , , , "
"   Rename Class,Rename Method,Pull Up Method,Move Method,Move Attribute,","Text Vertex Input/Output Format base classes overhaul The current way of implementing {{VertexInputFormat}} and {{VertexReader}} had bad smell.  It required users to understand how these two classes are glued together, and forced similar codes to be duplicated in every new input format.  (Similarly for the VertexOutputFormat and VertexWriter.)  Anyone who wants to create a new format should create an underlying record reader or writer at the right moment and delegate some calls to it, which seemed unnecessary detail being exposed.  Besides, type parameters had to appear all over every new format code, which was extremely annoying for both reading existing code and writing a new one.  I was very frustrated writing my first format code especially when I compared it to writing a new vertex code.  I thought writing a new input/output format should be as simple as vertex.

So, I have refactored {{TextVertexInputFormat}} and {{OutputFormat}} into new forms that have no difference in their interfaces, but remove a lot of burden for subclassing.  Instead of providing static VertexReader base classes, I made it a non-static inner-class of its format class, which helps eliminate the repeated code for gluing these two, already tightly coupled classes.  This has additional advantage of eliminating all the Generics type variables on the VertexReader side, which makes overall code much more concise.  I added several useful TextVertexReader base classes that can save efforts for implementing line-oriented formats.

Please comment if you see my proposed change have any impact on other aspects.  I'm unsure of how these additional layers of abstraction could affect performance.",", , , Duplicated Code, "
"   Rename Method,","Add thread and channel pooling to NettyClient and NettyServer Add thread and channel pooling on NettyClient and NettyServer.

Instead of a NettyClient's addressChannelMap being:

  address => Channel

it is:

  address => ChannelRotater

Originally part of GIRAPH-262, extracted here.",", "
"   Rename Method,Pull Up Method,Extract Method,Pull Up Attribute,","Improve netty reliability with retrying failed connections, tracking requests, thread-safe hash partitioning * Upgrade to the most recent stable version of Netty (3.5.3.Final)
* Try multiple connection attempts up to n failures
* Track requests throughout the system by keeping track of the request id and then matching the request id to the response (minor refactoring of WritableRequest to make requests simpler and support the request id)
* Improved handling of netty exceptions by dumping the exception stack to help debug failures
* Fixes bug in HashWorkerPartitioner by making partitionList thread-safe (this causes divide by zero exceptions in real life)

Currently, netty connection failures causes issues with more than 75 workers in my setup.  This allows us to reach over 200+ in a reasonably reliable network that doesn't kill connections.

This code passes the local Hadoop regressions and the single node Hadoop instance regressions.  It also succeeded on large runs (200+ workers) on a real Hadoop cluster.","Duplicated Code, Long Method, , Duplicated Code, Duplicated Code, "
"   Rename Method,","InputSplit Reservations are clumping, leaving many workers asleep while other process too many splits and get overloaded. With recent additions to the codebase, users here have noticed many workers are able to load input splits extremely quickly, and this has altered the behavior of Giraph during INPUT_SUPERSTEP when using the current algorithm for split reservations. A few workers process multiple splits (often overwhelming Netty and getting GC errors as they attempt to offload too much data too quick) while many (often most) of the others just sleep through the superstep, never successfully participating at all.

Essentially, the current algo is:

1. scan input split list, skipping nodes that are marked ""Finsihed""

2. grab the first unfinished node in the list (reserved or not) and check its reserved status.

3. if not reserved, attempt to reserve & return it if successful.

4. if the first one you check is already taken, sleep for way too long and only wake up if another worker finishes a split, then contend with that worker for another split, while the majority of the split list might sit idle, not actually checked or claimed by anyone yet.

This does not work. By making a few simple changes (and acknowledging that ZK reads are cheap, only writes are not) this patch is able to get every worker involved, and keep them in the game, ensuring that the INPUT_SUPERSTEP passes quickly and painlessly, and without overwhelming Netty by spreading the memory load the split readers bear more evenly. If the giraph.splitmb and -w options are set correctly, behavior is now exactly as one would expect it to be.

This also results in INPUT_SUPERSTEP passing more quickly, and survive the INPUT_SUPERSTEP for a given data load on less Hadoop memory slots.
 ",", "
"   Rename Method,Extract Method,","Netty requests should be reliable and implement exactly once semantics One of the biggest scalability challenges is getting Giraph to run reliably on a large number of tasks (i.e. > 200).  Several problems exist:

1) If the connection fails after the initial connection was made, the job will die.
2) Requests must be completed exactly once.  This is difficult to implement, but required since we cannot have multiple retried requests succeed (i.e. a vertex gets more messages than expected).
3) Sometimes there are unresolved addresses, causing failure.

This patch addresses these issues by re-establishing failed connections and keep tracking of every request sent to every worker.  If the request fails or passes a timeout, it will be resent.  The server will keep track of requests that succeeded to insure that the same request won't be processed more than once.  The structure for keeping track of the succeeded requests on the server is efficient for handling increasing request ids (IncreasingBitSet).  For handling unresolved addresses, I added retry logic to keep trying to resolve the problem.

This patch also adds several unit tests that use fault injection to simulate a lost response or a closed channel exception on the server.  It also has unittests for IncreasingBitSet to insure it is working correctly and efficiently.

This passes all unittests (including the new ones).  Additionally, I have some experience results as well.

Previously, I was unable to run reliably with more than 200 workers.  With this change I can reliably run 500+ workers.  I also ran with 600 workers successfully.  This is a really big reliability win for us.

I can see the code working to do reconnections and re-issue requests when necessary.  It's very cool.

I.e.

2012-08-18 00:16:52,109 INFO org.apache.giraph.comm.NettyClient: checkAndFixChannel: Fixing disconnected channel to xxx.xxx.xxx.xxx/xx.xx.xx.xx:30455, open = false, bound = false
2012-08-18 00:16:52,111 INFO org.apache.giraph.comm.NettyClient: checkAndFixChannel: Connected to xxx.xxx.xxx.xxx/xxx.xxx.xxx.xxx:30455!
2012-08-18 00:16:52,123 INFO org.apache.giraph.comm.NettyClient: checkAndFixChannel: Fixing disconnected channel to xxx.xxx.xxx.xxx/xxx.xxx.xxx.xxx, open = false, bound = false
2012-08-18 00:16:52,124 INFO org.apache.giraph.comm.NettyClient: checkAndFixChannel: Connected to xxx.xxx.xxx.xxx/xxx.xxx.xxx.xxx:30117!

","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,","InputSplit list can be long with many workers (and locality info) and should not be re-created every time a worker calls reserveInputSplit() While instrumenting the INPUT_SUPERSTEP and watching various runs, I see the input split list generated every time a worker calls reserveInputSplit is, for all intents and purposes, immutable per job. Therefore, we can save a fair amount of memory by not re-creating the list and re-querying ZooKeeper on each pass to claim another split. Only the reserved and finished children lists are ever mutated during the input phase of the job.
",", "
"   Rename Method,","Hide the SortedMap<I, Edge<I,E>> in Vertex from client visibility (impl. detail), replace with appropriate accessor methods As discussed on the list, and on GIRAPH-28, the SortedMap<I, Edge<I,E>> is an implementation detail which needs not be exposed to application developers - they need to iterate over the edges, and possibly access them one-by-one, and remove them (in the Mutable case), but they don't need the SortedMap, and creating primitive-optimized BasicVertex implementations is hampered by the fact that clients expect this Map to exist.",", "
"   Extract Interface,Push Down Attribute,","Open Netty client and server on master For GIRAPH-273 first thing we need is to open Netty communication on master, make connections to workers and make connections from workers to master.

Since it's already significant amount of code I'm opening a separate issue for it.",", , Large Class, "
"   Move Class,Move And Rename Class,","Add subpackages to comm Package comm became too big, it would be nice to add some subpackages to it.",", "
"   Rename Class,Rename Method,Extract Method,Inline Method,","Outgoing messages from current superstep should be grouped at the sender by owning worker, not by partition Currently, outgoing messages created by the Vertex#compute() cycle on each worker are stored and grouped by the partitionId on the destination worker to which the messages belong. This results in messages being duplicated on the wire per partition on a given receiving worker that has delivery vertices for those messages.

By partitioning the outgoing, current-superstep messages by destination worker, we can split them into partitions at insertion into a MessageStore on the destination worker. What we trade in come compute time while inserting at the receiver side, we gain in fine grained control over the real number of messages each worker caches outbound for any given worker before flushing, and how those flush messages are aggregated for delivery as well. Potentially, it allows for a great reduction in duplicate messages sent in situations like Vertex#sendMessageToAllEdges() -- see GIRAPH-322, GIRAPH-314. You get the idea.

This might be a poor idea, and it can certainly use some additional refinement, but it passes mvn verify and may even run ;) It interoperates with the disk spill code, but not as well as it could. Consider this a request for comment on the idea (and the approach) rather than a finished product.

Comments/ideas/help welcome! Thanks

","Duplicated Code, Long Method, , , "
"   Rename Method,","Ensure that subclassing BasicVertex is possible by user apps Original assumptions in Giraph were that all users would subclass Vertex (which extended MutableVertex extended BasicVertex).  Classes which wish to have application specific data structures (ie. not a TreeMap<I, Edge<I,E>>) may need to extend either MutableVertex or BasicVertex.  Unfortunately VertexRange extends ArrayList<Vertex>, and there are other places where the assumption is that vertex classes are either Vertex, or at least MutableVertex.

Let's make sure the internal APIs allow for BasicVertex to be the base class.",", "
"   Rename Method,Move Method,Move Attribute,","Write worker addresses to Zookeeper; move addresses and resolution to NettyClient In preparation for GIRAPH-273, we need to have addresses of all workers available, so I write them to Zookeeper along with the master address.

Since address resoultion is needed in both NettyWorkerClient and NettyMasterClient, I moved that to NettyClient, and also added a map taskId->address in there. Now NettyMasterClient and NettyWorkerClient don't need to take care of InetSocketAddresses, just task ids which they want to send messages to.",", , , "
"   Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Multithreading in input split loading and compute Cleaned up the WorkerClient hierarchy
- WorkerClientRequestProcessor is a request cache for every thread (input split loading / compute)
- With RPC gone, got rid of ugly WorkerClientServer and NettyWorkerClientServer
SendPartitionCache
Made GraphState immutable for multi-threading
Added multithreading for loading the input splits
Added multithreading for compute
Added thread-level debugging as an option
Added additional testing on the number of vertices, edges
Optimization on HashWorkerPartitioner to use CopyOnWriteArrayList instead of sychronized list (this is a bottleneck)
Added multithreaded TestPageRank test case

I ran the PageRankBenchmark on 20 workers with 10M vertices, 1B edges.  All supersteps are about the same time, so I just compared superstep 0 from every test.  Compute performance gains are quite nice (even a little faster than before with one thread).  Actual gains will depend heavily on the number of cores you have and possible parallelism of the application.

{code}
Trunk
# threads  compute time (secs)   total time (secs)
1          89                    97.543

Multithreading
1          86.70094              92.477
2          50.41521              57.850
4          38.07716              50.246
8          38.63188              45.940
16         22.999943             48.607
24         23.649189             45.112
32         21.412325             44.201
{code}

We also saw similar gains on the input split loading on an internal app. Future work can be to further improve the scalability of multithreading.

","Duplicated Code, Long Method, , , , , "
"   Rename Method,","Cleaner MutableVertex API Currently, MutableVertex requires the user to instantiate a vertex before creating the request.
Using instantiateVertex() also requires a downcast to MutableVertex.

What we can do instead is pass id and value (and optionally edges) to addVertexRequest(), and the instantiation/initialization will happen internally and safely.",", "
"   Pull Up Method,Move Method,Move Attribute,","More SendMessageCache improvements Having a lot of maps in SendMessageCache still makes it slow, so here is another step towards making it faster.",", , , Duplicated Code, "
"   Move Method,Extract Method,","Metrics Update Updating metrics to be more useful.

https://reviews.apache.org/r/7900/","Duplicated Code, Long Method, , , "
"   Move Class,Move Method,","Refactor / cleanups Some general thoughts I've jotted down while going through the code. Writing them here to start tracking progress for them.

1. Refactor giraph.graph to giraph.master, giraph.worker. The whole giraph.graph package name is bad in general I think.
2. Cleanup giraph.utils. For example move timers stuff to giraph.time.
3. Change module names to be more maven-esque, that is something like giraph-root, giraph-core, giraph-formats.
4. Remove WorkerClientServer. Is this needed anymore?
5. Cleanup MasterThread#run: long convoluted method.
6. Cleanup BspService#process: lots of duplication. Use a vector of events or something.
7. Cleanup Vertex class: seems to me it has too many methods and should be a simpler interface (maybe even eventually an actual interface! not an abstract class). Add something like a Vertexes/Vertices class with helper methods that use can use.
8. {Master,Worker}Observer. Discussed elsewhere already. ALlow users to easily plug in code at various points in the system. Essentially a cleaner implementation of e.g. WorkerContext
9. Cleanup GraphMapper. I don't see why we even call a map() method seeing as we are overriding run(). We are clearly not particularly ""mapreduce-y"" so we should make it our entry point more clear than a map(). Also I think we should have something like a WorkerThread similar to MasterThread and clean up all of this to just creare whichever threads the node is assigned roles of. 
10. Move examples and anything else not needed for a giraph library out into it's own package (like giraph-examples)?


If someone +1s the ideas I'll work up some patches. Feel free to add other cleanup things here as well.",", , "
"   Rename Class,Rename Method,Push Down Method,Move Method,Extract Method,Move Attribute,","Create BinaryCombiner ands specialized message store for it Current combiner interface is very general, but also doesn't provide the best performance. All the combiners we currently have are binary combiners, i.e. they can combine two messages into one. Having a lists around this simple concept makes it slower and requires more object creations.
Adding BinaryCombiner, and a specialized message store which will be used with it. This message store has only one message per vertex instead of having a collection per vertex.","Duplicated Code, Long Method, , , , , "
"   Rename Class,Extract Method,",Refactor / cleanup Hadoop Counters https://reviews.apache.org/r/7980,"Duplicated Code, Long Method, , "
"   Move Class,Rename Method,Push Down Method,Move Method,Extract Method,Push Down Attribute,Move Attribute,","Serialize the graph/message cache into byte[] for improving memory usage and compute speed Our entire graph is currently stored as Java objects in memory.  I added an option to keep only a representative vertex that serializes/deserializes on the fly and should be used with the new ByteArrayPartition.  In conjunction with a serialized client-side message cache, memory usage then loading shrinks to almost 1/10 of trunk and loads the input splits almost 3x faster (see input superstep times below).  I added a serializer based on Sun's unsafe methods that enables this memory savings with a very small performance hit (maybe a few 1-5% slower).  Compared to trunk, when serializing the messages with our faster serializer, compute time improves significantly as well against trunk (16.7 -> 12.31 for 2.5B edges, 2.97 -> 1.61 for 250M edges).  There are still further improvements to be made on the server side where we still store our messages in-memory.  I (or someone else) can do that in a later patch.  This also significantly reduces GC time, as there are less objects to GC.

- Improves byte[] serialization signficantly
-- Added ExtendedDataInput/ExtendedDataOutput interfaces to allow for some additional methods needed for byte[] serialization/deserialization
-- Add ExtendedByteArrayDataInput/ExtendedByteArrayDataoutput to serialize/deserialize Writables to a byte[]
-- Added DynamicChannelBufferOutputStream/DynamicChannelBufferInputStream to serialize/deserialize Writables to a DynamicChannelBuffer

- Gives you the choice of partition implementation (SimplePartition (default) or ByteArrayPartition -> (serialized vertices))
-- Added a new method to Partition called saveVertex(), which also the serialization back into the ByteArrayPartition or does nothing when using SimplePartition
- Gives you the choice of unsafe serialization (using Sun's unsafe class - default) or regular serialization
- Serializes the messages on the client cache into byte[] (saves memory and also serializes faster)
-- Created new ByteArrayVertexIdMessageCollection to support the serialized messages
-- SendVertexRequest now sends Partition objects rather than collections
- Adds 2 more options in PageRankBenchmark to try out RepresentationVertex or RepresentationVertex with unsafe serialization
- Fixed a bug in LongDoubleFloatDoubleVertex's readFields when edges aren't cleared before deserializing

- Added new unittests
-- Replaced TestEdgeListVertex with TestMutableVertex to test all our generic MutableVertex implementations
--- Added more serialization tests of different serialization
-- TestPartitionStores has more testing of unsafe serialization/deserialization

Testing:

All unittests pass
Distributed unittests pass - (except two that also fail in trunk)
Lots of PageRankBenchmark runs on a cluster

Benchmark results:

25 edges / vertex, 10M vertices, 10 workers
Trunk
INFO    2012-11-08 14:43:55,855 [load-0] org.apache.giraph.graph.InputSplitsCallable  - call: Loaded 1 input splits in 22.475897 secs, (v=1000000, e=25000000) 44492.105 vertices/sec, 1112302.6 edges/sec
INFO    2012-11-08 14:44:00,411 [main] org.apache.giraph.graph.BspServiceWorker  - finishSuperstep: Waiting on all requests, superstep -1 totalMem = 81728.6875M, maxMem = 81728.6875M, freeMem = 76580.54187774658M
INFO    2012-11-08 14:44:05,254 [compute-7] org.apache.giraph.graph.ComputeCallable  - call: Computation took 2.9732208 secs for 1 partitions on superstep 0.  Flushing started
INFO    2012-11-08 14:44:11,180 [main] org.apache.giraph.graph.BspServiceWorker  - finishSuperstep: Superstep 0, messages = 25000000 totalMem = 81728.6875M, maxMem = 81728.6875M, freeMem = 74781.9575881958M
Total (milliseconds)    62,413  0       62,413
Superstep 3 (milliseconds)      2,417   0       2,417
Setup (milliseconds)            2,731   0       2,731
Shutdown (milliseconds)         50      0       50
Superstep 0 (milliseconds)      10,654  0       10,654
Input superstep (milliseconds)  27,484  0       27,484
Superstep 2 (milliseconds)      9,475   0       9,475
Superstep 1 (milliseconds)      9,599   0       9,599
Total time of GC in milliseconds        225,052         0       225,052

25 edges / vertex, 10M vertices, 10 workers
SimplePartition + EdgeListVertex (after rebase)
INFO    2012-11-08 14:33:15,907 [load-0] org.apache.giraph.graph.InputSplitsCallable  - call: Loaded 1 input splits in 25.431986 secs, (v=1000000, e=25000000) 39320.562 vertices/sec, 983014.06 edges/sec
INFO    2012-11-08 14:33:17,501 [main] org.apache.giraph.graph.BspServiceWorker  - finishSuperstep: Waiting on all requests, superstep -1 totalMem = 81728.6875M, maxMem = 81728.6875M, freeMem = 76290.28507995605M
INFO    2012-11-08 14:33:20,175 [compute-2] org.apache.giraph.graph.ComputeCallable  - call: Computation took 2.0086238 secs for 1 partitions on superstep 0.  Flushing started
INFO    2012-11-08 14:33:26,667 [main] org.apache.giraph.graph.BspServiceWorker  - finishSuperstep: Superstep 0, messages = 25000000 totalMem = 81728.6875M, maxMem = 81728.6875M, freeMem = 73716.20901489258M
Trunk (after rebase)
Total (milliseconds)    68,113  0       68,113
Superstep 3 (milliseconds)      2,057   0       2,057
Setup (milliseconds)            9,765   0       9,765
Shutdown (milliseconds)         59      0       59
Superstep 0 (milliseconds)      9,180   0       9,180
Input superstep (milliseconds)  27,525  0       27,525
Superstep 2 (milliseconds)      9,600   0       9,600
Superstep 1 (milliseconds)      9,924   0       9,924
Total time of GC in milliseconds        216,345         0       216,345

250 edges / vertex, 10M vertices, 10 workers
ByteArrayPartition + UnsafeRepresentativeVertex + reuse vertexdata buffer + unsafe serialization (after rebase)
INFO    2012-11-08 14:33:09,822 [load-0] org.apache.giraph.graph.InputSplitsCallable  - call: Loaded 1 input splits in 9.3217535 secs, (v=1000000, e=25000000) 107275.95 vertices/sec, 2681898.8 edges/sec
INFO    2012-11-08 14:33:10,900 [main] org.apache.giraph.graph.BspServiceWorker  - finishSuperstep: Waiting on all requests, superstep -1 totalMem = 81728.6875M, maxMem = 81728.6875M, freeMem = 79974.63636779785M
INFO    2012-11-08 14:33:13,213 [compute-7] org.apache.giraph.graph.ComputeCallable  - call: Computation took 1.6110481 secs for 1 partitions on superstep 0.  Flushing started
INFO    2012-11-08 14:33:13,972 [main] org.apache.giraph.graph.BspServiceWorker  - finishSuperstep: Waiting on all requests, superstep 0 totalMem = 81728.6875M, maxMem = 81728.6875M, freeMem = 78228.54064941406M
Total (milliseconds)                    47,061          0       47,061
Superstep 3 (milliseconds)              2,175           0       2,175
Setup (milliseconds)                    3,018           0       3,018
Shutdown (milliseconds)                 1,050           0       1,050
Superstep 0 (milliseconds)              8,780           0       8,780
Input superstep (milliseconds)          10,952          0       10,952
Superstep 2 (milliseconds)              10,450          0       10,450
Superstep 1 (milliseconds)              10,633          0       10,633

250 edges / vertex, 10M vertices, 10 workers
Trunk
INFO    2012-11-08 14:46:25,304 [load-0] org.apache.giraph.graph.InputSplitsCallable  - call: Loaded 1 input splits in 167.02779 secs, (v=1000000, e=250000000) 5987.028 vertices/sec, 1496757.0 edges/sec
INFO    2012-11-08 14:46:35,558 [main] org.apache.giraph.graph.BspServiceWorker  - finishSuperstep: Waiting on all requests, superstep -1 totalMem = 81728.6875M, maxMem = 81728.6875M, freeMem = 38447.11888885498M
INFO    2012-11-08 14:46:52,963 [compute-14] org.apache.giraph.graph.ComputeCallable  - call: Computation took 16.770031 secs for 1 partitions on superstep 0.  Flushing started
INFO    2012-11-08 14:46:53,074 [main] org.apache.giraph.graph.BspServiceWorker  - finishSuperstep: Waiting on all requests, superstep 0 totalMem = 81728.6875M, maxMem = 81728.6875M, freeMem = 24629.869369506836M
Total (milliseconds)            568,094                                          0                  568,094
Superstep 3 (milliseconds)      2,344                                            0                  2,344
Setup (milliseconds)            2,748                                            0                  2,748
Shutdown (milliseconds)         47                                               0                  47
Superstep 0 (milliseconds)      67,853                                           0                  67,853
Input superstep (milliseconds)  177,722                                          0                  177,722
Superstep 2 (milliseconds)      247,518                                          0                  247,518
Superstep 1 (milliseconds)      69,856                                           0                  69,856
Total time of GC in milliseconds                                                 2,741,892          0   2,741,892

250 edges / vertex, 10M vertices, 10 workers
SimplePartition + EdgeListVertex (after rebase)
INFO    2012-11-08 14:19:57,774 [load-0] org.apache.giraph.graph.InputSplitsCallable  - call: Loaded 1 input splits in 172.17258 secs, (v=1000000, e=250000000) 5808.126 vertices/sec, 1452031.5 edges/sec
INFO    2012-11-08 14:20:04,864 [main] org.apache.giraph.graph.BspServiceWorker  - finishSuperstep: Waiting on all requests, superstep -1 totalMem = 81728.6875M, maxMem = 81728.6875M, freeMem = 37025.9013671875M
INFO    2012-11-08 14:20:17,453 [compute-6] org.apache.giraph.graph.ComputeCallable  - call: Computation took 11.959192 secs for 1 partitions on superstep 0.  Flushing started
INFO    2012-11-08 14:20:17,606 [main] org.apache.giraph.graph.BspServiceWorker  - finishSuperstep: Waiting on all requests, superstep 0 totalMem = 81728.6875M, maxMem = 81728.6875M, freeMem = 21953.103630065918M
Total (milliseconds)            470,845                                          0                  470,845
Superstep 3 (milliseconds)      2,595                                            0                  2,595
Setup (milliseconds)            1,774                                            0                  1,774
Shutdown (milliseconds)         54                                               0                  54
Superstep 0 (milliseconds)      59,609                                           0                  59,609
Input superstep (milliseconds)  179,665                                          0                  179,665
Superstep 2 (milliseconds)      165,848                                          0                  165,848
Superstep 1 (milliseconds)      61,296                                           0                  61,296
Total time of GC in milliseconds                                                 2,480,260          0   2,480,260

250 edges / vertex, 10M vertices, 10 workers
ByteArrayPartition + UnsafeRepresentativeVertex + reuse vertexdata buffer + unsafe serialization (after rebase)
INFO    2012-11-08 13:26:50,334 [load-0] org.apache.giraph.graph.InputSplitsCallable  - call: Loaded 1 input splits in 69.22095 secs, (v=1000000, e=250000000) 14446.494 vertices/sec, 3611623.5 edges/sec
INFO    2012-11-08 13:26:52,511 [main] org.apache.giraph.graph.BspServiceWorker  - finishSuperstep: Waiting on all requests, superstep -1 totalMem = 81728.6875M, maxMem = 81728.6875M, freeMem = 75393.74648284912M
INFO    2012-11-08 13:27:06,441 [compute-5] org.apache.giraph.graph.ComputeCallable  - call: Computation took 12.318953 secs for 1 partitions on superstep 0.  Flushing started
INFO    2012-11-08 13:27:06,483 [main] org.apache.giraph.graph.BspServiceWorker  - finishSuperstep: Waiting on all requests, superstep 0 totalMem = 81728.6875M, maxMem = 81728.6875M, freeMem = 62303.2106552124M
Total (milliseconds)            301,720                                          0                  301,720
Superstep 3 (milliseconds)      4,759                                            0                  4,759
Setup (milliseconds)            2,887                                            0                  2,887
Shutdown (milliseconds)         50                                               0                  50
Superstep 0 (milliseconds)      72,625                                           0                  72,625
Input superstep (milliseconds)  75,797                                           0                  75,797
Superstep 2 (milliseconds)      72,245                                           0                  72,245
Superstep 1 (milliseconds)      73,353                                           0                  73,353
Total time of GC in milliseconds                                                 716,930            0   716,930","Duplicated Code, Long Method, , , , , , "
"   Rename Method,Pull Up Method,Extract Method,",Aggregate metrics to Master See https://reviews.apache.org/r/8042/,"Duplicated Code, Long Method, , Duplicated Code, "
"   Rename Method,Extract Method,","Missing progress calls when stopping Netty server At the end of a long running job I got an exception about not reporting progress. The last log line was: ""stop: Halting netty server"", so I suspect it's because awaitUninterruptibly() call there.","Duplicated Code, Long Method, , "
"   Move Class,Rename Method,Move Method,Extract Method,Move Attribute,","Cleanup Configuration related things In this diff:

1) Create a conf/ package for configuration related things.
2) Create a GiraphClasses object from members of ImmutableClassesGiraphConfiguration. Use this class inside ImmutableClassesGiraphConfiguration itself.
3) Use GiraphClasses in tests to cleanup InternalVertexRunner, BspCase. Removes a lot of methods with many parameters. Makes things more type safe. Removes a lot of nulls in calling code.
4) Extract constants from GiraphConfiguration out into separate GiraphConstants. Personally I think these constants should be moved to the place where they are used (i.e. have ZKManager hold its configuration related constants), but I think this is a step in the right direction.

Generally makes things cleaner IMO.

https://reviews.apache.org/r/8437","Duplicated Code, Long Method, , , , "
"   Push Down Method,Push Down Attribute,","Make Vertex an Interface Made Vertex an interface, with DefaultVertex as its implementation.

https://reviews.apache.org/r/11682/",", , , "
"   Move Method,Extract Method,Move Attribute,","Cleanup GraphMapper I don't see why we even call a map() method seeing as we are overriding run(). We are clearly not particularly ""mapreduce-y"" so we should make it our entry point more clear than a map(). Also I think we should have something like a WorkerThread similar to MasterThread and clean up all of this to just creare whichever threads the node is assigned roles of.

Link to review board:

https://reviews.apache.org/r/8898/
","Duplicated Code, Long Method, , , , "
"   Move Method,Extract Method,Move Attribute,","Export Worker's Context/State to vertices through pre/post/Application/Superstep It would be quite useful for vertices to reach some worker-related information stored i.e. in the GraphState class.

This information could be exported as a parameter to pre/post/Application/Superstep like this:

public void preApplication(Configurable workerObject);
public void postApplication(Configurable workerObject);
public void preSuperstep(Configurable workerObject);
public void postSuperstep(Configurable workerObject);
public Configurable getWorkerObject();

Another possibility is to add a Context inner class to BasicVertex to store this information.","Duplicated Code, Long Method, , , , "
"   Rename Method,Pull Up Method,Move Method,","Add convergence detection to org.apache.giraph.examples.RandomWalkVertex I propose to add convergence detection to the RandomWalkVertex. Convergence is achieved when the overall absolute change (L1 norm) of the difference between the current and the previous probability vector becomes less than a given threshold. Convergence detection can be implemented via an additional aggregator and a check in the master compute function.

This change would make the class much easier to use as the users don't have to worry about the number of supersteps to execute, but can simply specify a high number as MAX_SUPERSTEPS and be sure that the algorithm convergences when acceptable quality of the result is reached.",", , Duplicated Code, "
"   Rename Class,Rename Method,Move Method,Extract Method,","Refactor platform-independent CLI argument parsing in GiraphRunner into a separate class In order to run on non Hadoop MR platforms, we will need to populate the GiraphConfiguration for our job in a platform-independent way so that all config options are available to whatever driver class initiates the Giraph job (not just GiraphRunner/GiraphJob.) This also serves to clean up GiraphRunner in general.

Passes 'mvn clean install'

Review Board URL: https://reviews.apache.org/r/9350/
","Duplicated Code, Long Method, , , "
"   Pull Up Method,Extract Method,Pull Up Attribute,","Create PartitionContext Right now we have WorkerContext which is accessible from vertex.compute, but it's not thread safe - usually if we want to use it with multithreaded computation we have to use synchronization. Instead we can create a ComputeContext, which is going to have one instance per compute thread.","Duplicated Code, Long Method, , Duplicated Code, Duplicated Code, "
"   Rename Class,Pull Up Method,Move Method,Extract Method,Pull Up Attribute,Move Attribute,","More efficient and flexible edge-based input The current implementation of edge-based input using mutations is not as memory-efficient as it could be, and also can't be used with immutable vertex classes.
By having an ad-hoc code path for edges sent during input superstep, we can achieve both memory efficiency and eliminate the current restriction.","Duplicated Code, Long Method, , , , Duplicated Code, Duplicated Code, "
"   Rename Method,Move Method,Extract Method,Move Attribute,","Range-partitioning and edge locality benchmark Range-based partitioning can drastically reduce network communication when using a vertex id space where close ids correspond to vertices likely to be connected.

We currently have an incomplete implementation of range-based partitioning that tries to be very generic (allowing arbitrarily different partition sizes).
Talking about this with Avery, we thought that for now it's better to add a simpler version (which tries to split as evenly as possible) and leave the current classes there in case someone wants to implement a more complex logic.

A nice-to-have is also extending the pseudo-random formats to generate a required ratio of partition-local edges, in order to estimate the impact of locality with benchmarks.","Duplicated Code, Long Method, , , , "
"   Extract Interface,Rename Method,Extract Method,","Allow in-place modification of edges This is a somewhat long term item.

Because of some optimized edge storage implementations (byte array, primitive array), we have a contract with the user that Edge objects returned by getEdges() are read-only.

One concrete example where in-place modification would be useful: in the weighted version of PageRank, you can store the weight sum and normalize each message sent, or you could more efficiently normalize the out-edges once in superstep 0.

The Pregel paper describes an OutEdgeIterator that allows for in-place modification of edges. I can see how that would be easy to implement in C++, where there is no need to reuse objects.

Giraph ""unofficially"" supports this if one is using generic collections to represent edges (e.g. ArrayList or HashMap).

It may be trickier in some optimized implementations, but in principle it should be doable.

One way would be to have some special MutableEdge implementation which calls back to the edge data structure in order to save modifications:

{code}
for (Edge<I, E> edge : getEdges()) {
  edge.setValue(newValue);
}
{code}

Another option would be to add a special set() method to our edge iterator, where one can replace the current edge:

{code}
for (EdgeIterator<I, E> it = getEdges().iterator(); it.hasNext();) {
  Edge<I, E> edge = it.next();
  edge.setValue(newValue);
  it.set(edge);
}
{code}

We could actually implement the first version as syntactic sugar on top of the second version (the special MutableEdge would need a reference to the iterator in order to call set(this)).","Duplicated Code, Long Method, , Large Class, "
"   Rename Method,Pull Up Method,Extract Method,",Clean up benchmarks Benchmark classes have a lot of duplicate options and duplicate code which handles CommandLine.,"Duplicated Code, Long Method, , Duplicated Code, "
"   Extract Interface,Extract Method,","Add support for multithreaded output Some output formats, like giraph-hive stuff, can write output in parallel. We should provide an option to choose number of threads to use for storing output.","Duplicated Code, Long Method, , Large Class, "
"   Rename Method,Extract Method,","Decouple vertices and edges in DiskBackedPartitionStore and avoid writing back edges when the algorithm does not change topology. Many algorithms work on a static graph. In these cases, when running out-of-core graph we end up writing back the edges that have not changed since we read them. By decoupling vertices and edges, we can write back only the freshly computed vertex values.","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,","Rename VertexEdges to OutEdges As we discussed in the mailing list, the current naming is a bit confusing.",", "
"   Rename Class,Move And Rename Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Decouple Vertex data and Computation, make Computation and Combiner classes switchable Currently, our Vertex class holds a lot of stuff which shouldn't be there, related to global graph and worker state. We should decouple vertex and computation, vertex should be provided by the infrastructure and user should implement the computation.

In many real-world applications there are several different stages, where different kind of computation is done and different type of messages are sent. This can be done currently by having complicated compute() and encoding the message type inside of the message. Much better would be to provide a way to change which Computation is used by Giraph. Applications can then be considered as pieces which can be put together in a pipeline coordinated by master.","Duplicated Code, Long Method, , , , , "
"   Rename Method,",Input superstep should support aggregators like any other superstep add aggregator for VertexReader to allow user to aggregate values at Input SuperStep,", "
"   Move And Rename Class,Move Class,Move Method,Move Attribute,","Clean up message stores The hierarchy of message stores is very complex at the moment, and message stores are required to implement some methods which are not used by the infrastructure (like addMessages(MessageStore), readFields/write, getNumberOfMessages) - they are only there because out-of-core stores need them. I'm working on some optimized message stores, so decided to clean this up first.",", , , "
"   Extract Method,Move Attribute,","Communication improvement using one-to-all message sending Using one-to-all message sending to send one message copy to multiple target vertices on the same worker.
To enable this feature, use conf.enableOneToAllMsgSending(). 

Pagerank benchmarking (Partitions: 640, Edges per vertex: 500, Vertices: 200000000, Workers: 40)
shows that the time per superstep is reduced by 40%. 
","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,Inline Method,","Giraph Application Master: move to new and stable YARN API Giraph was the early adopter of Hadoop YARN AM! Eli successfully wrote a Giraph AM based on Hadoop 2.0.x_alpha. However, in last few months, Yarn significantly *overhauled* its APIs and associated coding patterns. The new beta version is 2.1.x and I was told by Yarn-dev that current APIs will not change much.
In the above circumstances, we need to substantially overhaul Giraph AM as well to accommodate with the new Yarn API. Moreover, in newer YARN API, supporting kerberos security in AM becomes easier and more transparent.

Potential impact:
The upcoming Girpah AM will not work with earlier alpha Hadoop versions such as 2.0.3. I'm not sure if anyone is using Giraph AM in production. However, the more prevalent way of Giraph processing (MR-based) should continue to work.



    ","Duplicated Code, Long Method, , , "
"   Move And Rename Class,Move Method,","Efficient dense matrix aggregators In applications where a matrix is needed, is not efficient to have an aggregator per entry. This update provides the same functionality with an aggregator per matrix row. This implementation uses an array per row and is efficient when the matrices are dense.
",", , "
"   Rename Class,Pull Up Method,Move Method,Pull Up Attribute,","Improve GraphPartitionerFactory usage  Usage of GraphPartitionerFactory can be improved:

- defining custom partitioner needs extending 3 classes/interfaces, and defining multiple functions, which is more complex than needed 
- range partitioners are randomly assigning excess, instead of having each partition/worker having 1 consecutive range
- if maxPartitions is reached, we might be creating number of partitions that is not divisible with number of workers",", , Duplicated Code, Duplicated Code, "
"   Extract Method,Move Attribute,",Print job progress to command line Currently we print nothing about job progress to command line. We should track which stage are we in and how far in it are we.,"Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Don't expose the list holding the messages in BasicVertex I'm currently trying to implement my own memory efficient vertex (similar to LongDoubleFloatDoubleVertex) and ran into problems with getMsgList()

This method returns a list pointing to the messages of the vertex and it is modified externally (BasicRPCCommunications calls clear() and addAll() e.g.). This makes it very hard to use something else than a java.util.List internally (LongDoubleFloatDoubleVertex ""hacked"" around this) and it is generally dangerous to have the internal state of an object be modified externally. It also makes the code harder to read and understand.

I'd suggest to change the API to let a vertex handle the modifications itself internally (e.g. add something like pushMessages(...))","Duplicated Code, Long Method, , "
"   Rename Method,Push Down Method,Extract Method,Push Down Attribute,","Upgrade to netty 4 Off late netty 4 has earned so much praise in the community. For example, 
https://blog.twitter.com/2013/netty-4-at-twitter-reduced-gc-overhead

A switch to netty 4 enables a significant reduction in gc pressure and also huge performance gains. I started working on this last Sunday and have a patch that shows performance gains on the order of 15-25% (total execution time) for some applications at Facebook. However, I only tested it with hadoop_facebook. So there might be issues with SASL path. 

I will release the patch today and want to open up a discussion if anyone is using the secure feature anymore. If not we can just deprecate it.","Duplicated Code, Long Method, , , , "
"   Push Down Method,Push Down Attribute,","Specialized edge stores While doing some performance tuning I discovered that loading the edge store can be a very expensive operation. Similar to GIRAPH-704, the use of primitive maps can provide significant performance benefit. Part of the benefit comes with the lower memory overhead associated with the primitive maps however the larger benefit comes with the fact that you don't have to release and reconstruct the vertexId object every time a new vertex is encountered.

When processing a large graph with 4B vertices and 5B edges (3B of the edges loaded via EdgeInputFormat) the worker edge requests were taking ~15 seconds each, but after implementing the above suggestions that number dropped down sub-second.",", , , "
"   Rename Class,Rename Method,Move Method,Extract Method,Move Attribute,","support succinct representation of messages in messagestores currently one-to-all messages is useful only for reducing network transfer, but at the server side, before processingRequest, the messages are broken down into bytearrayvertexidmessages, thus, each message is still stored as many times as the number of vertices it is sent to. 

in this task we implements two message-stores that can represent messages in a very succinct way
especially useful when messages are of significant size & fanout is low [note that this is - useful only when no combiners are used]

this depends on GIRAPH-907 & GIRAPH-908","Duplicated Code, Long Method, , , , "
"   Move Class,Extract Method,Push Down Attribute,","Checkpointing improvements We need to address some issues with checkpointing:
1) worker2worker messages are not saved
2) BspServiceWorker does not compile under hadoop_0.23 profile
3) it would be nice to be able to manually checkpoint and stop any job at any point of time. 

","Duplicated Code, Long Method, , , "
"   Extract Method,Push Down Attribute,","Improve job tracking on command line Currently job client connects to Zookeeper and uses information from there to report progress on the command line. This means that if job fails there is no way for app to communicate back to job client why it failed, and also bunch of zookeeper exceptions gets thrown. 
Instead, we can use swift as a channel of communication between job client and master/workers. Also we can allow for MasterCompute/WorkerContext to print info directly to command line - is useful to report app related progress or for debugging.","Duplicated Code, Long Method, , , "
"   Rename Method,Move Method,","Upgrade to Gora 0.5 We recently released Gora 0.5
http://www.mail-archive.com/dev%40gora.apache.org/msg05236.html
Giraph should upgrade to 0.5 as there is better support with added modules which Giraph can take advantage of.",", , "
"   Move Class,Move Method,Extract Method,Move Attribute,","In-proc ZooKeeper server with Master process Currently by default zookeeper runs as a separate java process, on the same server where master runs. This prevents us from seeing zookeeper logs and makes it harder to debug memory issues.  We should be able to run zookeeper inside Master process and perhaps this should be default. ","Duplicated Code, Long Method, , , , "
"   Rename Class,Rename Method,","Improve naming for ReduceOperation reduceSingle/reducePartial can be slightly confusing.
Also OnSameReduceOperation should be named better",", "
"   Move Class,Extract Interface,","Organize source code for Apache Now that the initial import is complete, we need to organize the source code for the apache namespace and conventions. 

Things to do: 
- move package names to org.apache.gora 
- add license headers
- Edit Readme 
",", Large Class, "
"   Rename Method,Pull Up Method,Extract Method,Move Attribute,","Datastore for gora-dynamodb  As per comments on the Gora dev@ mailing list, and per our application submission [1] and ideas page [2], this issue should track, from start to finish the development, montoring, reviwing, testing and eventual implementation of the gora-dynamodb module for Gora.

[1] https://cwiki.apache.org/confluence/display/GORA/GSoC+2012
[2] https://cwiki.apache.org/confluence/display/GORA/GSoC+2012+Ideas+Page","Duplicated Code, Long Method, , , Duplicated Code, "
"   Rename Method,",Change CassandraClient#init() to CassandraClient#initialize() for consistency with other Datastores.  This makes drawing comparisons between the different Datastores easier and should have been kept consistent from the start anyway.,", "
"   Rename Method,",Change CassandraClient#init() to CassandraClient#initialize() for consistency with other Datastores.  This makes drawing comparisons between the different Datastores easier and should have been kept consistent from the start anyway.,", "
"   Rename Method,Extract Method,","gora-cassandra array type support  In order to support ARRAY in gora-cassandra, we have two scenarios as follows:
1) super column family like the current MAP implementation; or
2) single column to store all elements of ARRAY.
Each senario has pros and cons, but I'd prefer 1) and I have implemented a prototype.

1) super column family
** pros
- consistent with MAP
- each column stores primitive type value.
** cons
- ARRAY cannot be contained in RECORD or MAP.

2) single column
** pros
- complex type such as RECORD and MAP can have ARRAY value.
** cons
- large size of ARRAY requres a huge single column value.
- difficult to implement for STRING and BYTES (variable length).

Currently, super column is used for other complex types such as RECORD and MAP,
so it is consistent to use super column for ARRAY.
Considering this, the rule that complex type cannot have complex type value
seems a reasonable limitation, and it makes rule simple.

If we take 1) senario with super column family,
I can provide a patch of my implementation.
","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","gora-cassandra array type support  In order to support ARRAY in gora-cassandra, we have two scenarios as follows:
1) super column family like the current MAP implementation; or
2) single column to store all elements of ARRAY.
Each senario has pros and cons, but I'd prefer 1) and I have implemented a prototype.

1) super column family
** pros
- consistent with MAP
- each column stores primitive type value.
** cons
- ARRAY cannot be contained in RECORD or MAP.

2) single column
** pros
- complex type such as RECORD and MAP can have ARRAY value.
** cons
- large size of ARRAY requres a huge single column value.
- difficult to implement for STRING and BYTES (variable length).

Currently, super column is used for other complex types such as RECORD and MAP,
so it is consistent to use super column for ARRAY.
Considering this, the rule that complex type cannot have complex type value
seems a reasonable limitation, and it makes rule simple.

If we take 1) senario with super column family,
I can provide a patch of my implementation.
","Duplicated Code, Long Method, , "
"   Rename Class,Extract Interface,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Merge GORA_94 into Gora trunk This is a final issue to merge in GORA_94 into trunk.
I'll get a patch attached here which provides a diff people can apply against Gora trunk code base.
Within the scope of this patch, I am proposing to NOT release gora-dynamodb for 0.4 as there is additional thought required to get it working.
[~renato2099] and myself have been discussing this at ApacheCon and will address it in due course.
I would like to personally thank ALL contributors to GORA_94 branch over recent months. It became a bit of a monster and it a major contribution to Apache Gora. ","Duplicated Code, Long Method, , , , , Large Class, "
"   Rename Class,Extract Interface,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Merge GORA_94 into Gora trunk This is a final issue to merge in GORA_94 into trunk.
I'll get a patch attached here which provides a diff people can apply against Gora trunk code base.
Within the scope of this patch, I am proposing to NOT release gora-dynamodb for 0.4 as there is additional thought required to get it working.
[~renato2099] and myself have been discussing this at ApacheCon and will address it in due course.
I would like to personally thank ALL contributors to GORA_94 branch over recent months. It became a bit of a monster and it a major contribution to Apache Gora. ","Duplicated Code, Long Method, , , , , Large Class, "
"   Rename Class,Extract Interface,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Merge GORA_94 into Gora trunk This is a final issue to merge in GORA_94 into trunk.
I'll get a patch attached here which provides a diff people can apply against Gora trunk code base.
Within the scope of this patch, I am proposing to NOT release gora-dynamodb for 0.4 as there is additional thought required to get it working.
[~renato2099] and myself have been discussing this at ApacheCon and will address it in due course.
I would like to personally thank ALL contributors to GORA_94 branch over recent months. It became a bit of a monster and it a major contribution to Apache Gora. ","Duplicated Code, Long Method, , , , , Large Class, "
"   Rename Method,","Removal of _g_dirty field from _ALL_FIELDS array and Field Enum in Persistent classes In auto-generated persistent classes, we create an array field called ALL_FIELDS as you know. But this array also contains _g_dirty field, which is not a stored field at all. Maybe we should remove _g_dirty field from the array, since the array is used for getting all fields in the stored table. We can also remove it from Field enum, so the users do not know about the _g_dirty field.",", "
"   Rename Class,Move And Rename Class,Rename Method,Move Method,Extract Method,",Refactor gora-dynamodb to support avro serialization Currently the gora-dynamodb module supports only DynamoDB serialization but it should also support Avro based serialization.,"Duplicated Code, Long Method, , , "
"   Rename Method,","Upgrade HBase to 0.98 HBase 0.98 release is the current stable release.

Gora should be built based on HBase 0.98.",", "
"   Extract Method,Move Attribute,","Configure MongoDB ReadPreference and WriteConcern h5. Actual behavior
Current MongoStore implementation doesn't allow configuration of  [Read Preference|http://docs.mongodb.org/manual/core/read-preference/] and [Write Concern|http://docs.mongodb.org/manual/core/write-concern/] of MongoDB Java Driver

h5. Proposed improvement
Add ""gora.mongodb.readpreference"" and ""gora.mongodb.writeconcern"" properties to allow configuration during store initialization

Please review PR :
https://github.com/apache/gora/pull/28","Duplicated Code, Long Method, , , "
"   Rename Class,Extract Interface,Move Method,Extract Method,Inline Method,",Upgrade to Apache Avro 1.7.x I am not sure what this involves as of yet but I have a small feeling that it's going to be some reasonably major work...  ,"Duplicated Code, Long Method, , , , Large Class, "
"   Rename Method,Extract Method,",Upgrade to Apache Avro 1.7.x I am not sure what this involves as of yet but I have a small feeling that it's going to be some reasonably major work...  ,"Duplicated Code, Long Method, , "
"   Rename Method,Move Method,Extract Method,Inline Method,","Add support for multiple distinct operators in the same query block Impala only allows a single (DISTINCT columns) expression in each query.

{color:red}Note:
If you do not need precise accuracy, you can produce an estimate of the distinct values for a column by specifying NDV(column); a query can contain multiple instances of NDV(column). To make Impala automatically rewrite COUNT(DISTINCT) expressions to NDV(), enable the APPX_COUNT_DISTINCT query option.
{color}

{code}
[impala:21000] > select count(distinct i_class_id) from item;
Query: select count(distinct i_class_id) from item
Query finished, fetching results ...
16
Returned 1 row(s) in 1.51s
{code}

{code}
[impala:21000] > select count(distinct i_class_id), count(distinct i_brand_id) from item;
Query: select count(distinct i_class_id), count(distinct i_brand_id) from item
ERROR: com.cloudera.impala.common.AnalysisException: Analysis exception (in select count(distinct i_class_id), count(distinct i_brand_id) from item)
	at com.cloudera.impala.analysis.AnalysisContext.analyze(AnalysisContext.java:133)
	at com.cloudera.impala.service.Frontend.createExecRequest(Frontend.java:221)
	at com.cloudera.impala.service.JniFrontend.createExecRequest(JniFrontend.java:89)
Caused by: com.cloudera.impala.common.AnalysisException: all DISTINCT aggregate functions need to have the same set of parameters as COUNT(DISTINCT i_class_id); deviating function: COUNT(DISTINCT i_brand_id)
	at com.cloudera.impala.analysis.AggregateInfo.createDistinctAggInfo(AggregateInfo.java:196)
	at com.cloudera.impala.analysis.AggregateInfo.create(AggregateInfo.java:143)
	at com.cloudera.impala.analysis.SelectStmt.createAggInfo(SelectStmt.java:466)
	at com.cloudera.impala.analysis.SelectStmt.analyzeAggregation(SelectStmt.java:347)
	at com.cloudera.impala.analysis.SelectStmt.analyze(SelectStmt.java:155)
	at com.cloudera.impala.analysis.AnalysisContext.analyze(AnalysisContext.java:130)
	... 2 more
{code}

Hive supports this:
{code}
$ hive -e ""select count(distinct i_class_id), count(distinct i_brand_id) from item;""
Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties
Hive history file=/tmp/grahn/hive_job_log_grahn_201303052234_1625576708.txt
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201302081514_0073, Tracking URL = http://impala:50030/jobdetails.jsp?jobid=job_201302081514_0073
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=m0525.mtv.cloudera.com:8021 -kill job_201302081514_0073
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2013-03-05 22:34:43,255 Stage-1 map = 0%,  reduce = 0%
2013-03-05 22:34:49,323 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 4.81 sec
2013-03-05 22:34:50,337 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 4.81 sec
2013-03-05 22:34:51,351 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 4.81 sec
2013-03-05 22:34:52,360 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 4.81 sec
2013-03-05 22:34:53,370 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 4.81 sec
2013-03-05 22:34:54,379 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 4.81 sec
2013-03-05 22:34:55,389 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 8.58 sec
2013-03-05 22:34:56,402 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 8.58 sec
2013-03-05 22:34:57,413 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 8.58 sec
2013-03-05 22:34:58,424 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 8.58 sec
MapReduce Total cumulative CPU time: 8 seconds 580 msec
Ended Job = job_201302081514_0073
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 8.58 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 8 seconds 580 msec
OK
16	952
Time taken: 25.666 seconds
{code}","Duplicated Code, Long Method, , , , "
"   Rename Method,Extract Method,","Allow creating Avro tables without column definitions. Allow COMPUTE STATS to always work on Impala-created Avro tables. When trying to run {{COMPUTE STATS}} on a table I created without any column definition (all columns come from the Avro schema and the partition keys), it fails with the following error message:{code}
Query: compute stats mytable
ERROR: AnalysisException: Cannot COMPUTE STATS on Avro table 'mytable' because its column definitions do not match those in the Avro schema.
Missing column definition corresponding to Avro-schema column 'thefirstcolumn' of type 'STRING' at position '0'.
Please re-create the table with column definitions, e.g., using the result of 'SHOW CREATE TABLE'{code}

I feel this is somewhat related to [IMPALA-867|https://issues.cloudera.org/browse/IMPALA-867], and I also understand the workaround proposed in the error message (the same thing is proposed in the comments of [IMPALA-867|https://issues.cloudera.org/browse/IMPALA-867]).

The documentation states:{quote}Originally, Impala relied on users to run the Hive ANALYZE TABLE statement, but that method of gathering statistics proved unreliable and difficult to use. The Impala COMPUTE STATS statement is built from the ground up to improve the reliability and user-friendliness of this operation.{quote}

To me, having to re-create the table with column definitions in the Hive metastore is not so user-friendly. Since {{COMPUTE STATS}} was built from the ground up, can it not get the columns list from the schema and partitions, rather than use the Hive metastore for that?

Otherwise, I have to keep on re-creating the table... In case I use that workaround, how do I efficiently ""transfer"" all partitions to the new table?

-- 

*Update*
* As per Impala 1.4, CREATE TABLE will find the columns from the Avro schema
* What is still required is only the update of these columns as the schema evolves (at least when ALTER TABLE is used to change the schema URL, possibly also if the file on HDFS changes?)","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Unioning queries with subqueries could be supported The query below is rejected in analysis but could be allowed

{noformat}
Query: 
  select COUNT(*) FROM alltypestiny WHERE int_col = (SELECT 1) 
UNION 
  SELECT int_col FROM alltypestiny
ERROR: AnalysisException: Unsupported statement containing subqueries: SELECT count(*) FROM functional.alltypestiny WHERE int_col = (SELECT 1) UNION SELECT int_col FROM functional.alltypestiny
{noformat}

is the same as 

{noformat}
Query: 
  select count(*) 
  FROM functional.alltypestiny 
  LEFT SEMI JOIN (SELECT 1 `$c$1`) `$a$1` ON int_col = `$a$1`.`$c$1` 
UNION 
  SELECT int_col FROM functional.alltypestiny
+----------+
| count(*) |
+----------+
| 4        |
| 1        |
| 0        |
+----------+
{noformat}","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Slow DDL statements for tables with large number of partitions Impala users sometimes report that DDL statements (e.g. alter table partition set location...) are taking multiple seconds (>5) for partitioned tables with large number of partitions. The same operations are significantly faster in hive (sub-second response time). 

Use case:
* 2 node cluster
* Single table (24 columns, 3 partition keys) with 2500 partitions
* alter table foo partition (foo_i = i) set location 'hdfs://.....' takes approximately 5-6sec (0.2 in HIVE)
* 1 sec delay in the alter stmt is caused by https://issues.apache.org/jira/browse/HIVE-5524","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","SQL Support for Cached Replicas Impala should support increasing the cache replication factor for tables and partitions.

The currently proposed way of doing this is

{{[set] cached in 'pool' with replication = xxx}}","Duplicated Code, Long Method, , "
"   Extract Superclass,Extract Method,","Impala needs to support all operators in drop partitions (<, >, <>, !=, <=, >=) like hive does In hive we can do this
ALTER TABLE foo DROP PARTITION(ds < 'date')

And also have drop partitions in a range with all these operators.
<, >, <>, !=, <=, >=

https://issues.apache.org/jira/browse/HIVE-2908

We should have the same feature in impala too.","Duplicated Code, Long Method, , Duplicated Code, Large Class, "
"   Rename Method,",Impala should support CROSS JOIN Getting this error message when attempting a CROSS JOIN: ERROR: com.cloudera.impala.common.NotImplementedException: Join requires at least one equality predicate between the two tables.,", "
"   Rename Method,","Consider replacing constant exprs with literal equivalents Evaluating expressions that are the same for every row can be very expensive (note these numbers are taken from the debug build, so maybe this is not an issue in release?):

{code}
(15:33:02@desktop) ~/src/cloudera/impala (cdh5-trunk) $ impala-shell.sh -q ""SELECT 1.2345678 FROM tpch.lineitem"" -B 2>&1 > /dev/null
...
Fetched 6001215 row(s) in 61.33s

(15:34:06@desktop) ~/src/cloudera/impala (cdh5-trunk) $ impala-shell.sh -q ""SELECT CAST('1.2345678' AS DOUBLE) FROM tpch.lineitem"" -B 2>&1 > /dev/null
...
Fetched 6001215 row(s) in 74.86s
{code}",", "
"   Rename Method,","Support INSERT and LOAD DATA to S3 Impala can query from S3 but not write to S3.  A few mechanical changes are needed to make the insert/load data code use the correct filesystem rather than the table's base dir (see also IMPALA-1816), however the bigger issue that needs to be decided is:

These operations both rely on FileSystem.rename() at the coordinator to narrow the crash consistency window. But, S3 doesn't natively support rename, and rename() will actually do a copy and delete which is too slow and prone to failure, defeating the purpose.  A couple options that can be considered/explored: (a) simply document that S3 doesn't give the same amount of crash consistency, (b) try to use multi-part uploading to simulate the same effect (since parts are ""hidden"" until the multi-part is committed), or (c) enhance the metastore so that it can atomically add files (i.e. S3 objects) to tables.

Also, some regions provide eventual consistency, though maybe that's only for writes rather than object creation, so maybe not an issue.",", "
"   Rename Class,Rename Method,Pull Up Method,Extract Method,Pull Up Attribute,","Add support for nested loop joins Add support for nested loop joins in Impala. Primarily used in conjunction with nested types and in cases where non-equity join predicates are used with certain join types.

Example query that is currently not supported and could be executed when nested loop join is implemented:
{code}
select * from functional.alltypestiny t1 left outer join functional.alltypessmall t2 on t1.int_col < t2.int_col;
{code} 

Also, in the context of nested types the following query could be executed using a nested loop join:
{code}
select * from customer c left outer join c.orders;
{code}","Duplicated Code, Long Method, , Duplicated Code, Duplicated Code, "
"   Rename Class,Rename Method,Push Down Method,Push Down Attribute,","Add support for nested loop joins Add support for nested loop joins in Impala. Primarily used in conjunction with nested types and in cases where non-equity join predicates are used with certain join types.

Example query that is currently not supported and could be executed when nested loop join is implemented:
{code}
select * from functional.alltypestiny t1 left outer join functional.alltypessmall t2 on t1.int_col < t2.int_col;
{code} 

Also, in the context of nested types the following query could be executed using a nested loop join:
{code}
select * from customer c left outer join c.orders;
{code}",", , , "
"   Rename Class,Rename Method,Pull Up Method,Extract Method,Pull Up Attribute,","Add support for nested loop joins Add support for nested loop joins in Impala. Primarily used in conjunction with nested types and in cases where non-equity join predicates are used with certain join types.

Example query that is currently not supported and could be executed when nested loop join is implemented:
{code}
select * from functional.alltypestiny t1 left outer join functional.alltypessmall t2 on t1.int_col < t2.int_col;
{code} 

Also, in the context of nested types the following query could be executed using a nested loop join:
{code}
select * from customer c left outer join c.orders;
{code}","Duplicated Code, Long Method, , Duplicated Code, Duplicated Code, "
"   Rename Method,Extract Method,","Include the database comment when showing databases The create database/schema statement allows one to include a comment with the database.  However it doesn't seem to ever get displayed anywhere within impala.  It would be very useful if the show databases statement showed the comment associated with each database.  Also, the HUE metadata browser should similarly show the comment.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Add PERCENT_RANK, NTILE, CUME_DIST analytic window functions We should add common percentile-based analytic functions (PERCENT_RANK, NTILE, CUME_DIST). They should be able to be implemented as rewrites in the planner.","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,","Add support for DESCRIBE DATABASE similar to Hive Impala doesn't seem to currently support either the:

DESCRIBE DATABASE <db_name>;
or
DESCRIBE SCHEMA <schema_name>;

syntax that is available in Hive: https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-DescribeDatabase

This would be useful in retrieving the comment and location (and any other additional metadata that may be added) for a database.",", "
"   Rename Method,Extract Method,","Extrapolate the number of rows in a scan based on the rows/byte ratio *This JIRA is intended to address the following problems*
* Some partitions may be missing the #rows stat
* Some partitions may have the #rows stat but it is stale because files were added/dropped since computing the #rows stat

*The main idea is to use available #rows stats to extrapolate the missing stats*
* Store an additional statistic rows/byte in the TBLPROPERTIES of the table (could also be rows/kbyte or whatever seems most suitable)
* That statistic is computed as part of COMPUTE [INCREMENTAL] STATS on the impalad side, and then shipped to the catalogd for it to be stored in the Metastore
* During query planning we use the rows/byte statistic to estimate the number of rows scanned for *all* partitions regardless of whether a partition has #rows or not. The rationale is that the #rows of a partition may be outdated and using the rows/byte ratio is more robust to data changes.
* We should augment SHOW TABLE STATS to display the stored #rows as well as the extrapolated #rows.
* We should have some way of reporting the stored rows/byte ratio for debugging purposes (maybe SHOW TABLE STATS or EXPLAIN?)

*Additional considerations*
* A table could have mixed formats
* Even if a table has the same format, files could be compressed differently
* It seems reasonable to ignore these issues in the first cut

*Non-Goals*
* Estimate statistics if there are no stats at all, e.g. purely based on file size without knowing any #rows
* Extrapolate column stats like NDV in a similar fashion. That is a much more invasive change with a smaller impact.
","Duplicated Code, Long Method, , "
"   Rename Method,","Add endtime to impalad lineage output Currently we just include start time as ""timestamp"" in the lineage. endtime should be useful too.",", "
"   Move Method,Extract Method,Move Attribute,",Min/max values on partition columns Tableau runs MIN/MAX values on columns that are selected for filtering to determine the filter range in the UI. If the filter is on a partition column it would be great to be able to just use the partition metadata instead of running full scans.,"Duplicated Code, Long Method, , , , "
"   Move Method,Extract Method,",Impala should read java udfs created from Hive 0,"Duplicated Code, Long Method, , , "
"   Rename Method,","&raw param for debug web UI /varz and others similar to what query profiles have Raw text only no html param &raw would make it easier for people to copy and paste from the debug web UI without formatting problems. I would personally like to see this on /varz especially, metrics would be useful too, but is probably worth adding to all /urls.",", "
"   Extract Method,Inline Method,","Add a variant of alter table to manually add column stats Current we have SQLs to add row stats for partitioned/non_partitioned tables. For ex:

{noformat}
alter table analysis_data set tblproperties('numRows'='1001000000'); #non-partitioned
alter table partitioned_data set tblproperties ('numRows'='1030000'); #partitioned
{noformat}

We should have something similar for column stats too.","Duplicated Code, Long Method, , , "
"   Rename Class,Rename Method,Move Method,Extract Method,","Support virtual views using CREATE VIEW statement. Add support for virtual views using a syntax similar to MySql's:
http://dev.mysql.com/doc/refman/5.0/en/create-view.html","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Support Kudu UPSERT in Impala Add UPSERT to Impala. This also includes adding/updating relevant functional tests, stress tests, and query generator tests. It may impact our statistics story too (TBD, stats work tracked by IMPALA-2830).

This should add syntax which works like {{INSERT}} for Kudu tables, but specifying that Kudu perform an UPSERT rather than an INSERT.

{code}
UPSERT INTO table [( column [, ...] )] VALUES ( value-expression [,...] )
UPSERT INTO table [( column [, ...] )] SELECT select-expression
{code}","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,Extract Method,","Add support for 'LOAD DATA' statements It would be good to support the 'LOAD DATA' command in Impala. LOAD operations do not do any transformation of data, they are pure copy/move operations.

Syntax:
{code}
LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]
{code}

If LOCAL is not specified the command should move data from the given location to the table/partition storage location. If LOCAL is specified, the data is copied from the local file system. Impala should needs to support loading from HDFS locations, but loading from a local path may not be needed.","Duplicated Code, Long Method, , "
"   Rename Method,Inline Method,","Support column permutations on INSERT It would be good to support the following:

{code}
INSERT INTO tbl(b,c,d,a) SELECT ...
{code}

i.e. permutation of the columns from the {{SELECT}}, this makes it easier to write correct {{INSERT}} statements.",", , "
"   Rename Method,","Tuning parameters for Impala HBase Integration Impala's hbase integration does not tuning of the query sent to HBase in particular in particular the scan cache size, and disabling of the block cache.   Both parameters are documented has having a performance on queries and also the wider HBase platform.   Disabling the block cache results prevents churn on the region server heap, the scanner cache reduces round trips.  This is normally set via the setCacheBlocks and setCaching methods on the scanner API.",", "
"   Rename Method,","Tuning parameters for Impala HBase Integration Impala's hbase integration does not tuning of the query sent to HBase in particular in particular the scan cache size, and disabling of the block cache.   Both parameters are documented has having a performance on queries and also the wider HBase platform.   Disabling the block cache results prevents churn on the region server heap, the scanner cache reduces round trips.  This is normally set via the setCacheBlocks and setCaching methods on the scanner API.",", "
"   Move And Rename Class,Rename Method,Extract Method,Inline Method,","Make ""REFRESH"" command a SQL statement rather than RPC It would be good to make ""REFRESH"" a first-class SQL statement rather than just an RPC. This would allow users to submit refreshes outside of the impala-shell (for example - via JDBC/ODBC). Initially, we would need to support both a full catalog refresh as well as a table-level refresh:

REFRESH;
REFRESH <table name>;

IMPALA-339 may introduce some additional syntax to choose between a RELOAD and a REFRESH so that should be covered as well.

As part of the change the impala-shell should be updated to submit refreshes using the regular ""query"" API rather than calling ResetCatalog/ResetTable","Duplicated Code, Long Method, , , "
"   Move Method,Extract Method,","Impala ""show create table"" statement There is currently no ""show create table"" statement in Impala, would be useful to have.

Thanks

Hari","Duplicated Code, Long Method, , , "
"   Rename Class,Move And Rename Class,Rename Method,Extract Method,","Add support for User Authorization Currently all HDFS table access is done by the Impala user, even in Kerberized environments. Impala needs to support different ways to control access to the data in a cluster. There are a few different areas this includes:

1) SQL statement authorization on server objects  (database, tables, URIs, etc). There needs to be a mechanism to restrict access (and set different access levels) for users in the cluster.

2) We need the ability to do user impersonation, similar to Hive, as well as ACL control of different tables for different users/groups to cover the use cases where multiple groups within biz are sharing an HDFS cluster with Impala. Without this the lack of access control to all tables is a deal breaker.


Could we provide a rough timeline of when we think this feature could be implemented?","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Adopt Oracle-style hint placement for INSERT statements For consistency with Oracle we should consider accepting hints in the same places in SQL statements. For example, our current INSERT statements accepts hints right before the SELECT portion:
{code}
INSERT INTO t PARTITIONED(year,month) /*+ hints */ SELECT * FROM src;
{code}

The proposal is to accept hints immediately after INSERT like Oracle does:
{code}
INSERT /*+ hints */ INTO t PARTITIONED(year,month) SELECT * FROM src;
{code}

Ideally, we would not accept hints in multiple places to avoid confusion and to reduce the code and testing burden. Ceasing to recognize the old hint placement is a backwards incompatible change.","Duplicated Code, Long Method, , "
"   Move Method,Move Attribute,",Impala should build against latest Hadoop components Hive 2 and Hadoop 3 and HBase 3 made breaking API changes. This issue is to track progress on getting Impala to build against the new APIs.,", , , "
"   Rename Method,","Don't abort Catalog startup quickly if HMS is not present If the catalog daemon can't contact the HMS on startup, it will fail out of the {{Catalog}} constructor in {{MetaStoreClientPool.addClients()}}.

We might consider not doing so, but instead retry for a longer time to allow the catalog and the HMS to be started concurrently.",", "
"   Rename Method,Extract Method,",Introduce expr rewrite phase. To address issues like IMPALA-1286 we should introduce a new expr rewrite phase where analyzed exprs can be transformed with rules. The new phase could be similar to the subquery rewrite phase where we transform exprs in-place and then reset() and analyze() the whole statement again.,"Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Specify more options when adding new kudu columns Currently, due to limitations in the Kudu API, Impala imposes a number of constraints on new columns:
1. nullable columns can't have default values (KUDU-1747)
2. no encoding/compression/block size can be specified (KUDU-1746)
Now that Kudu jiras have been resolved, we should remove these constraints on the Impala side.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,Inline Method,","Remove duplication of isConstant() and IsConstant() in frontend and backend Currently Expr.isConstant() and Expr::IsConstant() in the frontend and backend have duplicate logic to do exactly the same analysis. They need to be kept exactly in sync to avoid problems. We should plumb through the value of isConstant() from the frontend to avoid this duplication.

We need to be a little careful about how we do this in the frontend: Alex Behn mentioned that storing state in Exprs was risky, and also that naively calling isConstant() for each expr node was problematic since it could result in traversing the Expr tree many times.","Duplicated Code, Long Method, , , "
"   Rename Class,Rename Method,","Support changing Kudu default and storage attributes Add Impala support for:
{code}
KUDU-861 Support changing default, storage attributes

This patch adds support for adding, changing, or removing column defaults
and changing the storage attributes of a column. Changes to a column are
encoded as a ColumnSchemaDelta, which can be merged with a ColumnSchema
to change it.

Changing type and nullability of a column is still unsupported.

No failures in 100+ iterations of alter_table-randomized-test.

I also ran into an issue with altering RLE columns:
1. Add an RLE-encoded column to a table
2. Alter the column
3. Scan the column
3 will cause a check failure on scanning to pos 0 of an empty RLE
block. Test and fix included.

Change-Id: I457d99ba2188ef6e439df47c0d94f2dc1a62ea6c
Reviewed-on: http://gerrit.cloudera.org:8080/4310
Tested-by: Kudu Jenkins
Reviewed-by: Dan Burkert <danburkert@apache.org>
{code}",", "
"   Rename Method,","ImpalaD should not open 21000 and 21050 Ports till Catalog is Received Currently ImpalaD's open the frontend connections and this results in query failures. The preferred behaviour would be that the ports remain closed till the catalog is received and for any reason is the SS connectivity is not established after reasonable attempts and timeouts, then the impalad to simply shut down.

{code}
impalad.INFO:I1216 17:39:40.437333 10463 jni-util.cc:166] com.cloudera.impala.common.AnalysisException: This Impala daemon is not ready to accept user requests. Status: Waiting for catalog update from the StateStore.
impalad.INFO:I1216 17:39:40.438743 10463 status.cc:112] AnalysisException: This Impala daemon is not ready to accept user requests. Status: Waiting for catalog update from the StateStore.
impalad.INFO:I1216 17:39:40.918184 10464 jni-util.cc:166] com.cloudera.impala.common.AnalysisException: This Impala daemon is not ready to accept user requests. Status: Waiting for catalog update from the StateStore.
impalad.INFO:I1216 17:39:40.918994 10464 status.cc:112] AnalysisException: This Impala daemon is not ready to accept user requests. Status: Waiting for catalog update from the StateStore.
impalad.INFO:I1216 17:39:44.129482 10465 jni-util.cc:166] com.cloudera.impala.common.AnalysisException: This Impala daemon is not ready to accept user requests. Status: Waiting for catalog update from the StateStore.
{code}

This will help especially when we have multiple impala'd behind a LB and connections can be directed to daemons with the catalog when some servers/impala services are been restarted for any reason.",", "
"   Move Method,Extract Method,","HDFS scans should operate with a constrained number of I/O buffers The HDFS scan nodes should be able to operate with a fixed number of I/O buffers in most cases (excluding very large rows). We should modify them to claim a reservation upfront and use this for all disk I/O.

This probably also requires switching DiskIoMgr to allocate memory from BufferPool.","Duplicated Code, Long Method, , , "
"   Move Method,Inline Method,","HDFS scans should operate with a constrained number of I/O buffers The HDFS scan nodes should be able to operate with a fixed number of I/O buffers in most cases (excluding very large rows). We should modify them to claim a reservation upfront and use this for all disk I/O.

This probably also requires switching DiskIoMgr to allocate memory from BufferPool.",", , , "
"   Move Method,Extract Method,","HDFS scans should operate with a constrained number of I/O buffers The HDFS scan nodes should be able to operate with a fixed number of I/O buffers in most cases (excluding very large rows). We should modify them to claim a reservation upfront and use this for all disk I/O.

This probably also requires switching DiskIoMgr to allocate memory from BufferPool.","Duplicated Code, Long Method, , , "
"   Rename Method,Pull Up Method,Extract Method,","Improve concurrency of DDL/DML operations during catalog updates Currently, long running DDL/DML operations can block other operations from making progress if they run concurrently with the getCatalogObjects() call that creates catalog updates. The reason is that while getCatalogObjects() holds the lock for its entire duration and also tries to acquire the locks for the tables it processes. If that operation is blocked by another operation on a table then any other, unrelated, catalog write operation cannot make any progress as it cannot acquire the catalog lock which is held by getCatalogObjects().

From a user'sÂ point of view, concurrent DDL/DML operations are executed serially and, consequently, the latency of DDL/DML operations may vary significantly. With the fix for this issue concurrent DDL/DML operations should allow to run concurrently and the throughput of these operations should increase significantly. At the same time, the latency of DDL/DML operations should not depend on any other operations that are running at the same time. It's important to note that when we talk about the latency of an operation it is with respect to the coordinator that initiates the operation; the fix doesn't do anything to improve the latency of broadcasting metadata changes through the statestore. Some common user case where this fix is applicable are the following:
 # Concurrent REFRESH operations onÂ different tables.Â 
 # Concurrent ALTER TABLE operations onÂ different tables.","Duplicated Code, Long Method, , Duplicated Code, "
"   Rename Method,","Reduce number of Kudu clients that get created Creating Kudu clients is very expensive as each will fetch metadata from the Kudu master. We can reduce the load on the Kudu master by reusing Kudu clients when possible. To start, we can use a single client for the entire BE and another for the entire FE.

This is dependent on a metadata invalidation improvement from Kudu (https://gerrit.cloudera.org/#/c/6719/)",", "
"   Rename Method,","Support for ORC format files In Hulu, we have supported ORC format files in our version of Impala. Will you accept this feature? Weâ€™re willing to contribute it to the community.",", "
"   Rename Method,","Don't synthesize block metadata in the catalog for S3/ADLS Today, the catalog synthesizes block metadata for S3/ADLS by just breaking up splittable files into ""blocks"" with the FileSystem's default block size. Rather than carrying these blocks around in the catalog and distributing them to all impalad's, we might as well generate the scan ranges on-the-fly during planning. That would save the memory and network bandwidth of blocks.

That does mean that the planner will have to instantiate and call the filesystem to get the default block size, but for these FileSystem's, that's just a matter of reading the config.

Perhaps the same can be done for HDFS erasure coding, though that depends on what a block location actually means in that context and whether they contain useful info.",", "
"   Rename Method,","Update DESCRIBE statement to respect column level privileges Currently if a user is granted select on a subset of columns on a table, the DESCRIBE command will show them all columns, and the DESCRIBE FORMATTED/EXTENDED is not allowed.

This change would update the DESCRIBE command that if a user has select on a subset of columns, it will only show the data from the columns the user has access to.Â  For DESCRIBE FORMATTED/EXTENDED, if a user has some column access, but not all columns, the Location, and View * Text would be removed from the additional metadata.

The purpose of this change is to increase consumability by allowing tools that allow users to browse data, such a for creating reports, to present only columns they have access to.Â  There is also a security aspect to this fix by not exposing additional data.Â  Other statements such a SHOW COLUMN STATS, will be handled by a separate Jira to be opened.

Â ",", "
"   Push Down Method,Push Down Attribute,","Remove builtins db from catalogd Currently builtins_db is created in the catalogd initialization. But later on when catalogd calls reset(), catalogd will replace its dbCache with DBs from HMS. As a result, a delete builtins_db operation is always in the initial catalog update sent to statestore. Though statestore doesn't send it to impalad because the initial catalog is not a delta and doesn't include delete operations, it's a potential problem. We should remove builtins db from catalogd because it's not needed there. ",", , , "
"   Rename Method,","Add JVM Pause Monitor to Impala Processes In IMPALA-3114, we added a pause monitor for Impala. In addition to that, we should port/borrow Hadoop's JvmPauseMonitor [https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/JvmPauseMonitor.java.]Â I believe that when the JVM is aggressively GCing, the C++ threads will continue to get scheduled (and won't log), but the Java ones will log. (I've definitely seen JvmPauseMonitor be accurate many times.)

[~bharathv], when you were testing this, were you able to reproduce it triggering when the JVM half was in ""GC hell""?",", "
"   Rename Method,","Include number of required threads in explain plan Impala has an internal notion of ""required threads"" to execute a fragment, e.g. the fragment execution thread and the first scanner thread. It's possible to compute the number of required threads per fragment instance based on the plan.

We should include this in the resource profile and expose it in the explain plan. This could then be a step toward implementing something like IMPALA-6035.",", "
"   Rename Method,","Improve partition pruning time Impala can roughly prune 10k partitions per sec. For huge tables, it might have >30k partitions. For queries that have tight SLA, the partition pruning time can be significant.

Hive's partition pruning is faster than Impala.",", "
"   Move Method,Extract Method,","Clean RiotReader RiotReader has lots of deprecated functions but for historical reasons, is visible in the public API package.

1. migrate out deprecated functions to RiotParsers in o.a.j.riot.lang.
2. Use RDFDataMgr where ever possible
3. Clean tests up.","Duplicated Code, Long Method, , , "
"   Move Class,Extract Method,","Algebra execution join library This JIRA is for refactoring and adding more join algorithms into a join library.

This is for joins between results from intermediate patterns, not joins that solve basic graph patterns.

Normal use is a index join algorithm but it has some requirements about variable scope.  ARQ falls back to a general join mechanism if the scoping requirements aren't met (this unusual).

The general join code is not good.  It should be, for example, a hash join if possible.

Relation to JENA-266: There is hash-based anti-join code in {{org.apache.jena.sparql.engine.index}} in support of {{MINUS}}. This is not a proposal to combine that code into the join library.  It is not immediately clear that code to cover all cases (inner join, left join and anti-join) at once is really a good idea if it leads to excessively complicated code.
","Duplicated Code, Long Method, , "
"   Pull Up Method,Pull Up Attribute,","Algebra execution join library This JIRA is for refactoring and adding more join algorithms into a join library.

This is for joins between results from intermediate patterns, not joins that solve basic graph patterns.

Normal use is a index join algorithm but it has some requirements about variable scope.  ARQ falls back to a general join mechanism if the scoping requirements aren't met (this unusual).

The general join code is not good.  It should be, for example, a hash join if possible.

Relation to JENA-266: There is hash-based anti-join code in {{org.apache.jena.sparql.engine.index}} in support of {{MINUS}}. This is not a proposal to combine that code into the join library.  It is not immediately clear that code to cover all cases (inner join, left join and anti-join) at once is really a good idea if it leads to excessively complicated code.
",", Duplicated Code, Duplicated Code, "
"   Rename Method,Pull Up Method,Extract Method,","Algebra execution join library This JIRA is for refactoring and adding more join algorithms into a join library.

This is for joins between results from intermediate patterns, not joins that solve basic graph patterns.

Normal use is a index join algorithm but it has some requirements about variable scope.  ARQ falls back to a general join mechanism if the scoping requirements aren't met (this unusual).

The general join code is not good.  It should be, for example, a hash join if possible.

Relation to JENA-266: There is hash-based anti-join code in {{org.apache.jena.sparql.engine.index}} in support of {{MINUS}}. This is not a proposal to combine that code into the join library.  It is not immediately clear that code to cover all cases (inner join, left join and anti-join) at once is really a good idea if it leads to excessively complicated code.
","Duplicated Code, Long Method, , Duplicated Code, "
"   Rename Method,","Iter - add document, add takeWhile, clean up. Some improvements for {{Iter}} and related Iterators.

* Documentation
* {{takeWhile}}, {{takeUntil}}, {{dropWhile}}, {{dropUntil}}
* Deprecate operations on {{Collection}}s better done with Java8 streams.
* Rework {{IteratorTruncate}} to use {{IteratorSlotted}}, not have it's own equivalent machinery.",", "
"   Rename Method,","Remove DatasetGraphCaching This class seems to do nothing much but does add complexity.  There is caching in {{DatasetImpl}} that competes with this caching.  Nowadays, Jena's storage systems are using {{GraphView}} which is very lightweight so caching the creation of graphs is not beneficial and the cache has to be concurrent and transaction safe.

The internal class {{DatasetGraphCaching.Helper}} is used only in SDB.  {{Helper}} could be moved to into SDB.
",", "
"   Rename Method,Inline Method,","Remove DatasetGraphCaching This class seems to do nothing much but does add complexity.  There is caching in {{DatasetImpl}} that competes with this caching.  Nowadays, Jena's storage systems are using {{GraphView}} which is very lightweight so caching the creation of graphs is not beneficial and the cache has to be concurrent and transaction safe.

The internal class {{DatasetGraphCaching.Helper}} is used only in SDB.  {{Helper}} could be moved to into SDB.
",", , "
"   Rename Method,","Remove DatasetGraphCaching This class seems to do nothing much but does add complexity.  There is caching in {{DatasetImpl}} that competes with this caching.  Nowadays, Jena's storage systems are using {{GraphView}} which is very lightweight so caching the creation of graphs is not beneficial and the cache has to be concurrent and transaction safe.

The internal class {{DatasetGraphCaching.Helper}} is used only in SDB.  {{Helper}} could be moved to into SDB.
",", "
"   Rename Method,","Remove DatasetGraphCaching This class seems to do nothing much but does add complexity.  There is caching in {{DatasetImpl}} that competes with this caching.  Nowadays, Jena's storage systems are using {{GraphView}} which is very lightweight so caching the creation of graphs is not beneficial and the cache has to be concurrent and transaction safe.

The internal class {{DatasetGraphCaching.Helper}} is used only in SDB.  {{Helper}} could be moved to into SDB.
",", "
"   Rename Method,Inline Method,","Remove DatasetGraphCaching This class seems to do nothing much but does add complexity.  There is caching in {{DatasetImpl}} that competes with this caching.  Nowadays, Jena's storage systems are using {{GraphView}} which is very lightweight so caching the creation of graphs is not beneficial and the cache has to be concurrent and transaction safe.

The internal class {{DatasetGraphCaching.Helper}} is used only in SDB.  {{Helper}} could be moved to into SDB.
",", , "
"   Rename Method,","Remove DatasetGraphCaching This class seems to do nothing much but does add complexity.  There is caching in {{DatasetImpl}} that competes with this caching.  Nowadays, Jena's storage systems are using {{GraphView}} which is very lightweight so caching the creation of graphs is not beneficial and the cache has to be concurrent and transaction safe.

The internal class {{DatasetGraphCaching.Helper}} is used only in SDB.  {{Helper}} could be moved to into SDB.
",", "
"   Extract Method,Inline Method,","Remove DatasetGraphCaching This class seems to do nothing much but does add complexity.  There is caching in {{DatasetImpl}} that competes with this caching.  Nowadays, Jena's storage systems are using {{GraphView}} which is very lightweight so caching the creation of graphs is not beneficial and the cache has to be concurrent and transaction safe.

The internal class {{DatasetGraphCaching.Helper}} is used only in SDB.  {{Helper}} could be moved to into SDB.
","Duplicated Code, Long Method, , , "
"   Rename Method,","Improve jena-base lib.Tuple Tuples, which are immutable, value-equality fixed length sequences of instances of the same type, accessed by index.

They are like {{T[]}} but immutable, with .equals based on contents, and could be improved to work independently of their original use in TDB.

Proposal:

#  Make Tuple an interface (simpler than current), with special implementations for low number of elements.
# {{ColumnMap}} ==> {{TupleMap}} and sort out the naming as much as possible to use as a way to manage index mapping, not just rewriting Tuples.

An intermediate step is to move {{ColumnMap}} into TDB.

This is not a proposal to add it to {{Triple}} or {{Quad}}.

{{Triple}} is not the same as a 3-length {{Tuple<Node>}}.  Triple elements are accessed by name ({{getSubject}} etc), not index.  SPO is just one possible order in which to think about triples but it has no special status. 
",", "
"   Extract Method,Push Down Attribute,","Independently Configurable BulitinRegistry for Jena Rules Engine An appealing case for the Jena Rules Engine is to use rules to trigger a set of actions in the rule heads that create side effects in the Java Universe:  consider (1) constructing and populating Java data structures in the rule head,  and (2) use of the rules engine in a ""reactive"" scenario where changes in the outside world are inserted into the graph as facts,  triggering actions on a distributed system in the heads.

In cases like this,  the creation of a new set of built-ins could occur somewhat frequently so efficiency matters here so I don't like the idea of this registered in some central registry so that we can pass in the built-in parameter with the resource to the factory,  so the advised method for configuration is to add a setter on the GenericRuleReasoner,  The only concern I have is ""order-of-operations"" involving initialization,  etc.","Duplicated Code, Long Method, , , "
"   Move And Rename Class,","Provide transaction promotion This JIRA is to add the ability for a read transaction to promote to a write transaction.

API changes are necessary to expose this feature properly and uniformly.

PR #161 provides the machinery for TDB. To avoid general API changes outside TDB, this happens automatically in {{DatasetGraphTransaction.getW()}} if enabled (by default it isn't, the PR makes no change to behaviour of TDB by default).  It needs to be enabled with {{DatasetGraphTransaction.promotion = true}}. PR#161 does contain internal changes (e.g. {{DatasetGraphWrapper}}) outside TDB.

This leads towards a general {{begin()}} for transactions. 

This JIRA is cover discussion on the API and record changes to the subsystems.
",", "
"   Rename Method,","Provide transaction promotion This JIRA is to add the ability for a read transaction to promote to a write transaction.

API changes are necessary to expose this feature properly and uniformly.

PR #161 provides the machinery for TDB. To avoid general API changes outside TDB, this happens automatically in {{DatasetGraphTransaction.getW()}} if enabled (by default it isn't, the PR makes no change to behaviour of TDB by default).  It needs to be enabled with {{DatasetGraphTransaction.promotion = true}}. PR#161 does contain internal changes (e.g. {{DatasetGraphWrapper}}) outside TDB.

This leads towards a general {{begin()}} for transactions. 

This JIRA is cover discussion on the API and record changes to the subsystems.
",", "
"   Rename Method,","Add support to VALUES in SelectBuilder It seems that QueryBuilder is lacking on support for building VALUES block, or there is no obvious method to do that.",", "
"   Rename Method,Extract Method,Inline Method,","Add support to VALUES in SelectBuilder It seems that QueryBuilder is lacking on support for building VALUES block, or there is no obvious method to do that.","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Provide detailed setup for RIOT parsing with a parser builder. Provide a parser builder for detailed setup of RDFParser.  

This is a new low level interface to the parsing process. It replaces and extends the machinery hidden inside {{RDFDataMgr}} ({{process}} and {{getReader}}) and {{RDFParserRegistry.ReaderRIOTLang}}.

It aligns with the changes to {{HttpOp}} to have a specific optional {{HttpClient}} (JENA-576 and related work) and so allows applications to control the HTTP setup without resorting to direct use of {{HttpOp}}.

More detailed control can be exposed, including language specific and specialized needs, for example [PR#211 ""preserve id of blanknodes in JSON-LD""|https://github.com/apache/jena/pull/211].

{{RDFDataMgr}} functions involving a Context can be can be deprecated.


","Duplicated Code, Long Method, , "
"   Rename Method,Move Method,Move Attribute,","Language-specific collation in ARQ As [discussed|http://markmail.org/message/v2bvsnsza5ksl2cv] on the users mailing list in October 2016, I would like to change ARQ collation of literal values to be language-aware and respect language-specific collation rules.

This would probably involve changing at least the [NodeUtils.compareLiteralsBySyntax|https://github.com/apache/jena/blob/master/jena-arq/src/main/java/org/apache/jena/sparql/util/NodeUtils.java#L199] method.

It currently sorts by lexical value first, then by language tag. Since the collation order needs to be stable across all possible literal values, I think the safest way would be to sort by language tag first, then by lexical value according to the collation rules for that language.

But what about subtags like {{@en-US}} or {{@pt-BR}}? Can they have different collation rules than the main language? It would be a bit strange if all {{@en-US}} literals sorted after {{@en}} literals...

It would be good to check how Dydra does this and possibly take the same approach. See the message linked above for further backgound.

I've been talking with [~kinow] about this and he may be interested in implementing it.",", , , "
"   Rename Method,","QueryIterRoot should not be overloaded for initial bindings. {{QueryIterRoot}} is also used for initial bindings but the code sometimes assumes that the root binding is the join identity iterator (one row, no columns).

{{QueryIterRoot}} should be reserved for for this case.

{{QueryIterator}} could have a method {{.isJoinIdentity()}} so that an iterator can be tested without peeking and without assuming the java class.
",", "
"   Move Class,Move And Rename Class,Rename Method,Move Method,Extract Method,Move Attribute,","Configurable ResultSet reading and writing Same patterns as {{RDFParser}}, {{RDFWriter}}.","Duplicated Code, Long Method, , , , "
"   Rename Class,Rename Method,Move Method,Extract Method,","Configurable ResultSet reading and writing Same patterns as {{RDFParser}}, {{RDFWriter}}.","Duplicated Code, Long Method, , , "
"   Rename Method,","Transaction promotion Expose the transaction promotion capabilities of TIM, TDB, and TDB2.
",", "
"   Rename Method,Extract Method,","Transaction promotion Expose the transaction promotion capabilities of TIM, TDB, and TDB2.
","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Transaction promotion Expose the transaction promotion capabilities of TIM, TDB, and TDB2.
","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,Extract Method,","Transaction promotion Expose the transaction promotion capabilities of TIM, TDB, and TDB2.
","Duplicated Code, Long Method, , "
"   Rename Method,Inline Method,","Refactor Fuseki configuration to make the faciities available from the embedded server This will allow the Fuseki basic server to process the server section of a full Fuseki configuration file.

",", , "
"   Move Method,Move Attribute,","Working with Blank Nodes with Fuseki Be able to work with blank nodes between client and Fuseki server.

This ticket is mainly about pulling together existing mechanisms.

This would be a new {{RDFConnectionFactory}} function to return a {{RDFConnection}} with the right settings for presevring blank node id. Having a new {{RDFConnectionFuseki}}, subclass of {{RDFConnectionRemote}}, to handle the right settings and not simply option for {{RDFConnectionRemote}}, because it is than a place for future Fuseki-specific operations.
 Â 
 When working with a Fuseki remote:
 * Use RDF Thrift (RDF and Result set forms) for efficiency
 * Preserve blank node ids.

Â ",", , , "
"   Rename Method,","Allow setting of RDF/XML Reader properties when using RIOT. RIOT does not currently have a mechanism for setting the ARP (Jena's RDF/XML parser) properties. Of all the parsers, only ARP has properties to set. The RIOT/ARP bridge , {{ReaderRIOTRDFXML}}, has no mechanism for passing property/value settings to ARP.

",", "
"   Move Class,Move Method,Move Attribute,","Add configurable filters and tokenizers In support of [Jena-1488|https://issues.apache.org/jira/browse/JENA-1488], this issue proposes to add a feature to allow including definedÂ filters and tokenizers, similar to {{DefinedAnalyzer}}, for the {{ConfigurableAnalyzer}}, allowingÂ configurableÂ arguments such as the {{excludeChars}}. I've looked at {{ConfigurableAnalyzer}} and its assembler and it should be straightforward.

I would add tokenizer and filter definitions to {{TextIndexLucene}} similar to the support for adding analyzers:
{code:java}
    text:defineFilters (
        [ text:defineFilter <#foo> ; 
          text:filter [ 
            a text:GenericFilter ;
            text:class ""fi.finto.FoldingFilter"" ;
            text:params (
                [ text:paramName ""excludeChars"" ;
                  text:paramType text:TypeString ; 
                  text:paramValue ""whatevercharstoexclude"" ]
                )
            ] ; 
          ]
      )
{code}
{{GenericFilterAssembler}} and {{GenericTokenizerAssmbler}} would make use of much of the code in {{GenericAnalyzerAssembler}}. The changes to {{ConfigurableAnalyzer}} and {{ConfigurableAnalyzerAssembler}} are straightforward and mostly involve retaining the resource URI rather than extracting the localName.

Such an addition willÂ make it easy to create new tokenizers and filters that could be dropped in by just adding the classes onto the jena/fuseki classpath or by referring to ones already included in Jena (via Lucene or otherwise) and putting the appropriate assembler bits in the configuration.",", , , "
"   Rename Method,Extract Method,","Allow addition of transaction components after initial setup. Currently:
 * TDB1: the set of {{TransactionLifecycle}} components is fixed
 * TDB2: the set of {{TransactionComponents}} is frozen during creation of a dataset.

In order to be able to add a text index into the transaction handling, allow new items to be added (carefully) into the transaction subsystem for a dataset.

This is necessary for JENA-1302.","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,","Allow addition of transaction components after initial setup. Currently:
 * TDB1: the set of {{TransactionLifecycle}} components is fixed
 * TDB2: the set of {{TransactionComponents}} is frozen during creation of a dataset.

In order to be able to add a text index into the transaction handling, allow new items to be added (carefully) into the transaction subsystem for a dataset.

This is necessary for JENA-1302.",", "
"   Move Class,Move And Rename Class,","Split module jena-fuseki-core into the engine and separate webapp. The module jena-fuseki-core has both the core of Fuseki (data registries, service servlets, the servlet filter) and the webapp code needed for the full server with UI.

The embedded Fuseki does not need the webapp.

Separating the two aspects into separate modules is cleaner and avoids the risk of webapp assumptions leaking into the non-webapp embedded server.

Â 

The key difference is that the embedded/base server makes no assumptions about disk, it is given datasets to manage, where as the webapp full/UI server has an on-disk configuration and database area.",", "
"   Move Class,Extract Method,","Split module jena-fuseki-core into the engine and separate webapp. The module jena-fuseki-core has both the core of Fuseki (data registries, service servlets, the servlet filter) and the webapp code needed for the full server with UI.

The embedded Fuseki does not need the webapp.

Separating the two aspects into separate modules is cleaner and avoids the risk of webapp assumptions leaking into the non-webapp embedded server.

Â 

The key difference is that the embedded/base server makes no assumptions about disk, it is given datasets to manage, where as the webapp full/UI server has an on-disk configuration and database area.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","TDB2 Loader: print progress of the overall load Some of the logging output is per file; It would be better if logged counts were for the whole load, not per file.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,Inline Method,","TransformFilterEquality does not handle starting OPTIONAL well There was one other case where our tests were stuck on a very slow query execution because transformFilterEquality failed to optimize. The problem is that the optimizer gives up whenever the WHERE clause starts with an OPTIONAL clause. The reason is that the generated algebraic formula starts with a TableUnit and this is not handled correctly. 

I have attached a patch which fixes the problem","Duplicated Code, Long Method, , , "
"   Extract Superclass,Rename Method,",XSDhexBinary and XSDbase64binary should share code via XSDBinaryBase 0,", Duplicated Code, Large Class, "
"   Rename Method,",Remove DAML+OIL support Remove (legacy) DAML+OIL support.,", "
"   Move Class,Extract Method,","RETE patch for faster forward rule execution From: http://mail-archives.apache.org/mod_mbox/jena-dev/201303.mbox/%3C20130317072414.327a09b7@tyrannox.fritz.box%3E

I would like to like to submit a patch which speeds up forward rule
execution in the RETE engine.

It uses only slightly more memory, but reduces the time needed for my
dataset of about 100.000 triples from beyond measuring to about 5
minutes.

It passes all tests so far. Am I missing something?","Duplicated Code, Long Method, , "
"   Rename Method,Move Method,","TDB stats-base reordering nees to treat RDF type differently. rdf:type is not like other predicates.

A large dataset with a small number of types can throw the stats planner.

Instead, collect rdf:type + actual type statistics.",", , "
"   Rename Method,Pull Up Method,Pull Up Attribute,","Remote updates do not respect srv:serviceContext This is a follow on issue prompted by JENA-405, that issue requested support for respecting the service context for remote SPARQL queries which allowed for setting certain parameters in a centralized place rather than every time you wanted to access a service.

It seems reasonable that we should do the same for updates but having looked at the code there appears to be no existing way to inject authentication parameters for updates.  This is a lacking feature since it would seem more likely that credentials would be needed for updates than queries.",", Duplicated Code, Duplicated Code, "
"   Rename Method,","ARQ should be able to optimize implicit joins and implicit left joins There is a class of useful optimizations that currently ARQ does not even attempt to apply which are usually referred to as implicit joins.

A trivial example is as follows:

SELECT *
WHERE
{
  ?x ?p1 ?o1 .
  ?y ?p2 ?o2 .
  FILTER(?x = ?y)
}

Currently this requires us to compute a cross product and then apply the filter, even with streaming evaluation this can be extremely costly.  The aim of this optimization is to produce a query like the following:

SELECT *
WHERE
{
  ?x ?p1 ?o1 .
  ?x ?p2 ?o2 .
  BIND(?x AS ?y)
}

This optimization can also be applied to some left joins where the implicit join applies across the join e.g.

SELECT *
WHERE
{
  ?x ?p1 ?o1 .
  OPTIONAL
  {
    ?y ?p2 ?o2 .
    FILTER(?x = ?y)
  }
}

This can be thought of as a generalization of TransformFilterEquality except covering the case where both items are variables.  Since both things are variables we need to be careful about when we apply this optimization since when = is used we need to guarantee that substituting one variable for the other does not alter the semantics of the query.

I believe the optimization is safe to apply providing that we can guarantee (as far as possible) that one variable is non-literal.  This can be done by inspecting the positions in which the mentioned variables are used and ensuring that at least one of the variables occurs in the graph, subject or predicate position.

Safety for left joins is a little more complex since we must ensure that at least one of the variables occurs in the RHS and we can only make the substitution in the RHS as otherwise we change the join semantics.",", "
"   Rename Method,Extract Method,","Unify HTTP usage and authentication mechanisms in ARQ Currently ARQ uses a mixture of HttpClient and HttpURLConnection to perform various HTTP operations e.g. SPARQL Queries, SPARQL Updates and SPARQL Graph Store Protocol.

This has the effect of making the code somewhat awkward to maintain and makes certain operations like authentication more complex than they need to be because different parts of the system support different modes of authentication.

For example currently SPARQL queries only support Basic Auth and they always pre-authenticate so they cannot do proxy auth or use any other kind of auth method.  On the other hand SPARQL updates use HttpClient which is capable of performing Basic, Digest, NTLM, SPNEGO and Kerberos for both normal and proxy auth but is never pre-authenticates.

This task proposes unifying all HTTP operations in ARQ to use Apache HttpClient since it is more flexible and introducing a more extensible framework for doing authentication.

In terms of HTTP unification we need to convert the following:
- HttpQuery should use HttpClient
- LocatorURL should use HttpClient

In terms of HTTP Authentication my idea is as follows, introduce a new interface HttpAuthenticator which provides an apply(AbstractHttpClient client, URI target) method.  All systems that may permit HTTP auth will allow use of an authenticator, providing a generic interface for authenticators will allow us to introduce authenticators for other auth schemes e.g. form based logins.

We can also provide authenticators that leverage existing mechanisms e.g. storing credentials in a service context which would be used by default.  Existing methods that accept username and password would use simpler authenticators.","Duplicated Code, Long Method, , "
"   Extract Method,Inline Method,","Unify HTTP usage and authentication mechanisms in ARQ Currently ARQ uses a mixture of HttpClient and HttpURLConnection to perform various HTTP operations e.g. SPARQL Queries, SPARQL Updates and SPARQL Graph Store Protocol.

This has the effect of making the code somewhat awkward to maintain and makes certain operations like authentication more complex than they need to be because different parts of the system support different modes of authentication.

For example currently SPARQL queries only support Basic Auth and they always pre-authenticate so they cannot do proxy auth or use any other kind of auth method.  On the other hand SPARQL updates use HttpClient which is capable of performing Basic, Digest, NTLM, SPNEGO and Kerberos for both normal and proxy auth but is never pre-authenticates.

This task proposes unifying all HTTP operations in ARQ to use Apache HttpClient since it is more flexible and introducing a more extensible framework for doing authentication.

In terms of HTTP unification we need to convert the following:
- HttpQuery should use HttpClient
- LocatorURL should use HttpClient

In terms of HTTP Authentication my idea is as follows, introduce a new interface HttpAuthenticator which provides an apply(AbstractHttpClient client, URI target) method.  All systems that may permit HTTP auth will allow use of an authenticator, providing a generic interface for authenticators will allow us to introduce authenticators for other auth schemes e.g. form based logins.

We can also provide authenticators that leverage existing mechanisms e.g. storing credentials in a service context which would be used by default.  Existing methods that accept username and password would use simpler authenticators.","Duplicated Code, Long Method, , , "
"   Extract Method,Move Attribute,","DatasetGraphTDB.deleteAny uses a loop to delete leading to a scaling problem. DatasetGraphTDB.deleteAny is relying on the DatasetGraphCaching default implementation.

It should work on NodeIds and also use the batching as seen in GraphTDB.removeWorker

Move GraphTDB.removeWorker code to DatsetGraphTDB and consolidate on using that.","Duplicated Code, Long Method, , , "
"   Push Down Method,Extract Method,","Fuseki should use dataset transactions where available, not parse to an intermediate datastructure Fuseki should detect when an update operation is on a real transactional dataset and avoid parsing the data into a RAM as a validation step.

This is especially the case for Graph Store Protocol operations.","Duplicated Code, Long Method, , , "
"   Rename Method,Move Method,Extract Method,","Fuseki should use dataset transactions where available, not parse to an intermediate datastructure Fuseki should detect when an update operation is on a real transactional dataset and avoid parsing the data into a RAM as a validation step.

This is especially the case for Graph Store Protocol operations.","Duplicated Code, Long Method, , , "
"   Move Method,Extract Method,Move Attribute,","Get rid of log4j dependency in jena-core Apache jena core seems to pull in log4j and slf4j-log4j12, since you use slf4j I would suppose I can plug in my own logging backend. If you have a hard dependency on log4j at least make it optional and don't force it in the core jena components.

Ultimately this avoids these kinds of issues:
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:.../target/universal/stage/lib/ch.qos.logback.logback-classic-1.0.13.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:.../target/universal/stage/lib/org.slf4j.slf4j-log4j12-1.6.4.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [ch.qos.logback.classic.util.ContextSelectorStaticBinder]","Duplicated Code, Long Method, , , , "
"   Rename Method,","Add read(Reader,...) to the general ReaderRIOT interface. 0",", "
"   Rename Class,Rename Method,Move Method,Extract Method,Move Attribute,",Improve filter placement optimization 0,"Duplicated Code, Long Method, , , , "
"   Move Method,Move Attribute,","Remove JMX support from ARQ ARQ has some JMX support but JMX is unavailable in verions environments (some OSGi (glassfish), Google App engine, systems using classloader restrictions (ant).

If JMX can't be started, ARQ operates as normal, but without the static information (e.g. versions) and live query counts.

The proposal is to remove JMX from ARQ on the assumption that, as a library, such information is better obtained by the using application.  

One such system might be Fuseki which already has a management information framework (it exposes statistics over HTTP).

",", , , "
"   Rename Method,","Do constant folding as part of query optimisation Currently Jena does not automatically simplify expressions by constant folding even though the function API actually has support for this already baked into it.

This issue will track work to integrate a transform for this into the standard optimiser.",", "
"   Extract Method,Inline Method,","Do constant folding as part of query optimisation Currently Jena does not automatically simplify expressions by constant folding even though the function API actually has support for this already baked into it.

This issue will track work to integrate a transform for this into the standard optimiser.","Duplicated Code, Long Method, , , "
"   Move Method,Move Attribute,","Make TDB datasets harder to corrupt This RFE comes out of discussions I had in person with Andy earlier this week.  On the mailing lists and Q&A sites we see a steady stream of questions from people who have corrupted TDB databases and it would be nice if we could put in place features that make this harder to do.

There are two main things we should do in the long term as I see it:

# Make using TDB non-transactonally more difficult
# Put in place some mechanism to make it difficult for multiple JVMs to access the same TDB dataset simultaneously

Me and Andy think the first could be achieved by making TDB datasets operation in auto-commit more rather than non-transactional mode by default.  In order to allow this we likely need upgradeable read transactions to be supported.  As part of this change non-transactional mode would still be supported but users would have to explicitly set some ""Here be Dragons"" style flag in order to do this.  Users who aren't using transactions currently would likely merely see performance drop since suddenly they are getting auto-commits on every operation but when they complain we can tell them they should be using transactions properly to ensure their TDB databases remain uncorrupted.

As far as the second point goes we could likely do this the way a lot of other applications do by having the code write a lock file to disk when a database is opened which contains the owning processes PID.  Whenever you go to open a database the presence of the lock file is checked for and if present the PID validated with the code refusing to open the database if the PIDs do not match.  There would likely need to be some code to cope with the case where the lock file gets left around and the owning PID is not alive but that shouldn't be too complicated.

Since these may be considered as substantial behavioural changes to TDB these may likely go into Jena 3",", , , "
"   Move And Rename Class,","Remove Table.matchRightLeft {{Table.matchRightLeft}} is join code for tables but rather inflexible.  It should not form part of the {{Table}} interface.

Put necessary code into a library.  Long term, a general library of inner and outer join algorithms will be introduced.",", "
"   Rename Method,Extract Method,","Remove Table.matchRightLeft {{Table.matchRightLeft}} is join code for tables but rather inflexible.  It should not form part of the {{Table}} interface.

Put necessary code into a library.  Long term, a general library of inner and outer join algorithms will be introduced.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Reduce costly string operations in utility class The enclosed patch improves performance for all kinds of string-serialization of rdf graphs. The patch improves the performance of ""UpdateRequest.output"" by approx. 30%, which makes remote sparql queries  a fair bit faster. The patch will probably improve performance across the board.","Duplicated Code, Long Method, , "
"   Rename Method,","SPARQL replace [users@ email report ""Replace doesn't error on patterns that match zero length strings""|http://mail-archives.apache.org/mod_mbox/jena-users/201407.mbox/%3CCA%2BQ4Jn%3DqOS85yCPGexqX%2BLFkX4Dw8QSQmBLq2kT05%2Btvj0xksA%40mail.gmail.com%3E].

{{replace(""abc"", "".*"", ""x"")}} returns {{""xx""}} because it uses Java's Matcher.replaceAll - it's one match for the whole of ""abc"" (it's a greedy pattern) and one match for the trailing empty string. Java returns {{""x""}} for an empty string, and {{""xx""}} for any non-empty string in place of ""abc"". 

F&O calls out this case and makes it an error.

http://www.w3.org/TR/xpath-functions/#func-replace

{quote}
An error is raised \[err:FORX0003\] if the pattern matches a zero-length string, that is, if the expression fn:matches("""", $pattern, $flags) returns true. It is not an error, however, if a captured substring is zero-length.
{quote}
",", "
"   Rename Method,Move Method,Extract Method,",Provide a registry for result set readers and writers. c.f. {{RDFDataMgr}} for graph and dataset I/O.,"Duplicated Code, Long Method, , , "
"   Extract Superclass,Extract Method,","Filter placement should be able to break up extend The following query demonstrates a query plan seen internally which is considered sub-optimal.

Consider the following query:

{noformat}
SELECT DISTINCT ?domainName
{
  { ?uri ?p ?o }
  UNION
  {
    ?sub ?p ?uri
    FILTER(isIRI(?uri))
  }
  BIND(str(?uri) as ?s)
  FILTER(STRSTARTS(?s, ""http://""))
  BIND(IRI(CONCAT(""http://"", STRBEFORE(SUBSTR(?s,8), ""/""))) AS ?domainName)
}
{noformat}

Which ARQ optimises as follows:

{noformat}
(distinct
  (project (?domainName)
    (filter (strstarts ?s ""http://"")
      (extend ((?s (str ?uri)) (?domainName (iri (concat ""http://"" (strbefore (substr ?s 8) ""/"")))))
        (union
          (bgp (triple ?uri ?p ?o))
          (filter (isIRI ?uri)
            (bgp (triple ?sub ?p ?uri))))))))
{noformat}

Which makes the query engine do a lot of work because it computes the both the {{BIND}} expressions for lots of possible solutions that will then be rejected when for many of them it would only be necessary to compute the first simple {{BIND}} function.

It would be better if the query was planned as follows:

{noformat}
(distinct
  (project (?domainName)
    (extend (?domainName (iri (concat ""http://"" (strbefore (substr ?s 8) ""/""))))
      (filter (strstarts ?s ""http://"")
        (extend (?s (str ?uri))
          (union
            (bgp (triple ?uri ?p ?o))
            (filter (isIRI ?uri)
              (bgp (triple ?sub ?p ?uri)))))))))
{noformat}

Essentially when we try to push a filter through an {{extend}} if we determine that we cannot push it through the extend we should see if we can split the {{extend}} instead thus resulting in a partial pushing.

Note that a user can re-write the original query to yield this plan if they make the second {{BIND}} a project expression like so:

{noformat}
SELECT DISTINCT (IRI(CONCAT(""http://"", STRBEFORE(SUBSTR(?s,8), ""/""))) AS ?domainName)
{
  { ?uri ?p ?o }
  UNION
  {
    ?sub ?p ?uri
    FILTER(isIRI(?uri))
  }
  BIND(str(?uri) as ?s)
  FILTER(STRSTARTS(?s, ""http://""))
}
{noformat}","Duplicated Code, Long Method, , Duplicated Code, Large Class, "
"   Rename Method,","Filter placement should be able to break up extend The following query demonstrates a query plan seen internally which is considered sub-optimal.

Consider the following query:

{noformat}
SELECT DISTINCT ?domainName
{
  { ?uri ?p ?o }
  UNION
  {
    ?sub ?p ?uri
    FILTER(isIRI(?uri))
  }
  BIND(str(?uri) as ?s)
  FILTER(STRSTARTS(?s, ""http://""))
  BIND(IRI(CONCAT(""http://"", STRBEFORE(SUBSTR(?s,8), ""/""))) AS ?domainName)
}
{noformat}

Which ARQ optimises as follows:

{noformat}
(distinct
  (project (?domainName)
    (filter (strstarts ?s ""http://"")
      (extend ((?s (str ?uri)) (?domainName (iri (concat ""http://"" (strbefore (substr ?s 8) ""/"")))))
        (union
          (bgp (triple ?uri ?p ?o))
          (filter (isIRI ?uri)
            (bgp (triple ?sub ?p ?uri))))))))
{noformat}

Which makes the query engine do a lot of work because it computes the both the {{BIND}} expressions for lots of possible solutions that will then be rejected when for many of them it would only be necessary to compute the first simple {{BIND}} function.

It would be better if the query was planned as follows:

{noformat}
(distinct
  (project (?domainName)
    (extend (?domainName (iri (concat ""http://"" (strbefore (substr ?s 8) ""/""))))
      (filter (strstarts ?s ""http://"")
        (extend (?s (str ?uri))
          (union
            (bgp (triple ?uri ?p ?o))
            (filter (isIRI ?uri)
              (bgp (triple ?sub ?p ?uri)))))))))
{noformat}

Essentially when we try to push a filter through an {{extend}} if we determine that we cannot push it through the extend we should see if we can split the {{extend}} instead thus resulting in a partial pushing.

Note that a user can re-write the original query to yield this plan if they make the second {{BIND}} a project expression like so:

{noformat}
SELECT DISTINCT (IRI(CONCAT(""http://"", STRBEFORE(SUBSTR(?s,8), ""/""))) AS ?domainName)
{
  { ?uri ?p ?o }
  UNION
  {
    ?sub ?p ?uri
    FILTER(isIRI(?uri))
  }
  BIND(str(?uri) as ?s)
  FILTER(STRSTARTS(?s, ""http://""))
}
{noformat}",", "
"   Rename Method,Extract Method,","Handling simple literals,language literals and xsd:string in RDF 1.1 In RDF 1.1, simple literals (no language tag, no mentioned datatype) 
a have datatype xsd:string.  Literals with language tag have
datatype rdf:langString.

Output should not explicitly use ^^xsd:string (or datatype=)

NodeFactory should catch no datatype and set xsd:string or rdf:langString as appropriate.
","Duplicated Code, Long Method, , "
"   Move Method,Move Attribute,","Blank Node output under Hadoop can cause identifiers to diverge in multi-stage pipelines In writing up the documentation on the RDF Tools for Hadoop and enumerating the possible issues that blank nodes imply I discovered an issue that I hadn't previously considered.

For a single job the input and output formats all ensure that blank nodes are consistently given the same identifiers if they had the same syntactic ID and were in the same file.  This is done even when a file is being read in multiple chunks by multiple map tasks.  However by its nature each reduce task will create an output file so potentially you can end up with blank nodes spread over multiple files.

However if we then read these files into a subsequent job the blank nodes may now be spread across multiple files so even though they were the same node originally our allocation policy will cause the identifiers to diverge and become distinct blank nodes which is incorrect behaviour.

Since there is no clear universal fix for this what I am considering doing is instead introducing a configuration setting that will allow the file path to be ignored for the purpose of blank node identifier allocations within a job.  This will mean that identifiers are purely allocated on the basis of the Job ID and thus the same syntactic ID in any file will result in the same blank node identifier.  As the user will hopefully will have left this turned off for the first job even if we start with the same syntactic ID but in different files the normal allocation policy for the first job should ensure unique IDs for the later jobs.

My next step on this is to implement a failing unit test (and then temporarily ignore it) which demonstrates this issue.",", , , "
"   Extract Method,Pull Up Attribute,","There is a need for a generic assembler for SecurityEvaluators. There is a SecuredAssembler that will assemble secured models but there is no generic SecurityEvaluator assembler that can work with most SecurityEvaluator implementations.

This improvement is to cover building a generic assembler that will pass parameters to the constructor for an instance of SecurityEvaluator.","Duplicated Code, Long Method, , Duplicated Code, "
"   Rename Method,",Use Java8 constructs with jena-core iterators For example: https://github.com/apache/jena/pull/54,", "
"   Rename Method,",Use Java8 constructs with jena-core iterators For example: https://github.com/apache/jena/pull/54,", "
"   Move Class,Rename Method,","Refactor graph/permissions interface layer The jena-permissions package was originally designed to be graph implementation agnostic.  To that end there is a layer that translates the jena based node, triple and statement objects into a slightly different format.  As jena-permissions is now part of Apache Jena proper this layer should be factored out to improve performance.

The results of this refactoring will be visible and probably disruptive to early adopters and so should be done as part of the 3.0 release.",", "
"   Rename Method,","Remove "".json"" from registration of RDF/JSON. To avoid confusion with JSON-LD,  remove "".json"" from registration of RDF/JSON.",", "
"   Move Class,Move And Rename Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","New consumer checklist We will use this JIRA to track the list of issues to resolve to get a working new consumer client. The consumer client can work in phases -

1. Add new consumer APIs and configs
2. Refactor Sender. We will need to use some common APIs from Sender.java (https://issues.apache.org/jira/browse/KAFKA-1316)
3. Add metadata fetch and refresh functionality to the consumer (This will require https://issues.apache.org/jira/browse/KAFKA-1316)
4. Add functionality to support subscribe(TopicPartition...partitions). This will add SimpleConsumer functionality to the new consumer. This does not include any group management related work.
5. Add ability to commit offsets to Kafka. This will include adding functionality to the commit()/commitAsync()/committed() APIs. This still does not include any group management related work.
6. Add functionality to the offsetsBeforeTime() API.
7. Add consumer co-ordinator election to the server. This will only add a new module for the consumer co-ordinator, but not necessarily all the logic to do group management. 

At this point, we will have a fully functional standalone consumer and a server side co-ordinator module. This will be a good time to start adding group management functionality to the server and consumer.

8. Add failure detection capability to the consumer when group management is used. This will not include any rebalancing logic, just the ability to detect failures using session.timeout.ms.
9. Add rebalancing logic to the server and consumer. This will be a tricky and potentially large change since it will involve implementing the group management protocol.
10. Add system tests for the new consumer
11. Add metrics 
12. Convert mirror maker to use the new consumer.
13. Convert perf test to use the new consumer
14. Performance testing and analysis.
15. Review and fine tune log4j logging","Duplicated Code, Long Method, , , , , "
"   Rename Class,Move Method,","Extend wire protocol to allow CRC32C Howdy

We are currently building out a number of Kafka consumers in Go, based on a patched version of the Sarama library that Shopify released a while back.

We have a reasonably fast serialization protocol (Cap'n Proto), a 10G network and lots of cores. We have various consumers computing all kinds of aggregates on a reasonably high volume access log stream (1.1e6 messages/sec peak, about 500-600 bytes per message uncompressed).

When profiling our consumer, our single hottest function (until we disabled it), was the CRC32 checksum validation, since the deserialization and aggregation in these consumers is pretty cheap.

We believe things could be improved by extending the wire protocol to support CRC-32C (Castagnoli), since SSE 4.2 has an instruction to accelerate its calculation.

https://en.wikipedia.org/wiki/SSE4#SSE4.2

It might be hard to use from Java, but consumers written in most other languages will benefit a lot.

To give you an idea, here are some benchmarks for the Go CRC32 functions running on a Intel(R) Core(TM) i7-3540M CPU @ 3.00GHz core:

BenchmarkCrc32KB 90196 ns/op 363.30 MB/s
BenchmarkCrcCastagnoli32KB 3404 ns/op 9624.42 MB/s

I believe BenchmarkCrc32 written in C would do about 600-700 MB/sec, and the CRC32-C speed should be close to what one achieves in Go.

(Met Todd and Clark at the meetup last night. Thanks for the great presentation!)",", , "
"   Move Class,Move And Rename Class,Move Method,","Integrate checkstyle for java code There are a lot of little style and layering problems that tend to creep into our code, especially with external patches and lax reviewers.

These are the usual style suspects--capitalization, spacing, bracket placement, etc.

My personal pet peave is a lack of clear thinking about layers. These layering problems crept in quite fast, and sad to say a number of them were accidentally caused by me. This is things like o.a.k.common depending on o.a.k.clients or the consumer depending on the producer.

I have a patch that integrates checkstyle to catch these issues at build time, and which corrects the known problems. There are a fair number of very small changes in this patch, all trivial.

Checkstyle can be slightly annoying, not least of which because it has a couple minor bugs around anonymous inner class formatting, but I find it is 98% real style issues so mostly worth it.",", , "
"   Rename Method,Extract Method,Inline Method,","Bound fetch response size (KIP-74) Currently the only bound on the fetch response size is max.partition.fetch.bytes * num_partitions. There are two problems:
1. First this bound is often large. You may chose max.partition.fetch.bytes=1MB to enable messages of up to 1MB. However if you also need to consume 1k partitions this means you may receive a 1GB response in the worst case!
2. The actual memory usage is unpredictable. Partition assignment changes, and you only actually get the full fetch amount when you are behind and there is a full chunk of data ready. This means an application that seems to work fine will suddenly OOM when partitions shift or when the application falls behind.

We need to decouple the fetch response size from the number of partitions.

The proposal for doing this would be to add a new field to the fetch request, max_bytes which would control the maximum data bytes we would include in the response.

The implementation on the server side would grab data from each partition in the fetch request until it hit this limit, then send back just the data for the partitions that fit in the response. The implementation would need to start from a random position in the list of topics included in the fetch request to ensure that in a case of backlog we fairly balance between partitions (to avoid first giving just the first partition until that is exhausted, then the next partition, etc).

This setting will make the max.partition.fetch.bytes field in the fetch request much less useful and we should discuss just getting rid of it.

I believe this also solves the same thing we were trying to address in KAFKA-598. The max_bytes setting now becomes the new limit that would need to be compared to max_message size. This can be much larger--e.g. setting a 50MB max_bytes setting would be okay, whereas now if you set 50MB you may need to allocate 50MB*num_partitions.

This will require evolving the fetch request protocol version to add the new field and we should do a KIP for it.","Duplicated Code, Long Method, , , "
"   Rename Class,Extract Method,","Expose a Partitioner interface in the new producer In the new producer you can pass in a key or hard code the partition as part of ProducerRecord.

Internally we are using a class
{code}
class Partitioner {
public int partition(String topic, byte[] key, Integer partition, Cluster cluster) {...}
}
{code}

This class uses the specified partition if there is one; uses a hash of the key if there isn't a partition but there is a key; and simply chooses a partition round robin if there is neither a partition nor a key.

However there are several partitioning strategies that could be useful that we don't support out of the box. 

An example would be having each producer periodically choose a random partition. This tends to be the most efficient since all data goes to one server and uses the fewest TCP connections, however it only produces good load balancing if there are many producers.

Of course a user can do this now by just setting the partition manually, but that is a bit inconvenient if you need to do that across a bunch of apps since each will need to remember to set the partition every time.

The idea would be to expose a configuration to set the partitioner implementation like
{code}
partitioner.class=org.apache.kafka.producer.DefaultPartitioner
{code}

This would default to the existing partitioner implementation.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Add a request timeout to NetworkClient Currently NetworkClient does not have a timeout setting for requests. So if no response is received for a request due to reasons such as broker is down, the request will never be completed.
Request timeout will also be used as implicit timeout for some methods such as KafkaProducer.flush() and kafkaProducer.close().
KIP-19 is created for this public interface change.
https://cwiki.apache.org/confluence/display/KAFKA/KIP-19+-+Add+a+request+timeout+to+NetworkClient","Duplicated Code, Long Method, , "
"   Rename Method,Inline Method,","Add a request timeout to NetworkClient Currently NetworkClient does not have a timeout setting for requests. So if no response is received for a request due to reasons such as broker is down, the request will never be completed.
Request timeout will also be used as implicit timeout for some methods such as KafkaProducer.flush() and kafkaProducer.close().
KIP-19 is created for this public interface change.
https://cwiki.apache.org/confluence/display/KAFKA/KIP-19+-+Add+a+request+timeout+to+NetworkClient",", , "
"   Rename Method,Extract Method,","Add a request timeout to NetworkClient Currently NetworkClient does not have a timeout setting for requests. So if no response is received for a request due to reasons such as broker is down, the request will never be completed.
Request timeout will also be used as implicit timeout for some methods such as KafkaProducer.flush() and kafkaProducer.close().
KIP-19 is created for this public interface change.
https://cwiki.apache.org/confluence/display/KAFKA/KIP-19+-+Add+a+request+timeout+to+NetworkClient","Duplicated Code, Long Method, , "
"   Rename Class,Move And Rename Class,Rename Method,","Fix capitalization in SSL classes I notice that all the SSL classes are using the convention SSLChannelBuilder, SSLConfigs, etc. Kafka has always used the convention SslChannelBuilder, SslConfigs, etc. See e.g. KafkaApis, ApiUtils, LeaderAndIsrRequest, ClientIdAndTopic, etc.

We should fix this.",", "
"   Rename Class,Rename Method,","Add support for ListGroups and DescribeGroup APIs Since the new consumer currently has no persistence in Zookeeper (pending outcome of KAFKA-2017), there is no way for administrators to investigate group status including getting the list of members in the group and their partition assignments. We therefore propose to modify GroupMetadataRequest (previously known as ConsumerMetadataRequest) to return group metadata when received by the respective group's coordinator. When received by another broker, the request will be handled as before: by only returning coordinator host and port information.

{code}
GroupMetadataRequest => GroupId IncludeMetadata
GroupId => String
IncludeMetadata => Boolean

GroupMetadataResponse => ErrorCode Coordinator GroupMetadata
ErrorCode => int16
Coordinator => Id Host Port
Id => int32
Host => string
Port => int32
GroupMetadata => State ProtocolType Generation Protocol Leader Members
State => String
ProtocolType => String
Generation => int32
Protocol => String
Leader => String
Members => [Member MemberMetadata MemberAssignment]
Member => MemberIp ClientId
MemberIp => String
ClientId => String
MemberMetadata => Bytes
MemberAssignment => Bytes
{code}

The request schema includes a flag to indicate whether metadata is needed, which saves clients from having to read all group metadata when they are just trying to find the coordinator. This is important to reduce group overhead for use cases which involve a large number of topic subscriptions (e.g. mirror maker).

Tools will use the protocol type to determine how to parse metadata. For example, when the protocolType is ""consumer"", the tool can use ConsumerProtocol to parse the member metadata as topic subscriptions and partition assignments. 

The detailed proposal can be found below.
https://cwiki.apache.org/confluence/display/KAFKA/KIP-40%3A+ListGroups+and+DescribeGroup
",", "
"   Move Method,Move Attribute,","Standardize new consumer exceptions The purpose of this ticket is to standardize and cleanup the exceptions thrown by the new consumer to ensure 1) that exceptions are only raised when there is no reasonable way of handling them internally, 2) that raised exceptions are documented properly, 3) that exceptions provide enough information for handling.

For all blocking methods, the following exceptions are possible:
- AuthorizationException (can only thrown if cluster is configured for authorization)
- WakeupException (only thrown with an explicit call to wakeup())
- ApiException (invalid session timeout, invalid groupId, inconsistent assignment strategy, etc.)

Additionally, the following methods have special exceptions.
poll():
- SerializationException (problems deserializing keys/values)
- InvalidOffsetException (only thrown if no reset policy is defined; includes OffsetOutOfRange and NoOffsetForPartition)

commit():
- CommitFailedException (only thrown if group management is enabled and a rebalance completed before the commit could finish)

position():
- InvalidOffsetException (same as above)",", , , "
"   Rename Method,Extract Method,","Implement max.poll.records for new consumer (KIP-41) Currently, the consumer.poll(timeout)

returns all messages that have not been acked since the last fetch
The only way to process a single message, is to throw away all but the first message in the list
This would mean we are required to fetch all messages into memory, and this coupled with the client being not thread-safe, (i.e. we cannot use a different thread to ack messages, makes it hard to consume messages when the order of message arrival is important, and a large number of messages are pending to be consumed)","Duplicated Code, Long Method, , "
"   Extract Method,Inline Method,","Connect should parallelize task start/stop The Herder implementations currently iterate over all connectors/tasks and sequentially start/stop them. We should parallelize this. This is less critical for {{StandaloneHerder}}, but pretty important for {{DistributedHerder}} since it will generally be managing more tasks and any delay starting/stopping a single task will impact every other task on the node (and can ultimately result in incorrect behavior in the case of a single offset commit in one connector taking too long preventing all of the rest from committing offsets).","Duplicated Code, Long Method, , , "
"   Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Rename SinkTask.onPartitionsAssigned/onPartitionsRevoked and Clarify Contract The purpose of the onPartitionsRevoked() and onPartitionsAssigned() methods exposed in Kafka Connect's SinkTask interface seems a little unclear and too closely tied to consumer semantics. From the javadoc, these APIs are used to open/close per-partition resources, but that would suggest that we should always get one call to onPartitionsAssigned() before writing any records for the corresponding partitions and one call to onPartitionsRevoked() when we have finished with them. However, the same methods on the consumer are used to indicate phases of the rebalance operation: onPartitionsRevoked() is called before the rebalance begins and onPartitionsAssigned() is called after it completes. In particular, the consumer does not guarantee a final call to onPartitionsRevoked(). 

This mismatch makes the contract of these methods unclear. In fact, the WorkerSinkTask currently does not guarantee the initial call to onPartitionsAssigned(), nor the final call to onPartitionsRevoked(). Instead, the task implementation must pull the initial assignment from the SinkTaskContext. To make it more confusing, the call to commit offsets following onPartitionsRevoked() causes a flush() on a partition which had already been revoked. All of this makes it difficult to use this API as suggested in the javadocs.

To fix this, we should clarify the behavior of these methods and consider renaming them to avoid confusion with the same methods in the consumer API. If onPartitionsAssigned() is meant for opening resources, maybe we can rename it to open(). Similarly, onPartitionsRevoked() can be renamed to close(). We can then fix the code to ensure that a typical open/close contract is enforced. This would also mean removing the need to pass the initial assignment in the SinkTaskContext. This would give the following API:

{code}
void open(Collection<TopicPartition> partitions);
void close(Collection<TopicPartition> partitions);
{code}

We could also consider going a little further. Instead of depending on onPartitionsAssigned() to open resources, tasks could open partition resources on demand as records are received. In general, connectors will need some way to close partition-specific resources, but there might not be any need to pass the full list of partitions to close since the only open resources should be those that have received writes since the last rebalance. In this case, we just have a single method:

{code}
void close();
{code}

The downside to this is that the difference between close() and stop() then becomes a little unclear.

Obviously these are not compatible changes and connectors would have to be updated.


","Duplicated Code, Long Method, , , , , "
"   Rename Method,Extract Method,","Keep track of connector and task status info, expose it via the REST API Relate to KAFKA-3054;
We should keep track of the status of connector and task during their startup, execution, and handle exceptions thrown by connector and task;
Users should be able to fetch these informations by REST API and send some necessary commands(reconfiguring, restarting, pausing, unpausing) to connectors and tasks by REST API;","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,Move Method,Extract Method,Inline Method,","Support single message transforms in Kafka Connect Users should be able to perform light transformations on messages between a connector and Kafka. This is needed because some transformations must be performed before the data hits Kafka (e.g. filtering certain types of events or PII filtering). It's also useful for very light, single-message modifications that are easier to perform inline with the data import/export.","Duplicated Code, Long Method, , , , "
"   Rename Method,Extract Method,","Support single message transforms in Kafka Connect Users should be able to perform light transformations on messages between a connector and Kafka. This is needed because some transformations must be performed before the data hits Kafka (e.g. filtering certain types of events or PII filtering). It's also useful for very light, single-message modifications that are easier to perform inline with the data import/export.","Duplicated Code, Long Method, , "
"   Rename Method,","KIP-31/KIP-32 clean-ups During review, I found a few things that could potentially be improved but were not important enough to block the PR from being merged.",", "
"   Rename Method,Extract Method,","Improve consumer rebalance error messaging A common problem with the new consumer is to have message processing take longer than the session timeout, causing an unexpected rebalance. Unfortunately, when this happens, the error messages are often cryptic (e.g. something about illegal generation) and contain no clear advice on what to do (e.g. increase session timeout). We should do a pass on error messages to ensure that users receive clear guidance on the problem and possible solutions.","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,","Streams: stop using ""job"" terminology, rename job.id to application.id Background: We stopped using the terminology of a ""job"" in the context of Kafka Streams. For example, the upcoming Streams docs do not refer to a ""job"" anymore; otherwise it's very confusing to readers that are familiar with ""jobs"" in Hadoop, Spark, Storm, etc. because there's no equivalent concept of a ""job"" in Streams.

We should update the Streams code (see ""cd streams/ ; git grep job"" for a starting point) to reflect this accordingly. Notably, the configuration option ""job.id"" should be changed to ""application.id"".",", "
"   Rename Method,","Improve protocol type errors when invalid sizes are received We currently don't perform much validation on the size value read by the protocol types. This means that we end up throwing exceptions like `BufferUnderflowException`, `NegativeArraySizeException`, etc. `Schema.read` catches these exceptions and adds some useful information like:

{code}
throw new SchemaException(""Error reading field '"" + fields[i].name +
""': "" +
(e.getMessage() == null ? e.getClass().getName() : e.getMessage()));
{code}

We could do even better by throwing a `SchemaException` with a more user friendly message.",", "
"   Rename Method,Pull Up Method,Move Method,Extract Method,Pull Up Attribute,Move Attribute,","Support session windows The Streams DSL currently does not provide session window as in the DataFlow model. We have seen some common use cases for this feature and it's better adding this support asap.

https://cwiki.apache.org/confluence/display/KAFKA/KIP-94+Session+Windows","Duplicated Code, Long Method, , , , Duplicated Code, Duplicated Code, "
"   Rename Class,Move And Rename Class,Move Method,Extract Method,Move Attribute,","Support session windows The Streams DSL currently does not provide session window as in the DataFlow model. We have seen some common use cases for this feature and it's better adding this support asap.

https://cwiki.apache.org/confluence/display/KAFKA/KIP-94+Session+Windows","Duplicated Code, Long Method, , , , "
"   Move Class,Move And Rename Class,Extract Interface,Move Method,Extract Method,Move Attribute,","Support session windows The Streams DSL currently does not provide session window as in the DataFlow model. We have seen some common use cases for this feature and it's better adding this support asap.

https://cwiki.apache.org/confluence/display/KAFKA/KIP-94+Session+Windows","Duplicated Code, Long Method, , , , Large Class, "
"   Rename Method,","Support session windows The Streams DSL currently does not provide session window as in the DataFlow model. We have seen some common use cases for this feature and it's better adding this support asap.

https://cwiki.apache.org/confluence/display/KAFKA/KIP-94+Session+Windows",", "
"   Move Method,Extract Method,","KIP-146: Support per-connector/per-task classloaders in Connect Currently we just use the default ClassLoader in Connect. However, this limits how we can compatibly load conflicting connector plugins. Ideally we would use a separate class loader per connector/task that is instantiated to avoid potential conflicts.

Note that this also opens up options for other ways to provide jars to instantiate connectors. For example, Spark uses this to dynamically publish classes defined in the REPL and load them via URL: https://ardoris.wordpress.com/2014/03/30/how-spark-does-class-loading/ But much simpler examples (include URL in the connector class instead of just class name) are also possible and could be a nice way to more support dynamic sets of connectors, multiple versions of the same connector, etc.
","Duplicated Code, Long Method, , , "
"   Rename Method,Pull Up Method,Move Method,Extract Method,Inline Method,","Add capability to specify replication compact option for stream store Currently state store replication always go through a compact kafka topic. For some state stores, e.g. JoinWindow, there are no duplicates in the store, there is not much benefit using a compacted topic.
The problem of using compacted topic is the records can stay in kafka broker forever. In my use case, my key is ad_id, it's incrementing all the time, not bounded, I am worried the disk space on broker for that topic will go forever.
I think we either need the capability to purge the compacted records on broker, or allow us to specify different compact option for state store replication.","Duplicated Code, Long Method, , , , Duplicated Code, "
"   Rename Class,Rename Method,Move Method,","Consolidate tumbling windows and hopping windows We currently have two separate implementations for tumbling windows and hopping windows, even though tumbling windows are simply a specialization of hopping windows. We should thus consolidate/merge the two separate implementations into a new TimeWindows / TimeWindow.",", , "
"   Rename Method,Extract Method,","Move kafka-streams test fixtures into a published package The KStreamTestDriver and related fixtures defined in streams/src/test/java/org/apache/kafka/test would be useful to developers building applications on top of Kafka Streams, but they are not currently exposed in a package. 

I propose moving this directory to live under streams/fixtures/src/main and creating a new 'streams:fixtures' project in the gradle configuration to publish these as a separate package. 

KIP: https://cwiki.apache.org/confluence/display/KAFKA/KIP-247%3A+Add+public+test+utils+for+Kafka+Streams","Duplicated Code, Long Method, , "
"   Rename Method,Move Method,Extract Method,","Higher granularity streams metrics  Originally proposed by [~guozhang] in https://github.com/apache/kafka/pull/1362#issuecomment-218326690

We can consider adding metrics for process / punctuate / commit rate at the granularity of each processor node in addition to the global rate mentioned above. This is very helpful in debugging.

We can consider adding rate / total cumulated metrics for context.forward indicating how many records were forwarded downstream from this processor node as well. This is helpful in debugging.

We can consider adding metrics for each stream partition's timestamp. This is helpful in debugging.

Besides the latency metrics, we can also add throughput latency in terms of source records consumed.

","Duplicated Code, Long Method, , , "
"   Rename Method,",Allow setting of default topic configs via StreamsConfig Kafka Streams currently allows you to specify a replication factor for changelog and repartition topics that it creates. It should also allow you to specify any other TopicConfig. These should be used as defaults when creating Internal topics. The defaults should be overridden by any configs provided by the StateStoreSuppliers etc.,", "
"   Rename Class,Extract Method,Inline Method,Move Attribute,",Unify store and downstream caching in streams This is an umbrella story for capturing changes to processor caching in Streams as first described in KIP-63. https://cwiki.apache.org/confluence/display/KAFKA/KIP-63%3A+Unify+store+and+downstream+caching+in+streams,"Duplicated Code, Long Method, , , , "
"   Rename Method,Extract Method,","Warn that kafka-connect group.id must not conflict with connector names If the group.id value happens to have the same value as a connector names the following error will be issued:

{quote}
Attempt to join group connect-elasticsearch-indexer failed due to: The group member's supported protocols are incompatible with those of existing members.
{quote}

Maybe the documentation for Distributed Worker Configuration group.id could be worded:

{quote}
A unique string that identifies the Connect cluster group this worker belongs to. This value must be different than all connector configuration 'name' properties.
{quote}
","Duplicated Code, Long Method, , "
"   Move Method,Extract Method,","Add Helper Functions Into TestUtils Per guidance from [~guozhang] from PR #1477 move helper functions from RegexSourceIntegrationTest (getProducerConfig, getConsumerConfig, getStreamsConfig into TestUtils and parameterize as appropriate. Also look into adding a {{waitUntil(Condition condition)}} type construct to wait for a condition to be met without relying on using Thread.sleep","Duplicated Code, Long Method, , , "
"   Rename Class,Rename Method,Extract Method,Inline Method,Move Attribute,",Allow consumer to send heartbeats in background thread (KIP-62) This ticket covers the implementation of KIP-62 as documented here: https://cwiki.apache.org/confluence/display/KAFKA/KIP-62%3A+Allow+consumer+to+send+heartbeats+from+a+background+thread.,"Duplicated Code, Long Method, , , , "
"   Rename Method,Extract Method,","Split the ProducerBatch and resend when received RecordTooLargeException We recently see a few cases where RecordTooLargeException is thrown because the compressed message sent by KafkaProducer exceeded the max message size.

The root cause of this issue is because the compressor is estimating the batch size using an estimated compression ratio based on heuristic compression ratio statistics. This does not quite work for the traffic with highly variable compression ratios. 

For example, if the batch size is set to 1MB and the max message size is 1MB. Initially a the producer is sending messages (each message is 1MB) to topic_1 whose data can be compressed to 1/10 of the original size. After a while the estimated compression ratio in the compressor will be trained to 1/10 and the producer would put 10 messages into one batch. Now the producer starts to send messages (each message is also 1MB) to topic_2 whose message can only be compress to 1/5 of the original size. The producer would still use 1/10 as the estimated compression ratio and put 10 messages into a batch. That batch would be 2 MB after compression which exceeds the maximum message size. In this case the user do not have many options other than resend everything or close the producer if they care about ordering.

This is especially an issue for services like MirrorMaker whose producer is shared by many different topics.

To solve this issue, we can probably add a configuration ""enable.compression.ratio.estimation"" to the producer. So when this configuration is set to false, we stop estimating the compressed size but will close the batch once the uncompressed bytes in the batch reaches the batch size.","Duplicated Code, Long Method, , "
"   Move Method,Extract Method,Move Attribute,",SSL support for Connect REST API Currently the Connect REST API only supports http. We should also add SSL support so access to the REST API can be secured.,"Duplicated Code, Long Method, , , , "
"   Extract Method,Move Attribute,","Allow per stream/table timestamp extractor At the moment the timestamp extractor is configured via a {{StreamConfig}} value to {{KafkaStreams}}. That means you can only have a single timestamp extractor per app, even though you may be joining multiple streams/tables that require different timestamp extraction methods.

You should be able to specify a timestamp extractor via {{KStreamBuilder.stream()/table()}}, just like you can specify key and value serdes that override the StreamConfig defaults.

KIP: https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=68714788

Specifying a per-stream extractor should only be possible for sources, but not for intermediate topics. For PAPI we cannot enforce this, but for DSL {{through()}} should not allow to set a custom extractor by the user. In contrast, with regard to KAFKA-4785, is must internally set an extractor that returns the record's metadata timestamp in order to overwrite the global extractor from {{StreamsConfig}} (ie, set {{FailOnInvalidTimestampExtractor}}). This change should be done in KAFKA-4785 though.","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Decouple flush and offset commits It is desirable to have, in addition to the time-based flush interval, volume or size-based commits. E.g. a sink connector which is buffering in terms of number of records may want to request a flush when the buffer is full, or when sufficient amount of data has been buffered in a file.

Having a method like say {{requestFlush()}} on the {{SinkTaskContext}} would allow for connectors to have flexible policies around flushes. This would be in addition to the time interval based flushes that are controlled with {{offset.flush.interval.ms}}, for which the clock should be reset when any kind of flush happens.

We should probably also support requesting flushes via the {{SourceTaskContext}} for consistency though a use-case doesn't come to mind off the bat.","Duplicated Code, Long Method, , "
"   Move And Rename Class,Extract Interface,Extract Method,","Add Record Headers Currently headers are not natively supported unlike many transport and messaging platforms or standard, this is to add support for headers to kafka

This JIRA is related to KIP found here:
https://cwiki.apache.org/confluence/display/KAFKA/KIP-82+-+Add+Record+Headers
","Duplicated Code, Long Method, , Large Class, "
"   Rename Method,Extract Method,",Enable JAAS configuration for Kafka clients without a config file See KIP-85 for details: https://cwiki.apache.org/confluence/display/KAFKA/KIP-85%3A+Dynamic+JAAS+configuration+for+Kafka+clients,"Duplicated Code, Long Method, , "
"   Move Class,Rename Class,Move Method,Extract Method,Move Attribute,",KIP-86: Configurable SASL callback handlers Implementation of KIP-86: https://cwiki.apache.org/confluence/display/KAFKA/KIP-86%3A+Configurable+SASL+callback+handlers,"Duplicated Code, Long Method, , , , "
"   Move Method,Inline Method,Move Attribute,","Simplify KTableSource With the new ""interactive queries"" feature, source tables are always materialized. Thus, we can remove the stale flag {{KTableSoure#materialized}} (which is always true now) to simply to code.",", , , , "
"   Rename Method,Extract Method,","RocksDB checkpoint files lost on kill -9 Right now, the checkpoint files for logged RocksDB stores are written during a graceful shutdown, and removed upon restoration. Unfortunately this means that in a scenario where the process is forcibly killed, the checkpoint files are not there, so all RocksDB stores are rematerialized from scratch on the next launch.

In a way, this is good, because it simulates bootstrapping a new node (for example, its a good way to see how much I/O is used to rematerialize the stores) however it leads to longer recovery times when a non-graceful shutdown occurs and we want to get the job up and running again.

It seems that two possible things to consider:

- Simply do not remove checkpoint files on restoring. This way a kill -9 will result in only repeating the restoration of all the data generated in the source topics since the last graceful shutdown.

- Continually update the checkpoint files (perhaps on commit) -- this would result in the least amount of overhead/latency in restarting, but the additional complexity may not be worth it.


https://cwiki.apache.org/confluence/display/KAFKA/KIP-116%3A+Add+State+Store+Checkpoint+Interval+Configuration","Duplicated Code, Long Method, , "
"   Rename Method,",Kafka Streams resetter is slow because it joins the same group for each topic The resetter is joining the same group for each topic which takes ~10secs in my testing. This makes the reset very slow when you have a lot of topics.,", "
"   Rename Method,",Kafka Streams resetter is slow because it joins the same group for each topic The resetter is joining the same group for each topic which takes ~10secs in my testing. This makes the reset very slow when you have a lot of topics.,", "
"   Rename Method,","Remove caching of dirty and removed keys from StoreChangeLogger The StoreChangeLogger currently keeps a cache of dirty and removed keys and will batch the changelog records such that we don't send a record for each update. However, with KIP-63 this is unnecessary as the batching and de-duping is done by the caching layer. Further, the StoreChangeLogger relies on context.timestamp() which is likely to be incorrect when caching is enabled",", "
"   Rename Class,Rename Method,Move Method,Extract Method,Inline Method,Push Down Attribute,Move Attribute,","Replace MessageSet usage with client-side equivalents Currently we have two separate implementations of Kafka's message format and log structure, one on the client side and one on the server side. Once KAFKA-2066 is merged, we will only be using the client side objects for direct serialization/deserialization in the request APIs, but we we still be using the server-side MessageSet objects everywhere else. Ideally, we can update this code to use the client objects everywhere so that future message format changes only need to be made in one place. This would eliminate the potential for implementation differences and gives us a uniform API for accessing the low-level log structure.","Duplicated Code, Long Method, , , , , , "
"   Rename Method,","Refactor Connect backing stores for thread-safety 
In Kafka Connect there has been already significant provisioning for multi-threaded execution with respect to classes implementing backing store interfaces. 

A requirement for 
[KAFKA-3008|https://issues.apache.org/jira/browse/KAFKA-3008] is to tighten thread-safety guarantees in these implementations, especially for ConfigBackingStore and StatusBackingStore, and this will be the focus of the current ticket. 
",", "
"   Pull Up Method,Extract Method,","Add consumer.close(timeout, unit) for graceful close with timeout KAFKA-3703 implements graceful close of consumers with a hard-coded timeout of 5 seconds. For consistency with the producer, add a close method with configurable timeout for Consumer.

{quote}
public void close(long timeout, TimeUnit unit);
{quote}

Since this is a public interface change, this change requires a KIP.","Duplicated Code, Long Method, , Duplicated Code, "
"   Rename Class,Move Method,Extract Method,Move Attribute,","Per listener security setting overrides (KIP-103) This is a follow-up to KAFKA-4565 where most of KIP-103 was implemented. I quote the missing bit from the KIP:

""Finally, we make it possible to provide different security (SSL and SASL) settings for each listener name by adding a normalised prefix (the listener name is lowercased) to the config name. For example, if we wanted to set a different keystore for the CLIENT listener, we would set a config with name listener.name.client.ssl.keystore.location. If the config for the listener name is not set, we will fallback to the generic config (i.e. ssl.keystore.location) for compatibility and convenience. For the SASL case, some configs are provided via a JAAS file, which consists of one or more entries. The broker currently looks for an entry named KafkaServer. We will extend this so that the broker first looks for an entry with a lowercased listener name followed by a dot as a prefix to the existing name. For the CLIENT listener example, the broker would first look for client.KafkaServer with a fallback to KafkaServer, if necessary.""

KIP link for details:
https://cwiki.apache.org/confluence/display/KAFKA/KIP-103%3A+Separation+of+Internal+and+External+traffic","Duplicated Code, Long Method, , , , "
"   Rename Method,Extract Method,","Parametrize streams benchmarks to run at scale The streams benchmarks (in SimpleBenchmark.java and triggered through kafka/tests/kafkatest/benchmarks/streams/streams_simple_benchmark_test.py) run as single-instance, with a simple 1 broker Kafka cluster. 

We need to parametrize the tests so they can run at scale, e.g., with 10-100 KafkaStreams instances and similar number of brokers.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Improve diagnostics for SASL authentication failures At the moment, broker closes the client connection if SASL authentication fails. Clients see this as a connection failure and do not get any feedback for the reason why the connection was closed. Producers and consumers retry, attempting to create successful connections, treating authentication failures as transient failures. There are no log entries on the client-side which indicate that any of these connection failures were due to authentication failure.

This JIRA will aim to improve diagnosis of authentication failures with the changes described in [KIP-152|https://cwiki.apache.org/confluence/display/KAFKA/KIP-152+-+Improve+diagnostics+for+SASL+authentication+failures].

This JIRA also does not change handling of SSL authentication failures. javax.net.debug provides sufficient diagnostics for this case. SSL changes are harder to do while preserving backward compatibility.



","Duplicated Code, Long Method, , "
"   Move Method,Move Attribute,","Exploit #peek to implement #print() and other methods From: https://github.com/apache/kafka/pull/2493#pullrequestreview-22157555

Things that I can think of:

- print / writeAsTest can be a special impl of peek; KStreamPrint etc can be removed.
- consider collapse KStreamPeek with KStreamForeach with a flag parameter indicating if the acted key-value pair should still be forwarded.

",", , , "
"   Rename Method,",Expose states of active tasks to public API https://cwiki.apache.org/confluence/display/KAFKA/KIP+130%3A+Expose+states+of+active+tasks+to+KafkaStreams+public+API,", "
"   Rename Method,Extract Method,",Add Exactly-Once Semantics to Streams https://cwiki.apache.org/confluence/display/KAFKA/KIP-129%3A+Streams+Exactly-Once+Semantics,"Duplicated Code, Long Method, , "
"   Rename Method,","Only log invalid user configs and overwrite with correct one Streams does not allow to overwrite some config parameters (eg, {{enable.auto.commit}}) Currently, we throw an exception, but this is actually not required, as Streams can just ignore/overwrite the user provided value.

Thus, instead of throwing, we should just log a WARN message and overwrite the config with the values that suits Streams. (atm it's only one parameter {{enable.auto.commit}}), but with exactly-once it's going to be more (cf. KAFKA-4923). Thus, the scope of this ticket depends when it will be implemented (ie, before or after KAFKA-4923).

This ticket should also include JavaDoc updates that explain what parameters cannot be specified by the user.",", "
"   Rename Method,Extract Method,","Improve internal Task APIs Currently, the internal interface for tasks is not very clean and it's hard to reason about the control flow when tasks get closes, suspended, resumed etc. This makes exception handling particularly hard.

We want to refactor this part of the code to get a clean control flow and interface.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,Inline Method,","Improve internal Task APIs Currently, the internal interface for tasks is not very clean and it's hard to reason about the control flow when tasks get closes, suspended, resumed etc. This makes exception handling particularly hard.

We want to refactor this part of the code to get a clean control flow and interface.","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Add option ""dry run"" to Streams application reset tool We want to add an option to Streams application reset tool, that allow for a ""dry run"". Ie, only prints what topics would get modified/deleted without actually applying any actions.","Duplicated Code, Long Method, , "
"   Move And Rename Class,Rename Method,Move Method,Extract Method,Push Down Attribute,Move Attribute,","Range Scan for Windowed State Stores Windowed state stores currently do not support key range scans, even though it seems reasonable to be able to – at least in a given window – do the same operations you would do on a key-value store.","Duplicated Code, Long Method, , , , , "
"   Rename Method,","Application Reset Tool does not need to seek for internal topics As KAFKA-4456 got resolved, there is no modify offsets of internal topics with the application reset tool, as those offsets will be deleted anyway.",", "
"   Rename Method,","New Short serializer, deserializer, serde There is no Short serializer/deserializer in the current clients component.

It could be useful when using Kafka-Connect to write data to databases with SMALLINT fields (or similar) and avoiding conversions to int improving a bit the performance in terms of memory and network.",", "
"   Rename Class,Rename Method,","Changes to punctuate semantics (KIP-138) This ticket is to track implementation of 
[KIP-138: Change punctuate semantics|https://cwiki.apache.org/confluence/display/KAFKA/KIP-138%3A+Change+punctuate+semantics]",", "
"   Move Method,Extract Method,","Support ExtendedDeserializer in Kafka Streams KIP-82 introduced the concept of message headers and introduced an ExtendedDeserializer interface that allowed a Deserializer to access those message headers.

Change Kafka Streams to support the use of ExtendedDeserializer to provide compatibility with Kafka Clients that use the new header functionality.","Duplicated Code, Long Method, , , "
"   Move Method,Move Attribute,","Add ability to batch restore and receive restoration stats. Currently, when restoring a state store in a Kafka Streams application, we put one key-value at a time into the store. 

This task aims to make this recovery more efficient by creating a new interface with ""restoreAll"" functionality allowing for bulk writes by the underlying state store implementation. 

The proposal will also add ""beginRestore"" and ""endRestore"" callback methods potentially used for 
Tracking when the bulk restoration process begins and ends.
Keeping track of the number of records and last offset restored.



KIP: https://cwiki.apache.org/confluence/display/KAFKA/KIP-167%3A+Add+interface+for+the+state+store+restoration+process",", , , "
"   Rename Method,Extract Method,","Streams should not suspend tasks twice Currently, Streams suspends tasks on rebalance and closes suspended tasks if not reassigned. During close, {{suspend()}} is called a second time, also calling {{Processor.close()}} for all nodes again.

It would be safer to only call {{suspend()}} once in case users have non-idempotent operations in {{Processor.close()}} method and might thus fail. (cf. KAFKA-5167)","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Formatting verifiable producer/consumer output in a similar fashion Hi,
following the proposal to have verifiable producer/consumer providing a very similar output where the ""timestamp"" is always the first column followed by ""name"" event and then all the specific data for such event.
It includes a verifiable producer refactoring for having that in the same way as verifiable consumer.

Thanks,
Paolo","Duplicated Code, Long Method, , "
"   Rename Class,Move Method,Move Attribute,","Reduce classes needed for LeaderAndIsrPartitionState and MetadataPartitionState It will be cleaner to replace LeaderAndIsrPartitionState and MetadataPartitionState in LeaderAndIsr.scala with org.apache.kafka.common.requests.PartitionState and
UpdateMetadataRequest.PartitionState respectively.
",", , , "
"   Rename Method,Extract Method,",KIP-182: Reduce Streams DSL overloads and allow easier use of custom storage engines 0,"Duplicated Code, Long Method, , "
"   Extract Interface,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Refactor StreamThread to separate concerns and enable better testability {{StreamThread}} does a lot of stuff, i.e., managing and creating tasks, getting data from consumers, updating standby tasks, punctuating, rebalancing etc. With the current design it is extremely hard to reason about and is quite tightly coupled. 
We need to start to tease out some of the separate concerns from StreamThread, ie, TaskManager, RebalanceListener etc.","Duplicated Code, Long Method, , , , , Large Class, "
"   Rename Method,Move Method,Extract Method,Move Attribute,",Implement KafkaPrincipalBuilder interface with support for SASL (KIP-189) This issue covers the implementation of [KIP-189|https://cwiki.apache.org/confluence/display/KAFKA/KIP-189%3A+Improve+principal+builder+interface+and+add+support+for+SASL].,"Duplicated Code, Long Method, , , , "
"   Rename Method,","Add AdminClient.createPartitions() It should be possible to increase the partition count using the AdminClient. 

See [KIP-195|https://cwiki.apache.org/confluence/display/KAFKA/KIP-195%3A+AdminClient.increasePartitions]",", "
"   Rename Method,Extract Method,Inline Method,","Introduce delivery.timeout.ms producer config (KIP-91) We propose adding a new timeout delivery.timeout.ms. The window of enforcement includes batching in the accumulator, retries, and the inflight segments of the batch. With this config, the user has a guaranteed upper bound on when a record will either get sent, fail or expire from the point when send returns. In other words we no longer overload request.timeout.ms to act as a weak proxy for accumulator timeout and instead introduce an explicit timeout that users can rely on without exposing any internals of the producer such as the accumulator. 

See [KIP-91|https://cwiki.apache.org/confluence/display/KAFKA/KIP-91+Provide+Intuitive+User+Timeouts+in+The+Producer] for more details.","Duplicated Code, Long Method, , , "
"   Rename Class,Move Method,Extract Method,Inline Method,Move Attribute,","Handle SSL authentication failures as non-retriable exceptions in clients KIP-152 improves diagnostics for SASL authentication failures and propagates SASL authentication failures to producers and consumers. For SSL authentication, we can't have protocol changes, but we should try and adopt the same behaviour if possible.","Duplicated Code, Long Method, , , , , "
"   Rename Class,Rename Method,",Handle authentication failures from transactional producer and KafkaAdminClient Follow on from KAFKA-5854 to handle authentication failures better for transactional producer API and KafkaAdminClient,", "
"   Rename Method,Extract Method,","Improve the quota throttle communication. Currently if a client is throttled duet to quota violation, the broker will only send back a response to the clients after the throttle time has passed. In this case, the clients don't know how long the response will be throttled and might hit request timeout before the response is returned. As a result the clients will retry sending a request and results a even longer throttle time. 

The above scenario could happen when a large clients group sending records to the brokers. We saw this when a MapReduce job pushes data to the Kafka cluster. 

To improve this, the broker can return the response with throttle time immediately after processing the requests. After that, the broker will mute the channel for this client. A correct client implementation should back off for that long before sending the next request. If the client ignored the throttle time and send the next request immediately, the channel will be muted and the request won't be processed until the throttle time has passed. 

A KIP will follow with more details.","Duplicated Code, Long Method, , "
"   Extract Method,Inline Method,","KIP-222: Add ""describe consumer groups"" and ""list consumer groups"" to KafkaAdminClient {{KafkaAdminClient}} does not allow to get information about consumer groups. This feature is supported by old {{kafka.admin.AdminClient}} though. 

We should add {{KafkaAdminClient#describeConsumerGroups()}} and {{KafkaAdminClient#listConsumerGroup()}}. 

Associated KIP: KIP-222","Duplicated Code, Long Method, , , "
"   Rename Method,","Provide for custom error handling when Kafka Streams fails to produce This is an issue related to the following KIP: 
https://cwiki.apache.org/confluence/display/KAFKA/KIP-210+-+Provide+for+custom+error+handling++when+Kafka+Streams+fails+to+produce",", "
"   Rename Method,","Postpone normal processing of tasks within a thread until restoration of all tasks have completed Let's say a stream thread hosts multiple tasks, A and B. At the very beginning when A and B are assigned to the thread, the thread state is {{TASKS_ASSIGNED}}, and the thread start restoring these two tasks during this state using the restore consumer while using normal consumer for heartbeating. 

If task A's restoration has completed earlier than task B, then the thread will start processing A immediately even when it is still in the {{TASKS_ASSIGNED}} phase. But processing task A will slow down restoration of task B since it is single-thread. So the thread's transition to {{RUNNING}} when all of its assigned tasks have completed restoring and now can be processed will be delayed. 

Note that the streams instance's state will only transit to {{RUNNING}} when all of its threads have transit to {{RUNNING}}, so the instance's transition will also be delayed by this scenario. 

We'd better to not start processing ready tasks immediately, but instead focus on restoration during the {{TASKS_ASSIGNED}} state to shorten the overall time of the instance's state transition.",", "
"   Rename Method,Push Down Method,Extract Method,Push Down Attribute,","Make Repartition Topics Transient Unlike changelog topics, the repartition topics could just be short-lived. Today users have different ways to configure them with short retention such as enforce a short retention period or use AppendTime for repartition topics. All these would be cumbersome and Streams should just do this for the users. 

One way to do it is use the “purgeData” admin API (KIP-107) such that after the offset of the input topics are committed, if the input topics are actually repartition topics, we would purge the data immediately.","Duplicated Code, Long Method, , , , "
"   Move Class,Rename Method,Extract Method,","Make Repartition Topics Transient Unlike changelog topics, the repartition topics could just be short-lived. Today users have different ways to configure them with short retention such as enforce a short retention period or use AppendTime for repartition topics. All these would be cumbersome and Streams should just do this for the users. 

One way to do it is use the “purgeData” admin API (KIP-107) such that after the offset of the input topics are committed, if the input topics are actually repartition topics, we would purge the data immediately.","Duplicated Code, Long Method, , "
"   Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Add the AdminClient in Streams' KafkaClientSupplier We will add Java AdminClient to Kafka Streams, in order to replace the internal StreamsKafkaClient. More details can be found in KIP-220 (https://cwiki.apache.org/confluence/display/KAFKA/KIP-220%3A+Add+AdminClient+into+Kafka+Streams%27+ClientSupplier)","Duplicated Code, Long Method, , , , , "
"   Rename Method,Extract Method,","Add the AdminClient in Streams' KafkaClientSupplier We will add Java AdminClient to Kafka Streams, in order to replace the internal StreamsKafkaClient. More details can be found in KIP-220 (https://cwiki.apache.org/confluence/display/KAFKA/KIP-220%3A+Add+AdminClient+into+Kafka+Streams%27+ClientSupplier)","Duplicated Code, Long Method, , "
"   Rename Method,","report a metric of the lag between the consumer offset and the start offset of the log Currently, the consumer reports a metric of the lag between the high watermark of a log and the consumer offset. It will be useful to report a similar lag metric between the consumer offset and the start offset of the log. If this latter lag gets close to 0, it's an indication that the consumer may lose data soon.",", "
"   Rename Method,Move Method,Move Attribute,","DNS alias support for secured connections It seems clients can't use a dns alias in front of a secured Kafka cluster. 

So applications can only specify a list of hosts or IPs in bootstrap.servers instead of an alias encompassing all cluster nodes. 

Using an alias in bootstrap.servers results in the following error : 

javax.security.sasl.SaslException: An error: (java.security.PrivilegedActionException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Fail to create credential. (63) - No service creds)]) occurred when evaluating SASL token received from the Kafka Broker. Kafka Client will go to AUTH_FAILED state. [Caused by javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Fail to create credential. (63) - No service creds)]] 

When using SASL/Kerberos authentication, the kafka server principal is of the form kafka@kafka/broker1.hostname.com@EXAMPLE.COM 

Kerberos requires that the hosts can be resolved by their FQDNs. 

During SASL handshake, the client will create a SASL token and then send it to kafka for auth. 
But to create a SASL token the client first needs to be able to validate that the broker's kerberos is a valid one. 

There are 3 potential options : 

1. Creating a single kerberos principal not linked to a host but to an alias and reference it in the broker jaas file. 
But I think the kerberos infrastructure would refuse to validate it, so the SASL handshake would still fail 

2. Modify the client bootstrap mechanism to detect whether bootstrap.servers contains a dns alias. If it does, resolve and expand the alias to retrieve all hostnames behind it and add them to the list of nodes. 
This could be done by modifying parseAndValidateAddresses() in ClientUtils 

3. Add a cluster.alias parameter that would be handled by the logic above. Having another parameter to avoid confusion on how bootstrap.servers works behind the scene. 

Thoughts ? 
I would be happy to contribute the change for any of the options. 

I believe the ability to use a dns alias instead of static lists of brokers would bring good deployment flexibility. 
",", , , "
"   Rename Method,",Have State Stores Restore Before Initializing Toplogy Streams should restore state stores (if needed) before initializing the topology.,", "
"   Rename Method,",Introduce Incremental FetchRequests to Increase Partition Scalability Introduce Incremental FetchRequests to Increase Partition Scalability. See https://cwiki.apache.org/confluence/display/KAFKA/KIP-227%3A+Introduce+Incremental+FetchRequests+to+Increase+Partition+Scalability,", "
"   Rename Method,",Change LogSegment.delete to deleteIfExists and harden log recovery Fixes KAFKA-6194 and a delete while open issue (KAFKA-6322 and KAFKA-6075) and makes the code more robust.,", "
"   Rename Method,Extract Method,Inline Method,","Improve Streams metrics for skipped records Copy this from KIP-210 discussion thread: 

{quote} 
Note that currently we have two metrics for `skipped-records` on different 
levels: 

1) on the highest level, the thread-level, we have a `skipped-records`, 
that records all the skipped records due to deserialization errors. 
2) on the lower processor-node level, we have a 
`skippedDueToDeserializationError`, that records the skipped records on 
that specific source node due to deserialization errors. 


So you can see that 1) does not cover any other scenarios and can just be 
thought of as an aggregate of 2) across all the tasks' source nodes. 
However, there are other places that can cause a record to be dropped, for 
example: 

1) https://issues.apache.org/jira/browse/KAFKA-5784: records could be 
dropped due to window elapsed. 
2) KIP-210: records could be dropped on the producer side. 
3) records could be dropped during user-customized processing on errors. 
{quote} 

[~guozhang] Not sure what you mean by ""3) records could be dropped during user-customized processing on errors."" 

Btw: we also drop record with {{null}} key and/or value for certain DSL operations. This should be included as well. 

KIP: : https://cwiki.apache.org/confluence/display/KAFKA/KIP-274%3A+Kafka+Streams+Skipped+Records+Metrics","Duplicated Code, Long Method, , , "
"   Move Method,Inline Method,Move Attribute,","Improve Streams metrics for skipped records Copy this from KIP-210 discussion thread: 

{quote} 
Note that currently we have two metrics for `skipped-records` on different 
levels: 

1) on the highest level, the thread-level, we have a `skipped-records`, 
that records all the skipped records due to deserialization errors. 
2) on the lower processor-node level, we have a 
`skippedDueToDeserializationError`, that records the skipped records on 
that specific source node due to deserialization errors. 


So you can see that 1) does not cover any other scenarios and can just be 
thought of as an aggregate of 2) across all the tasks' source nodes. 
However, there are other places that can cause a record to be dropped, for 
example: 

1) https://issues.apache.org/jira/browse/KAFKA-5784: records could be 
dropped due to window elapsed. 
2) KIP-210: records could be dropped on the producer side. 
3) records could be dropped during user-customized processing on errors. 
{quote} 

[~guozhang] Not sure what you mean by ""3) records could be dropped during user-customized processing on errors."" 

Btw: we also drop record with {{null}} key and/or value for certain DSL operations. This should be included as well. 

KIP: : https://cwiki.apache.org/confluence/display/KAFKA/KIP-274%3A+Kafka+Streams+Skipped+Records+Metrics",", , , , "
"   Push Down Method,Extract Method,","Allow timestamp manipulation in Processor API Atm, Kafka Streams only has a defined ""contract"" about timestamp propagation at the Processor API level: all processor within a sub-topology, see the timestamp from the input topic record and this timestamp will be used for all result record when writing them to an topic, too. 

For the DSL and also for custom operator, it would be desirable to allow timestamp manipulation for at Processor level for individual records that are forwarded. 

KIP-251: https://cwiki.apache.org/confluence/display/KAFKA/KIP-251%3A+Allow+timestamp+manipulation+in+Processor+API","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Use single-point queries than range queries for windowed aggregation operators Today for windowed aggregations in Streams DSL, the underlying implementation is leveraging the fetch(key, from, to) API to get all the related windows for a single record to update. However, this is a very inefficient operation with significant amount of CPU time iterating over window stores. On the other hand, since the operator implementation itself have full knowledge of the window specs it can actually translate this operation into multiple single-point queries with the accurate window start timestamp, which would largely reduce the overhead. 

The proposed approach is to add a single fetch API to the WindowedStore and use that in the KStreamWindowedAggregate / KStreamWindowedReduce operators.","Duplicated Code, Long Method, , "
"   Rename Method,Inline Method,","Re-write simple benchmark in system tests with JMXTool The current SimpleBenchmark is recording the num.records actively in order to compute throughput and latency, which introduces extra cost plus is less accurate for benchmarking purposes; instead, it's better to use JmxMixin with SimpleBenchmark to record metrics.",", , "
"   Rename Method,","Delay initiating the txn on producers until initializeTopology with EOS turned on In Streams EOS implementation, the created producers for tasks will initiate a txn immediately after being created in the constructor of `StreamTask`. However, the task may not process any data and hence producer may not send any records for that started txn for a long time because of the restoration process. And with default txn.session.timeout valued at 60 seconds, it means that if the restoration takes more than that amount of time, upon starting the producer will immediately get the error that its producer epoch is already old. 

To fix this, we should consider instantiating the txn only after the restoration phase is done. Although this may have a caveat that if the producer is already fenced, it will not be notified until then, in initializeTopology. But I think this should not be a correctness issue since during the restoration process we do not make any changes to the processing state.",", "
"   Rename Method,Extract Method,Inline Method,",The Trogdor coordinator should track task statuses The Trogdor coordinator should track task statuses,"Duplicated Code, Long Method, , , "
"   Rename Method,Move Method,Extract Method,","Reduce Kafka Streams Footprint The persistent storage footprint of a Kafka Streams application contains the following aspects: 
# The internal topics created on the Kafka cluster side. 
# The materialized state stores on the Kafka Streams application instances side. 

There have been some questions about reducing these footprints, especially since many of them are not necessary. For example, there are redundant internal topics, as well as unnecessary state stores that takes up space but also affect performance. When people are pushing Streams to production with high traffic, this issue would be more common and severe. Reducing the footprint of Streams have clear benefits for reducing resource utilization of Kafka Streams applications, and also not creating pressure on broker's capacities.","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,Inline Method,","Reduce Kafka Streams Footprint The persistent storage footprint of a Kafka Streams application contains the following aspects: 
# The internal topics created on the Kafka cluster side. 
# The materialized state stores on the Kafka Streams application instances side. 

There have been some questions about reducing these footprints, especially since many of them are not necessary. For example, there are redundant internal topics, as well as unnecessary state stores that takes up space but also affect performance. When people are pushing Streams to production with high traffic, this issue would be more common and severe. Reducing the footprint of Streams have clear benefits for reducing resource utilization of Kafka Streams applications, and also not creating pressure on broker's capacities.","Duplicated Code, Long Method, , , "
"   Move Class,Move And Rename Class,Rename Method,Move Method,Inline Method,Move Attribute,","Reduce Kafka Streams Footprint The persistent storage footprint of a Kafka Streams application contains the following aspects: 
# The internal topics created on the Kafka cluster side. 
# The materialized state stores on the Kafka Streams application instances side. 

There have been some questions about reducing these footprints, especially since many of them are not necessary. For example, there are redundant internal topics, as well as unnecessary state stores that takes up space but also affect performance. When people are pushing Streams to production with high traffic, this issue would be more common and severe. Reducing the footprint of Streams have clear benefits for reducing resource utilization of Kafka Streams applications, and also not creating pressure on broker's capacities.",", , , , "
"   Rename Method,Extract Method,","Remove deprecated APIs from KIP-120 and KIP-182 in Streams As we move on to the next major release 2.0, we can consider removing the deprecated APIs from KIP-120 and KIP-182.","Duplicated Code, Long Method, , "
"   Rename Class,Extract Method,Move Attribute,","Remove deprecated APIs from KIP-120 and KIP-182 in Streams As we move on to the next major release 2.0, we can consider removing the deprecated APIs from KIP-120 and KIP-182.","Duplicated Code, Long Method, , , "
"   Rename Method,Inline Method,","Remove deprecated APIs from KIP-120 and KIP-182 in Streams As we move on to the next major release 2.0, we can consider removing the deprecated APIs from KIP-120 and KIP-182.",", , "
"   Rename Method,Inline Method,","Remove deprecated APIs from KIP-120 and KIP-182 in Streams As we move on to the next major release 2.0, we can consider removing the deprecated APIs from KIP-120 and KIP-182.",", , "
"   Rename Method,Extract Method,","KIP-244: Add Record Header support to Kafka Streams Processor API Add support for headers on Streams Processor API. 

KIP documentation: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-244%3A+Add+Record+Header+support+to+Kafka+Streams+Processor+API] ","Duplicated Code, Long Method, , "
"   Rename Class,Extract Method,","Externalize Secrets for Kafka Connect Configurations Kafka Connect's connector configurations have plaintext passwords, and Connect stores these in cleartext either on the filesystem (for standalone mode) or in internal topics (for distributed mode). 

Connect should not store or transmit cleartext passwords in connector configurations. Secrets in stored connector configurations should be allowed to be replaced with references to values stored in external secret management systems. Connect should provide an extension point for adding customized integrations, as well as provide a file-based extension as an example. Second, a Connect runtime should be allowed to be configured to use one or more of these extensions, and allow connector configurations to use placeholders that will be resolved by the runtime before passing the complete connector configurations to connectors. This will allow existing connectors to not see any difference in the configurations that Connect provides to them at startup. And third, Connect's API should be changed to allow a connector to obtain the latest connector configuration at any time. 

","Duplicated Code, Long Method, , "
"   Rename Method,","Remove caching wrapper stores if cache-size is configured to zero bytes Users can disable caching globally by setting the cache size to zero in their config. However, this does only effectively disable the caching layer, but the code is still in place. 

We should consider to remove the caching wrappers completely for this case. The tricky part is, that we insert the caching layer at compile time, ie, when calling `StreamsBuilder#build()` – at this point, we don't know the configuration yet. Thus, we need to find a way to rewrite the topology after it is passed to `KafkaStreams` if case caching size is set to zero. 

KIP: [KIP-356: Add withCachingDisabled() to StoreBuilder|https://cwiki.apache.org/confluence/display/KAFKA/KIP-356%3A+Add+withCachingDisabled%28%29+to+StoreBuilder?src=jira]",", "
"   Rename Method,Extract Method,Move Attribute,","Decrease consumer request timeout to 30s Per KIP-266 discussion, we should lower the request timeout. We should also add new logic to override this timeout for the JoinGroup request.","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Add support for Custom SASL extensions in OAuth authentication KIP: [here|https://cwiki.apache.org/confluence/display/KAFKA/KIP-342%3A+Add+support+for+Custom+SASL+extensions+in+OAuth+authentication] 

Kafka currently supports non-configurable SASL extensions in its SCRAM authentication protocol for delegation token validation. It would be useful to provide configurable SASL extensions for the OAuthBearer authentication mechanism as well, such that clients could attach arbitrary data for the principal authenticating into Kafka. This way, a custom principal can hold information derived from the authentication mechanism, which could prove useful for better tracing and troubleshooting, for example. This can be done in a way which allows for easier extendability in future SASL mechanisms.","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,Extract Method,Inline Method,","Add support for Custom SASL extensions in OAuth authentication KIP: [here|https://cwiki.apache.org/confluence/display/KAFKA/KIP-342%3A+Add+support+for+Custom+SASL+extensions+in+OAuth+authentication] 

Kafka currently supports non-configurable SASL extensions in its SCRAM authentication protocol for delegation token validation. It would be useful to provide configurable SASL extensions for the OAuthBearer authentication mechanism as well, such that clients could attach arbitrary data for the principal authenticating into Kafka. This way, a custom principal can hold information derived from the authentication mechanism, which could prove useful for better tracing and troubleshooting, for example. This can be done in a way which allows for easier extendability in future SASL mechanisms.","Duplicated Code, Long Method, , , "
"   Rename Method,","KIP-328: Add Window Grace Period (and deprecate Window Retention) As described in [https://cwiki.apache.org/confluence/display/KAFKA/KIP-328%3A+Ability+to+suppress+updates+for+KTables] 

  

This ticket only covers the grace period portion of the work.",", "
"   Rename Method,","Streams should be more fencing-sensitive during task suspension under EOS When EOS is turned on, Streams did the following steps: 

1. InitTxn in task creation. 
2. BeginTxn in topology initialization. 
3. AbortTxn in clean shutdown. 
4. CommitTxn in commit(), which is called in suspend() as well. 

Now consider this situation, with two thread (Ta) and (Tb) and one task: 

1. originally Ta owns the task, consumer generation is 1. 
2. Ta is un-responsive to send heartbeats, and gets kicked out, a new generation 2 is formed with Tb in it. The task is migrated to Tb while Ta does not know. 
3. Ta finally calls `consumer.poll` and was aware of the rebalance, it re-joins the group, forming a new generation of 3. And during the rebalance the leader decides to assign the task back to Ta. 
4.a) Ta calls onPartitionRevoked on the task, suspending it and call commit. However if there is no data ever sent since `BeginTxn`, this commit call will become a no-op. 
4.b) Ta then calls onPartitionAssigned on the task, resuming it, and then calls BeginTxn. Then it was encountered a ProducerFencedException, incorrectly. 

The root cause is that, Ta does not trigger InitTxn to claim ""I'm the newest for this txnId, and am going to fence everyone else with the same txnId"", so it was mistakenly treated as the old client than Tb. 

Note that this issue is not common, since we need to encounter a txn that did not send any data at all to make its commitTxn call a no-op, and hence not being fenced earlier on. 

One proposal for this issue is to close the producer and recreates a new one in `suspend` after the commitTxn call succeeded and `startNewTxn` is false, so that the new producer will always `initTxn` to fence others.",", "
"   Rename Method,","DriverQueryHook should support postSelect option also. as of now DriverQueryHook has a preLaunch() method which is called just before launching the query . 

We should also have an option to interact with hook on postSelect() which will be called once query has been accepted by lens server and appropriate driver has been selected for it.


Also as of now, the preLaunch() is called by individual driver. This should be moved to server and drivers should not manage this operation. 
",", "
"   Rename Method,","Convert dimension filter to fact filters for perfomace improvement on outer join queries Filters like the following can be converted to fact filters. This will help in improving query performance by pushing down filter to driving table i.e fact.

e.g. 
where dim.name in ('x', 'y') 
becomes
where fact.dimid in (select dim.id from dim where dim.name in ('x','y'))",", "
"   Move Method,Extract Method,","Multi storage queries are not written properly when storage start and end time is configured *Storages* 
S1 : no start and end time 
S2 : name=""cube.storagetable.end.times"" value=""2016-03-31""
S3 : name=""cube.storagetable.start.times"" value=""2016-04-01""
S4 : name=""cube.storagetable.start.times"" value=""2016-04-01""
*Query*
cube select burn from mycube where time_range_in(d_time, '2016-03-30-00', '2016-04-01-23') --async
All storages are skipped in this case.
Expected a Union between S2 and S3","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Ability to load different instances of same driver class Currently we are loading only one driver instance per class. We should be able to load multiple different instances of a driver class.

For example, There can be multiple HiveDrivers talking to different HiveServers sitting on each colo. Or different JDBCDrivers talking to different dbs.","Duplicated Code, Long Method, , "
"   Rename Method,Pull Up Method,Extract Method,Inline Method,","Resolve issues with case when aggregate expressions with dim-attributes conditions  Right now, users write query like the following :

select sum(case when dim1=x then msr1 else 0 end), msr2 from cube where time_range_in(...)

And if dim1 and msr2 are not queryable, query fails saying Fields cannot be queried together.

but the query should be accepted.

The same works fine if the above is created as cube expression.","Duplicated Code, Long Method, , , Duplicated Code, "
"   Rename Class,Move And Rename Class,Rename Method,Move Method,Extract Method,Inline Method,",Improve Scheduler API messages   This patch includes more clearer scheduler APIs.,"Duplicated Code, Long Method, , , , "
"   Rename Method,","Thread should have ability to wait for Events to be processed When a thread notifies event service to process any event, it should have a way to wait for it to be finished.
{code}
LensEvent event = new LensEvent();
synchronized(event) {
   eventservice.notifyEvent(event);
   event.wait();
}
{code}
The EventHandler will do a notifyall() on the event it is done handling. 
{code}
public void run() {
      try {
        Class<? extends LensEvent> evtClass = event.getClass();
        // Call listeners directly listening for this event type
        handleEvent(eventListeners.get(evtClass), event);
        Class<?> superClass = evtClass.getSuperclass();

        // Call listeners which listen of super types of this event type
        while (LensEvent.class.isAssignableFrom(superClass)) {
          if (eventListeners.containsKey(superClass)) {
            handleEvent(eventListeners.get(superClass), event);
          }
          superClass = superClass.getSuperclass();
        }
      } finally {
        synchronized (event) {
          event.notifyAll();
        }
      }
    }
{code}",", "
"   Move Class,Extract Method,","Add data completeness checker Though lens has partition registration being done whenever data is available, there is no guarantee the partition registered is complete. There can be different ways to know if the data is complete for partition. One option could be to have a partition property saying whether it is complete or not. Other could be to do a http call to another hosted service and more.

Proposal here is to add an interface for DataCompletenessChecker and do the check while resolving partitions.

Here are some of the capabilities we would like to add in Lens :
# Lens will check partition existence first, if it exists, then check the completeness percentage. If the completeness percentage is less than a configured threshold (default should be 98, 99 or even 100), Lens will fail the query.
# Lens's accept query on partial data will accept on incomplete data as well.
# Lens will also option to override the completeness percentage threshold value at query level
# Lens will still have look ahead capability of daily being incomplete, then it will union with hourly. 
# If same measure is there in two different facts , Lens will we pick the one with higher availability.
# In case of completeness percentage threshold missed, Lens will respond back with available percentage.
","Duplicated Code, Long Method, , "
"   Move Class,Rename Method,Move Method,Extract Method,Move Attribute,","Support Fact to Fact Union Currently Lens supports Union-ing data across different storages in a single Fact. With this JIRA Lens server will be able to Union Data Across Facts too.

","Duplicated Code, Long Method, , , , "
"   Rename Method,","Add examples execution to ML Create a utility that can read the ml params, test table etc from the conf and execute it via a script",", "
"   Pull Up Method,Extract Method,","Session lifecycle events If a service is maintaining some session specific data events like session started or session closed would be useful for init/cleanup of resources.

This would be also useful in maintaining session history.","Duplicated Code, Long Method, , Duplicated Code, "
"   Rename Method,Move Method,Extract Method,","Allow start and end times for columns in fact tables We can have columns being promoted/demoted from one aggregate fact to other, with usage of columns in reports.

We need this for the following use cases:

# If field is moved to a lower cost fact: Since the fact is having lower cost it will be picked up for processing, but if the life of column queried does not fit in the range queried, a higher level fact should be picked up. If lower level fact is picked, it would lead to wrong results.

# If field is moved to a higher cost fact, i.e. its life ending in a lower cost fact. Again, depending on the range queried, either lower or higher level fact should be picked up. It will be more like optimization for this case. 
","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Allow timed dimensions of cube to be non-partitioned columns as well Here is a use case:
Say cube is having timed dimensions as it and et. If queries come on et and no storage partitioned on et, we fail the queries currently. But if there is notion of et=it plus or minus 10 and storage is partitioned on it, the timerange on et can be resolved to it partitions.
And if et is direct column along with partition mapping, et filter also needs applied along with partition lookup for it.

The requirement is to accept cube definition with such timed dimensions and resolve corresponding partitions in StorageTableResolver.
","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,Move Attribute,",Driver Specific Configuration should be set in Query Context Driver specific query conf is being merged currently at multiple places in code. Consolidating this in DriverSelectorQueryContext ,"Duplicated Code, Long Method, , , "
"   Rename Method,",Minor refactoring changes in Query Rewriter interface  QueryRewrite interface  needs to be moved into server-api and some minor method signature changes,", "
"   Move Class,Move Method,Extract Method,Move Attribute,","Add latency metering for each resolver in cube query rewriter Since cube query rewriter go through many resolvers sequentially, we want to find how much time is taken by each resolver in each phase.","Duplicated Code, Long Method, , , , "
"   Move Class,Move Method,Move Attribute,","Add latency metering metrics for all api exposed through REST To understand the latencies provided by lens server, we want to include metering for all the api.

We can explore meters in https://dropwizard.github.io/metrics/3.1.0/manual/core/#meters",", , , "
"   Rename Class,Move And Rename Class,Rename Method,","Use Algorithm instead of Trainer in all apis Currently ""Algorithm"" and ""Trainer"" is used interchangeably which could cause confusion. Rename classes and methods to use ""Algorithm"" at ALL places.",", "
"   Rename Class,Move And Rename Class,Rename Method,","Use Algorithm instead of Trainer in all apis Currently ""Algorithm"" and ""Trainer"" is used interchangeably which could cause confusion. Rename classes and methods to use ""Algorithm"" at ALL places.",", "
"   Rename Method,Extract Method,","Use cube query context for setting priority We can avoid explain call for olap queries by using cubequerycontext for setting priority.

The proposal is to extract queryplan information from cubequerycontext and use that directly","Duplicated Code, Long Method, , "
"   Extract Method,Inline Method,",Add api for adding partitions in batch This jira is for just adding the api. The performance improvements will be taken care of in a separate jira,"Duplicated Code, Long Method, , , "
"   Rename Method,",from lens cli given a dimension find out all the dimtables currently we are able to get dimension from dimtable. but opposite is not there to do. can we add this in lens cli so that given a dimension we can figure out all the dimtables belonged to that dimension.,", "
"   Rename Method,","Configurability in QueryExecutionServiceImpl * DriverSelector: right now it's hard coded to MinQueryCostSelector. Making it configurable via conf
* Query acceptors now loaded through server conf
* renamed variable acceptedQueries to queuedQueries
* test cases for acceptors
",", "
"   Rename Method,Move Method,Extract Method,",Modifying concept of 'latest' partition for dimtables 0,"Duplicated Code, Long Method, , , "
"   Rename Method,",Add cli command to print query details This command prints details as per LensQuery on cli,", "
"   Rename Method,",add jar should be able to take regex path and should be able to add multiple jars 0,", "
"   Rename Class,Rename Method,",Add cli command to show all queryable fields with description for a cube 0,", "
"   Rename Method,Move Method,Extract Method,Move Attribute,",Union support across storage tables in multi fact query We need to have support for union across storage tables of one/all  facts tables taking part in multi fact query.,"Duplicated Code, Long Method, , , , "
"   Move Class,Rename Method,Extract Method,",Add presubmit hook in UserConfigLoader and call it from drivers Right now the hadoop queue where the hive job is triggered is fixed. This needs to be dynamic. We have one requirement where our yarn cluster has hierarchical queues based on job priority. We need capability to be able to choose the right queue based on priority of the job,"Duplicated Code, Long Method, , "
"   Move Method,Extract Method,","Support single columns as expressions - more like alias Right now seeing issues with single column expressions.

It is not putting aggregate around if the column is an expression. Also not replacing alias correctly if an alias is passed.

","Duplicated Code, Long Method, , , "
"   Rename Method,","Refactoring of testQueryCommands test case There are too many test cases running in testQueryCommands test case. It is considerably time consuming to debug a failing test case within  testQueryCommands. If these test cases are modularized and broken into separate test cases, it will be much helpful in faster debugging.

{code}
@Test
  public void testQueryCommands() throws Exception {
    client = new LensClient();
    client.setConnectionParam(""lens.query.enable.persistent.resultset.indriver"", ""false"");
    setup(client);
    LensQueryCommands qCom = new LensQueryCommands();
    qCom.setClient(client);
    resDir = new File(""target/results"");
    assertTrue(resDir.exists() || resDir.mkdirs());
    testExecuteSyncQuery(qCom);
    testExecuteAsyncQuery(qCom);
    testSyncResults(qCom);
    testExplainQuery(qCom);
    testExplainFailQuery(qCom);
    testPreparedQuery(qCom);
    testShowPersistentResultSet(qCom);
    testPurgedFinishedResultSet(qCom);
    testFailPreparedQuery(qCom);
    // run all query commands with query metrics enabled.
    client = new LensClient();
    client.setConnectionParam(""lens.query.enable.persistent.resultset.indriver"", ""false"");
    client.setConnectionParam(""lens.query.enable.metrics.per.query"", ""true"");
    qCom.setClient(client);
    String result = qCom.getAllPreparedQueries(""all"", """", -1, -1);
    assertEquals(result, ""No prepared queries"");
    testExecuteSyncQuery(qCom);
    testExecuteAsyncQuery(qCom);
    testSyncResults(qCom);
    testExplainQuery(qCom);
    testExplainFailQuery(qCom);
    testPreparedQuery(qCom);
    testShowPersistentResultSet(qCom);
    testPurgedFinishedResultSet(qCom);
    testFailPreparedQuery(qCom);
  }
{code}",", "
"   Rename Method,Extract Method,","Applying Query Launching Constraints before allowing a query to be launched Lens will always accept a new query from a user and put it in a scheduling queue for processing.

Next candidate query picked up from scheduling queue for processing will be launched only if all query constraints evaluated on candidate query and launched queries allows the candidate query to be launched, otherwise the candidate query will be added to waiting queries.

When any launched query is finished, a waiting query selector will select waiting queries for rescheduling using a list of waiting queries selection policy. Every waiting query selection policy will return a list of eligible waiting queries to be rescheduled. Query Selector will calculate intersection of multiple list of eligible waiting queries and add the result of intersection to scheduling queue for reprocessing.

At initialization, Query Constraints and Waiting Query Selection Policies will be configured using configuration values.

New Query Constraints and Waiting Query Selection Policies can be added at runtime, without rebuilding and deploying lens module. Drivers should be allowed to add more Query Constraints and Waiting Queries Selection policies.

Waiting Queries will be persisted across server restarts.

Query Constraint 1: Allow a candidate query to be launched, only if, cumulative query cost of all current queries launched by the user, who submitted candidate query, is less than  a cumulative query cost ceiling for launching a new query. 

Waiting Query Selection Policy 1: Select all waiting queries of the user whose query has finished.

Query Constraint 2: Allow a candidate query to be launched, only if, current concurrent queries launched on the selected driver are less than max concurrent queries allowed on the driver. 

Waiting Query Selection Policy 2: Select all waiting queries of the driver whose query has finished.","Duplicated Code, Long Method, , "
"   Move Class,Rename Method,Move Method,","Using Duration and Fact Weight Based Query Cost Calculator for Hive Driver Along with this, we can add a new field in QueryCost to return the query cost calculated by the implementation. normalizedQueryCost could be one name for that field.",", , "
"   Move Method,Extract Method,","File name suggestions in cli when path is expected https://github.com/spring-projects/spring-shell/blob/master/src/main/java/org/springframework/shell/commands/ScriptCommands.java


path args are String for now, making them File should enable suggestions. ","Duplicated Code, Long Method, , , "
"   Rename Method,Move Method,",Update partition command in lens Our Requirement is to be able to change path of an existing partition. ,", , "
"   Rename Method,Extract Method,",Queries get purged to DB as soon as they finish Query Result Mail is not working when finished queries is set to zero as the queries are immediately going to the DB,"Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Allow column name mapping for few/all columns in underlying storage tables This improvement proposed is to accept column mapping for few or all columns in underlying storage tables - which allows column to be different in underlying storage than column in fact/dimtable

For example:
Fact1 has col1

S1_Fact1 has col1
S2_Fact2 has col1_variant

S2_Fact2 can have column mapping property specified as col1:col1_variant. If S2_Fact2 becomes the eligible storage table, then query should be written with col1_variant if col1 is queried.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Allow chain ref columns to have multiple chain destinations Right now, we allow ReferenceDimAttribute to be a chain ref column which specifies chain_name, and ref_col from the chain. This improvement proposed is to allow different chain_name, ref_col pair to be accepted.

Usecase :

We have multiple time dimension tables : say HourDim, DayDim and MonthDim.

All of them have timestamp as field which can be mapped to a time field on cube. So, there will be three different chains associated with this reference attribute and optionally the referring column can be different as well.","Duplicated Code, Long Method, , "
"   Move Method,Extract Method,","Support for !=, in and not in predicates in elastic search driver 0","Duplicated Code, Long Method, , , "
"   Rename Class,Move Class,Move Method,Extract Method,Move Attribute,","Remove accepting TableReferences for ReferenceDimAttribute We have JoinChains that can be defined for reference dim attribute. Having both TableReferences and JoinChains is confusing for user. We should remove support for table references and only allow reference dim-attribute to be defined as chained column.

This will simplify the spec to user and also remove a lot of code.","Duplicated Code, Long Method, , , , "
"   Move Class,Move And Rename Class,Rename Method,Move Method,Extract Method,","Query failure retries for transient errors There have to be retries for query failures for transient errors like network errors (Hive server not reachable/ Metastore not reachable/ DB not reachable). Retries should be available for each phase - submission, execution, updating status, fetching results and formatting.

Right now, any such failure results in marking query as failed.","Duplicated Code, Long Method, , , "
"   Move Method,Extract Method,","Option to do flattening of columns on bridge tables later With support to flatten (aggregate) columns from bridge table joins added in LENS-752, this is enhancement to apply aggregate later to applying filters or expression over the columns.

For ex :

With schema example given LENS-752, if user queries revenue with a filter :

cube select usersport.name, revenue where time_range_in(dt,x,y) and usersport.name in ('Cricket').

The query should have an option to apply filter before aggregate or not.

Similarly :
cube select (case when usersport.name='cricket' then 'CKT' when usersport.name='football' then 'FB' else 'NA' end), revenue where time_range_in(dt,x,y).

The query should have option to apply expression before aggregate or not.","Duplicated Code, Long Method, , , "
"   Rename Method,","Slow response times for /metastore/nativetables API To obtain list of native tables, CubeMetastoreService does the following -
1. Fetches the list of tables ( one MetastoreClient call)
2. Filters out the cube tables from the list. The filtering happens by looking at the table properties from the Table object. This table object is obtained with another Metastore call. So, If there are 'n' tables, there will be 'n' metastore calls.

Here is the code snippet :


  private List<String> getTablesFromDB(LensSessionHandle sessionid,
    String dbName, boolean prependDbName)
    throws MetaException, UnknownDBException, HiveSQLException, TException, LensException {
    List<String> tables = getSession(sessionid).getMetaStoreClient().getAllTables(
      dbName);
    List<String> result = new ArrayList<String>();
    if (tables != null && !tables.isEmpty()) {
      Iterator<String> it = tables.iterator();
      while (it.hasNext()) {
        String tblName = it.next();
        org.apache.hadoop.hive.metastore.api.Table tbl =
          getSession(sessionid).getMetaStoreClient().getTable(dbName, tblName);
        if (tbl.getParameters().get(MetastoreConstants.TABLE_TYPE_KEY) == null) {
          if (prependDbName) {
            result.add(dbName + ""."" + tblName);
          } else {
            result.add(tblName);
          }
        }
      }
    }
    return result;
  }


Instead of this, we can directly fetch the list of table objects for our list of table names in a single API call using getMetaStoreClient().getTableObjectsByName() method.

Currently, one of our databases contain 8000+ tables which leads to very very long response times.
   ",", "
"   Rename Method,",Expression pushdown for query optimisation in JDBC Currently Columnar rewriter skips rewrite in case of expressions used in select query. This ticket is to add the improvement in the rewriter to rewrite them and pushdown to fact subquery. ,", "
"   Rename Method,Extract Method,","queries where results of two storage tables of same fact are unioned, the rows should be aggregated 0","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,",Allow per-queue driver max launched queries constraints 0,"Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,Inline Method,","Enable streaming results for queries Users and Applications would want to not only persist but also stream the results of queries that finish fast and produce a small result set.  This JIRA is to track support for this feature

","Duplicated Code, Long Method, , , "
"   Rename Method,","Session close should not result in queued query failures In the current scenario, if the queries are queued from lens side (because of throttling), then these queries fails on session close.",", "
"   Move And Rename Class,",Add error codes for jdbc query execution exception As part of LENS-750 we have implemented hive error codes for queries with tables or columns doesn't exist. The same need to be implemented for jdbc as well. Ideally we can rename the LensHiveErrorCode to LensDriverErrorCode so that other drivers can use it.,", "
"   Rename Method,Move Attribute,",Update CLI to show streaming results This can be taken up once LENS-901 is committed.,", , "
"   Pull Up Method,Inline Method,Pull Up Attribute,",Persist Query Priority in Lens DB  As of now Query Priority is not persisted. The query details API shows query priority only until query is in memory. Once purged this info is not available .,", , Duplicated Code, Duplicated Code, "
"   Rename Method,Pull Up Method,Pull Up Attribute,","Add suport for Join chains on dimension tables Currently join chains are supported only on cubes . This needs to be supported on dimension tables as well for dimension only queries 

",", Duplicated Code, Duplicated Code, "
"   Extract Interface,Rename Method,Move Method,Extract Method,Pull Up Attribute,Move Attribute,","Add cube-segmentation for base cube With cube segmentation a cube can have multiple cubes and all these child cubes together will make the cube complete. 

CubeSegmentation and  CubeFactTable will sit together, which means it can belong to only one base cube. A base cube can have one or more cube segmentations. Fields of segmentation will be intersection of all columns of its cubes. Segmentation will have weight to compare with its buddies (facts or other segmentations). Also it can have start and end time defined or it can derive from its underline facts. 

eg: 
base_cube
  |_fact1
  |_fact2
  |_cube_segment1
     |_cube1
        |_fact_11
        |_fact_12
        ... 
      ...
  |_cube_segment2
     |_cube2
        |_fact_21
        |_fact_22
        ... 
      ...




","Duplicated Code, Long Method, , , , Large Class, Duplicated Code, "
"   Rename Class,Extract Superclass,Rename Method,","QueryContextPriorityComparator should compare the priorities instead of cost Right now, we are comparing queries on query cost and queries with same priority can change position in queue, because of cost values being different.

Instead we should compare on priority instead of cost sothat the position of query does not change within the same priority bucket.",", Duplicated Code, Large Class, "
"   Move Class,Move And Rename Class,Rename Method,Move Method,",Move hot deployment with runtime The hot deployment is now handled in the Axis2 modules which isn't right if we want to make it in common with the JBI IL implementation. Have to move that in bpel-runtime and clean it up (fix the couple of additional things needed to comply with deployment spec).,", , "
"   Rename Method,","ExtensionActivity and ExtensionAssignOperation: Support for parser and compiler BPEL's extension mechanisms are very powerful to ease data manipulation or debugging.

Aim of this task is to bring in an implementation for correctly parsing and compiling <extensionActivity>s and <extensionAssignOperation>s. Therefore the API needs to be slightly extended with a plugin mechanism for ""extension bundles"". Bundles are related to a specific extension namespace and can consist of both ExtensionActivity and ExtensionAssignOperation implementations. Such bundles can be registered to the engine using the OdeConfiguation properties files.

Parser and compiler will be extended to cope with <extensions>, <extension>, <extensionActivity> and <extensionAssignOperation> elements.

The runtime part needs some further discussion and will be addressed in an other task.",", "
"   Move Class,Extract Interface,Move Method,Move Attribute,",ExtensionActivity and ExtensionAssignOperation: Runtime support This ticket addresses the development of runtime support for pluggable extensionActivities and extensionAssignOperations.,", , , Large Class, "
"   Move Class,Move Method,",Expose Process Management API on JBI bus Expose Process Management API on JBI bus,", , "
"   Move Class,Move Method,",Expose Process Management API on JBI bus Expose Process Management API on JBI bus,", , "
"   Move Class,Push Down Method,Move Method,Extract Method,Push Down Attribute,","http:binding support Add support for http:binding in ODE.

A solution could be to extract an interface from org.apache.ode.axis2.ExternalService. The current ExternalService would become the soap:binding implementation of this new interface while this jira will provide the http:binding implementation.
The instantiation of one of these implementation would take in place in org.apache.ode.axis2.ODEServer#createExternalService. All the information required to pick the right implementation is passed to this method already.

http://www.w3.org/TR/wsdl#_http
","Duplicated Code, Long Method, , , , , "
"   Rename Method,Move Method,",Deployment using a Web Service interface Allow for remote deployment using a web service. The deployment package (basically a zip containing the directory with all necessary files) should be sent as an attachment to the message.,", , "
"   Move Class,Move Method,Move Attribute,",Deployment using a Web Service interface Allow for remote deployment using a web service. The deployment package (basically a zip containing the directory with all necessary files) should be sent as an attachment to the message.,", , , "
"   Rename Method,",Deployment using a Web Service interface Allow for remote deployment using a web service. The deployment package (basically a zip containing the directory with all necessary files) should be sent as an attachment to the message.,", "
"   Rename Method,","Axis2TestBase: centralize server management in base class The org.apache.ode.axis2.Axis2TestBase raison d'etre is to embed an ODE server for test purpose. So concrete test classes may deploy processes, invoke operations, etc.

However as of today the server management is not at the base class level. Subclasses are responsible for starting/stopping the server. And if a test case does not stop the server the next test case to be executed will fail to start the server (since the port is already used etc).
Moreover management interface is kind of fuzzy (you have to call start to start, but server.stop() to shutdown).

This fix aims at centralize the server management in the base class.
Server is started/stopped in JUnit setUp/tearDown methods. So subclasses do not have to worry about anymore.
Of course if subclasses have to perform custom setUp/tearDown steps, it's their responsability to call the super methods.",", "
"   Move Class,Rename Method,Extract Method,",support PUT and DELETE methods 0,"Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,Pull Up Method,Pull Up Attribute,Move Attribute,","EPR configuration *Abstract:*
Add support for external services configuration at runtime.
Some settings (like timeout, proxy, headers etc) may have to be defined dynamically and on a service-basis.

This could be done with a property file (per deployment unit).

*Details:*
This issue adds a mechanism to pass properties from the ProcessConf to the Integration Layer, and leverages it to configure external services.

The ProcessConf interface has a method Map<String, String> getProperties(String...).
This method is meant to expose properties to the Integration Layer. These properties have to be specific to the process configuration. It's the implementor responsability to define what these properties are, what they are used for. The String array received as argument may be used to specify filters, criteria to get only a subset of the properties.
Default Implementation

The default implementation will use the first two parameters as service name and port name.

This method is backed by a property file named integration-layer.properties. This file must be placed in the deployment unit.
",", , Duplicated Code, Duplicated Code, "
"   Rename Class,Rename Method,","EPR configuration *Abstract:*
Add support for external services configuration at runtime.
Some settings (like timeout, proxy, headers etc) may have to be defined dynamically and on a service-basis.

This could be done with a property file (per deployment unit).

*Details:*
This issue adds a mechanism to pass properties from the ProcessConf to the Integration Layer, and leverages it to configure external services.

The ProcessConf interface has a method Map<String, String> getProperties(String...).
This method is meant to expose properties to the Integration Layer. These properties have to be specific to the process configuration. It's the implementor responsability to define what these properties are, what they are used for. The String array received as argument may be used to specify filters, criteria to get only a subset of the properties.
Default Implementation

The default implementation will use the first two parameters as service name and port name.

This method is backed by a property file named integration-layer.properties. This file must be placed in the deployment unit.
",", "
"   Rename Class,Rename Method,Pull Up Method,Pull Up Attribute,Move Attribute,","EPR configuration *Abstract:*
Add support for external services configuration at runtime.
Some settings (like timeout, proxy, headers etc) may have to be defined dynamically and on a service-basis.

This could be done with a property file (per deployment unit).

*Details:*
This issue adds a mechanism to pass properties from the ProcessConf to the Integration Layer, and leverages it to configure external services.

The ProcessConf interface has a method Map<String, String> getProperties(String...).
This method is meant to expose properties to the Integration Layer. These properties have to be specific to the process configuration. It's the implementor responsability to define what these properties are, what they are used for. The String array received as argument may be used to specify filters, criteria to get only a subset of the properties.
Default Implementation

The default implementation will use the first two parameters as service name and port name.

This method is backed by a property file named integration-layer.properties. This file must be placed in the deployment unit.
",", , Duplicated Code, Duplicated Code, "
"   Rename Class,Rename Method,","EPR configuration *Abstract:*
Add support for external services configuration at runtime.
Some settings (like timeout, proxy, headers etc) may have to be defined dynamically and on a service-basis.

This could be done with a property file (per deployment unit).

*Details:*
This issue adds a mechanism to pass properties from the ProcessConf to the Integration Layer, and leverages it to configure external services.

The ProcessConf interface has a method Map<String, String> getProperties(String...).
This method is meant to expose properties to the Integration Layer. These properties have to be specific to the process configuration. It's the implementor responsability to define what these properties are, what they are used for. The String array received as argument may be used to specify filters, criteria to get only a subset of the properties.
Default Implementation

The default implementation will use the first two parameters as service name and port name.

This method is backed by a property file named integration-layer.properties. This file must be placed in the deployment unit.
",", "
"   Rename Method,",REST Connector 0,", "
"   Rename Method,",REST Connector 0,", "
"   Rename Method,","Allow BPEL Processes To Be Provided Over JMS In general, the requirements as follows:

a) Allow two or more processes to provide the same service over (the same) JMS Queue endpoint.
b) Allow two or more processes to provide the same port type (but different service) over (the same) JMS Queue endpoint.
c) Allow two or more processes to provide the same service over (the same) JMS Topic endpoint.
d) Allow two or more processes to provide the same port type (but different service) over (the same) JMS Topic endpoint.
e) Allow a process to invoke a service (or process) over a JMS endpoint.

We work with the following assumptions:
a) Operations provided over JMS Topics must be one-way (to avoid multiple responses per request)
b) Operations provided over JMS queues may be either one-way or two-way.
c) As per Axis2 protocol, non-durable or non-existent destination names will be qualified with either dynamicQueues or dynamicTopics.

The limitations in the existing code base are:
a) The name that we assign to axis services is derived from the soap:location endpoint, which is assumed to follow an HTTP scheme.
b) It is not possible to have two processes provide the same service, as that leads to naming conflicts.
c) By default, the JMS transport is not enabled in Axis2.

The proposed (verified) solution is:
a) For testing purposes, enable the JMS transport in axis2.xml. Note that by default, this will be not be turned on. The configuration of JMS in Axis2, and setup of the JMS broker is left as an exercise for the user/developer.
b) Derive service names from jms endpoints, without making the assumptions made for HTTP endpoints. Further, qualify the JMS endpoint with the bundle, diagram and  process name, so as to make it unique (this is necessary to avoid the naming conflict). To be precise, the JMS URI template is as follows:
${deploy_serverUrl}${deploy_baseSoapServicesUrl}/${deploy_bundleNcName}/${diagram_relativeURL}/${processLocalName}/${jmsDestinationName}
c) Extract the jms destination name from the service name, and set it as the value of the JMSConstants.DEST_PARAM of the axis service (this is required so that the JMS Connection Factory creates the right destination for that endpoint)
d) Store the axis service in ODEServer against the unique name as was derived above. Use that name while destroying that service as well.
e) As far as requirement (e), I believe this works out of the box.
f) As far as assumption (a), I believe this constraint should be enforced by the modeler. Also, the modeler should enforce assumption (c) for proper provisioning of processes over JMS.",", "
"   Rename Class,Extract Method,","List deployed packages and processes Provide a way, through the management API, to list which packages have been deployed, which processes are included in these packages and which package a process belongs to.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Publish/Subscribe across processes By default, a SOAP request is targeted at a specific BPEL process in ODE. At times, though, one might want to publish the request simultaneously to multiple BPEL processes, especially if the invocations are one-way.

This issue describes an implementation of such a feature in the BPEL runtime, in a way that is agnostic of the integration layer and transport bindings.

In order to facilitate message publishing, processes must have a way to subscribe to messages. While there are many ways to register subscriptions, we chose a implicit mechanism of subscription, wherein no new deployment artifacts are required.  In our approach, if two or more processes provide the same (i.e., shared) service, messages targeted at the endpoint of that service will essentially fan out to each of those (subscribing) processes.

In general, there are two paths that need to be considered:
a) Out-Of-Process invocation of the shared service: This follows the path outlined in the BpelServer.createMessageExchange() method. For shared services, we create a new kind of Brokered MEX that clones and pushes the message to each of the ""subscribing"" process.

b) In-Process invocation of the shared service: This follows the path outlined in the BpelProcess.invokePartner() method, which bypasses the MEXs and creates the MEXDAOs directly.  Again, we clone and push the message to each ""subscribing"" process.

During registration, services will now be associated with a list of processes that provide it, which could potentially be of any size. The endpoint is physically activated with the integration layer when the first process registers on it, and is physically deactivated when the last process de-registers from it. Care must be taken though, to remove any older versions of processes in the server's map. 

Also, in order to handle two-way pub-subs gracefully, we take the response from one of the processes and return that to the end-consumer. Ideally, the design-time tooling should take care to prevent pub-sub across any services whose operations are not one-way.","Duplicated Code, Long Method, , "
"   Rename Method,",support ws-security for external services 0,", "
"   Rename Method,Extract Method,",support ws-security for external services 0,"Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,",Process Instance Cleanup Add ability to automatically remove process instance data when the instance completes/terminates. ,"Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Support multiple matching correlation sets on IMAs WS-BPEL 2.0 spec specifies on how multiple matching correlation sets should be handled in section 9.2.

When multiple correlation sets are used in an IMA with initiate=""no"", a message MUST match all such correlation sets for that message to be delivered to the activity in the given process instance.

Currently, ODE supports multiple initiating correlation sets but supports only one matching correlation set. The change includes:

1. On an IMA, when multiple correlation sets are specified, all the initiating correlation sets should be created newly and the remaining correlation sets that are not bound to correlation keys should be used as a combined key to match to an instance.
2. Instead of a single CorrelationKey object, a set of CorrelationKeys should be used in serializing the key set to database and in comparing against set of correlation set values from the incoming message.","Duplicated Code, Long Method, , "
"   Rename Method,","improve rakefile to enable testing with different dbms 1. With user settings, make the database properties over-ridable.
2. Build testing environments where all dbms are tested nightly.",", "
"   Rename Method,","Support initiate=""join"" for receives with no createInstance For now, the initiate=""join"" value for correlation is only supported on receives with createInstance=""true"". Implement it in other cases as well (with the correlation comparison logic).",", "
"   Rename Method,","Improve process versioning in JBI Each time you redeploy a service assembly in servicemix, there's a new process version registered in ODE. Also an old entry (old cbp) is deleted, which causes old instances to throw 'error reloading compiled process' error. 
",", "
"   Rename Method,","Better error reporting for WS clients Currently, when a process is called and some error happens during processing, the error is not communicated back to the client. Instead, all the client gets is a timeout exception which can be confusing.

Here is an example:

2009-03-05 19:30:06,873 ERROR [org.apache.ode.axis2.ODEService] Timeout or
execution error when waiting for response to MEX {MyRoleMex#632 [Client
96038a45-1409-4bda-ab33-81fd29de4a48-3] calling
{http://www.intalio.com/bpms/workflow/ib4p_20051115}UIFWService.completeTask(...)}
java.util.concurrent.TimeoutException: Message exchange
org.apache.ode.bpel.engine.MyRoleMessageExchangeImpl$ResponseFuture@e8b20a
timed out when waiting for a response!
java.util.concurrent.TimeoutException: Message exchange
org.apache.ode.bpel.engine.MyRoleMessageExchangeImpl$ResponseFuture@e8b20a
timed out when waiting for a response!
       at
org.apache.ode.bpel.engine.MyRoleMessageExchangeImpl$ResponseFuture.get(MyRoleMessageExchangeImpl.java:241)
       at
org.apache.ode.axis2.ODEService.onAxisMessageExchange(ODEService.java:152)
       at
org.apache.ode.axis2.hooks.ODEMessageReceiver.invokeBusinessLogic(ODEMessageReceiver.java:67)
       at
org.apache.ode.axis2.hooks.ODEMessageReceiver.invokeBusinessLogic(ODEMessageReceiver.java:50)
       at
org.apache.axis2.receivers.AbstractMessageReceiver.receive(AbstractMessageReceiver.java:96)
       at org.apache.axis2.engine.AxisEngine.receive(AxisEngine.java:145)

The proposal is to report back to the client any error during processing of the request (by default). This feature could be turned off for security reasons since it may create a risk of information disclosure.",", "
"   Move And Rename Class,Rename Method,","Unpack details blob from ODE_JOB table Related discussion is here:
http://markmail.org/thread/iiy6utvsqew6nn6m",", "
"   Rename Method,Move Method,Move Attribute,","Unpack details blob from ODE_JOB table Related discussion is here:
http://markmail.org/thread/iiy6utvsqew6nn6m",", , , "
"   Move And Rename Class,Rename Method,","Unpack details blob from ODE_JOB table Related discussion is here:
http://markmail.org/thread/iiy6utvsqew6nn6m",", "
"   Rename Method,",Support setting mutliple message mappers Currently the engine supports having multiple message mappers registered however there is no way for the user to configure it to do so. Add a way for the user to specify multiple mappers.,", "
"   Rename Method,",Support setting mutliple message mappers Currently the engine supports having multiple message mappers registered however there is no way for the user to configure it to do so. Add a way for the user to specify multiple mappers.,", "
"   Extract Method,Inline Method,","De-Normalizing Large Data Currently, in the hibernate implementation of the process data access object (DAO) interface, all of the large (read blob) values is stored not in the table where it belongs, but rather in a detached table called LARGE_DATA. Examples of such dependent tables include those that hold the state of BPEL instances, BPEL events, SOAP messages, WSDL partner links, and XML variables, among other things. Inevitably, the LARGE_DATA table ends up becoming the bottleneck, because it forces us to not only execute a large number of joins but also hold that many more locks. As a result, the (hibernate) DAO layer takes longer to read/write/delete process data, and may potentially deadlock on the LARGE_DATA table. 

The obvious way out of this mess is to move the blob column from the LARGE_DATA table to the table where it is currently referenced through a foreign key. However, care must be taken to migrate the schema and data of existing servers at the time of upgrade. The upgrade path is described below, where the dependent table refers to the table that currently has a foreign key reference into the parent (i.e., LARGE_DATA) table: 

a) For each such foreign key in the dependent table, add the corresponding blob column(s) in the dependent table.
b) For each such foreign key in the dependent table, copy the blob value from the corresponding row of the parent into the corresponding column of the dependent that was added in step (a).
c) Drop the foreign keys in the dependent table that refer to the LARGE_DATA table, and the LARGE_DATA table itself. Finally, increment the version of the ODE schema (to indicate that the schema has been changed).

Needless to say, we must be prepared for scenarios wherein the server was upgraded but the schema wasn't (for whatever reason). We do so by checking the ODE schema version at the time of server startup, and failing gracefully if it doesn't match the expected value. Note that we consciously chose not to automate the upgrade path as part of the migration handler, primarily due to the long-running nature of the transaction.

As a result of this change, we observed a significant improvement in the performance of the hibernate-based process server (between 30-40%). However, individual results may vary. 

Note that the downside to moving the blob column into the dependent table is that we may inadvertently end up reading the blob property as a side-effect of an unrelated query on that table. As you may have guessed, that was the motivation for introducing the LARGE_DATA table in the first place. Fortunately, there are ways to mitigate against such cases, which include (a) using lazy fetching of the blob properties in the problematic dependent table, or (b) re-introducing a large data table specifically for the problematic dependent table, and using join fetching to work around the N+1 select problem. We plan on implementing such optimizations on a case-by-case basis, if and when required.","Duplicated Code, Long Method, , , "
"   Rename Method,",Implement process context propagation Implementation of the feature specified at http://ode.apache.org/process-contexts.html,", "
"   Move Class,Rename Method,Move Method,Extract Method,Move Attribute,","Separate configuration and runtime process representations Right now the ProcessDAO is a mix of both configuration information (deploy date, custom properties, active/retired, serialized compiled process) and runtime information (instances, correlators) which isn't so nice. It's also the source of many deployment problems as we have some duplicate information between the filesystem (the deployed process bundles) and the database.

To clean that up I'm going to introduce some sort of process configuration store (still haven't found a really impressive name for it). It would handle process deployment, maintaining these deployments over time (which involves redeployment or undeployment but could also be versioning in the future) and configuration changes. The runtime would use it to get information about processes currently deployed to activate them.

That would both introduce a cleaner separation between runtime and static process definitions and would be much more in line with the deployment specifications. It would also make clustering easier as configuration could be easily propagated. ","Duplicated Code, Long Method, , , , "
"   Push Down Method,Move Method,Extract Method,Push Down Attribute,Move Attribute,",Implement OSGi packaging for ODE Implement OSGi packaging for ODE. Make sure ODE is consumable as a set of bundles.,"Duplicated Code, Long Method, , , , , , "
"   Rename Method,","Speed up listAllProcesses listAllProcesses in ProcessAndInstanceManagementImpl.java does a lot of queries to database in order to fetch instance summary. This causes transaction timeout. on ODE load.

Disabling instance summary is a workaround:

diff --git a/bpel-runtime/src/main/java/org/apache/ode/bpel/engine/ProcessAndInstanceManagementImpl.java b/bpel-runtime/src/main/java/org/apache/ode/bpel/engine/ProcessAndInstanceManagementImpl.java
index 47ada7a..0317864 100644
--- a/bpel-runtime/src/main/java/org/apache/ode/bpel/engine/ProcessAndInstanceManagementImpl.java
+++ b/bpel-runtime/src/main/java/org/apache/ode/bpel/engine/ProcessAndInstanceManagementImpl.java
@@ -805,6 +805,7 @@ public class ProcessAndInstanceManagementImpl implements InstanceManagement, Pro
         depinfo.setDocument(pconf.getBpelDocument());
         depinfo.setDeployDate(toCalendar(pconf.getDeployDate()));
         depinfo.setDeployer(pconf.getDeployer());
+ /*
         if (custom.includeInstanceSummary()) {
             TInstanceSummary isum = info.addNewInstanceSummary();
             genInstanceSummaryEntry(conn, isum.addNewInstances(), TInstanceStatus.ACTIVE, pconf);
@@ -814,7 +815,7 @@ public class ProcessAndInstanceManagementImpl implements InstanceManagement, Pro
             genInstanceSummaryEntry(conn, isum.addNewInstances(), TInstanceStatus.SUSPENDED, pconf);
             genInstanceSummaryEntry(conn, isum.addNewInstances(), TInstanceStatus.TERMINATED, pconf);
             getInstanceSummaryActivityFailure(conn, isum, pconf);
- }
+ }*/
 
         if (custom.includeDocumentLists()) {
             TProcessInfo.Documents docinfo = info.addNewDocuments(); ",", "
"   Move Method,Move Attribute,","document() function in XSL scripts can not resolve remote documents The document() function is used in XSL script (via doXSLTransform) to load external documents. The current implementation is only considering files in the local file system. In addition it should be able also load resources from HTTP/HTTPS endpoints.

I suggest to convert the URI to an URL and use URL connections to fetch the contents. This covers file:, http:, https and some other internet protocols.",", , , "
"   Push Down Method,Extract Method,",Fix circular dependencies in jacob JacobObject which is root abstraction for the pi calculus engine depends on JacobVPU which is the engine implementation. Need to clean up the abstractions a little bit.,"Duplicated Code, Long Method, , , "
"   Rename Method,",Fix circular dependencies in jacob JacobObject which is root abstraction for the pi calculus engine depends on JacobVPU which is the engine implementation. Need to clean up the abstractions a little bit.,", "
"   Rename Method,","Unit tests for PDFBox features We're upgrading the pdfbox we use and to ensure there aren't any regressions, while also learning pdfbox, we are unit testing some of the classes, especially the PDF ""primitive"" objects (COS level).",", "
"   Rename Method,",Pattern colorspace support PDFBox doesn't support PDPattern colorspaces,", "
"   Rename Class,Rename Method,Move Method,Extract Method,",Pattern colorspace support PDFBox doesn't support PDPattern colorspaces,"Duplicated Code, Long Method, , , "
"   Rename Method,",Pattern colorspace support PDFBox doesn't support PDPattern colorspaces,", "
"   Move And Rename Class,Pull Up Method,",Pattern colorspace support PDFBox doesn't support PDPattern colorspaces,", Duplicated Code, "
"   Rename Method,","Color conversion for PDJpegs using a DeviceN colorspace PDFBOX-1116 and PDFBOX-1154 already added some color conversions for PDJpegs and PDPixelMaps, except one for handling DeviceN colorspaces.",", "
"   Rename Method,","Speed up LZWFilter decoding I noticed that the LZW decoder performance can be improved: it's
allocating a new byte[] for every byte it visits in the stream.  This
is actually an O(N^2) cost, but N is typically fairly small.

I changed LZWDictionary to use its own private growable byte[] to
accumulate each added byte.  I also changed it to not pre-enroll all
initial (0-255) codes, but instead add it (lazily) on demand if the
code is used.

I also randomized the TestFilters test, and mixed in some
""more predictable"" patterns, so we get better testing of the filters.
If the test fails it prints the seed used for the random numbers, so
we can reproduce the failure.
",", "
"   Move Method,Extract Method,Move Attribute,","Add ""Save as image"" to PDFReader Extend the PDFReader with a ""Save as image"" function so that it'd be easy to save the current page as png","Duplicated Code, Long Method, , , , "
"   Rename Method,","Refactor the PdfA parser To fix the PDFBox-1274 issue, the  validation of PDF/A needs a refactoring.
Currently, each XRef entry is checked independently. 
Most of the time, this is enough because the required information to validate the object are present in the object.

For the issue PDFBox-1274,  the object validation should access to the page that uses the object.

After the refactoring the valdiation unit will be the PDPage.

",", "
"   Move Class,Rename Method,Move Method,Extract Method,Move Attribute,","Refactor the PdfA parser To fix the PDFBox-1274 issue, the  validation of PDF/A needs a refactoring.
Currently, each XRef entry is checked independently. 
Most of the time, this is enough because the required information to validate the object are present in the object.

For the issue PDFBox-1274,  the object validation should access to the page that uses the object.

After the refactoring the valdiation unit will be the PDPage.

","Duplicated Code, Long Method, , , , "
"   Rename Method,","Refactor the PdfA parser To fix the PDFBox-1274 issue, the  validation of PDF/A needs a refactoring.
Currently, each XRef entry is checked independently. 
Most of the time, this is enough because the required information to validate the object are present in the object.

For the issue PDFBox-1274,  the object validation should access to the page that uses the object.

After the refactoring the valdiation unit will be the PDPage.

",", "
"   Rename Method,Inline Method,",Update PDPage to enum 0,", , "
"   Rename Method,Extract Method,","xmpbox refactoring Today, some xmp schema and some objects are missing.
The interfaces are close to jempbox but not really similar.
Convenience method for creation of schema are missing.

The aim of this issue is to :
* make interface similar
* create missing object and methods
* clean useless code
","Duplicated Code, Long Method, , "
"   Move Method,Extract Method,Move Attribute,","Support lucene 3.6.0 Fedora rawhide has updated to lucene 3.6.0.  When trying to compile pdfbox against it I get:

[ERROR] /builddir/build/BUILD/pdfbox-1.7.0/lucene/src/main/java/org/apache/pdfbox/lucene/IndexFiles.java:[27,29] error: cannot find symbol
[ERROR]   symbol:   class HTMLDocument
  location: package org.apache.lucene.demo

Looks like the demo package in lucene has change quite a lot, including changing the artifaceId to lucene-demo:

diff -up pdfbox-1.7.0/lucene/pom.xml.lucene pdfbox-1.7.0/lucene/pom.xml
--- pdfbox-1.7.0/lucene/pom.xml.lucene  2012-07-05 09:16:35.056582368 -0600
+++ pdfbox-1.7.0/lucene/pom.xml 2012-07-05 09:45:08.069655661 -0600
@@ -47,7 +47,7 @@
     </dependency>
     <dependency>
       <groupId>org.apache.lucene</groupId>
-      <artifactId>lucene-demos</artifactId>
+      <artifactId>lucene-demo</artifactId>
       <version>${lucene.version}</version>
     </dependency>
   </dependencies>
","Duplicated Code, Long Method, , , , "
"   Rename Method,","Improve handling of multiline text boxes The current implementation for setting the appearance of content that is added to a multiline text box is incorrect in a number of ways:
* Doesn't position the start of the text in the correct location
* Incorrectly uses font size '0' instead of auto-sizing the font
* Doesn't break up very long lines
* If the font size is very large, then the next line is started too close to the previous line.",", "
"   Rename Method,","Improve handling of multiline text boxes The current implementation for setting the appearance of content that is added to a multiline text box is incorrect in a number of ways:
* Doesn't position the start of the text in the correct location
* Incorrectly uses font size '0' instead of auto-sizing the font
* Doesn't break up very long lines
* If the font size is very large, then the next line is started too close to the previous line.",", "
"   Move Method,Extract Method,Move Attribute,","Improve handling of multiline text boxes The current implementation for setting the appearance of content that is added to a multiline text box is incorrect in a number of ways:
* Doesn't position the start of the text in the correct location
* Incorrectly uses font size '0' instead of auto-sizing the font
* Doesn't break up very long lines
* If the font size is very large, then the next line is started too close to the previous line.","Duplicated Code, Long Method, , , , "
"   Rename Method,Extract Method,","PDF signature improvements Hallo,
i have some signing improvements and want to contribute it.

Changelog:
- add ability to sign documents with xref streams (big thanks to A. Funk). This will significantly improve the signature creation, due to the fact that there are more and more documents with xref streams out there.
- add ability to handle documents with hybrid xref (xref stream and table as fallback)
- parsing incremental updated documents where the offsets of the xref entries doesn't match exactly the position of the object. The conflict solver tollerate objects that are +- 4 bytes out of the offset. 
- fix COSString parsing if there are malformed chars inside a hex string.
- removed some confusing logging in the conflict solver
- add ability to create and sign signature fields.
- add ability to create pades signature timestamps (PAdES Part4)
- improved the signature search and added new convenience methods to PDDocument.
- add new methods to the PDSignature object (seed value dict) 
- add examples for signing pdf documents (just basic signatures, i try to add some for advanced signing with SignatureOptions and maybe a example for visual signing)

hope i don't forgot something. 

All the changes are made on a pdfbox fork on github. A pull request will follow in a moment.

Best regards
Thomas","Duplicated Code, Long Method, , "
"   Move Class,Move Method,Extract Method,Move Attribute,","OverlayPDF logic should be moved into a library class We're finding that the OverlayPDF command line utility fixes problems observed when using the Overlay class. However, with the Overlay class we can operate it as a library call, but the OverlayPDF command line utility can't be, particularly given we work with PDDocuments.

I've just had someone copy the entire OverlayPDF class into one of our own and modify it to accept pre-loaded PDDocuments instead of filenames. It would be far more constructive to extract the core logic into a library class and have the CLI class drive it.

If we had a proper clone method it would be good to create a completely new PDDocument to serve as the new returned document instead of mutating the inputs too...
","Duplicated Code, Long Method, , , , "
"   Rename Method,","Replace external glyphlist.txt with our onw implementation According to the header of glyphlist.txt Adobe encourages people to use the content of the file to create their own implementation of the glyphlist:

# Permission is hereby granted, free of charge, to any person obtaining a
# copy of this documentation file, to create their own derivative works
# from the content of this document to use, copy, publish, distribute,
# sublicense, and/or sell the derivative works, and to permit others to do
# the same, provided that the derived work is not represented as being a
# copy or version of this document.

To get rid of the external dependency we should follow that advise and create our own class providing the information of the glyphlist.txt
",", "
"   Rename Method,","Improve pdfbox tests I'd like to improve the tests for rendering.

org/apache/pdfbox/util/TestPDFToImage.java is disabled in pdfbox\pom.xml . This has been disabled since 2009 ?! So I enabled it here.

The subdir ""rendering"" is missing in pdfbox\target\test-output for these tests

When a test fails because the rendered image is not identical, no detailed message appears on the console. It appears only in pdfbox.log and not on the console.

this is because of the settings in
pdfbox\src\test\resources\logging.properties

If this is on purpose, please change the texts in pdfbox\src\test\java\org\apache\pdfbox\util\*.java from

""One or more failures, see test log for details""

to

""One or more failures, see test logfile 'pdfbox.log' for details""

I wanted to attach a PDF with ccitt g4 compression and its rendering created with the 1.8.2 version, but it doesn't work out, seems that CIB generates files that can be rendered properly with 1.8.2. However I attach the TIFF g4 file, and a JBIG2 test file from it. I don't have access to a Xerox WorkCentre (enter jbig2 in google news :-) ) so I used a free service, so there's a watermark.

It should be included into
pdfbox\src\test\resources\input\rendering
I have created the image myself and I give it into the public domain.

If my suggestion is accepted, it would be nice if people could create files that fail in current versions or have failed in old versions, and release these files to the public domain, so that they can be added to the tests.",", "
"   Move Method,Extract Method,Move Attribute,",Remove the print and the convertToImage stuff from PDPage and PDDocument  Move the printing/convertToImage stuff from PDPage and PDDocument to the PDPageable class to remove/reduce all the AWT-stuff from the PDFBox-core classes.,"Duplicated Code, Long Method, , , , "
"   Rename Method,","[PATCH] Visible Signature using PDFbox In order to sign document with visible signature we have very bad solution at the moment:  passing a PDF as an InputStream that serves as a template for the appearance settings is very inconvenient. So, Now Everything is well and fixed! You only set image with location, zoom, width, height, and etc, and everything will be added automatically. I've just already done this, and I will upload my patches too. I have wrote example too, in order to see how to use it. Everything is easy!",", "
"   Rename Class,Extract Interface,Rename Method,Extract Method,","[PATCH] Parser for Type 1 Fonts This patch adds a parser for Type 1 fonts to FontBox and makes use of it in PDFBox for rendering Type 1 glyphs. This should fix various issues with the JVM crashing and rendering fonts incorrectly.

It was necessary to modify Type1CharStringParser to handle the `callothersubr` command and correctly handle subroutines. Likewise, Type1CharString was modified to support ""flex"".

This patch does not remove the AWT fallback for non-embedded and standard 14 fonts because an entirely new fallback system is needed and suitable fonts will need to be shipped as part of PDFBox. This needs to be discussed on the mailing list and/or in follow-on issue.

Note: To keep this patch small I have not replaced any of the existing ad-hoc Type 1 parsing code in PDType1Font and preflight. Those classes retain their original code which can be replaced in subsequent patches/refactoring. I can open follow-on issues for these.

~~~

As well as the patch, the these files were added:
+ /pdfbox/src/main/java/org/apache/pdfbox/pdfviewer/font/Type1Glyph2D.java
+ /fontbox/src/main/java/org/apache/fontbox/encoding/CustomEncoding.java
+ /fontbox/src/main/java/org/apache/fontbox/type1/Token.java
+ /fontbox/src/main/java/org/apache/fontbox/type1/Type1CharStringReader.java
+ /fontbox/src/main/java/org/apache/fontbox/type1/Type1Font.java
+ /fontbox/src/main/java/org/apache/fontbox/type1/Type1Lexer.java
+ /fontbox/src/main/java/org/apache/fontbox/type1/Type1Mapping.java
+ /fontbox/src/main/java/org/apache/fontbox/type1/Type1Parser.java

And this file was removed:
- /pdfbox/src/main/java/org/apache/pdfbox/pdfviewer/font/CFFGlyph2D.java
","Duplicated Code, Long Method, , Large Class, "
"   Move Method,Inline Method,","Refactor color spaces I'm currently working on this, so I wanted to open an issue to let everyone know.

Color spaces need to be refactored in 2.0.0. Tilman noticed slowness in PDFBOX-1851 due to using ICC profiles and calling ColorSpace#toRGB for every pixel. For example, the file from PDFBOX-1851 went from rendering in 4 seconds to taking over 60 seconds.

The solution is to use ColorConvertOp to convert an entire BufferedImage in one go, taking advantage of AWT's native color management module. Color conversions done this way are almost instantaneous, even for large images.

The current design of color spaces within PDFBox depends upon conversions being done on a per-pixel basis, so a significant refactoring is needed in order to convert images using ColorConvertOp without having to resort to per-pixel calls in cases such as a Separation color space which uses a CMYK alternate color space via a tint-transform.

The color space handling code is also tightly coupled to image handling. The various classes which read images each have their own color handling code which rely on per-pixel conversions. For this reason any color space refactoring must also included a significant refactoring of image handling code. This is an opportunity to refactor all color handling so that it is encapsulated within the color space classes, allowing downstream users to call toRGB(float[]) or toRGB(BufferedImage) and not need to worry about tint transforms and the like.

===========

Here's a summary of the changes:

- PDCcitt has been removed, its reading capability has moved to CCITTFaxFilter and writing capability has moved to CCITTFactory.

- PDJpeg has been removed. JPEG reading is now done by new code in DCTFilter which correctly handles CMYK/YCCK color. This fixes various files where images appeared like negatives. JPEG writing is done by new code in JPEGFactory.

- cleaned up JBIG2Filter

- cleaned up JPXFilter, in particular calling decode() caused the stream dictionary to be updated, which was unsafe. I've also added a special JPXColorSpace which wraps the embedded AWT color space of a JPX BufferedImage, this replaces the need for the awkward mapping of ColorSpace to PDColorSpace.

- Added better error messages for missing JAI plugins (JPX, JBIG2). A special exception, MissingImageReaderException is now thrown.

- PDXObjectForm has been renamed to PDFormXObject to match the PDF spec.
- PDXObjectImage has been renamed in the same manner.
- PDInlinedImage has been renamed to PDInlineImage for the same reason.
- CCITTFaxDecodeFilter has been renamed to CCITTFaxFilter for consistency with the other filters.

- ImageParameters has been removed, it was used to represent inline image parameters which are now simply members of PDInlineImage.

- added PDColor which represents a color value, including patterns, it is immutable for ease of use.

- removed PDColorState which was a container for both a color and a color space, in almost every case it was used to represent a color and so has been replaced by PDColor and occasionally PDColorSpace.

- moved most of the functionality of PDXObject into its subclasses

- rewrote almost all color handling code in all PDColorSpace subclasses, including fixing the calculations for l*a*b, DeviceN, and indexed color spaces. 

- all color spaces now implement a toRGB(float[]) function for color conversion, so external consumers of color spaces no longer have to know about internals such as tint transforms.

- image color conversion is now performed in one operation, using ColorConvertOp, rather than pixel-by-pixel, this speeds up ICC transforms by many orders of magnitude. Color spaces now expose a special method toImageRGB(Raster) for this purpose. This fixes some known performance issues with certain files.

- updated Type1, Axial, Radial, and Gouraud shading contexts to call the new toRGB functions. This is an interim measure, for better performance the color conversion should instead be done using toImageRGB after the entire gradient is drawn to the raster.

- creation of AWT Paint has been moved inside color spaces, hiding the details from the caller. It is no longer possible to get an AWT Color from a color space, only a Paint may be obtained.

- removed PDColorSpaceFactory and moved its functionality into PDColorSpace.

- moved some of the new shading and tiling pattern code to PDPattern so that toPaint() is encapsulated in the color space.

- new PDImage interface which is implemented by both PDInlineImage and PDImageXObject

- Image XObject image reading, masking  and stencilling code has been rewritten, resulting in the removal of CompositeImage.

- new SampledImageReader performs image reading for all formats, including JPEG and CCITT. The format itself is simply a filter, as is the case in the PDF spec. New image reading handles decode arrays, interpolation, and conversion of all image types to efficient 8bpp rasters. This replaces PDPixelMap as well as reading code from PDJpeg and PDCcitt. Handling of decod arrays fixes various issues where images were inverted, especially inline images in Type 3 fonts.

- removed SetNonStrokingICCBasedColor, SetNonStrokingIndexed, SetNonStrokingPattern, SetNonStrokingSeparation, SetStrokingICCBasedColor, SetStrokingIndexed, SetStrokingPattern, SetStrokingSeparation, and replaced them with SetColor.
",", , , "
"   Rename Method,","Refactor color spaces I'm currently working on this, so I wanted to open an issue to let everyone know.

Color spaces need to be refactored in 2.0.0. Tilman noticed slowness in PDFBOX-1851 due to using ICC profiles and calling ColorSpace#toRGB for every pixel. For example, the file from PDFBOX-1851 went from rendering in 4 seconds to taking over 60 seconds.

The solution is to use ColorConvertOp to convert an entire BufferedImage in one go, taking advantage of AWT's native color management module. Color conversions done this way are almost instantaneous, even for large images.

The current design of color spaces within PDFBox depends upon conversions being done on a per-pixel basis, so a significant refactoring is needed in order to convert images using ColorConvertOp without having to resort to per-pixel calls in cases such as a Separation color space which uses a CMYK alternate color space via a tint-transform.

The color space handling code is also tightly coupled to image handling. The various classes which read images each have their own color handling code which rely on per-pixel conversions. For this reason any color space refactoring must also included a significant refactoring of image handling code. This is an opportunity to refactor all color handling so that it is encapsulated within the color space classes, allowing downstream users to call toRGB(float[]) or toRGB(BufferedImage) and not need to worry about tint transforms and the like.

===========

Here's a summary of the changes:

- PDCcitt has been removed, its reading capability has moved to CCITTFaxFilter and writing capability has moved to CCITTFactory.

- PDJpeg has been removed. JPEG reading is now done by new code in DCTFilter which correctly handles CMYK/YCCK color. This fixes various files where images appeared like negatives. JPEG writing is done by new code in JPEGFactory.

- cleaned up JBIG2Filter

- cleaned up JPXFilter, in particular calling decode() caused the stream dictionary to be updated, which was unsafe. I've also added a special JPXColorSpace which wraps the embedded AWT color space of a JPX BufferedImage, this replaces the need for the awkward mapping of ColorSpace to PDColorSpace.

- Added better error messages for missing JAI plugins (JPX, JBIG2). A special exception, MissingImageReaderException is now thrown.

- PDXObjectForm has been renamed to PDFormXObject to match the PDF spec.
- PDXObjectImage has been renamed in the same manner.
- PDInlinedImage has been renamed to PDInlineImage for the same reason.
- CCITTFaxDecodeFilter has been renamed to CCITTFaxFilter for consistency with the other filters.

- ImageParameters has been removed, it was used to represent inline image parameters which are now simply members of PDInlineImage.

- added PDColor which represents a color value, including patterns, it is immutable for ease of use.

- removed PDColorState which was a container for both a color and a color space, in almost every case it was used to represent a color and so has been replaced by PDColor and occasionally PDColorSpace.

- moved most of the functionality of PDXObject into its subclasses

- rewrote almost all color handling code in all PDColorSpace subclasses, including fixing the calculations for l*a*b, DeviceN, and indexed color spaces. 

- all color spaces now implement a toRGB(float[]) function for color conversion, so external consumers of color spaces no longer have to know about internals such as tint transforms.

- image color conversion is now performed in one operation, using ColorConvertOp, rather than pixel-by-pixel, this speeds up ICC transforms by many orders of magnitude. Color spaces now expose a special method toImageRGB(Raster) for this purpose. This fixes some known performance issues with certain files.

- updated Type1, Axial, Radial, and Gouraud shading contexts to call the new toRGB functions. This is an interim measure, for better performance the color conversion should instead be done using toImageRGB after the entire gradient is drawn to the raster.

- creation of AWT Paint has been moved inside color spaces, hiding the details from the caller. It is no longer possible to get an AWT Color from a color space, only a Paint may be obtained.

- removed PDColorSpaceFactory and moved its functionality into PDColorSpace.

- moved some of the new shading and tiling pattern code to PDPattern so that toPaint() is encapsulated in the color space.

- new PDImage interface which is implemented by both PDInlineImage and PDImageXObject

- Image XObject image reading, masking  and stencilling code has been rewritten, resulting in the removal of CompositeImage.

- new SampledImageReader performs image reading for all formats, including JPEG and CCITT. The format itself is simply a filter, as is the case in the PDF spec. New image reading handles decode arrays, interpolation, and conversion of all image types to efficient 8bpp rasters. This replaces PDPixelMap as well as reading code from PDJpeg and PDCcitt. Handling of decod arrays fixes various issues where images were inverted, especially inline images in Type 3 fonts.

- removed SetNonStrokingICCBasedColor, SetNonStrokingIndexed, SetNonStrokingPattern, SetNonStrokingSeparation, SetStrokingICCBasedColor, SetStrokingIndexed, SetStrokingPattern, SetStrokingSeparation, and replaced them with SetColor.
",", "
"   Pull Up Method,Pull Up Attribute,","Shading package: Move ""function"" methods to base class and more refactoring I'm planning to move the ""function"" methods to the PDShadingResources class. Reasons: 1) duplicate code (partly introduced by myself), 2) this will allow me to handle functions in type 4 & 5 shading. Currently I can't, because these shading context classes use a common class (GouraudShadingContext), but the PDShadingType 4 and 5 have their function methods.",", Duplicated Code, Duplicated Code, "
"   Extract Superclass,Pull Up Attribute,","Implement shading with Coons and tensor-product patch meshes Of the seven shading methods described in the PDF specification, type 6 (Coons patch meshes) and type 7 (Tensor-product patch meshes) haven't been implemented. I have done type 1, 4 and 5, but I don't know the math for type 6 and 7. My math days are decades away.

Knowledge prerequisites: 
- java, although you don't have to be a java ace, just feel confortable
- math: you should know what ""cubic BÃ©zier curves"", ""Degenerate BÃ©zier curves"", ""bilinear interpolation"", ""tensor-product"", ""affine transform matrix"" and ""Bernstein polynomials"" are, or be able to learn it
- maven (basic)
- svn (basic)
- an IDE like Netbeans or Eclipse or IntelliJ (basic)
- ideally, you are either a math student who likes to program, or a computer science student who is specializing in graphics.

A first look at PDFBOX: try the command utility here:
https://pdfbox.apache.org/commandline/#pdfToImage
and use your favorite PDF, or the PDFs mentioned in PDFBOX-615, these have the shading types that are already implemented.

Some simple source code to convert to images:

String filename = ""blah.pdf"";
PDDocument document = PDDocument.loadNonSeq(new File(filename), null);
List<PDPage> pdPages = document.getDocumentCatalog().getAllPages();
int page = 0;
for (PDPage pdPage : pdPages)
{
++page;
BufferedImage bim = RenderUtil.convertToImage(pdPage, BufferedImage.TYPE_BYTE_BINARY, 300);
ImageIO.write(bim, ""png"", new File(filename+page+"".png""));
}
document.close();

You are not starting from scratch. The implementation of type 4 and 5 shows you how to read parameters from the PDF and set the graphics. You don't have to learn the complete PDF spec, only 15 pages related to the two shading types, and 6 pages about shading in general. The PDF specification is here:
http://www.adobe.com/devnet/pdf/pdf_reference.html

The tricky parts are:
- decide whether a point(x,y) is inside or outside a patch
- decide the color of a point within the patch

To get an idea about the code, look at the classes GouraudTriangle, GouraudShadingContext, Type4ShadingContext and Vertex here
https://svn.apache.org/viewvc/pdfbox/trunk/pdfbox/src/main/java/org/apache/pdfbox/pdmodel/graphics/shading/
or download the whole project from the repository.
https://pdfbox.apache.org/downloads.html#scm
If you want to see the existing code in the debugger with a Gouraud shading, try this file:
http://asymptote.sourceforge.net/gallery/Gouraud.pdf

Testing:
I have attached several example PDFs. To see which one has which shading, open them with an editor like NOTEPAD++, and search for ""/ShadingType"" (without the quotes). If your images are rendering like the example PDFs, then you were successful.

Optional:
Review and optimize the complete shading package for speed; implement cubic spline interpolation for type 0 (sampled) functions (that one is really low-low priority, see details by looking up ""cubic spline interpolation"" in the PDF spec, which tells that it is disregarded in printing, and I don't have a test PDF).

Mentor: Tilman Hausherr (European timezone, languages: german, english, french)
",", Duplicated Code, Large Class, Duplicated Code, "
"   Rename Class,Rename Method,","Implement shading with Coons and tensor-product patch meshes Of the seven shading methods described in the PDF specification, type 6 (Coons patch meshes) and type 7 (Tensor-product patch meshes) haven't been implemented. I have done type 1, 4 and 5, but I don't know the math for type 6 and 7. My math days are decades away.

Knowledge prerequisites: 
- java, although you don't have to be a java ace, just feel confortable
- math: you should know what ""cubic BÃ©zier curves"", ""Degenerate BÃ©zier curves"", ""bilinear interpolation"", ""tensor-product"", ""affine transform matrix"" and ""Bernstein polynomials"" are, or be able to learn it
- maven (basic)
- svn (basic)
- an IDE like Netbeans or Eclipse or IntelliJ (basic)
- ideally, you are either a math student who likes to program, or a computer science student who is specializing in graphics.

A first look at PDFBOX: try the command utility here:
https://pdfbox.apache.org/commandline/#pdfToImage
and use your favorite PDF, or the PDFs mentioned in PDFBOX-615, these have the shading types that are already implemented.

Some simple source code to convert to images:

String filename = ""blah.pdf"";
PDDocument document = PDDocument.loadNonSeq(new File(filename), null);
List<PDPage> pdPages = document.getDocumentCatalog().getAllPages();
int page = 0;
for (PDPage pdPage : pdPages)
{
++page;
BufferedImage bim = RenderUtil.convertToImage(pdPage, BufferedImage.TYPE_BYTE_BINARY, 300);
ImageIO.write(bim, ""png"", new File(filename+page+"".png""));
}
document.close();

You are not starting from scratch. The implementation of type 4 and 5 shows you how to read parameters from the PDF and set the graphics. You don't have to learn the complete PDF spec, only 15 pages related to the two shading types, and 6 pages about shading in general. The PDF specification is here:
http://www.adobe.com/devnet/pdf/pdf_reference.html

The tricky parts are:
- decide whether a point(x,y) is inside or outside a patch
- decide the color of a point within the patch

To get an idea about the code, look at the classes GouraudTriangle, GouraudShadingContext, Type4ShadingContext and Vertex here
https://svn.apache.org/viewvc/pdfbox/trunk/pdfbox/src/main/java/org/apache/pdfbox/pdmodel/graphics/shading/
or download the whole project from the repository.
https://pdfbox.apache.org/downloads.html#scm
If you want to see the existing code in the debugger with a Gouraud shading, try this file:
http://asymptote.sourceforge.net/gallery/Gouraud.pdf

Testing:
I have attached several example PDFs. To see which one has which shading, open them with an editor like NOTEPAD++, and search for ""/ShadingType"" (without the quotes). If your images are rendering like the example PDFs, then you were successful.

Optional:
Review and optimize the complete shading package for speed; implement cubic spline interpolation for type 0 (sampled) functions (that one is really low-low priority, see details by looking up ""cubic spline interpolation"" in the PDF spec, which tells that it is disregarded in printing, and I don't have a test PDF).

Mentor: Tilman Hausherr (European timezone, languages: german, english, french)
",", "
"   Rename Method,","Implement shading with Coons and tensor-product patch meshes Of the seven shading methods described in the PDF specification, type 6 (Coons patch meshes) and type 7 (Tensor-product patch meshes) haven't been implemented. I have done type 1, 4 and 5, but I don't know the math for type 6 and 7. My math days are decades away.

Knowledge prerequisites: 
- java, although you don't have to be a java ace, just feel confortable
- math: you should know what ""cubic BÃ©zier curves"", ""Degenerate BÃ©zier curves"", ""bilinear interpolation"", ""tensor-product"", ""affine transform matrix"" and ""Bernstein polynomials"" are, or be able to learn it
- maven (basic)
- svn (basic)
- an IDE like Netbeans or Eclipse or IntelliJ (basic)
- ideally, you are either a math student who likes to program, or a computer science student who is specializing in graphics.

A first look at PDFBOX: try the command utility here:
https://pdfbox.apache.org/commandline/#pdfToImage
and use your favorite PDF, or the PDFs mentioned in PDFBOX-615, these have the shading types that are already implemented.

Some simple source code to convert to images:

String filename = ""blah.pdf"";
PDDocument document = PDDocument.loadNonSeq(new File(filename), null);
List<PDPage> pdPages = document.getDocumentCatalog().getAllPages();
int page = 0;
for (PDPage pdPage : pdPages)
{
++page;
BufferedImage bim = RenderUtil.convertToImage(pdPage, BufferedImage.TYPE_BYTE_BINARY, 300);
ImageIO.write(bim, ""png"", new File(filename+page+"".png""));
}
document.close();

You are not starting from scratch. The implementation of type 4 and 5 shows you how to read parameters from the PDF and set the graphics. You don't have to learn the complete PDF spec, only 15 pages related to the two shading types, and 6 pages about shading in general. The PDF specification is here:
http://www.adobe.com/devnet/pdf/pdf_reference.html

The tricky parts are:
- decide whether a point(x,y) is inside or outside a patch
- decide the color of a point within the patch

To get an idea about the code, look at the classes GouraudTriangle, GouraudShadingContext, Type4ShadingContext and Vertex here
https://svn.apache.org/viewvc/pdfbox/trunk/pdfbox/src/main/java/org/apache/pdfbox/pdmodel/graphics/shading/
or download the whole project from the repository.
https://pdfbox.apache.org/downloads.html#scm
If you want to see the existing code in the debugger with a Gouraud shading, try this file:
http://asymptote.sourceforge.net/gallery/Gouraud.pdf

Testing:
I have attached several example PDFs. To see which one has which shading, open them with an editor like NOTEPAD++, and search for ""/ShadingType"" (without the quotes). If your images are rendering like the example PDFs, then you were successful.

Optional:
Review and optimize the complete shading package for speed; implement cubic spline interpolation for type 0 (sampled) functions (that one is really low-low priority, see details by looking up ""cubic spline interpolation"" in the PDF spec, which tells that it is disregarded in printing, and I don't have a test PDF).

Mentor: Tilman Hausherr (European timezone, languages: german, english, french)
",", "
"   Rename Method,Inline Method,","Implement shading with Coons and tensor-product patch meshes Of the seven shading methods described in the PDF specification, type 6 (Coons patch meshes) and type 7 (Tensor-product patch meshes) haven't been implemented. I have done type 1, 4 and 5, but I don't know the math for type 6 and 7. My math days are decades away.

Knowledge prerequisites: 
- java, although you don't have to be a java ace, just feel confortable
- math: you should know what ""cubic BÃ©zier curves"", ""Degenerate BÃ©zier curves"", ""bilinear interpolation"", ""tensor-product"", ""affine transform matrix"" and ""Bernstein polynomials"" are, or be able to learn it
- maven (basic)
- svn (basic)
- an IDE like Netbeans or Eclipse or IntelliJ (basic)
- ideally, you are either a math student who likes to program, or a computer science student who is specializing in graphics.

A first look at PDFBOX: try the command utility here:
https://pdfbox.apache.org/commandline/#pdfToImage
and use your favorite PDF, or the PDFs mentioned in PDFBOX-615, these have the shading types that are already implemented.

Some simple source code to convert to images:

String filename = ""blah.pdf"";
PDDocument document = PDDocument.loadNonSeq(new File(filename), null);
List<PDPage> pdPages = document.getDocumentCatalog().getAllPages();
int page = 0;
for (PDPage pdPage : pdPages)
{
++page;
BufferedImage bim = RenderUtil.convertToImage(pdPage, BufferedImage.TYPE_BYTE_BINARY, 300);
ImageIO.write(bim, ""png"", new File(filename+page+"".png""));
}
document.close();

You are not starting from scratch. The implementation of type 4 and 5 shows you how to read parameters from the PDF and set the graphics. You don't have to learn the complete PDF spec, only 15 pages related to the two shading types, and 6 pages about shading in general. The PDF specification is here:
http://www.adobe.com/devnet/pdf/pdf_reference.html

The tricky parts are:
- decide whether a point(x,y) is inside or outside a patch
- decide the color of a point within the patch

To get an idea about the code, look at the classes GouraudTriangle, GouraudShadingContext, Type4ShadingContext and Vertex here
https://svn.apache.org/viewvc/pdfbox/trunk/pdfbox/src/main/java/org/apache/pdfbox/pdmodel/graphics/shading/
or download the whole project from the repository.
https://pdfbox.apache.org/downloads.html#scm
If you want to see the existing code in the debugger with a Gouraud shading, try this file:
http://asymptote.sourceforge.net/gallery/Gouraud.pdf

Testing:
I have attached several example PDFs. To see which one has which shading, open them with an editor like NOTEPAD++, and search for ""/ShadingType"" (without the quotes). If your images are rendering like the example PDFs, then you were successful.

Optional:
Review and optimize the complete shading package for speed; implement cubic spline interpolation for type 0 (sampled) functions (that one is really low-low priority, see details by looking up ""cubic spline interpolation"" in the PDF spec, which tells that it is disregarded in printing, and I don't have a test PDF).

Mentor: Tilman Hausherr (European timezone, languages: german, english, french)
",", , "
"   Extract Superclass,Pull Up Method,Pull Up Attribute,","Implement shading with Coons and tensor-product patch meshes Of the seven shading methods described in the PDF specification, type 6 (Coons patch meshes) and type 7 (Tensor-product patch meshes) haven't been implemented. I have done type 1, 4 and 5, but I don't know the math for type 6 and 7. My math days are decades away.

Knowledge prerequisites: 
- java, although you don't have to be a java ace, just feel confortable
- math: you should know what ""cubic BÃ©zier curves"", ""Degenerate BÃ©zier curves"", ""bilinear interpolation"", ""tensor-product"", ""affine transform matrix"" and ""Bernstein polynomials"" are, or be able to learn it
- maven (basic)
- svn (basic)
- an IDE like Netbeans or Eclipse or IntelliJ (basic)
- ideally, you are either a math student who likes to program, or a computer science student who is specializing in graphics.

A first look at PDFBOX: try the command utility here:
https://pdfbox.apache.org/commandline/#pdfToImage
and use your favorite PDF, or the PDFs mentioned in PDFBOX-615, these have the shading types that are already implemented.

Some simple source code to convert to images:

String filename = ""blah.pdf"";
PDDocument document = PDDocument.loadNonSeq(new File(filename), null);
List<PDPage> pdPages = document.getDocumentCatalog().getAllPages();
int page = 0;
for (PDPage pdPage : pdPages)
{
++page;
BufferedImage bim = RenderUtil.convertToImage(pdPage, BufferedImage.TYPE_BYTE_BINARY, 300);
ImageIO.write(bim, ""png"", new File(filename+page+"".png""));
}
document.close();

You are not starting from scratch. The implementation of type 4 and 5 shows you how to read parameters from the PDF and set the graphics. You don't have to learn the complete PDF spec, only 15 pages related to the two shading types, and 6 pages about shading in general. The PDF specification is here:
http://www.adobe.com/devnet/pdf/pdf_reference.html

The tricky parts are:
- decide whether a point(x,y) is inside or outside a patch
- decide the color of a point within the patch

To get an idea about the code, look at the classes GouraudTriangle, GouraudShadingContext, Type4ShadingContext and Vertex here
https://svn.apache.org/viewvc/pdfbox/trunk/pdfbox/src/main/java/org/apache/pdfbox/pdmodel/graphics/shading/
or download the whole project from the repository.
https://pdfbox.apache.org/downloads.html#scm
If you want to see the existing code in the debugger with a Gouraud shading, try this file:
http://asymptote.sourceforge.net/gallery/Gouraud.pdf

Testing:
I have attached several example PDFs. To see which one has which shading, open them with an editor like NOTEPAD++, and search for ""/ShadingType"" (without the quotes). If your images are rendering like the example PDFs, then you were successful.

Optional:
Review and optimize the complete shading package for speed; implement cubic spline interpolation for type 0 (sampled) functions (that one is really low-low priority, see details by looking up ""cubic spline interpolation"" in the PDF spec, which tells that it is disregarded in printing, and I don't have a test PDF).

Mentor: Tilman Hausherr (European timezone, languages: german, english, french)
",", Duplicated Code, Large Class, Duplicated Code, Duplicated Code, "
"   Move Class,Move And Rename Class,Rename Method,Move Method,Extract Method,Inline Method,","PDFImageWriter doesn't make use of PDFStreamEngine PDFImageWriter is a subclass of PDFStreamEngine, however it never uses any of its functionality, the writeImage methods could be marked as static and behave in the same manner.

The relationship between PDFImageWriter, RenderUtil, and ImageIOUtil no longer matches its historical origins and needs to be refactored.","Duplicated Code, Long Method, , , , "
"   Rename Method,","Exception Refactoring (Don't wrap Exceptions with COSVisitorException) COSVisitorException is redundant, it is a simple wrapper for SignatureException, CryptographyException and NoSuchAlgorithmException and should be replaced by those exceptions directly.

For example, we can replace:

public void write(PDDocument doc) throws COSVisitorException

With:

public void write(PDDocument doc) throws IOException, CryptographyException

and so on...",", "
"   Move And Rename Class,","DocumentEncryption and PDFEncryption are deprecated and should be removed DocumentEncryption and PDFEncryption in org.apache.pdfbox.encryption are deprecated and should be removed in 2.0.0.

The ARCFour class can then be made package-private in org.apache.pdfbox.pdmodel.encryption.",", "
"   Rename Class,Rename Method,Move Method,Extract Method,Inline Method,Push Down Attribute,","Standardise AcroForm Fields While working on adding the patch in PDFBOX-1847 I noticed that the digital signature form field, PDSignature is deprecated, having been replaced by PDSignatureField.

Currently some aspects of the fields do not correspond with the PDF specification, in particular the hierarchy of the fields and their naming. There are currently 43 open issues for the AcroForm component and no issues have been closed since 2011, so I've attempted some basic refactoring to give us a clean slate for adding new features and fixing old bugs.

Here's the current hierarchy of fields in PDFBox:

PDField
    PDChoiceButton
        PDCheckbox
        PDRadioCollection
    PDPushButton
    PDVariableText
        PDChoiceField
        PDTextbox
    PDSignatureField
    PDUnknownField

And here's the actual hierarchy from the PDF specification:

Field
    Button
        Check Box
        Radio Button
        Pushbutton
    Text
    Choice
        List Box
        Combo Box
    Signature

Note that PDPushButton and PDTextbox are in the wrong place in the hierarchy and List Box and Combo Box are missing.","Duplicated Code, Long Method, , , , , "
"   Extract Superclass,Extract Method,","Move SecurityHandler to PDEncryptionDictionary In the PDF specification a ""security handler"" is defined in the ""encryption dictionary"", but in PDFBox the security handler belongs to the document itself.

It is not difficult to move the SecurityHandler field of PDDocument to the PDEncryptionDictionary class, and we can do so without introducing any breaking changes.

We can deprecate methods at this point rather than removing them, until we're happy with feedback.

Also, we generally don't use the postfix ""Dictionary"" because most classes in the PD model represent dictionaries anyway, so we should rename PDEncryptionDictionary to PDEncryption. Luckily we can just deprecate the old class and add a new subclass with the desired name. This avoids breaking changes until we're happy with the new state of things.","Duplicated Code, Long Method, , Duplicated Code, Large Class, "
"   Rename Method,","Support creating PDF from lossless encoded images Currently we support the insertion of TIFF and JPEG into a PDF, but not PNG. We can pass a BufferedImage, but this one will be JPEG compressed which is not a good thing for graphics with sharp edges. I suggest that we support PNG as well. It is possible because the Flate Filter supports both directions.

My implementation (coming in a few minutes) is just an RGB based start that begs for improvement.",", "
"   Rename Method,","Add filter parameter to PDImageXObject(document, filteredStream) constructor  I am adding a third parameter to
{code}
public PDImageXObject(PDDocument document, InputStream filteredStream)
{code}
i.e. changing it to
{code}
public PDImageXObject(PDDocument document, InputStream filteredStream, COSBase cosFilter)
{code}
because in the code, the filter is always set afterwards. My change improves code clarity. The caller _knows_ what filter was used because he used it to prepare the filteredStream content.

WDYT about also adding width, height, bpc and colorspace to that constructor? These four parameters are always used.

This cool guy (enter his name on youtube) has arguments to use constructors parameters instead of setters:
http://misko.hevery.com/2009/02/19/constructor-injection-vs-setter-injection/

IMHO, mixing constructor initialization and setter initialization looks confusing.",", "
"   Rename Class,Rename Method,Pull Up Method,","Implement transparency groups The attached PDF uses transparency groups, blending and soft masks to create the rounded corners and shades behind images. It appears that these features are not implemented in PDFBox. An implementation proposal is attached in the TransparencyGroup.patch. The basic idea is to create a buffered image, draw the transparency group content onto it and then use the result to produce the soft mask or draw the image on the original g2d.

Note: I am not the (only) author of the proposed change. It was developed in our company few years ago in sources based on a 1.7.x version of PDFBox, mostly by a guy who already left. Over the years, merging of the work done in PDFBox main stream into our source base has become impossible due to many refactorings and other deep going changes done. Now we would like to go the opposite way - where possible - bring the changes and fixes we have done into PDFBox main stream and start to use it in our installations.",", Duplicated Code, "
"   Rename Method,Extract Method,","Implement transparency groups The attached PDF uses transparency groups, blending and soft masks to create the rounded corners and shades behind images. It appears that these features are not implemented in PDFBox. An implementation proposal is attached in the TransparencyGroup.patch. The basic idea is to create a buffered image, draw the transparency group content onto it and then use the result to produce the soft mask or draw the image on the original g2d.

Note: I am not the (only) author of the proposed change. It was developed in our company few years ago in sources based on a 1.7.x version of PDFBox, mostly by a guy who already left. Over the years, merging of the work done in PDFBox main stream into our source base has become impossible due to many refactorings and other deep going changes done. Now we would like to go the opposite way - where possible - bring the changes and fixes we have done into PDFBox main stream and start to use it in our installations.","Duplicated Code, Long Method, , "
"   Rename Method,Move Method,","Support for multipage TIFFs in CCITTFactory, makes PDFBox capable of doing tiff2pdf I created a patch based on Sergey Ushakov's work that handles multipage TIFFs. This allows fast and efficient conversion from TIFF to PDF

The general approach is to provide a new factory method that accepts an image (page) number, and then appropriate page number is located when the CCITT stream is being extracted.

There's a minor inefficiency in this approach because the seek starts from the beginning for each page, causing O(N^2) algorithm when extracting every page, but maximum size for file appears to be 2 GB and the cost for finding a single page will still be low, so I bet this will never come up in practice.

There is no method that tells how many pages TIFF files have. I opted to simply return null from the factory method that accepts page number if there is no such page, so users can use this as condition to break from a TIFF to PDF conversion loop.",", , "
"   Rename Method,","Optimize clipping As already stated in a TODO comment in PageDrawer, the call of Graphics2D#setClip() is time and memory consuming. The attached patch optimizes clipping by calling Graphics2D#setClip() only if the clipping path has changed. The effect depends on the document, e.g. the attached one renders in 10.5s without the optimization and in 5.5 seconds in the optimized version.

The clipping has to be re-applied whenever the transform in Graphics2D changes. This is not explicitly checked for, the implementation rather depends on the cached value being reset manually. Currently this is only needed at one place when processing annotations (AcroForms). Also, the implementation relies upon the clipping path object stored in PDGraphicsState to never change so that a comparison using == can be used. This works fine, but needs a bit of awareness in future changes. To make the design more clean, the clipping path could be made private to PDGraphcisState and thus really ""immutable"" from outside.",", "
"   Rename Method,Extract Method,Inline Method,","Optimize clipping As already stated in a TODO comment in PageDrawer, the call of Graphics2D#setClip() is time and memory consuming. The attached patch optimizes clipping by calling Graphics2D#setClip() only if the clipping path has changed. The effect depends on the document, e.g. the attached one renders in 10.5s without the optimization and in 5.5 seconds in the optimized version.

The clipping has to be re-applied whenever the transform in Graphics2D changes. This is not explicitly checked for, the implementation rather depends on the cached value being reset manually. Currently this is only needed at one place when processing annotations (AcroForms). Also, the implementation relies upon the clipping path object stored in PDGraphicsState to never change so that a comparison using == can be used. This works fine, but needs a bit of awareness in future changes. To make the design more clean, the clipping path could be made private to PDGraphcisState and thus really ""immutable"" from outside.","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,Push Down Attribute,","Optimize clipping As already stated in a TODO comment in PageDrawer, the call of Graphics2D#setClip() is time and memory consuming. The attached patch optimizes clipping by calling Graphics2D#setClip() only if the clipping path has changed. The effect depends on the document, e.g. the attached one renders in 10.5s without the optimization and in 5.5 seconds in the optimized version.

The clipping has to be re-applied whenever the transform in Graphics2D changes. This is not explicitly checked for, the implementation rather depends on the cached value being reset manually. Currently this is only needed at one place when processing annotations (AcroForms). Also, the implementation relies upon the clipping path object stored in PDGraphicsState to never change so that a comparison using == can be used. This works fine, but needs a bit of awareness in future changes. To make the design more clean, the clipping path could be made private to PDGraphcisState and thus really ""immutable"" from outside.","Duplicated Code, Long Method, , , "
"   Rename Method,","Clean up PDFStreamEngine and PDFTextStripper PDFStreamEngine and PDFTextStripper don't really meet our coding conventions and have several unused methods and deprecated code which can safely be removed.

This should clear the way to fixing some bugs in PDFStreamEngine, PDFTextStripper and the various PDFont classes related to text encoding.",", "
"   Rename Method,","Clean up PDFStreamEngine and PDFTextStripper PDFStreamEngine and PDFTextStripper don't really meet our coding conventions and have several unused methods and deprecated code which can safely be removed.

This should clear the way to fixing some bugs in PDFStreamEngine, PDFTextStripper and the various PDFont classes related to text encoding.",", "
"   Rename Method,","Clean up PDFStreamEngine and PDFTextStripper PDFStreamEngine and PDFTextStripper don't really meet our coding conventions and have several unused methods and deprecated code which can safely be removed.

This should clear the way to fixing some bugs in PDFStreamEngine, PDFTextStripper and the various PDFont classes related to text encoding.",", "
"   Rename Method,Move Method,Inline Method,Move Attribute,","Font Refactoring To fix bugs such as PDFBOX-2140 and to enable Unicode TTF embedding we need to sort out long-standing font/text encoding issues. The main issue is that encoding is done in an ad-hoc manner, sometimes in the PDFont subclasses, sometimes elsewhere. For example TTFGlyph2D does its own decoding, and this code is copy & pasted into PDTrueTypeFont. Likewise, PDFont handles CMaps and Encodings despite the fact that these two encoding methods are mutually exclusive. The end result is that the process of reading Encodings/CMaps is often following rules which are completely invalid for that font type but mostly work by luck.

Phase 1

- Refactor PDFont subclasses to remove setXXX methods which allow the object to be corrupted. Proper use of inheritance can remove all cases where public setXXX methods are used during font loading.

- Clean up TTF loading and the loadTTF in anticipation of Unicode TTF embedding, FontBox's TrueTypeFont class is externally mutable via setXXX methods used only by TTFParser: these can be made package-private.

- the Encoding class and EncodingManager could do with some cleaning up prior to further refactoring.

- PDSimpleFont does not do anything, its functionality should be moved into its superclass, PDFont.

- PDFont#determineEncoding() loads CMaps when only Encodings are applicable, and vice versa. Loading needs to be pushed down into the appropriate subclasses, as a starting point the relevant code should at least be copied into the relevant subclasses ready for further refactoring.

- TTFGlyph2D does its own decoding of char codes, rather than using the font's #encode method (fair enough because #encode is broken) and there's a copy and pasted version of the same code in PDTrueTypeFont - we need to consolidate this code into PDTrueTypeFont where it belongs.

Phase 2

- Refactor loading of CMaps and Encodings from font dictionaries, this will involve changes to PDFont and its subclasses to delegate loading to subclasses where it can be properly encapsulated

- May need to alter the class hierarchy w.r.t CIDFont to facilitate this, as CIDFont isn't really a PDFont - it's parent Type0 font is responsible for its CMap. We'll see.

Phase 3

- Refactor the decoding of character codes by PDFont and its subclasses, this will involve replacing the #getCodeFromArray, #encode and #encodeToCID methods.

- Fix decoding of content stream character codes in PDFStreamEngine, using the newly refactored PDFont and using the current font's CMap to determine the code width.
",", , , , "
"   Move Class,Move And Rename Class,Extract Interface,Move Method,Move Attribute,","Font Refactoring To fix bugs such as PDFBOX-2140 and to enable Unicode TTF embedding we need to sort out long-standing font/text encoding issues. The main issue is that encoding is done in an ad-hoc manner, sometimes in the PDFont subclasses, sometimes elsewhere. For example TTFGlyph2D does its own decoding, and this code is copy & pasted into PDTrueTypeFont. Likewise, PDFont handles CMaps and Encodings despite the fact that these two encoding methods are mutually exclusive. The end result is that the process of reading Encodings/CMaps is often following rules which are completely invalid for that font type but mostly work by luck.

Phase 1

- Refactor PDFont subclasses to remove setXXX methods which allow the object to be corrupted. Proper use of inheritance can remove all cases where public setXXX methods are used during font loading.

- Clean up TTF loading and the loadTTF in anticipation of Unicode TTF embedding, FontBox's TrueTypeFont class is externally mutable via setXXX methods used only by TTFParser: these can be made package-private.

- the Encoding class and EncodingManager could do with some cleaning up prior to further refactoring.

- PDSimpleFont does not do anything, its functionality should be moved into its superclass, PDFont.

- PDFont#determineEncoding() loads CMaps when only Encodings are applicable, and vice versa. Loading needs to be pushed down into the appropriate subclasses, as a starting point the relevant code should at least be copied into the relevant subclasses ready for further refactoring.

- TTFGlyph2D does its own decoding of char codes, rather than using the font's #encode method (fair enough because #encode is broken) and there's a copy and pasted version of the same code in PDTrueTypeFont - we need to consolidate this code into PDTrueTypeFont where it belongs.

Phase 2

- Refactor loading of CMaps and Encodings from font dictionaries, this will involve changes to PDFont and its subclasses to delegate loading to subclasses where it can be properly encapsulated

- May need to alter the class hierarchy w.r.t CIDFont to facilitate this, as CIDFont isn't really a PDFont - it's parent Type0 font is responsible for its CMap. We'll see.

Phase 3

- Refactor the decoding of character codes by PDFont and its subclasses, this will involve replacing the #getCodeFromArray, #encode and #encodeToCID methods.

- Fix decoding of content stream character codes in PDFStreamEngine, using the newly refactored PDFont and using the current font's CMap to determine the code width.
",", , , Large Class, "
"   Rename Method,","Font Refactoring To fix bugs such as PDFBOX-2140 and to enable Unicode TTF embedding we need to sort out long-standing font/text encoding issues. The main issue is that encoding is done in an ad-hoc manner, sometimes in the PDFont subclasses, sometimes elsewhere. For example TTFGlyph2D does its own decoding, and this code is copy & pasted into PDTrueTypeFont. Likewise, PDFont handles CMaps and Encodings despite the fact that these two encoding methods are mutually exclusive. The end result is that the process of reading Encodings/CMaps is often following rules which are completely invalid for that font type but mostly work by luck.

Phase 1

- Refactor PDFont subclasses to remove setXXX methods which allow the object to be corrupted. Proper use of inheritance can remove all cases where public setXXX methods are used during font loading.

- Clean up TTF loading and the loadTTF in anticipation of Unicode TTF embedding, FontBox's TrueTypeFont class is externally mutable via setXXX methods used only by TTFParser: these can be made package-private.

- the Encoding class and EncodingManager could do with some cleaning up prior to further refactoring.

- PDSimpleFont does not do anything, its functionality should be moved into its superclass, PDFont.

- PDFont#determineEncoding() loads CMaps when only Encodings are applicable, and vice versa. Loading needs to be pushed down into the appropriate subclasses, as a starting point the relevant code should at least be copied into the relevant subclasses ready for further refactoring.

- TTFGlyph2D does its own decoding of char codes, rather than using the font's #encode method (fair enough because #encode is broken) and there's a copy and pasted version of the same code in PDTrueTypeFont - we need to consolidate this code into PDTrueTypeFont where it belongs.

Phase 2

- Refactor loading of CMaps and Encodings from font dictionaries, this will involve changes to PDFont and its subclasses to delegate loading to subclasses where it can be properly encapsulated

- May need to alter the class hierarchy w.r.t CIDFont to facilitate this, as CIDFont isn't really a PDFont - it's parent Type0 font is responsible for its CMap. We'll see.

Phase 3

- Refactor the decoding of character codes by PDFont and its subclasses, this will involve replacing the #getCodeFromArray, #encode and #encodeToCID methods.

- Fix decoding of content stream character codes in PDFStreamEngine, using the newly refactored PDFont and using the current font's CMap to determine the code width.
",", "
"   Rename Class,Move Method,Inline Method,Move Attribute,","Font Refactoring To fix bugs such as PDFBOX-2140 and to enable Unicode TTF embedding we need to sort out long-standing font/text encoding issues. The main issue is that encoding is done in an ad-hoc manner, sometimes in the PDFont subclasses, sometimes elsewhere. For example TTFGlyph2D does its own decoding, and this code is copy & pasted into PDTrueTypeFont. Likewise, PDFont handles CMaps and Encodings despite the fact that these two encoding methods are mutually exclusive. The end result is that the process of reading Encodings/CMaps is often following rules which are completely invalid for that font type but mostly work by luck.

Phase 1

- Refactor PDFont subclasses to remove setXXX methods which allow the object to be corrupted. Proper use of inheritance can remove all cases where public setXXX methods are used during font loading.

- Clean up TTF loading and the loadTTF in anticipation of Unicode TTF embedding, FontBox's TrueTypeFont class is externally mutable via setXXX methods used only by TTFParser: these can be made package-private.

- the Encoding class and EncodingManager could do with some cleaning up prior to further refactoring.

- PDSimpleFont does not do anything, its functionality should be moved into its superclass, PDFont.

- PDFont#determineEncoding() loads CMaps when only Encodings are applicable, and vice versa. Loading needs to be pushed down into the appropriate subclasses, as a starting point the relevant code should at least be copied into the relevant subclasses ready for further refactoring.

- TTFGlyph2D does its own decoding of char codes, rather than using the font's #encode method (fair enough because #encode is broken) and there's a copy and pasted version of the same code in PDTrueTypeFont - we need to consolidate this code into PDTrueTypeFont where it belongs.

Phase 2

- Refactor loading of CMaps and Encodings from font dictionaries, this will involve changes to PDFont and its subclasses to delegate loading to subclasses where it can be properly encapsulated

- May need to alter the class hierarchy w.r.t CIDFont to facilitate this, as CIDFont isn't really a PDFont - it's parent Type0 font is responsible for its CMap. We'll see.

Phase 3

- Refactor the decoding of character codes by PDFont and its subclasses, this will involve replacing the #getCodeFromArray, #encode and #encodeToCID methods.

- Fix decoding of content stream character codes in PDFStreamEngine, using the newly refactored PDFont and using the current font's CMap to determine the code width.
",", , , , "
"   Move Class,Rename Class,Move And Rename Class,Rename Method,Move Method,Extract Method,","(Graphics) Operator Refactoring I'm in the process of porting a fairly complex program which uses the 1.8 API over to 2.0, as a way of finding out where the rough edges in 2.0 are. The app which I'm porting hooks into many of the graphics operators and subclasses PageDrawer to get access to the PDF's graphics state.

It turns out that this doesn't work very well, especially in 2.0 where more of the PageDrawer's state is private and we have the additional complexity of transparency groups.

The main issue is that the graphics operators are coupled to PageDrawer, but I'm not interested in the AWT rendering, I just need a way to hook into the graphics operations - subclassing the operators has proven to be a poor solution as there are cases where calling super.process() doesn't provide enough flexibility.

So here's my solution: in the same way that text processing was recently factored-out into PDFTextStreamEngine for end-users to subclass, I'd like to do the same with graphics operations. Instead of the graphics operators being coupled to PageDrawer, which is only one possible implementation of graphics handling, we can move the methods which the operators call up into a new subclass of PDFStreamEngine, let's call it PDFGraphicsStreamEngine. This class can then be subclassed by anyone interested in hooking into the graphics operations, including PageDrawer.

With the new callbacks for text handling already in PDFTextStreamEngine and the addition of new graphics callbacks in PDFGraphicsStreamEngine, most of the time it shouldn't be necessary for end-users to need to override the operator classes to get access to the information they need, which would be a huge benefit :)

This will involve a bunch of changes to operators, so I'll take the chance to do some general cleaning up while I'm at it: the operator classes haven't received much attention for a while. With more callbacks in PDFStreamEngine et al, we're moving towards a point where the operator classes are becoming almost an internal part of the PDFBox API: might be something to think about for the future.","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Add missing values to PDComplexFileSpecification The class PDComplexFileSpecification needs to be improved as follows:

- analog to ""get/setFileXXX"" we should add the missing get/setEmbeddedFileUnicode
- we should rename getUnicodeFile to getFileUnicode to be inline with the other getters and add a setter for that value as well
- according to the spec, the Dos, Unix and Mac mutations shouldn't be used anymore, therefore we should rearrange the order in ""getFilename""
","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Improve XRef self healing mechanism PDFBOX-1769 introduced a ""self healing"" mechanism to repair corrupt XRef offsets. But that one was just a starter and there remain a lot of issues to be solved. I'm planing to solve at least some of them.

All fixes and improvements are targeting the non-sequential parser and I won't port those changes to the old parser.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Improve XRef self healing mechanism PDFBOX-1769 introduced a ""self healing"" mechanism to repair corrupt XRef offsets. But that one was just a starter and there remain a lot of issues to be solved. I'm planing to solve at least some of them.

All fixes and improvements are targeting the non-sequential parser and I won't port those changes to the old parser.","Duplicated Code, Long Method, , "
"   Rename Method,","Remove usage of AWT fonts We're still using AWT fonts to render the ""standard 14"" built-in fonts, which causes rendering problems and encoding issues (see  PDFBOX-2140). We're also using AWT for some fallback fonts.

Removal of these AWT fonts isn't too difficult, we need to load the fonts using the existing PDFFontManager mechanism which has recently been added. All missing TrueType fonts loaded from disk have been using SystemFontManager for a number of weeks now. 

We should ship some sensible default fonts with PDFBox, such as the Liberation fonts (see PDFBOX-2169, PDFBOX-2263), in case PDFFontManager can't find anything suitable, rather than falling back to the default TTF font, but by default we'll probe the system for suitable fonts.",", "
"   Rename Method,Extract Method,","Remove usage of AWT fonts We're still using AWT fonts to render the ""standard 14"" built-in fonts, which causes rendering problems and encoding issues (see  PDFBOX-2140). We're also using AWT for some fallback fonts.

Removal of these AWT fonts isn't too difficult, we need to load the fonts using the existing PDFFontManager mechanism which has recently been added. All missing TrueType fonts loaded from disk have been using SystemFontManager for a number of weeks now. 

We should ship some sensible default fonts with PDFBox, such as the Liberation fonts (see PDFBOX-2169, PDFBOX-2263), in case PDFFontManager can't find anything suitable, rather than falling back to the default TTF font, but by default we'll probe the system for suitable fonts.","Duplicated Code, Long Method, , "
"   Rename Method,","Remove Jempbox subproject Following up PDFBOX-2107 I'm finally going to remove the Jempbox subproject.

We discussed that topic several times, IMHO always with the same result: discontinue Jempbox in favor of XMPBox.

Those users who still prefer Jempbox might use the 1.8.x version, which still should work even in combination with 2.0",", "
"   Rename Method,","Overhaul the appearance generation for PDF forms The appearance handling for forms in 1.x is limited and does not reflect all settings possible for form fields. In addition the current code is not very modular and does not follow the box model used for form fields. 

Unfortunately only the basics of form handling are defined in the PDF spec. The details like padding of boxes, text placement etc. have to be determined by looking at how Adobe forms are generated.

Update: The file from PDFBOX-2310 has bad rendering which might be related?",", "
"   Push Down Method,Move Method,Extract Method,Move Attribute,","Overhaul the appearance generation for PDF forms The appearance handling for forms in 1.x is limited and does not reflect all settings possible for form fields. In addition the current code is not very modular and does not follow the box model used for form fields. 

Unfortunately only the basics of form handling are defined in the PDF spec. The details like padding of boxes, text placement etc. have to be determined by looking at how Adobe forms are generated.

Update: The file from PDFBOX-2310 has bad rendering which might be related?","Duplicated Code, Long Method, , , , , "
"   Rename Method,","Overhaul the appearance generation for PDF forms The appearance handling for forms in 1.x is limited and does not reflect all settings possible for form fields. In addition the current code is not very modular and does not follow the box model used for form fields. 

Unfortunately only the basics of form handling are defined in the PDF spec. The details like padding of boxes, text placement etc. have to be determined by looking at how Adobe forms are generated.

Update: The file from PDFBOX-2310 has bad rendering which might be related?",", "
"   Rename Method,","Overhaul the appearance generation for PDF forms The appearance handling for forms in 1.x is limited and does not reflect all settings possible for form fields. In addition the current code is not very modular and does not follow the box model used for form fields. 

Unfortunately only the basics of form handling are defined in the PDF spec. The details like padding of boxes, text placement etc. have to be determined by looking at how Adobe forms are generated.

Update: The file from PDFBOX-2310 has bad rendering which might be related?",", "
"   Rename Method,Push Down Method,Inline Method,","Overhaul the appearance generation for PDF forms The appearance handling for forms in 1.x is limited and does not reflect all settings possible for form fields. In addition the current code is not very modular and does not follow the box model used for form fields. 

Unfortunately only the basics of form handling are defined in the PDF spec. The details like padding of boxes, text placement etc. have to be determined by looking at how Adobe forms are generated.

Update: The file from PDFBOX-2310 has bad rendering which might be related?",", , , "
"   Rename Method,Extract Method,","Overhaul the appearance generation for PDF forms The appearance handling for forms in 1.x is limited and does not reflect all settings possible for form fields. In addition the current code is not very modular and does not follow the box model used for form fields. 

Unfortunately only the basics of form handling are defined in the PDF spec. The details like padding of boxes, text placement etc. have to be determined by looking at how Adobe forms are generated.

Update: The file from PDFBOX-2310 has bad rendering which might be related?","Duplicated Code, Long Method, , "
"   Move Method,Extract Method,","Overhaul the appearance generation for PDF forms The appearance handling for forms in 1.x is limited and does not reflect all settings possible for form fields. In addition the current code is not very modular and does not follow the box model used for form fields. 

Unfortunately only the basics of form handling are defined in the PDF spec. The details like padding of boxes, text placement etc. have to be determined by looking at how Adobe forms are generated.

Update: The file from PDFBOX-2310 has bad rendering which might be related?","Duplicated Code, Long Method, , , "
"   Rename Class,Rename Method,Extract Method,","Remove .properties file usage in PDFStreamEngine PDFStreamEngine and its subclasses currently load a list of OperatorProcessor classes from .properties files. This makes it difficult to override the classes, as the .properties files also need to be copied and pasted if changes are made. Worse still, subclasses in other modules are known to load .properties files from the pdfbox module which breaks OSGi, as discovered in PDFBOX-2358.

There is currently an API, registerOperatorProcessor() which performs the same role. This should be adapted for use in subclasses, perhaps deprecated and replaced with something less brittle which doesn't require manually assigning operators to specific strings such as ""BT"" or ""T*"".","Duplicated Code, Long Method, , "
"   Rename Method,","Improve high-level font APIs The PDFont and Type1Equivalent APIs could expose some higher-level details, such as a consistent way to get names and Type1Equivalent instances.

Some of the other font formats could also do with APIs exposing some specific useful internals such as GIDs. I'm going to add these as I find that I have a need for them during development and debugging.",", "
"   Rename Method,Extract Method,","Move caching outside of PDResources *Note:* This issue is based on a discussion which occurred regarding PDFBOX-2301 but is actually a separate issue.

Currently we cache the page resources in PDResources which belongs to a specific PDPage. This causes two problems, 1) users who want to hold many PDPage objects in memory will have high memory use (but this is often by accident*). 2) By caching resources in PDPage we only get to keep that cache for the lifetime of the page, which e.g. in PDFRenderer is a single page only. That means that a font which appears on 40 pages has to be parsed 40 times, which causes slow running times, but also memory thrashing as objects are destroyed frequently only to be re-created.

What PDFRenderer really needs is not page-wide caching but document-wide caching, so that it can cache fonts, cmaps, color profiles, etc. only once. But that won't work for images, because they're too large. What we're beginning to realise is that caching is use-case specific and probably shouldn't be built-in to PDFBox's pdmodel. Instead we should removing resource caching from PDPage/PDResources and implement custom caching in PDFRenderer and other downstream classes such as PDFTextStripper. I'll happily volunteer myself. The existing high-level PDFBox APIs will continue to ""just work"" and power users will get a level of control that they appreciate.

This strategy could be enhanced by removing memory-hungry methods on PDResources such as getFonts() and getXObjects() which force all resources of a particular type to be loaded, whether or not they are needed, or actually used in the content stream. They would be replaced by methods to retrieve a single resource, e.g. getFont(name).

---

\* There probably isn't a legitimate use case for 1) any more, we've solved the issues which we used to have with image caching (in fact, the clearCache() method actually no longer needs to be called by PDFRenderer, though it currently is). The real problem is that it's easy to accidentally retain PDPage objects, the PDDocument#getDocumentCatalog().getAllPages() method is dangerous as looping over it will cause pages to be retained during processing, like so:

{code}
for (PDPage page : document.getDocumentCatalog().getAllPages()) // java.util.List
{
     // ... this is idiomatic in PDFBox 1.8
} 
// List returned by getAllPages() kept in scope until here (bad)
{code}

I added of couple of methods a while ago to avoid this by fetching each PDPage one at a time, and this is now used internally in PDFBox to avoid the memory problems we used to have:

{code}
for (int i = 0; i < document.getNumberOfPages(); i++)
{
    PDPage page = document.getPage(i);
    // ... this is the new 2.0 way
    // current page falls out of scope here (good)
}
{code}

To solve this problem, we could change getAllPages() so that instead of returning a List it returns an Iterator<PDPage>, which would provide a nicer API than getPage(int) and most existing code will continue to work. This is also an opportunity to also fix type safety issues due to PDPageNode and incorrect handling of the page tree (this is similar to the issue we had recently with the acroform field tree).","Duplicated Code, Long Method, , "
"   Rename Method,","Page tree handling needs rewriting The way in which PDFBox handles the Page tree needs to be rewritten, preferably from scratch. Currently the document catalog returns the raw objects from the page tree, wrapped in either a PDPage or PDPageNode.

We need to abstract over the page tree and get rid of PDPageNode, we should provide methods which can add/remove PDPage objects *only*. The existing low-level access to the page tree is not needed at the PD-level.

Inheritance of page properties such as crop box, resources, and rotation should be reimplemented to use whatever new page tree abstraction we invent. We can finally remove the old broken methods which didn't look up the inheritance tree when retrieving these values.",", "
"   Rename Method,Extract Method,Inline Method,","Page tree handling needs rewriting The way in which PDFBox handles the Page tree needs to be rewritten, preferably from scratch. Currently the document catalog returns the raw objects from the page tree, wrapped in either a PDPage or PDPageNode.

We need to abstract over the page tree and get rid of PDPageNode, we should provide methods which can add/remove PDPage objects *only*. The existing low-level access to the page tree is not needed at the PD-level.

Inheritance of page properties such as crop box, resources, and rotation should be reimplemented to use whatever new page tree abstraction we invent. We can finally remove the old broken methods which didn't look up the inheritance tree when retrieving these values.","Duplicated Code, Long Method, , , "
"   Extract Method,Inline Method,","Page tree handling needs rewriting The way in which PDFBox handles the Page tree needs to be rewritten, preferably from scratch. Currently the document catalog returns the raw objects from the page tree, wrapped in either a PDPage or PDPageNode.

We need to abstract over the page tree and get rid of PDPageNode, we should provide methods which can add/remove PDPage objects *only*. The existing low-level access to the page tree is not needed at the PD-level.

Inheritance of page properties such as crop box, resources, and rotation should be reimplemented to use whatever new page tree abstraction we invent. We can finally remove the old broken methods which didn't look up the inheritance tree when retrieving these values.","Duplicated Code, Long Method, , , "
"   Move Method,Extract Method,Inline Method,Move Attribute,","Page tree handling needs rewriting The way in which PDFBox handles the Page tree needs to be rewritten, preferably from scratch. Currently the document catalog returns the raw objects from the page tree, wrapped in either a PDPage or PDPageNode.

We need to abstract over the page tree and get rid of PDPageNode, we should provide methods which can add/remove PDPage objects *only*. The existing low-level access to the page tree is not needed at the PD-level.

Inheritance of page properties such as crop box, resources, and rotation should be reimplemented to use whatever new page tree abstraction we invent. We can finally remove the old broken methods which didn't look up the inheritance tree when retrieving these values.","Duplicated Code, Long Method, , , , , "
"   Rename Method,Extract Method,","Page tree handling needs rewriting The way in which PDFBox handles the Page tree needs to be rewritten, preferably from scratch. Currently the document catalog returns the raw objects from the page tree, wrapped in either a PDPage or PDPageNode.

We need to abstract over the page tree and get rid of PDPageNode, we should provide methods which can add/remove PDPage objects *only*. The existing low-level access to the page tree is not needed at the PD-level.

Inheritance of page properties such as crop box, resources, and rotation should be reimplemented to use whatever new page tree abstraction we invent. We can finally remove the old broken methods which didn't look up the inheritance tree when retrieving these values.","Duplicated Code, Long Method, , "
"   Rename Method,","Page tree handling needs rewriting The way in which PDFBox handles the Page tree needs to be rewritten, preferably from scratch. Currently the document catalog returns the raw objects from the page tree, wrapped in either a PDPage or PDPageNode.

We need to abstract over the page tree and get rid of PDPageNode, we should provide methods which can add/remove PDPage objects *only*. The existing low-level access to the page tree is not needed at the PD-level.

Inheritance of page properties such as crop box, resources, and rotation should be reimplemented to use whatever new page tree abstraction we invent. We can finally remove the old broken methods which didn't look up the inheritance tree when retrieving these values.",", "
"   Rename Method,","Page tree handling needs rewriting The way in which PDFBox handles the Page tree needs to be rewritten, preferably from scratch. Currently the document catalog returns the raw objects from the page tree, wrapped in either a PDPage or PDPageNode.

We need to abstract over the page tree and get rid of PDPageNode, we should provide methods which can add/remove PDPage objects *only*. The existing low-level access to the page tree is not needed at the PD-level.

Inheritance of page properties such as crop box, resources, and rotation should be reimplemented to use whatever new page tree abstraction we invent. We can finally remove the old broken methods which didn't look up the inheritance tree when retrieving these values.",", "
"   Rename Method,","Page tree handling needs rewriting The way in which PDFBox handles the Page tree needs to be rewritten, preferably from scratch. Currently the document catalog returns the raw objects from the page tree, wrapped in either a PDPage or PDPageNode.

We need to abstract over the page tree and get rid of PDPageNode, we should provide methods which can add/remove PDPage objects *only*. The existing low-level access to the page tree is not needed at the PD-level.

Inheritance of page properties such as crop box, resources, and rotation should be reimplemented to use whatever new page tree abstraction we invent. We can finally remove the old broken methods which didn't look up the inheritance tree when retrieving these values.",", "
"   Rename Method,Extract Method,","Page tree handling needs rewriting The way in which PDFBox handles the Page tree needs to be rewritten, preferably from scratch. Currently the document catalog returns the raw objects from the page tree, wrapped in either a PDPage or PDPageNode.

We need to abstract over the page tree and get rid of PDPageNode, we should provide methods which can add/remove PDPage objects *only*. The existing low-level access to the page tree is not needed at the PD-level.

Inheritance of page properties such as crop box, resources, and rotation should be reimplemented to use whatever new page tree abstraction we invent. We can finally remove the old broken methods which didn't look up the inheritance tree when retrieving these values.","Duplicated Code, Long Method, , "
"   Rename Method,Inline Method,","Make the non-sequential parser the default parser As proposed by Maruan on dev@, we should make the non-sequentatial parser the default parser. The different  load-methods should be simplified in that context, so that all load/loadNonSeq will be replaced by a load method.",", , "
"   Rename Method,","Make the non-sequential parser the default parser As proposed by Maruan on dev@, we should make the non-sequentatial parser the default parser. The different  load-methods should be simplified in that context, so that all load/loadNonSeq will be replaced by a load method.",", "
"   Rename Method,","xref stream is saved as table When saving a PDDocument, PdfBox seems to always write an xref table, even when the original file contains an xref stream.

To reproduce, load a PDF file (like the one attached) with PDDocument#load (or PDDocument#loadNonSeq, same result) and then save it with PDDocument#save to another file.

It seems to me that the problem is in COSWriter#doWriteXRef. When COSDocument#isXRefStream is true, the xref entries should be wrapped in a stream, but they're written to output one by one. I think that part should look more like its counterpart in COSWriter#doWriteXRefInc.

I made some changes to doWriteXRef accordingly and it seems to work for PDFs that have never been incrementally updated but leads to corrupt files when the PDF has been incrementally updated before :(",", "
"   Rename Method,","Share functionality between Page Tree and Field Tree The PDFs page tree and AcroForms field tree share some common functionality e.g. resolving inheritable attributes, iterating through leafs and such which could be combined into a PDTree class.",", "
"   Rename Method,Move Method,Extract Method,","Share functionality between Page Tree and Field Tree The PDFs page tree and AcroForms field tree share some common functionality e.g. resolving inheritable attributes, iterating through leafs and such which could be combined into a PDTree class.","Duplicated Code, Long Method, , , "
"   Rename Method,","Share functionality between Page Tree and Field Tree The PDFs page tree and AcroForms field tree share some common functionality e.g. resolving inheritable attributes, iterating through leafs and such which could be combined into a PDTree class.",", "
"   Rename Method,","Further align AcroForms and Fields PDModel with PDF specification The PDModel for AcroForms and the fields being part of it  (o.a.p.pdmodel.interactive.form) needs to be enhanced. Sample issues:

 - RadioButton and Checkbox donâ€™t support DV entries although they should
 - inheritable attributes only support inheritance either form their parent or the root of the AcroForms model which is not inline with the specification",", "
"   Push Down Method,Push Down Attribute,","Further align AcroForms and Fields PDModel with PDF specification The PDModel for AcroForms and the fields being part of it  (o.a.p.pdmodel.interactive.form) needs to be enhanced. Sample issues:

 - RadioButton and Checkbox donâ€™t support DV entries although they should
 - inheritable attributes only support inheritance either form their parent or the root of the AcroForms model which is not inline with the specification",", , , "
"   Push Down Method,Move Method,","Further align AcroForms and Fields PDModel with PDF specification The PDModel for AcroForms and the fields being part of it  (o.a.p.pdmodel.interactive.form) needs to be enhanced. Sample issues:

 - RadioButton and Checkbox donâ€™t support DV entries although they should
 - inheritable attributes only support inheritance either form their parent or the root of the AcroForms model which is not inline with the specification",", , , "
"   Rename Method,Pull Up Method,Pull Up Attribute,","[PATCH] Two PDFont to create PDF documents in CJK and non-ISO-8859-1 languages I made two PDFont classes for creating PDF documents in CJK and non-ISO-8859-1 languages.

One is PDType0CJKFont. This is for using CJK fonts included in the Asian font package of Adobe Reader. This font doesn't require the target font at the time of creating PDF documentary. This font uses UTF-16 as a text code and supports surrogate pair characters.

The other is PDType0UnicodeFont. This is for using TrueType Type0 Font which can deal with any Unicode characters like a ArialUnicodeMS. Only the characters which are used actually in the document are embedde. Realizing this, you have to call the PDType0Unicode.reloadFont() method just before closing PDPageContentStream. I think this specification is ugly, but I could not thought of a suitable way to remove this spec. This font uses the original glyph code of the embedded font as a text code and supports surrogate pair characters too.

Example programs using these two fonts are also attached.",", Duplicated Code, Duplicated Code, "
"   Extract Method,Move Attribute,","Improve PDFDebugger (This is an idea for the [Google Summer of Code 2015|https://www.google-melange.com/])

Our command line utility PDFDebugger (part of the command line pdfbox-app get it [here|https://pdfbox.apache.org/downloads.html], read description [here|https://pdfbox.apache.org/commandline/], see the source code [here|https://svn.apache.org/viewvc/pdfbox/trunk/tools/src/main/java/org/apache/pdfbox/tools/PDFDebugger.java?view=markup&sortby=date]) needs some improvements:
   - hex view
   - view of non printable characters
   - âœ“ saving streams
   - binary copy & paste
   - âœ“ Create a status line that shows where we are in the tree. (Like in the Windows REGEDIT)
   - âœ“ Copy the current tree string into the clipboard (useful in discussions about details of a PDF)
   - âœ“ (Optional, not sure if easy) Jump to specific place in the tree by entering tree string
   - âœ“ ability to search in streams (very useful for content streams and meta data)
   - âœ“ show images that are streams
   - âœ“ show PDIndexed color lookup table, show the index value, the base and RGB color value sets when the mouse moves
   - âœ“ show PDSeparation color
   - âœ“ show PDDeviceN colors
   - optional, idea should be developed a bit: show meaningful explanation on some attributes, e.g. ""appearance stream"" when hovering over /AP
   - show font encodings and characters
   - âœ“ display flag bits (e.g. Annotation flags) in a way that is easy to understand. There are probably others, I assume that the main work needs to be done only once
   - edit attributes (should be possible to enter values as decimal, hex or binary)
   - edit streams, while keeping or changing the compression filter
   - save altered PDF 
   - âœ“ color mark of certain PDF operators, especially Q...q and text operators (BT...ET). Ideally, it should help the user understand the ""bracketing"" of these operators, i.e. understand where a sequence starts and where it ends. (See ""operator summary"" in the PDF Spec) Other ""important"" operators I can think of are the matrix, font and color operators. A cool advanced thing would be to show the current color or the font in a popup when hovering above such an operator.

To see a product with a similar purpose that is better than PDFDebugger, watch [this video|https://www.youtube.com/watch?v=g-QcU9B4qMc].

I'm not asking to implement a clone of that product (I don't use it, all I know is that video), but we at PDFBox really need something that makes PDF debugging easier. As an example of how the current PDFDebugger prevented me from finding a bug quickly, see PDFBOX-2401 and search for ""PDFDebugger"".

Prerequisites:
- java programming, especially the GUI components
- the ability to understand existing source code

Using external software components is possible (must have Apache License or a compatible one), but should be decided on a case-by-case basis, we don't want to get too big.

Development strategy: go from the easy to the difficult. The wished features are already sorted this way (mostly).

Get introduced: [download the source code with svn|https://pdfbox.apache.org/downloads.html#scm] and build it with maven. Run PDFDebugger and view some PDFs to see the components of a PDF. Start with the file of PDFBOX-2401. Read up something about the structure of PDF on the web or from the [PDF Specification|https://www.adobe.com/devnet/pdf/pdf_reference.html].

Mentor: Tilman Hausherr (European timezone, languages: german, english, french). To see the GSoC2014 project I mentored, go to PDFBOX-1915.","Duplicated Code, Long Method, , , "
"   Extract Method,Inline Method,","Improve PDFDebugger (This is an idea for the [Google Summer of Code 2015|https://www.google-melange.com/])

Our command line utility PDFDebugger (part of the command line pdfbox-app get it [here|https://pdfbox.apache.org/downloads.html], read description [here|https://pdfbox.apache.org/commandline/], see the source code [here|https://svn.apache.org/viewvc/pdfbox/trunk/tools/src/main/java/org/apache/pdfbox/tools/PDFDebugger.java?view=markup&sortby=date]) needs some improvements:
   - hex view
   - view of non printable characters
   - âœ“ saving streams
   - binary copy & paste
   - âœ“ Create a status line that shows where we are in the tree. (Like in the Windows REGEDIT)
   - âœ“ Copy the current tree string into the clipboard (useful in discussions about details of a PDF)
   - âœ“ (Optional, not sure if easy) Jump to specific place in the tree by entering tree string
   - âœ“ ability to search in streams (very useful for content streams and meta data)
   - âœ“ show images that are streams
   - âœ“ show PDIndexed color lookup table, show the index value, the base and RGB color value sets when the mouse moves
   - âœ“ show PDSeparation color
   - âœ“ show PDDeviceN colors
   - optional, idea should be developed a bit: show meaningful explanation on some attributes, e.g. ""appearance stream"" when hovering over /AP
   - show font encodings and characters
   - âœ“ display flag bits (e.g. Annotation flags) in a way that is easy to understand. There are probably others, I assume that the main work needs to be done only once
   - edit attributes (should be possible to enter values as decimal, hex or binary)
   - edit streams, while keeping or changing the compression filter
   - save altered PDF 
   - âœ“ color mark of certain PDF operators, especially Q...q and text operators (BT...ET). Ideally, it should help the user understand the ""bracketing"" of these operators, i.e. understand where a sequence starts and where it ends. (See ""operator summary"" in the PDF Spec) Other ""important"" operators I can think of are the matrix, font and color operators. A cool advanced thing would be to show the current color or the font in a popup when hovering above such an operator.

To see a product with a similar purpose that is better than PDFDebugger, watch [this video|https://www.youtube.com/watch?v=g-QcU9B4qMc].

I'm not asking to implement a clone of that product (I don't use it, all I know is that video), but we at PDFBox really need something that makes PDF debugging easier. As an example of how the current PDFDebugger prevented me from finding a bug quickly, see PDFBOX-2401 and search for ""PDFDebugger"".

Prerequisites:
- java programming, especially the GUI components
- the ability to understand existing source code

Using external software components is possible (must have Apache License or a compatible one), but should be decided on a case-by-case basis, we don't want to get too big.

Development strategy: go from the easy to the difficult. The wished features are already sorted this way (mostly).

Get introduced: [download the source code with svn|https://pdfbox.apache.org/downloads.html#scm] and build it with maven. Run PDFDebugger and view some PDFs to see the components of a PDF. Start with the file of PDFBOX-2401. Read up something about the structure of PDF on the web or from the [PDF Specification|https://www.adobe.com/devnet/pdf/pdf_reference.html].

Mentor: Tilman Hausherr (European timezone, languages: german, english, french). To see the GSoC2014 project I mentored, go to PDFBOX-1915.","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Improve PDFDebugger (This is an idea for the [Google Summer of Code 2015|https://www.google-melange.com/])

Our command line utility PDFDebugger (part of the command line pdfbox-app get it [here|https://pdfbox.apache.org/downloads.html], read description [here|https://pdfbox.apache.org/commandline/], see the source code [here|https://svn.apache.org/viewvc/pdfbox/trunk/tools/src/main/java/org/apache/pdfbox/tools/PDFDebugger.java?view=markup&sortby=date]) needs some improvements:
   - hex view
   - view of non printable characters
   - âœ“ saving streams
   - binary copy & paste
   - âœ“ Create a status line that shows where we are in the tree. (Like in the Windows REGEDIT)
   - âœ“ Copy the current tree string into the clipboard (useful in discussions about details of a PDF)
   - âœ“ (Optional, not sure if easy) Jump to specific place in the tree by entering tree string
   - âœ“ ability to search in streams (very useful for content streams and meta data)
   - âœ“ show images that are streams
   - âœ“ show PDIndexed color lookup table, show the index value, the base and RGB color value sets when the mouse moves
   - âœ“ show PDSeparation color
   - âœ“ show PDDeviceN colors
   - optional, idea should be developed a bit: show meaningful explanation on some attributes, e.g. ""appearance stream"" when hovering over /AP
   - show font encodings and characters
   - âœ“ display flag bits (e.g. Annotation flags) in a way that is easy to understand. There are probably others, I assume that the main work needs to be done only once
   - edit attributes (should be possible to enter values as decimal, hex or binary)
   - edit streams, while keeping or changing the compression filter
   - save altered PDF 
   - âœ“ color mark of certain PDF operators, especially Q...q and text operators (BT...ET). Ideally, it should help the user understand the ""bracketing"" of these operators, i.e. understand where a sequence starts and where it ends. (See ""operator summary"" in the PDF Spec) Other ""important"" operators I can think of are the matrix, font and color operators. A cool advanced thing would be to show the current color or the font in a popup when hovering above such an operator.

To see a product with a similar purpose that is better than PDFDebugger, watch [this video|https://www.youtube.com/watch?v=g-QcU9B4qMc].

I'm not asking to implement a clone of that product (I don't use it, all I know is that video), but we at PDFBox really need something that makes PDF debugging easier. As an example of how the current PDFDebugger prevented me from finding a bug quickly, see PDFBOX-2401 and search for ""PDFDebugger"".

Prerequisites:
- java programming, especially the GUI components
- the ability to understand existing source code

Using external software components is possible (must have Apache License or a compatible one), but should be decided on a case-by-case basis, we don't want to get too big.

Development strategy: go from the easy to the difficult. The wished features are already sorted this way (mostly).

Get introduced: [download the source code with svn|https://pdfbox.apache.org/downloads.html#scm] and build it with maven. Run PDFDebugger and view some PDFs to see the components of a PDF. Start with the file of PDFBOX-2401. Read up something about the structure of PDF on the web or from the [PDF Specification|https://www.adobe.com/devnet/pdf/pdf_reference.html].

Mentor: Tilman Hausherr (European timezone, languages: german, english, french). To see the GSoC2014 project I mentored, go to PDFBOX-1915.","Duplicated Code, Long Method, , "
"   Move Method,Move Attribute,","Improve PDFDebugger (This is an idea for the [Google Summer of Code 2015|https://www.google-melange.com/])

Our command line utility PDFDebugger (part of the command line pdfbox-app get it [here|https://pdfbox.apache.org/downloads.html], read description [here|https://pdfbox.apache.org/commandline/], see the source code [here|https://svn.apache.org/viewvc/pdfbox/trunk/tools/src/main/java/org/apache/pdfbox/tools/PDFDebugger.java?view=markup&sortby=date]) needs some improvements:
   - hex view
   - view of non printable characters
   - âœ“ saving streams
   - binary copy & paste
   - âœ“ Create a status line that shows where we are in the tree. (Like in the Windows REGEDIT)
   - âœ“ Copy the current tree string into the clipboard (useful in discussions about details of a PDF)
   - âœ“ (Optional, not sure if easy) Jump to specific place in the tree by entering tree string
   - âœ“ ability to search in streams (very useful for content streams and meta data)
   - âœ“ show images that are streams
   - âœ“ show PDIndexed color lookup table, show the index value, the base and RGB color value sets when the mouse moves
   - âœ“ show PDSeparation color
   - âœ“ show PDDeviceN colors
   - optional, idea should be developed a bit: show meaningful explanation on some attributes, e.g. ""appearance stream"" when hovering over /AP
   - show font encodings and characters
   - âœ“ display flag bits (e.g. Annotation flags) in a way that is easy to understand. There are probably others, I assume that the main work needs to be done only once
   - edit attributes (should be possible to enter values as decimal, hex or binary)
   - edit streams, while keeping or changing the compression filter
   - save altered PDF 
   - âœ“ color mark of certain PDF operators, especially Q...q and text operators (BT...ET). Ideally, it should help the user understand the ""bracketing"" of these operators, i.e. understand where a sequence starts and where it ends. (See ""operator summary"" in the PDF Spec) Other ""important"" operators I can think of are the matrix, font and color operators. A cool advanced thing would be to show the current color or the font in a popup when hovering above such an operator.

To see a product with a similar purpose that is better than PDFDebugger, watch [this video|https://www.youtube.com/watch?v=g-QcU9B4qMc].

I'm not asking to implement a clone of that product (I don't use it, all I know is that video), but we at PDFBox really need something that makes PDF debugging easier. As an example of how the current PDFDebugger prevented me from finding a bug quickly, see PDFBOX-2401 and search for ""PDFDebugger"".

Prerequisites:
- java programming, especially the GUI components
- the ability to understand existing source code

Using external software components is possible (must have Apache License or a compatible one), but should be decided on a case-by-case basis, we don't want to get too big.

Development strategy: go from the easy to the difficult. The wished features are already sorted this way (mostly).

Get introduced: [download the source code with svn|https://pdfbox.apache.org/downloads.html#scm] and build it with maven. Run PDFDebugger and view some PDFs to see the components of a PDF. Start with the file of PDFBOX-2401. Read up something about the structure of PDF on the web or from the [PDF Specification|https://www.adobe.com/devnet/pdf/pdf_reference.html].

Mentor: Tilman Hausherr (European timezone, languages: german, english, french). To see the GSoC2014 project I mentored, go to PDFBOX-1915.",", , , "
"   Rename Method,","Improve PDFDebugger (This is an idea for the [Google Summer of Code 2015|https://www.google-melange.com/])

Our command line utility PDFDebugger (part of the command line pdfbox-app get it [here|https://pdfbox.apache.org/downloads.html], read description [here|https://pdfbox.apache.org/commandline/], see the source code [here|https://svn.apache.org/viewvc/pdfbox/trunk/tools/src/main/java/org/apache/pdfbox/tools/PDFDebugger.java?view=markup&sortby=date]) needs some improvements:
   - hex view
   - view of non printable characters
   - âœ“ saving streams
   - binary copy & paste
   - âœ“ Create a status line that shows where we are in the tree. (Like in the Windows REGEDIT)
   - âœ“ Copy the current tree string into the clipboard (useful in discussions about details of a PDF)
   - âœ“ (Optional, not sure if easy) Jump to specific place in the tree by entering tree string
   - âœ“ ability to search in streams (very useful for content streams and meta data)
   - âœ“ show images that are streams
   - âœ“ show PDIndexed color lookup table, show the index value, the base and RGB color value sets when the mouse moves
   - âœ“ show PDSeparation color
   - âœ“ show PDDeviceN colors
   - optional, idea should be developed a bit: show meaningful explanation on some attributes, e.g. ""appearance stream"" when hovering over /AP
   - show font encodings and characters
   - âœ“ display flag bits (e.g. Annotation flags) in a way that is easy to understand. There are probably others, I assume that the main work needs to be done only once
   - edit attributes (should be possible to enter values as decimal, hex or binary)
   - edit streams, while keeping or changing the compression filter
   - save altered PDF 
   - âœ“ color mark of certain PDF operators, especially Q...q and text operators (BT...ET). Ideally, it should help the user understand the ""bracketing"" of these operators, i.e. understand where a sequence starts and where it ends. (See ""operator summary"" in the PDF Spec) Other ""important"" operators I can think of are the matrix, font and color operators. A cool advanced thing would be to show the current color or the font in a popup when hovering above such an operator.

To see a product with a similar purpose that is better than PDFDebugger, watch [this video|https://www.youtube.com/watch?v=g-QcU9B4qMc].

I'm not asking to implement a clone of that product (I don't use it, all I know is that video), but we at PDFBox really need something that makes PDF debugging easier. As an example of how the current PDFDebugger prevented me from finding a bug quickly, see PDFBOX-2401 and search for ""PDFDebugger"".

Prerequisites:
- java programming, especially the GUI components
- the ability to understand existing source code

Using external software components is possible (must have Apache License or a compatible one), but should be decided on a case-by-case basis, we don't want to get too big.

Development strategy: go from the easy to the difficult. The wished features are already sorted this way (mostly).

Get introduced: [download the source code with svn|https://pdfbox.apache.org/downloads.html#scm] and build it with maven. Run PDFDebugger and view some PDFs to see the components of a PDF. Start with the file of PDFBOX-2401. Read up something about the structure of PDF on the web or from the [PDF Specification|https://www.adobe.com/devnet/pdf/pdf_reference.html].

Mentor: Tilman Hausherr (European timezone, languages: german, english, french). To see the GSoC2014 project I mentored, go to PDFBOX-1915.",", "
"   Rename Method,","Improve PDFDebugger (This is an idea for the [Google Summer of Code 2015|https://www.google-melange.com/])

Our command line utility PDFDebugger (part of the command line pdfbox-app get it [here|https://pdfbox.apache.org/downloads.html], read description [here|https://pdfbox.apache.org/commandline/], see the source code [here|https://svn.apache.org/viewvc/pdfbox/trunk/tools/src/main/java/org/apache/pdfbox/tools/PDFDebugger.java?view=markup&sortby=date]) needs some improvements:
   - hex view
   - view of non printable characters
   - âœ“ saving streams
   - binary copy & paste
   - âœ“ Create a status line that shows where we are in the tree. (Like in the Windows REGEDIT)
   - âœ“ Copy the current tree string into the clipboard (useful in discussions about details of a PDF)
   - âœ“ (Optional, not sure if easy) Jump to specific place in the tree by entering tree string
   - âœ“ ability to search in streams (very useful for content streams and meta data)
   - âœ“ show images that are streams
   - âœ“ show PDIndexed color lookup table, show the index value, the base and RGB color value sets when the mouse moves
   - âœ“ show PDSeparation color
   - âœ“ show PDDeviceN colors
   - optional, idea should be developed a bit: show meaningful explanation on some attributes, e.g. ""appearance stream"" when hovering over /AP
   - show font encodings and characters
   - âœ“ display flag bits (e.g. Annotation flags) in a way that is easy to understand. There are probably others, I assume that the main work needs to be done only once
   - edit attributes (should be possible to enter values as decimal, hex or binary)
   - edit streams, while keeping or changing the compression filter
   - save altered PDF 
   - âœ“ color mark of certain PDF operators, especially Q...q and text operators (BT...ET). Ideally, it should help the user understand the ""bracketing"" of these operators, i.e. understand where a sequence starts and where it ends. (See ""operator summary"" in the PDF Spec) Other ""important"" operators I can think of are the matrix, font and color operators. A cool advanced thing would be to show the current color or the font in a popup when hovering above such an operator.

To see a product with a similar purpose that is better than PDFDebugger, watch [this video|https://www.youtube.com/watch?v=g-QcU9B4qMc].

I'm not asking to implement a clone of that product (I don't use it, all I know is that video), but we at PDFBox really need something that makes PDF debugging easier. As an example of how the current PDFDebugger prevented me from finding a bug quickly, see PDFBOX-2401 and search for ""PDFDebugger"".

Prerequisites:
- java programming, especially the GUI components
- the ability to understand existing source code

Using external software components is possible (must have Apache License or a compatible one), but should be decided on a case-by-case basis, we don't want to get too big.

Development strategy: go from the easy to the difficult. The wished features are already sorted this way (mostly).

Get introduced: [download the source code with svn|https://pdfbox.apache.org/downloads.html#scm] and build it with maven. Run PDFDebugger and view some PDFs to see the components of a PDF. Start with the file of PDFBOX-2401. Read up something about the structure of PDF on the web or from the [PDF Specification|https://www.adobe.com/devnet/pdf/pdf_reference.html].

Mentor: Tilman Hausherr (European timezone, languages: german, english, french). To see the GSoC2014 project I mentored, go to PDFBOX-1915.",", "
"   Rename Method,","Improve PDFDebugger (This is an idea for the [Google Summer of Code 2015|https://www.google-melange.com/])

Our command line utility PDFDebugger (part of the command line pdfbox-app get it [here|https://pdfbox.apache.org/downloads.html], read description [here|https://pdfbox.apache.org/commandline/], see the source code [here|https://svn.apache.org/viewvc/pdfbox/trunk/tools/src/main/java/org/apache/pdfbox/tools/PDFDebugger.java?view=markup&sortby=date]) needs some improvements:
   - hex view
   - view of non printable characters
   - âœ“ saving streams
   - binary copy & paste
   - âœ“ Create a status line that shows where we are in the tree. (Like in the Windows REGEDIT)
   - âœ“ Copy the current tree string into the clipboard (useful in discussions about details of a PDF)
   - âœ“ (Optional, not sure if easy) Jump to specific place in the tree by entering tree string
   - âœ“ ability to search in streams (very useful for content streams and meta data)
   - âœ“ show images that are streams
   - âœ“ show PDIndexed color lookup table, show the index value, the base and RGB color value sets when the mouse moves
   - âœ“ show PDSeparation color
   - âœ“ show PDDeviceN colors
   - optional, idea should be developed a bit: show meaningful explanation on some attributes, e.g. ""appearance stream"" when hovering over /AP
   - show font encodings and characters
   - âœ“ display flag bits (e.g. Annotation flags) in a way that is easy to understand. There are probably others, I assume that the main work needs to be done only once
   - edit attributes (should be possible to enter values as decimal, hex or binary)
   - edit streams, while keeping or changing the compression filter
   - save altered PDF 
   - âœ“ color mark of certain PDF operators, especially Q...q and text operators (BT...ET). Ideally, it should help the user understand the ""bracketing"" of these operators, i.e. understand where a sequence starts and where it ends. (See ""operator summary"" in the PDF Spec) Other ""important"" operators I can think of are the matrix, font and color operators. A cool advanced thing would be to show the current color or the font in a popup when hovering above such an operator.

To see a product with a similar purpose that is better than PDFDebugger, watch [this video|https://www.youtube.com/watch?v=g-QcU9B4qMc].

I'm not asking to implement a clone of that product (I don't use it, all I know is that video), but we at PDFBox really need something that makes PDF debugging easier. As an example of how the current PDFDebugger prevented me from finding a bug quickly, see PDFBOX-2401 and search for ""PDFDebugger"".

Prerequisites:
- java programming, especially the GUI components
- the ability to understand existing source code

Using external software components is possible (must have Apache License or a compatible one), but should be decided on a case-by-case basis, we don't want to get too big.

Development strategy: go from the easy to the difficult. The wished features are already sorted this way (mostly).

Get introduced: [download the source code with svn|https://pdfbox.apache.org/downloads.html#scm] and build it with maven. Run PDFDebugger and view some PDFs to see the components of a PDF. Start with the file of PDFBOX-2401. Read up something about the structure of PDF on the web or from the [PDF Specification|https://www.adobe.com/devnet/pdf/pdf_reference.html].

Mentor: Tilman Hausherr (European timezone, languages: german, english, french). To see the GSoC2014 project I mentored, go to PDFBOX-1915.",", "
"   Rename Method,Extract Method,","Subset embedded TTF fonts Now that PDFBOX-922 is fixed, we have working TTF embedding. However, the entire font is embedded, which is rather large. We already have a TTFSubsetter class in FontBox, which is never used, so we should make use of it.","Duplicated Code, Long Method, , "
"   Rename Class,Inline Method,",Improve examples Our examples are in need of improvement before 2.0 is released. Many examples are using obsolete APIs and don't follow our coding conventions. Many are too simple or too complex.,", , "
"   Extract Method,Inline Method,",Improve examples Our examples are in need of improvement before 2.0 is released. Many examples are using obsolete APIs and don't follow our coding conventions. Many are too simple or too complex.,"Duplicated Code, Long Method, , , "
"   Rename Method,","Improve PDPageContentStream API The PDPageContentStream API uses some methods with incorrect and misleading names, and some unusual choices of parameters. These can be fairly easily cleaned up.",", "
"   Rename Method,Extract Method,","Improve PDPageContentStream API The PDPageContentStream API uses some methods with incorrect and misleading names, and some unusual choices of parameters. These can be fairly easily cleaned up.","Duplicated Code, Long Method, , "
"   Rename Method,","Improve PDPageContentStream API The PDPageContentStream API uses some methods with incorrect and misleading names, and some unusual choices of parameters. These can be fairly easily cleaned up.",", "
"   Rename Method,","Allow sharing of COS objects between different documents A number of users on the mailing list have asked about how to import pages from other PDFs as forms, our current solution is LayerUtility, which is depends on PDFCloneUtility. Both these classes are surprisingly complex for what should be a simple task.

The two main tasks which these classes perform is copying the page's COSStream and cloning every relevant COS object. However, there seems to be no real need to do any of this copying and cloning - there's nothing about any of the COS objects which is specific to a given document. While a COSStream can share the same backing file as the COSDocument, this isn't a problem for COSWriter, even then we need only make sure that an exception is thrown if a COSStream is used after its parent COSDocument is closed.

Note that there *is* one artificial dependency between COSDictionary and COSArrays and their parent COSDocument, that is that calling close() on the COSDocument clears the contents of all child COSDictionary and COSArrays. However, there's no need for this, it seems to have come about due to some long past confusion regarding how garbage collection works in Java - we all know that it's not necessary to set objects to null or clear lists when we are done with them.

I propose that we get rid of the unnecessary object and list clearing in COSDocument#close() and add some checks to COSStream to throw user-friendly exceptions when reading from a closed backing stream. This will allow us to directly share COS objects between different COSDocuments, allowing simple ""x = y"" copying and making LayerUtility and PDFCloneUtility unnecessary. Instead of:

{code}
COSStream pageStream = (COSStream)page.getStream().getCOSObject();
PDStream newStream = new PDStream(targetDoc, pageStream.getUnfilteredStream(), false);
PDFormXObject form = new PDFormXObject(newStream);

PDResources pageRes = page.getResources();
PDResources formRes = new PDResources();
PDFCloneUtility cloner = new PDFCloneUtility(document);
cloner.cloneMerge(pageRes, formRes);
form.setResources(formRes);
{code}

We could have:
{code}
PDFormXObject form = new PDFormXObject(page.getStream());
form.setResources(page.getResources());
{code}",", "
"   Move Class,Extract Method,","Provide easier access to AcroForm field tree The current implementation of the AcroForm field retrieval methods donâ€™t provide an easy access to get to all fields as 
 - one needs to retrieve the documents root fields
 - check if these are non-terminal fields
 - retrieve their childs
 - move on until all terminal fields have been retrieved

There should be a way to easier get access to all terminal fields  ","Duplicated Code, Long Method, , "
"   Move Method,Move Attribute,",Remove old parser After making the non-sequential parser the default parser (PDFBOX-2430) and enabling signing with the non-sequential parser it is time to remove the old one.,", , , "
"   Rename Class,Rename Method,Push Down Method,Push Down Attribute,",Remove old parser After making the non-sequential parser the default parser (PDFBOX-2430) and enabling signing with the non-sequential parser it is time to remove the old one.,", , , "
"   Rename Method,Extract Method,",Support TTC font files We need to support TrueType collections (TTC) files. It shouldn't be too difficult as they are simply a bundle of TTF files in a single container.,"Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Make it easier to work with RadioButton Groups The current implementation of Radio Buttons in PDFBox 2.0 should be improved by
- renaming getOptions() to make it clearer that this gets the potential export values (although the dictionary entry is called /Opt)
- make it easier to inspect the possible values one can set for the various individual buttons","Duplicated Code, Long Method, , "
"   Rename Class,Move And Rename Class,Extract Interface,Rename Method,Extract Method,Inline Method,","Overhaul font substitution The improved font substitution mechanisms in 2.0 are not quite sufficient to handle all PDFs. Specifically, CJK substitution and substitution of TTF in place of CFF fonts is not possible with the current design.

The CJK problems can be seen in PDFBOX-2509 and PDFBOX-2563, which does not solve the problem. Additional font API weaknesses can be found in PDFBOX-2578 and PDFBOX-2366. This meta-issue aims to address all of those sub-issues.

The current problems are:

- FontBox does not provide a generic font type, so we have handle TrueTypeFont, CFFFont, and Type1Font separately. This hinders cross-format substitution.
- ExternalFonts has no knowledge of the CIDSystemInfo which is necessary for CJK substitution
- FontProvider contains too much public logic which should be internal to PDFBox, e.g. substitution logic, this makes it brittle and means we won't be able to add additional logic after 2.0 is released, e.g. CJK substitution.
- Too much confusion about the role of ExternalFonts, particularly with regards to mapping of built-in fonts and the definition of substitute vs. fallback font.
- ExternalFonts is a black box: the user cannot tell whether the font returned is an exact match, or a last-resort fallback.
- Confusing font substitution API, users preferred having a flat file format
- PDSimpleFont#getEncoding() can return null for TTFs which use built-in encodings. This has caused a lot of bugs - there must be a better way.
- We still have some confusing names, for example a CustomEncoding is known as a ""built-in encoding"" in the spec.
- There is no fallback CFF font, we resort to AdobeBlank instead, which has no rendering.","Duplicated Code, Long Method, , , Large Class, "
"   Rename Method,","Align annotation and form public API The public API for annotation and form differs in 

- visibility for flag fields
- method naming conventions",", "
"   Rename Class,Rename Method,Extract Method,Inline Method,Move Attribute,","Simplify COSStream encoding and decoding Performance issues and memory usage issues surrounding streams are one of the few things blocking the release of 2.0 (see  PDFBOX-2301, PDFBOX-2882, PDFBOX-2883).

Though we've managed to reduce some of the memory used by RandomAccessBuffer and to take advantage of buffering of scratch files, we still have problems with the amount of memory which COSStream holds onto. Changes introduced in 2.0 have resulted in COSStreams having a very complex relationship with classes which hold a lot of memory in complex ways (e.g. the fields: tempBuffer, filteredBuffer, unfilteredBuffer, filteredStream, unFilteredStream, scratchFile). Access to scratch file pages in particular does not seem to be well regulated, especially with regards to multithreading (an avenue we'd at least like to leave open).

Given recent flux, I'm doubtful that we can ship the current API for COSStream w.r.t. RandomAccess without shipping performance issues or flaws which will be unfixable without breaking changes.

One of the recent changes to COSStream is that it now exposes a RandomAccess, this is so that PDFStreamParser can parse content streams (as well as other subclasses which handle xref and object streams). However, streams are fundamentally not random access - stream filters are sequential. While the consumer of a stream may wish to buffer the data (in memory or scratch) for random access, COSStream itself does not need to expose such an elaborate API - many pieces of gymnastics are performed inside COSStream to present this illusion, at significant cost. We should remove that.

But what about providing a RandomAccess for PDFStreamParser, PDFObjectStreamParser, and PDFXrefStreamParser? It turns out that those classes don't actually perform random I/O. They perform sequential I/O with a buffer for peek/unread.

We need to simplify to get 2.0 fast, lean, and maintainable. Here's what I think we should do:

1. Split the interfaces for sequential and random I/O
- Introduce a new SequentialSource interface for sequential I/O, with thin wrappers for RandomAccessRead and InputStream.
- BaseParser will use SequentialSource rather than RandomAccessRead (this will be inherited by PDFStreamParser, PDFObjectStreamParser, and PDFXrefStreamParser).
- COSParser will use RandomAccessRead and pass a SequentialSource wrapper to it's superclass, BaseParser.

2. Remove RandomAccess APIs from COSStream, expose only InputStream and OutputStream, as we used to do. We can pass an InputStream to PDFStreamParser using a wrapper which implements SequentialSource. This will remove tempBuffer, filteredBuffer, and unfilteredBuffer from COSStream, all of which hold memory.","Duplicated Code, Long Method, , , , "
"   Rename Method,","Remove COSStreamArray / SequenceRandomAccessRead This ties in with my COSStream simplification in PDFBOX-2893.

COSStreamArray is a troublesome abstraction, it's not a real COS object and it's the only COS object which can be generated _after_ parsing. Look at the implementation of COSStreamArray, most methods throw an exception because it's _not_ a COSStream - it violates the contact of the very thing it claims to be. Even PDPageContentStream has to use instanceof to ""peer through""  the abstraction of COSStreamArray.

There's no reason to have this class, other than to duck-tape flaws in 1.8's APIs, namely that PDPage#getStream() returns a PDStream and PDFStreamParser expects a PDStream, yet both of these may be arrays of streams.

We can fix this in 2.0 by getting rid of the erroneous PDPage#getStream() and by exposing the array of streams, rather than attempting to hide them. Hopefully this will also fix existing errors which may be lurking throughout the codebase (see first comment, below) which are associated with mistaking COSStreamArray for a COSStream. We can still provide an InputStream API which abstracts over the array of streams, because there's nothing wrong with that - so users can have the same simple and convenient experience.

An added benefit of doing this is that it will allow us to remove SequenceRandomAccessRead, a highly complex memory-holding class.",", "
"   Move Class,Push Down Method,Push Down Attribute,","Make FontMapper into a singleton interface As discussed on the dev mailing list, some users want to provide completely custom implementations of FontMapper. It seems that the easiest way to do this is to make the static FontMapper into a singleton which can be replaced entirely with a user's implementation.",", , , "
"   Rename Method,","Right now PDFBOX does not permit to sign multiple files while calling an external signing service. Since to sign a PDF you forced the implementation of the SignatureInterface interface, is not possible to prepare N hashes from N PDF files and then send them to a signing service that accepts multiple hashes with a single signon.
For example if I use an OTP signing service.

What would be nice to have is to separate the hash calculation from the signing. 

Instead to implement the Interface I would like to have something like this:

1) calculate hash from document with the new signature dictionary bytes
2) sign the hash
3) insert the signature into pdf

This way I could achieve to sign for example 100 pdf files calling the service once.
Right now must ask the user to sign in 100 times.

Thanks in advance.
Andrea.


 

",", "
"   Extract Method,Inline Method,","Cache glyph table to optimize concurrent access If several threads convert several pdf to png (one thread access to a single document at a time) they are a contention on a lock in GlythTable. Jstack shows that all threads are in state blocked on the synchronized block  in the getGlyph method. The lock is necessary, it's ok, but degrades performance.

This patch cache glyphs already read. 
With the patch PDFBOX-3080, the follow benchmark compare 1000 pdf conversions with 1, 8, and 50 threads.

|| Simulation|| PDF 2.0-SNAPSHOT || With this patch + PDFBOX3080 ||
|| 1000 conversions / 1 thread | 120 s | 71 s|
|| 1000 conversions / 8 threads | 76 s | 28 s|
|| 1000 conversions / 50 threads | 81 s | 33 s|","Duplicated Code, Long Method, , , "
"   Move And Rename Class,","Reduce amount of intermediate data and objects to reduce memory footprint/complexity The CFFParser holds a lot of intermediate data and produces a lot of objects to do so. The idea is to reduce the amount of such objects and data to reduce the memory footprint and the complexity.

- the class IndexData holds intermediate data creates byte array everytime when getBytes is called. I'm going to replace the class with a simple list to reduce the memory footprint and the complexity
- remove unused members of private classes
- create a list of strings instead of a list of byte arrays which is used to create those strings",", "
"   Move Method,Extract Method,","Reduce amount of intermediate data and objects to reduce memory footprint/complexity The CFFParser holds a lot of intermediate data and produces a lot of objects to do so. The idea is to reduce the amount of such objects and data to reduce the memory footprint and the complexity.

- the class IndexData holds intermediate data creates byte array everytime when getBytes is called. I'm going to replace the class with a simple list to reduce the memory footprint and the complexity
- remove unused members of private classes
- create a list of strings instead of a list of byte arrays which is used to create those strings","Duplicated Code, Long Method, , , "
"   Rename Method,","PDImageXObject.createFromFile should relies on header bytes PDImageXObject.createFromFile currently relies on file extension to select the correct factory.
Often, file extension is not set or not correct.
It should be better to use the first bytes.

I did something similar here if it can helps: https://github.com/sismics/docs/blob/master/docs-core/src/main/java/com/sismics/util/mime/MimeTypeUtil.java#L26",", "
"   Rename Method,","Rename structure element setter of PDOutlineItem While playing around with the library I stumbled over a naming inconsistency between the getter and setter of a {{PDOutlineItem}} structure element.

The getter is named {{getStructureElement}} whereas the setter is named {{setStructuredElement}} which is a bit confusing.",", "
"   Rename Method,Pull Up Method,","Refactor to allow tsa timestamping for visible signatures Put TSA stuff into SignatureBase, allow tsa parameter.",", Duplicated Code, "
"   Rename Class,Rename Method,",Move Glyph2D functionality into PDFont subclasses The Glyph2D classes perform some extra normalisation and substitution of glyph paths so that they are ready for final rendering. It would be better for this functionality to be part of the various PDFont subclasses so that it's easy to get access to the final glyph bounds. We've had [user requests for this|http://pdfbox-dev.markmail.org/search/?q=glyph2d#query:glyph2d+page:1+mid:ww4vslm4xnztxvol+state:results].,", "
"   Rename Method,","Add COSBoolean(false) as option in PDDocumentCatalog's getOpenAction Over on Tika, we've started allowing users to extract PDActions.  In our recent regression tests, we found a few new exceptions while trying to get the OpenAction from PDDocumentCatalog.

The easy one to fix is: 

{noformat}
java.io.IOException: Unknown OpenAction false
	at org.apache.pdfbox.pdmodel.PDDocumentCatalog.getOpenAction(PDDocumentCatalog.java:261)
	at org.apache.tika.parser.pdf.AbstractPDF2XHTML.startDocument(AbstractPDF2XHTML.java:460)

{noformat}

The object is a COSBoolean with value=false.

I'll open a separate issue for the other new (to us) exception.",", "
"   Rename Method,","Improve and refactor RemoveAllText example Refactor double code and include patterns, not just xobject forms when going through resources. This will be a better template for utilities that work on the content stream tokens, e.g.
https://stackoverflow.com/a/45259160/535646",", "
"   Rename Method,","Add Certificate Dictionary to seed value in signature field This dictionary is important as it gives the ability to put certificate constraints on a signature field, like if you want signatures that are signed by a specific issuer or authority to only be used in a field.

currently tested Issuer constraint and it worked, acrobat reader ignores other certificates and only allow the issuer given to sign the field. documentation is not complete waiting for the initial acceptance to complete.

new class PDSeedValueCertificate is added which refers to this certificate.
PDSeedValue is modified to add the new dictionary.
COSName is modified to add the new pdf names that are included in the dictionary.

reference for this dictionary can be found in PDF reference 1.7 section 12.7.4.5 table 235 page 457 in here http://www.adobe.com/content/dam/acom/en/devnet/pdf/PDF32000_2008.pdf
 or chapter 8 table 8.84 page 700 in here http://archimedespalimpsest.net/Documents/External/pdf_reference_1-7.pdf

and in here
https://www.adobe.com/devnet-docs/acrobatetk/tools/DigSig/Acrobat_DigitalSignatures_in_PDF.pdf

this is my first contribution, hope everything goes well.",", "
"   Rename Method,Extract Method,","Implement show text with positioning operator (TJ) Why: The TJ operator is required to properly implement text justification in unicode fonts. The word spacing operator (Tw) is not sufficient because of note[1] from the PDF specification.

Github user backslash47 has provided a basic implementation if that is of any help:
https://github.com/backslash47/pdfbox/commit/3c528295b16445e58dc9fe895f78384221452be2

Thanks,
Daniel.

[1] Note: Word spacing is applied to every occurrence of the single-byte character code 32 in a string when using a simple font or a composite font that defines code 32 as a single-byte code. It does not apply to occurrences of the byte value 32 in multiple-byte codes. 

Example code:

{code:java}
import java.io.InputStream;
import java.util.ArrayList;
import java.util.List;

import org.apache.pdfbox.pdmodel.PDDocument;
import org.apache.pdfbox.pdmodel.PDPage;
import org.apache.pdfbox.pdmodel.PDPageContentStream;
import org.apache.pdfbox.pdmodel.PDPageContentStream.AppendMode;
import org.apache.pdfbox.pdmodel.common.PDRectangle;
import org.apache.pdfbox.pdmodel.font.PDFont;
import org.apache.pdfbox.pdmodel.font.PDType0Font;
import org.apache.pdfbox.util.Matrix;

public class TextWithPositioningExample {

	public static void main(String[] args) throws Exception {
		doIt(""Hello World, this is a test!"", ""justify-example.pdf"");
	}

	/**
	 * This example shows how to justify a string using the showTextWithPositioning method.
	 * First only spaces are adjusted, and then every letter.
	 */
	public static void doIt(String message, String outfile) throws Exception {

		// the document
		try (PDDocument doc = new PDDocument();
			 InputStream is = PDDocument.class.getResourceAsStream(""/org/apache/pdfbox/resources/ttf/LiberationSans-Regular.ttf"")) {

			final float FONT_SIZE = 20.0f;

			// Page 1
			PDFont font = PDType0Font.load(doc, is, true);
			//PDFont font = PDType1Font.COURIER;
			PDPage page = new PDPage(PDRectangle.A4);
			doc.addPage(page);
			
			// Get the non-justified string width in text space units.
			float stringWidth = font.getStringWidth(message) * FONT_SIZE;
			
			// Get the string height in text space units.
			float stringHeight = font.getFontDescriptor().getFontBoundingBox().getHeight() * FONT_SIZE;
			
			// Get the width we have to justify in.
			PDRectangle pageSize = page.getMediaBox();
			
			PDPageContentStream contentStream = new PDPageContentStream(doc,
					page, AppendMode.OVERWRITE, false);
		
			contentStream.beginText();
			contentStream.setFont(font, FONT_SIZE);
			
			// Start at top of page.
			contentStream.setTextMatrix(Matrix.getTranslateInstance(0, pageSize.getHeight() - stringHeight / 1000f));
			
			// First show non-justified.
			contentStream.showText(message);
			
			// Move to next line.
			contentStream.setTextMatrix(Matrix.getTranslateInstance(0, pageSize.getHeight() - ((stringHeight / 1000f) * 2)));
			
			// Now show word justified.
			// The space we have to make up, in text space units.
			float justifyWidth = ((pageSize.getWidth() * 1000f) - (stringWidth));
			
			List<Object> text = new ArrayList<>();
			String[] parts = message.split(""\\s"");
			
			float spaceWidth = (justifyWidth / (parts.length - 1)) / FONT_SIZE;

			for (int i = 0; i < parts.length; i++) {
				if (i != 0) {
					text.add("" "");
					// Positive values move to the left, negative to the right.
					text.add(Float.valueOf(-spaceWidth));
				}
				text.add(parts[i]);
			}
			contentStream.showTextWithPositioning(text.toArray());
			contentStream.setTextMatrix(Matrix.getTranslateInstance(0, pageSize.getHeight() - ((stringHeight / 1000f) * 3)));
			
			// Now show letter justified.
			text = new ArrayList<>();
			justifyWidth = ((pageSize.getWidth() * 1000f) - stringWidth);
			float extraLetterWidth = (justifyWidth / (message.codePointCount(0, message.length()) - 1)) / FONT_SIZE;
			
			for (int i = 0; i < message.length();) {
				if (i != 0) {
					text.add(Float.valueOf(-extraLetterWidth));
				}
				
				text.add(String.valueOf(Character.toChars(message.codePointAt(i))));
				
				i += Character.charCount(message.codePointAt(i));
			}
			contentStream.showTextWithPositioning(text.toArray());;
			
			// Finish up.
			contentStream.endText();
			contentStream.close();

			doc.save(outfile);
		}
	}
}

{code}
","Duplicated Code, Long Method, , "
"   Rename Method,Move Method,","Into existing signature embedded signed timestamp for validation I would like to contribute a new example for embedded Timestamping.
 The Timestamp is beeing embedded into the existing Signature (which has so be prepared big enough for it). So that the document does not get changed. 
 This Step is a preparation for the LTV and includes some reorganisation for Validation-Purposes.
{code:java}
exec:java -X -Dexec.mainClass=""org.apache.pdfbox.examples.signature.validation.CreateEmbeddedValidation"" -Dexec.args=""${infile} -tsa ${tsa}""
{code}
CreateSignature has been changed to add SignatureOptions, where we can choose the size of the signature.",", , "
"   Rename Method,","Vertical text creation I needed to output vertical Japanese text, but was stymied by several limitations:
* No API to load a TTF as Identity-V encoding
* No support for 'vert' glyph substitution
* No support for vertical metrics ('vhea' and 'vmtx' tables are parsed but not used at all)

I have attached a series of patches that implement the above features. Highlights:
* The GSUB glyph substitution table is parsed (limitation: type 1 lookups only; this is sufficient for many features including 'vert'/'vrt2' vertical glyph substitution)
* Cmap lookup makes use of GSUB when features are enabled on a TTF
* 'vhea' and 'vmtx' metrics are applied to PDCIDFont when appropriate, and are embedded/subsetted correctly through the DW2/W2 CIDFont dictionary
* An API has been added for loading a TTF as a vertical font, setting Identity-V encoding and enabling 'vert'/'vrt2' substitution

Each patch could approximately be split out into a separate ticket, if desired.

Also attached is some sample code that exercises these patches and illustrates the effect of vertical glyph positioning. The sample output PDF is also attached.",", "
"   Extract Interface,Extract Method,","Vertical text creation I needed to output vertical Japanese text, but was stymied by several limitations:
* No API to load a TTF as Identity-V encoding
* No support for 'vert' glyph substitution
* No support for vertical metrics ('vhea' and 'vmtx' tables are parsed but not used at all)

I have attached a series of patches that implement the above features. Highlights:
* The GSUB glyph substitution table is parsed (limitation: type 1 lookups only; this is sufficient for many features including 'vert'/'vrt2' vertical glyph substitution)
* Cmap lookup makes use of GSUB when features are enabled on a TTF
* 'vhea' and 'vmtx' metrics are applied to PDCIDFont when appropriate, and are embedded/subsetted correctly through the DW2/W2 CIDFont dictionary
* An API has been added for loading a TTF as a vertical font, setting Identity-V encoding and enabling 'vert'/'vrt2' substitution

Each patch could approximately be split out into a separate ticket, if desired.

Also attached is some sample code that exercises these patches and illustrates the effect of vertical glyph positioning. The sample output PDF is also attached.","Duplicated Code, Long Method, , Large Class, "
"   Rename Method,","Improve html output Would like to improve the html output of pdf files for arabic rendering. The attached file has changes that should improve the way the -html option works. Now, output files are tagged with the .html extension. We also added <DOCTYPE> information as well as a <meta> tag which writes the appropriate encoding of the file. Cleaned up a lot of code from PDFTextStripper and PDFText2HTML which wasn't being used. Added ability to set the <title> tag of the html document to be the title given in the pdf document information if it exists. Otherwise it will guess a title from the beginning first lines of the file. ",", "
"   Rename Method,Extract Method,","[PATCH] Improvements for bitmap production (resolution and color depth) The attached patch improves the PDFToImage utility to support a resolution and color depth setting.
-resolution 300dpi creates a 300 dpi bitmap
-color rgba creates an RGB 24bit image with an 8bit alpha channel
-color rgb creates an RGB 24bit image
-color gray creates an 8bit gray image
-color indexed creates an 8bit color image with 256 indexed colors (that was what was produced before the change)
-color bilevel creates a 1bit bi-level image

The patch also fixes various Checkstyle issues in the classes I touched. I've tried to preserve method backwards-compatibility. However, I've changed PDFImageWriter.WriteImage to PDFImageWriter.writeImage. Method names should always start with a lower-case character. PageDrawer has similar problems which I didn't fix, however.

@Daniel: Please note that the changes makes the TestPDFToImage test case fail because I've changed the default settings for PDFToImage to ""rgb"" and the screen resolution (usually 96dpi). Before the settings were equivalent to ""-resolution 144 -color indexed"". Furthermore, the output formats that support a resolution value through standard ImageIO metadata will now contain the resolution information. That alone will make the reference PNGs different even if I had set the default settings to the old settings and effectively created the same images on the pixel level. You may need to decide on the settings for the test and recreate the reference bitmaps.","Duplicated Code, Long Method, , "
"   Rename Method,","Prepare PDFBox for release To prepare PDFBox for release we have to check on the build-process (directory structure, targets etc.) and to check that everything is according to the Apache policies.
",", "
"   Rename Method,","Prepare PDFBox for release To prepare PDFBox for release we have to check on the build-process (directory structure, targets etc.) and to check that everything is according to the Apache policies.
",", "
"   Rename Method,Extract Method,","Improved PDF Text Extraction that notes paragraph boundaries The current behavior of the org.apache.pdfbox.util.PDFTextStripper class is to ignore paragraph demarcation in the text.  It basically just renders each line of text as it discovers it, separating each line equally with the same line separator.

This makes it difficult to identify paragraph (or even page) starts and stops in the extracted text.  This is often necessary for text processing that needs to work with logical 'chunks' of text.  Further, rendering into other formats (such as HTML or XML) is facilitated by resolving the document into more discrete logical text chunks.

The request here is for improved text extraction that provides more discrete instrumentation of the parsing, allowing one to identify / tag paragraph starts and stops.


","Duplicated Code, Long Method, , "
"   Rename Method,","Fallback mechanism for broken CFF fonts PDFBOX-542 has not proven to be sufficiently foolproof for real-world PDF documents.

PDFBox should fallback to a Type1 font with appropriate warning message (this is the default behaviour with PDFBox 0.8.0 and earlier versions) when there is a problem parsing/interpreting a CFF (aka Type1C) font.",", "
"   Rename Method,Extract Method,Inline Method,","Support missing Text Rendering Modes when rendering a PDF Of the 7 different Text Rendering Modes only mode 0 (Fill Text) is correctly implemented. Mode 1 (Stroke Text) falls back to Mode 0 and the others are not implemented. I'm looking to implement the missing modes (at least some of them).

Before doing so I'm proposing a structural change to when rendering really occurs. Currently it's done within the PDxxxFont classes. I'd rather implement the (AWT) text output in PageDrawer (or helper classes within the same package) and use the font classes to return an AWT font by adding a getAwtFont method. Doing so we get a better separation between the PDF related stuff (PDxxx) and applications like PageDrawer. The current rendering specific code within the PDxxxFont classes can be retained for compatibility and marked deprecated at a later stage.

WDYT?","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Refactoring rendering-related classes/methods for extensibility Some of the classes/methods in the rendering area assume they have access to a Graphics2D object.

This assumption breaks when using the .Net version of PDFBox.  Some judicious refactoring permits PageDrawer to be extended in .Net and key methods to be overriden.

I am continuing this refactoring for better rendering support in .Net.

Andreas recently asked that code committed to SVN also be tied to a Jira issue -- a good idea really -- so I'm putting this in as an issue.","Duplicated Code, Long Method, , "
"   Pull Up Method,Extract Method,","Implementation of additional CMAP Formats for TrueType fonts Hi, 

According to the Apple's ""TrueType Reference Manual"" and the Microsoft's ""TrueType 1.0 Font Files Technical Specification"", there are several CMap formats.
Currently FontBox implements formats 0, 4 and 6.

In attachment you can find a patch which implements formats 2, 8, 10, 12 and 13 according to my understanding of the following links : 

http://www.microsoft.com/typography/otspec/cmap.htm (OpenType Specification)
http://developer.apple.com/fonts/TTRefMan/RM06/Chap6cmap.html

This patch includes changes proposal of the JIRA Issues :
PDFBOX-668, PDFBOX-670, PDFBOX-691 

I hope this patch will help you.
Regards,
Eric
","Duplicated Code, Long Method, , Duplicated Code, "
"   Rename Method,","Implementation of function types 0,2 and 3 to be used in Separation and DeviceN colorspaces I'm working on an implementation of the function types 0,2 and 3 which are used within Separation and DeviceN colorspaces. 
",", "
"   Move Method,Extract Method,Move Attribute,","Optional Content Groups (OCGs aka layers): initial support I'm currently writing some code for a client to create and manipulate optional content groups (OCGs aka layers) on the PD-layer. The goal is to do overlays in a way that the various combined pages each are on a separate layer that can be switched on and off. Please note that this is only a start and, for example, does not include conditional rendering of layers. I'll submit a patch for review in a couple of days. I've got the necessary PD layer code already and now have to upgrade the Overlay utility.","Duplicated Code, Long Method, , , , "
"   Move Class,Move Method,","Rename Struts Action 2 to Struts 2 As discussed on dev, we should rename all instances of Struts Action 2 to Struts 2.  This includes:
 - org.apache.struts.action2 to org.apache.struts2
 - Struts Action Framework 2 to Struts 2
 - SAF2 to Struts 2
 - All documentation and javadoc references


See also 
* [STR-2898] ""Rename Struts Action 1 to Struts 1""
",", , "
"   Move Method,Move Attribute,","Support action names with slashes To better support ReST-style wildcard schemes (""/article/123/view""), the default action mapper should support action names that contain slashes.  To do this, instead of assuming everything before the last slash is the namespace, the configured namespaces should be iterated over and explicitly matched.",", , , "
"   Rename Method,","Generate Taglib TLD from annotations/xdoclet tags Previously in WebWork 2, the taglib TLD was generated using XDoclet from XDoclet tags in the source code.  With the move to Maven 2 and Java 5, the ant tasks that performed this task have been removed and XDoclet doesn't work with Java 5 source.  Therefore, either we should migrate the process to a new XDoclet 2 plugin or using some Java 5 annotation processing tool.",", "
"   Rename Method,","Clean URL (ReST-style) support via action mapping Struts should support clean urls, making it possible to use them with the capability to fully customize the URL's where necessary.",", "
"   Rename Method,","Clean URL (ReST-style) support via action mapping Struts should support clean urls, making it possible to use them with the capability to fully customize the URL's where necessary.",", "
"   Extract Superclass,Rename Method,","Refactor ActionContextCleanUp and DispatcherFilter to have common logics in an abstract super class Refactor ActionContextCleanUp and DispatcherFilter to have common logics in an abstract super class

see http://forums.opensymphony.com/thread.jspa?threadID=48630&tstart=0 for more info.",", Duplicated Code, Large Class, "
"   Rename Method,Pull Up Method,Extract Method,Pull Up Attribute,","Refactor ActionContextCleanUp and DispatcherFilter to have common logics in an abstract super class Refactor ActionContextCleanUp and DispatcherFilter to have common logics in an abstract super class

see http://forums.opensymphony.com/thread.jspa?threadID=48630&tstart=0 for more info.","Duplicated Code, Long Method, , Duplicated Code, Duplicated Code, "
"   Rename Method,","Add a Dependency Injection library to wire internals of Struts framework and plugins At the core of Struts, we should have a simple dependency injection engine that wires together major Struts components and constants/settings.  This would allow us to support more powerful, self-configuring plugins and simplify our internal architecture, removing the need for most statics and factories.",", "
"   Rename Method,","Allow codebehind to select the starting directory point  Here's a simple patch to allow a new default ""root"" directory to be used when searching templates.  I created it because I had a wish to keep jsp and css in independent directories.

The configuration property is known as  ""struts.codebehind.pathPrefix"" and requires a trailing slash 
It's set to default to ""/"" in both the java and in the struts-plugin.xml 

as an addendum, per a request, i've also added the default constant for ""struts.codebehind.defaultPackage"" to help the codebehind plugin remain selfcontained.",", "
"   Push Down Method,Move Method,Push Down Attribute,Move Attribute,","Separate dojo-related tags into their own plugin The dojo-specific tags should be removed into their own plugin.  This includes simple wrapper tags like the datepicker, but also extensions like the anchor and div tags.

---------- Forwarded message ----------
From: Don Brown <mrdon@twdata.org>
Date: Dec 27, 2006 6:13 PM
Subject: [proposal] Tag reorganization
To: Struts Developers List <dev@struts.apache.org>

Ok, let's put this into a formal proposal. I propose that we extract
any Dojo-related tags and behaviors into a new tag library. This new
library could extend the old tags for the purposes of adding
Dojo-related attributes, however, it should be its own logical tag
library. We should keep the old themeable tags, but look to speed up
the simple theme, probably through a Java rendering engine.

The new Dojo-based tags will contain:
 - The date picker
 - The rich text editor
 - Any remote div-loading tags
 - Extensions of existing tags to add new Dojo-related attributes and
rendering. If it isn't possible to extend the existing tags/components
and add new parameters, then we'll make it so.
 - Dojo. We'll pull all the Dojo files out of core.

We will be left with the original, themeable tags. These tags will
still use the theme engines and we'll keep the xhtml theme, but write or
include a simple tooltip Javascript library to we don't need to import
all of Dojo. Ideally, I'd like to see the simple tags rendered with
Java for superior performance, but that part of the proposal can wait.
These tags will retain as much as backwards compatibility as possible,
with the exception of the missing Ajax theme. Any ajax functionality
that used DWR will remain in the old tags and be moved to the xhtml theme.

I'd like to move on this quickly and have it completed within two weeks.

Thoughts?

Don

----------

For more, see

* http://mail-archive.com/dev%40struts.apache.org/msg26550.html
* http://mail-archive.com/dev%40struts.apache.org/msg26635.html ",", , , , , "
"   Move Method,Move Attribute,","Separate dojo-related tags into their own plugin The dojo-specific tags should be removed into their own plugin.  This includes simple wrapper tags like the datepicker, but also extensions like the anchor and div tags.

---------- Forwarded message ----------
From: Don Brown <mrdon@twdata.org>
Date: Dec 27, 2006 6:13 PM
Subject: [proposal] Tag reorganization
To: Struts Developers List <dev@struts.apache.org>

Ok, let's put this into a formal proposal. I propose that we extract
any Dojo-related tags and behaviors into a new tag library. This new
library could extend the old tags for the purposes of adding
Dojo-related attributes, however, it should be its own logical tag
library. We should keep the old themeable tags, but look to speed up
the simple theme, probably through a Java rendering engine.

The new Dojo-based tags will contain:
 - The date picker
 - The rich text editor
 - Any remote div-loading tags
 - Extensions of existing tags to add new Dojo-related attributes and
rendering. If it isn't possible to extend the existing tags/components
and add new parameters, then we'll make it so.
 - Dojo. We'll pull all the Dojo files out of core.

We will be left with the original, themeable tags. These tags will
still use the theme engines and we'll keep the xhtml theme, but write or
include a simple tooltip Javascript library to we don't need to import
all of Dojo. Ideally, I'd like to see the simple tags rendered with
Java for superior performance, but that part of the proposal can wait.
These tags will retain as much as backwards compatibility as possible,
with the exception of the missing Ajax theme. Any ajax functionality
that used DWR will remain in the old tags and be moved to the xhtml theme.

I'd like to move on this quickly and have it completed within two weeks.

Thoughts?

Don

----------

For more, see

* http://mail-archive.com/dev%40struts.apache.org/msg26550.html
* http://mail-archive.com/dev%40struts.apache.org/msg26635.html ",", , , "
"   Rename Method,","Add highlight effect to ""targets"" for the ajax tags Add the following properties to bind, submit, and anchor:

""highlightColor""
""highlighDuration""

if highlightColor is specified, dojo.lfx.html.highlight() will be applied to the ""targets"" elements",", "
"   Rename Class,Rename Method,","Create JSONValidationInterceptor * Fix ajax tags to use the JSON generated by this interceptor to perform ajax validation
* Fix showcase example",", "
"   Push Down Method,Extract Method,","Tags that push values into the value stack should use the ""var"" attribute to specify the name of the variable * Deprecate id on tags that are using it
* Deprecate ""name"" on ""set"" tag","Duplicated Code, Long Method, , , "
"   Move Class,Rename Method,Move Method,Move Attribute,","Rework FilterDispatcher to be pluggable Currently, the majority of how Struts handles actions, static resources and passing the request down the filter chain is handled in FitlerDispatcher via protected methods. In order to change this functionality, plugins and applications must write their own filter and change their web.xml file to use it.

It would be nice if the FitlerDispatcher was completely pluggable and all of its operations were performed via well defined interfaces. Plugins could then inject different implementations of these interfaces allowing modifications to the main handling of the HTTP request and response.",", , , "
"   Move Method,Extract Method,","Rework FilterDispatcher to be pluggable Currently, the majority of how Struts handles actions, static resources and passing the request down the filter chain is handled in FitlerDispatcher via protected methods. In order to change this functionality, plugins and applications must write their own filter and change their web.xml file to use it.

It would be nice if the FitlerDispatcher was completely pluggable and all of its operations were performed via well defined interfaces. Plugins could then inject different implementations of these interfaces allowing modifications to the main handling of the HTTP request and response.","Duplicated Code, Long Method, , , "
"   Rename Method,",Add filename argument to acceptFile method in FileUploadInterceptor It would be quite usefull to extend the acceptFile  method in FileUploadInterceptor with an additional argument that passes in the original filename. This allows you to filter uploads based on its extension / name and not merely on the content type. ,", "
"   Rename Method,","Paramters not being set in JFreeChart Plugin <action name=""ch"" class=""org.someorg.chart.ChartAction"">
		<result name=""success"" type=""chart"">
			<param name=""width"">${width}</param>
			<param name=""height"">${height}</param>
		</result>
</action>

In the above case I am getting an IllegalArgumentException, although I have defined width and height in ChartAction


public class ChartAction extends ActionSupport
{

	private static final long serialVersionUID = -4845276888116145855L;
	
	private Integer width = 200;
	private Integer height = 400;


	public String execute() throws Exception
	{
		chart = new chart..
		return SUCCESS;
	}

	public Integer getWidth()
	{
		return width;
	}

	public void setWidth(Integer width)
	{
		this.width = width;
	}

	public Integer getHeight()
	{
		return height;
	}

	public void setHeight(Integer height)
	{
		this.height = height;
	}
}
",", "
"   Move Class,Move Attribute,","Anchor tag should be able to build URLs like the URL tag does Some refactoring needed, as for URLRenderer depends on URL class. A new interface URLProvider will be introduced, to decouple URLRenderer  from URL.",", , "
"   Rename Method,Extract Method,","Update ClassFinder and PackageBasedActionConfigBuilder to avoid loading unnecessary classes in package scan, avoid spurious warnings and other benefits By default, the PackageBasedActionConfigBuilder used in the Convention plugin uses the ClassFinder utility class from XWorks to look for potential action classes on the classpath.  The behavior of the ClassFinder is to scan every .class file on the classpath and attempt to load it.  Once all classes have been loaded, the PackageBasedActionConfigBuilder evaluates each loaded class to see if it is an action (is in the right package?  does it have the proper name?  does it implement Action?).

While this works fine, the process of loading EVERY classes on the classpath can produce a ton of error messages as the ClassFinder attempts to load classes that potentially have unsatisfied dependencies or other issues.  While these are non-fatal messages, they tend to initially confuse users (like me!).  See a couple examples I found Googling:

http://www.mail-archive.com/user@struts.apache.org/msg85317.html
http://www.mail-archive.com/google-appengine-java@googlegroups.com/msg00048.html
http://article.gmane.org/gmane.comp.jakarta.struts.user/168495

I can give more justifications why this is annoying if needed, but even better, here's a patch to fix the issue.  It's a little tricky because it requires a patch to ClassFinder (which is in xworks... hopefully you guys can deal with that?) and to PackageBasedActionConfigBuilder.  The patch to ClassFinder allows specifying an optional class name filter to prevent the load of classes.  The patch to PackageBasedActionConfigBuilder constructs an appropriate filter from the various package hints specified in the class.  All together, we get the exact same class loading behavior without all those annoying error messages.

Thoughts?
","Duplicated Code, Long Method, , "
"   Rename Method,","Several improvements - Method to check which HTTP methods are allowed using OPTIONS.
- Default implementation for POST and PUT if client sends request header ""expect: 100-continue"".
- Verification of action returns to allow String, HttpHeaders, Result or null.
- The default method ""index"" in ""RestActionSupport"" return Object instead String in order to allow for HttpHeaders or Result to be returned.
- Result processing is delayed after all interceptors have finished.
- New configuration parameter to show the processing time via log4j. Will show action processing time plus interceptors and result processing time (JSP, XML, ...).
- The new flow of result processing is:
    - The content is selected (model, exception or errors list)
    - Use HttpHeaders to apply the headers, etc.
    - If the result code is 304 the result is not processed.
    - If the result is a HttpHeaderResult, it is executed regardless of representation.
    - The result is always returned according to the requested representation.
    - If there are errors, it will look for a result named ""default-error"" to be able to configure the error page. It could be configured in the web.xml but the request representation would not be respected.
		",", "
"   Move Method,Extract Method,","Correct support for include/exclude parameters JSONResult.setIncludeProperties() got the algorithm right, but using regular expressions to match OGNL expressions is *painful*.  The attached patches add support for wildcard patterns to JSONResult and JSONInterceptor, and update JSONInterceptor.setIncludeProperties() to use the correct algorithm.

The patches also add include/exclude support to JSONCleaner, to support filtering on the input data.  (Configuration for JSONInterceptor and JSONResult only handle output filtering.)

There are also a couple of code clean-up changes, to eliminate code duplication.","Duplicated Code, Long Method, , , "
"   Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Bring Portlet 2.0 (JSR286) plugin from sandbox to trunk The the Portlet 2.0 plugin in sandbox is, besides Portlet event handling, feature complete and should usable in production environments. The event feature is isolated and may be delivered later

Steps needed:
1. sync changes from trunk plugin
2. call a vote
3. move plugin as a full replacement for existing portlet plugin, since it is downward compatible","Duplicated Code, Long Method, , , , , "
"   Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Bring Portlet 2.0 (JSR286) plugin from sandbox to trunk The the Portlet 2.0 plugin in sandbox is, besides Portlet event handling, feature complete and should usable in production environments. The event feature is isolated and may be delivered later

Steps needed:
1. sync changes from trunk plugin
2. call a vote
3. move plugin as a full replacement for existing portlet plugin, since it is downward compatible","Duplicated Code, Long Method, , , , , "
"   Rename Class,Push Down Method,Extract Method,Push Down Attribute,","Convert UrlHelper into a real bean When converted into a bean, dependency can be inject instead using ActionContext.getContext().getContainer();","Duplicated Code, Long Method, , , , "
"   Extract Method,Inline Method,","Update description of missing extension points in BeanSelectionProvider These two aliases [1] aren't documented

alias(UnknownHandlerManager.class, StrutsConstants.STRUTS_UNKNOWN_HANDLER_MANAGER, builder, props);
alias(UrlHelper.class, StrutsConstants.STRUTS_URL_HELPER, builder, props);

Review struts-default.xml if other aren't missing as well","Duplicated Code, Long Method, , , "
"   Rename Class,Move Class,Move Method,Extract Method,Inline Method,Move Attribute,","Convert FileManager into a bean Right now FileManager is mostly an utility class with static methods and static state. It should be converted into a bean to allow change implementation to meet specific Application Server's needs, like JBoss or WebLogic.","Duplicated Code, Long Method, , , , , "
"   Move Class,Move And Rename Class,Push Down Method,Move Method,Extract Method,Push Down Attribute,","Struts2 OSGi plugin does not work with GlassFish The OSGi plugin does not work with GlassFish where Felix is already included. When including in the Struts web application a bundle containing an activator class the following exception occurs:

java.lang.ClassCastException: myosgi.Activator cannot be cast to org.osgi.framework.BundleActivator

while when including a bundle with a class which implements BundleContextAware the following exception occurs:

Exception starting filter struts2 java.lang.LinkageError: loader constraint violation: loader (instance of org/apache/felix/framework/searchpolicy/ContentClassLoader) previously initiated loading for a different type with name ""org/osgi/framework/BundleContext""

It also seems that the plugin uses an old version of Felix which could be (at least partly) responsible for the errors since GlassFish uses a newer one.

More details on the errors also available in [this stackoverflow post|http://stackoverflow.com/questions/14200300/using-struts2-osgi-plugin-with-glassfish]","Duplicated Code, Long Method, , , , , "
"   Rename Class,Move Class,Push Down Method,Push Down Attribute,","Allow to use custom JSONwriter Throws when accessing to a private inner class in that method:

private void map(Map map, Method method) throws JSONException

May be pass when trying to access a private class. example source code should be:

{code:java}
private void map(Map map, Method method) throws JSONException {
    this.add(""{"");
...
    while (it.hasNext()) {
        Map.Entry entry = (Map.Entry) it.next();
        Object key = entry.getKey();
        String expr = null;
        if (this.buildExpr) {
            try {
                if (key == null) {
                    LOG.error(""Cannot build expression for null key in "" + this.exprStack);
                    continue;
                } else {
                    expr = this.expandExpr(key.toString());
                    if (this.shouldExcludeProperty(expr)) {
                        continue;
                    }
                    expr = this.setExprStack(expr);
                }
            }
            catch (Exception ex) {
                LOG.error(""Error: "" + ex.getLocalizedMessage());
                continue;
            }
        }
        if (hasData) {
            this.add(',');
        }
...
    this.add(""}"");
}
{code}",", , , "
"   Rename Method,Move Method,Push Down Attribute,",Define factories for all types supported by ObjectFactory 0,", , , "
"   Extract Method,Pull Up Attribute,","Allow ""class"" attribute on Struts tags In building a JSP, and working on web related things outside of the Java environment, there are lots of tags which all receive the ""class"" attribute. The Struts developer must _remember_ to call the attribute cssClass instead. Typing muscle memory drives me to half of the time typing ""class"" instead, which leads to HTML which reads, 'class=""class java.util.HashMap""'

Why not just allow ""class"" like the rest of the HTML world? Why do we need to be different? I have a billion things to remember when web developing, this shouldn't be one of them.

We don't even have to to deprecate or obsolete cssClass, just also allow ""class""... please!","Duplicated Code, Long Method, , Duplicated Code, "
"   Rename Class,Move Method,","Support for JDK 8 Lambdas Struts stumbles when encountering lambda expressions in JDK 8. Looks like org.objwectweb.asm dependency needs to be updated...

{code}
2014-05-18 10:21:41,111 ERROR (com.opensymphony.xwork2.util.finder.ClassFinder:38) - Unable to read class [jdk8test.actions.Lambda]
java.lang.ArrayIndexOutOfBoundsException: 52264
	at org.objectweb.asm.ClassReader.readClass(Unknown Source)
	at org.objectweb.asm.ClassReader.accept(Unknown Source)
	at org.objectweb.asm.ClassReader.accept(Unknown Source)
	at com.opensymphony.xwork2.util.finder.ClassFinder.readClassDef(ClassFinder.java:717)
	at com.opensymphony.xwork2.util.finder.ClassFinder.<init>(ClassFinder.java:112)
	at org.apache.struts2.convention.PackageBasedActionConfigBuilder.findActions(PackageBasedActionConfigBuilder.java:390)
	at org.apache.struts2.convention.PackageBasedActionConfigBuilder.buildActionConfigs(PackageBasedActionConfigBuilder.java:347)
	at org.apache.struts2.convention.ClasspathPackageProvider.loadPackages(ClasspathPackageProvider.java:53)
	at com.opensymphony.xwork2.config.impl.DefaultConfiguration.reloadContainer(DefaultConfiguration.java:268)
	at com.opensymphony.xwork2.config.ConfigurationManager.getConfiguration(ConfigurationManager.java:67)
	at org.apache.struts2.dispatcher.Dispatcher.init_PreloadConfiguration(Dispatcher.java:445)
	at org.apache.struts2.dispatcher.Dispatcher.init(Dispatcher.java:489)
	at org.apache.struts2.dispatcher.ng.InitOperations.initDispatcher(InitOperations.java:74)
	at org.apache.struts2.dispatcher.ng.filter.StrutsPrepareAndExecuteFilter.init(StrutsPrepareAndExecuteFilter.java:57)
	at org.apache.catalina.core.ApplicationFilterConfig.initFilter(ApplicationFilterConfig.java:281)
	at org.apache.catalina.core.ApplicationFilterConfig.getFilter(ApplicationFilterConfig.java:262)
	at org.apache.catalina.core.ApplicationFilterConfig.<init>(ApplicationFilterConfig.java:107)
	at org.apache.catalina.core.StandardContext.filterStart(StandardContext.java:4775)
	at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:5452)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:150)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1559)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1549)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:744)
{code}

Simple test case attached.

tar xvf jdk8test.tar
cd jdk8test
mvn tomcat7:run
http://localhost:8080/jdk8test

",", , "
"   Rename Method,Move Method,Inline Method,",Drop deprecated API 0,", , , "
"   Rename Method,",Drop deprecated API 0,", "
"   Move Class,Move Method,Move Attribute,",Drop deprecated API 0,", , , "
"   Rename Method,","Support latest stable AngularJS version in maven angularjs archetype - Upgrade maven archetype to latest stable version 1.4.2
- Improve example code",", "
"   Rename Class,Rename Method,","Support latest stable AngularJS version in maven angularjs archetype - Upgrade maven archetype to latest stable version 1.4.2
- Improve example code",", "
"   Move Method,Move Attribute,","Support latest stable AngularJS version in maven angularjs archetype - Upgrade maven archetype to latest stable version 1.4.2
- Improve example code",", , , "
"   Rename Method,",Add dedicated class to represent Http Parameters Right now {{parameters}} are represented by a {{Map}} and a lot of logic is duplicated. There is no way to check if given parameter was already evaluated.,", "
"   Rename Method,","ConversionErrorInterceptor to extend MethodFilterInterceptor Would it be possible to modify the {{ConversionErrorInterceptor}} to extend {{MethodFilterInterceptor}} so I can exclude the validation on certain methods?
 ie
{code:xml}
<interceptor-ref name=""conversionError"">
    <param name=""excludeMethods"">execute,cancel*</param>
</interceptor-ref>
{code}

It seems always to be called (needs to be like the validator/workflow)

I have noticed that if I there is a conversion error on a screen and I return with a redirectAction (and the action has a {{STORE}} and the destination action has a {{RETRIEVE}}) the conversion error shows on the destination action screen.

Although I still get in dev mode:

{noformat}
Error setting expression 'bean.weight' with value ['gggg', ]
{noformat}

as this comes from the params interceptor (and I do not want to exclude this on my cancel), I guess we will have to live with this.",", "
"   Rename Method,","ConversionErrorInterceptor to extend MethodFilterInterceptor Would it be possible to modify the {{ConversionErrorInterceptor}} to extend {{MethodFilterInterceptor}} so I can exclude the validation on certain methods?
 ie
{code:xml}
<interceptor-ref name=""conversionError"">
    <param name=""excludeMethods"">execute,cancel*</param>
</interceptor-ref>
{code}

It seems always to be called (needs to be like the validator/workflow)

I have noticed that if I there is a conversion error on a screen and I return with a redirectAction (and the action has a {{STORE}} and the destination action has a {{RETRIEVE}}) the conversion error shows on the destination action screen.

Although I still get in dev mode:

{noformat}
Error setting expression 'bean.weight' with value ['gggg', ]
{noformat}

as this comes from the params interceptor (and I do not want to exclude this on my cancel), I guess we will have to live with this.",", "
"   Move Class,Move Method,Move Attribute,",Merge two existing I18NInterceptors into one Both existing {{I18NInterceptor}} s should be merged into one,", , , "
"   Rename Method,","<s:text/> tag should not evaluate defaultMessage against a ValueStack by default Right now {{<s:text/>}} tag will perform evaluation of a {{defaultMessage}} against a ValueStack by default. In most cases the {{defaultMessage}} is set to value of {{name}} attribute and can be easily used wrong by a developer. Evaluation must be performed only on purpose.

This change affects also {{<s:label/>}} tag and {{label}} attribute of all {{UIBean}}s",", "
"   Rename Method,",HttpParameters should behave like a Map 0,", "
"   Rename Method,Extract Method,","AnnotationWorkflowInterceptor should supports non-public annotated methods {code:java}
@Before
protected String prepare(){
    //TODO
    return null;
}
{code}
[https://github.com/apache/struts/blob/master/core/src/main/java/com/opensymphony/xwork2/interceptor/annotations/AnnotationWorkflowInterceptor.java#L115]
{code:java}
List<Method> methods = new ArrayList<>(AnnotationUtils.getAnnotatedMethods(action.getClass(), Before.class));
{code}
[https://github.com/apache/struts/blob/master/core/src/main/java/com/opensymphony/xwork2/util/AnnotationUtils.java#L123]
{code:java}
for (Method m : clazz.getMethods()) 
{code}
clazz.getMethods() only return public methods, so method ""prepare"" will be excluded, and protected modifier is a good practice for intercept method.We should improve AnnotationUtils.getAnnotatedMethods() to return all methods. Perhaps use an ConcurrentHashMap as cache is much better.","Duplicated Code, Long Method, , "
"   Rename Method,","Buffer/Flush behaviour in FreemarkerResult Scenario: the application use freemarker with a {{TemplateExceptionHandler.RETHROW_HANDLER}} policy, but occasionally needs to produce large XML (20~200Mb) and goes out of memory.

In [FreemarkerResult|http://grepcode.com/file/repo1.maven.org/maven2/org.apache.struts/struts2-core/2.5-BETA1/org/apache/struts2/views/freemarker/FreemarkerResult.java#191] there are two possible behaviours (line 191):

 * *Buffer-behaviour*: the whole template is processed and if everything is OK it is flushed to the output, otherwise an exception is thrown and handled at higher level before any output has been sent. This is intended to be used when {{TemplateExceptionHandler.RETHROW_HANDLER}} is active
* *Flush-behaviour*: template is processed and flushed according to freemarker library policies, used with any other {{TemplateExceptionHandler}}

Since {{TemplateExceptionHandler}} cannot be switched for a given request (it is a global configuration embedded in {{FreemarkerManager}}) there is no way to force a Flush-behaviour. (you can only force a Buffer-behaviour using {{isWriteIfCompleted}})

I implemented a more flexible solution that let you force the behaviour in both ways:

{code:title=FreemarkerResult.java|borderStyle=solid}
    final boolean willUsebufferedWriter;
    if (useBufferedWriter != null){
        willUsebufferedWriter = useBufferedWriter;
    }else{
        willUsebufferedWriter = configuration.getTemplateExceptionHandler() == TemplateExceptionHandler.RETHROW_HANDLER;
    }
                
    if (willUsebufferedWriter){
    ...
    }else{
    ...
    }       
{code}

where {{useBufferedWriter}} is a parameter that can be modified per request

{code}
<result type=""freemarker"">
    <param name=""location"">big_feed.ftl</param>
    <param name=""contentType"">text/xml</param>
    <param name=""useBufferedWriter"">false</param>
</result>
{code}


",", "
"   Move Class,Extract Superclass,Move Method,Move Attribute,","DefaultLocalizedTextProvider refactoring Now that DefaultLocalizedTextProvider is a bean, would it be possible to refactor the code to allow an override so we can change the default behavior on the package searching for the resource.properties files.  Currently is does an extensive search up the class interfaces first which on busy screens slows things down and is an unnecessary overhead. 

ie As I have migrated from struts1 my main UI resources are in the default ApplicationResources.properties file and are shared across struts ui classes in various packages. I would not want to duplicate the .properties entries for maintenance etc.
I would then want to search the default ApplicationResources.properties first and then package class (for validation messages) and then possibly up the interfaces (which would not make sense for me as I do not use this logic).

Discussion related to this
http://markmail.org/message/v2oc6c35swfwzwid",", , , Duplicated Code, Large Class, "
"   Rename Method,",Fallback to ActionContext if container is null in ActionSupport RIght now an action extending {{ActionSupport}} cannot be created manually as it requires {{Container}} to be injected.,", "
"   Move Class,Move And Rename Class,","Asynchronous action method User will be able to return java.util.concurrent.Callable<String> in their actions. Struts when sees such result, runs resultCode = result.call(); in it's own managed thread pool but exits from servlet's main thread with a null result, i.e. gives back main thread to container and leaves response open for concurrent processing. When resultCode = result.call(); returned, Struts calls javax.servlet.AsyncContext.dispatch() and resumes request processing within a container's thread servlet to generate the appropriate result for user according to resultCode.

This adds better support for SLS (Short request processing, Long action execution, Short response processing) via Servlet 3's Async API.

Support of other cases like SSL (e.g. a download server) or LLL(e.g. a video converter server) is still open.",", "
"   Rename Method,","Debug tag should not display anything when not in dev mode I noticed that the debug tag displays the content of the value stack independently of the value of devMode.

I wonder if it would not be more secure to do not display anything if devMode=false.

I can imagine a developer forgetting to remove such kind of debug tags before the app goes to production. Making it silent in production mode would reduce the risk to display sensitive data.",", "
"   Rename Class,Rename Method,","Refactor Configuration classes The Configuration classes (see com\opensymphony\webwork\config\*.java) are poorly designed and should be refactored.

Suggestions:

1) Are there XWork classes we can use? Seems like the XWork configs should be flexible enough to say something like ""here are my config files, use them..."".

2) Move webwork.config.XMLActionConfiguration to com.opensymphony.webwork.config.XMLActionConfiguration -- or use the XWork XML config stuff (seems like it should be taken care of in there?). I don't know why that class is in the webwork.config (old) package when it's pretty integral to everything.

3) Make Configuration abstract and fold in DefaultConfiguration into it. There is no reason to have all the *Impl(..) methods when the class should just be abstract anyway. All the code used in DefaultConfiguration can be easily moved into Configuration.

4) Remove get(String) method that returns an object. Is this even really used? We should just stipulate that a property is a String only.
",", "
"   Rename Method,","Implement new Aware interfaces that are using withXxxx pattern instead of setters In matter of security I wonder if we should stop using setters in internal API. Like in {{SessionAware}} interface we use {{setSession()}} and each actions must implement this method. Then we have a logic to avoid mapping incoming values to {{setSession()}} to permit injecting values into Session.

Instead of {{setSession()}} we can use {{withSession()}} or {{applySession()}} - the same can be applied to any *Aware interface.",", "
"   Rename Method,","Implement new Aware interfaces that are using withXxxx pattern instead of setters In matter of security I wonder if we should stop using setters in internal API. Like in {{SessionAware}} interface we use {{setSession()}} and each actions must implement this method. Then we have a logic to avoid mapping incoming values to {{setSession()}} to permit injecting values into Session.

Instead of {{setSession()}} we can use {{withSession()}} or {{applySession()}} - the same can be applied to any *Aware interface.",", "
"   Rename Method,","Add hlog number metric in regionserver Add hlog number metric in regionserver. 

We can use this metric to alert about memstore flush because of too many hlogs.
",", "
"   Rename Method,","Stronger validation of key unwrapping In EncryptionUtil#unwrapKey we use a CRC32 to validate the successful unwrapping of a data key. I chose a CRC32 to limit overhead. There is only a 1 in 2^32 chance of a random collision, low enough to be extremely unlikely. However, I was talking with my colleague Jerry Chen today about this. A cryptographic hash would lower the probability to essentially zero and we are only wrapping data keys once per HColumnDescriptor and once per HFile, saving a few bytes here and there only really. Might as well use the SHA of the data key and in addition consider running AES in GCM mode to cover that hash as additional authenticated data.",", "
"   Move Method,Move Attribute,","Regionservers should report detailed health to master; master should flag troubled regionservers in UI Regionservers should report detailed health to master. The master should flag troubled regionservers in the UI.

The concern at the moment is primarily heap. Regionservers should report used, committed, and max heap metrics in the periodic report. The master should flag in the regionserver list on /master.jsp those regionservers where available heap is below a configurable threshold. 
",", , , "
"   Rename Method,","Add read log size per second metrics for replication source The current metrics of replication source contain logEditsReadRate, shippedBatchesRate, etc, which could indicate how fast the data replicated to peer cluster to some extent. However, it is not clear enough to know how many bytes replicating to peer cluster from these metrics. In production environment, it may be important to know the size of replicating data per second because the services may be affected if the network become busy.",", "
"   Move Class,Pull Up Method,","Improve determinism and debugability of TestAccessController Separate grant and revoke API invocations to static helper methods in SecureTestUtils. Wait for permissions cache updates using a Predicate. Log the API calls, state checks, and waits.",", Duplicated Code, "
"   Move Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","refactor AsyncProcess AsyncProcess currently has two patterns of usage, one from HTable flush w/o callback and with reuse, and one from HCM/HTable batch call, with callback and w/o reuse. In the former case (but not the latter), it also does some throttling of actions on initial submit call, limiting the number of outstanding actions per server.
The latter case is relatively straightforward. The former appears to be error prone due to reuse - if, as javadoc claims should be safe, multiple submit calls are performed without waiting for the async part of the previous call to finish, fields like hasError become ambiguous and can be used for the wrong call; callback for success/failure is called based on ""original index"" of an action in submitted list, but with only one callback supplied to AP in ctor it's not clear to which submit call the index belongs, if several are outstanding.

I was going to add support for HBASE-10070 to AP, and found that it might be difficult to do cleanly.

It would be nice to normalize AP usage patterns; in particular, separate the ""global"" part (load tracking) from per-submit-call part.
Per-submit part can more conveniently track stuff like initialActions, mapping of indexes and retry information, that is currently passed around the method calls.
-I am not sure yet, but maybe sending of the original index to server in ""ClientProtos.MultiAction"" can also be avoided.- Cannot be avoided because the API to server doesn't have one-to-one correspondence between requests and responses in an individual call to multi (retries/rearrangement have nothing to do with it)
","Duplicated Code, Long Method, , , , , "
"   Rename Method,","Auto detect data block encoding in HFileOutputFormat Currently, one has to specify the data block encoding of the table explicitly using the config parameter ""hbase.mapreduce.hfileoutputformat.datablock.encoding"" when doing a bulkload load. This option is easily missed, not documented and also works differently than compression, block size and bloom filter type, which are auto detected. 

The solution would be to add support to auto detect datablock encoding similar to other parameters. 

The current patch does the following:
1. Automatically detects datablock encoding in HFileOutputFormat.
2. Keeps the legacy option of manually specifying the datablock encoding
around as a method to override auto detections.
3. Moves string conf parsing to the start of the program so that it fails
fast during starting up instead of failing during record writes. It also
makes the internals of the program type safe.
4. Adds missing doc strings and unit tests for code serializing and
deserializing config paramerters for bloom filer type, block size and
datablock encoding.",", "
"   Rename Method,","refactor deferred-log-flush/Durability related interface/code/naming to align with changed semantic of the new write thread model By the new write thread model introduced by [HBASE-8755|https://issues.apache.org/jira/browse/HBASE-8755], some deferred-log-flush/Durability API/code/names should be change accordingly:
1. no timer-triggered deferred-log-flush since flush is always done by async threads, so configuration 'hbase.regionserver.optionallogflushinterval' is no longer needed
2. the async writer-syncer-notifier threads will always be triggered implicitly, this semantic is that it always holds that 'hbase.regionserver.optionallogflushinterval' > 0, so deferredLogSyncDisabled in HRegion.java which affects durability behavior should always be false
3. what HTableDescriptor.isDeferredLogFlush really means is the write can return without waiting for the sync is done, so the interface name should be changed to isAsyncLogFlush/setAsyncLogFlush to reflect their real meaning",", "
"   Rename Method,","RegionServer graceful stop / decommissioning Right now, we have a weird way of node decommissioning / graceful stop, which is a graceful_stop.sh bash script, and a region_mover ruby script, and some draining server support which you have to manually write to a znode (really!). Also draining servers is only partially supported in LB operations (LB does take that into account for roundRobin assignment, but not for normal balance) 
See 
http://hbase.apache.org/book/node.management.html and HBASE-3071

I think we should support graceful stop as a first class citizen. Thinking about it, it seems that the difference between regionserver stop and graceful stop is that regionserver stop will close the regions, but the master will only assign them after the znode is deleted. 

In the new master design (or even before), if we allow RS to be able to close regions on its own (without master initiating it), then graceful stop becomes regular stop. The RS already closes the regions cleanly, and will reject new region assignments, so that we don't need much of the balancer or draining server trickery. 

This ties into the new master/AM redesign (HBASE-5487), but still deserves it's own jira. Let's use this to brainstorm on the design. 
",", "
"   Rename Method,","Improvements to the import flow Following improvements can be made to the Import logic

a) Make the import extensible (i.e., remove the filter from being a static member of Import and make it an instance variable of the mapper, make the mappers or variables of interest protected. )

b) Make sure that the Import calls filterRowKey method of the filter (Useful if we want to filter the data of an organization based on the row key or using filters like PrefixFilter which filter the data in filterRowKey method rather than the filterKeyValue method). The existing test case in TestImportExport#testWithFilter works with this assumption but is so far successful because there is only one row inserted into the table.

c) Provide an option to specify the durability during the import (Specifying the Durability as SKIP_WAL would improve the performance of restore considerably.) [~lhofhansl] suggested that this should be a parameter to the import.

d) Some minor refactoring to avoid building a comma separated string for the filter args.",", "
"   Move Class,Extract Interface,Move Method,Extract Method,Move Attribute,","HConnection interface is public but is used internally, and contains a bunch of methods HConnection has too many methods for a public interface, and some of these should not be public.
It is used extensively for internal purposes, so we keep adding methods to it that may not make sense for public interface.

The idea is to create a separate internal interface inheriting HConnection, copy some methods to it and deprecate them on HConnection. New methods for internal use would be added to new interface; the deprecated methods would eventually be removed from public interface.","Duplicated Code, Long Method, , , , Large Class, "
"   Rename Method,Move Attribute,","Refactor PerformanceEvaluation tool PerfEval is kind of a mess. It's painful to add new features because the test options are itemized and passed as parameters to internal methods. Serialization is hand-rolled and tedious. Ensuring support for mapreduce mode is a chore because of it.

This patch refactors the tool. Options are now passed around to methods and such as a POJO instead of one-by-one. Get rid of accessors that don't help anyone. On the mapreduce side, serialization is now handled using json (jackson is a dependency anyway) instead of the hand-rolled regex we used before. Also do away with custom InputSplit and FileFormat, instead using Text and NLineInputFormat. On the local mode side, combine 1 client and N clients into the same implementation. That implementation now uses an ExecutorService, so we can later decouple number of client workers from number of client tasks. Finally, drop a bunch of confusing local state, instead use the new TestOptions POJO as a parameter to static methods.",", , "
"   Move Class,Move Method,Extract Method,Inline Method,Move Attribute,","HBase REST xml/json improvements I've begun work on creating a REST based interface for HBase that can use both JSON and XML and would be extensible enough to add new formats down the road. I'm at a point with this where I would like to submit it for review and to get feedback as I continue to work towards new features.

Attached to this issue you will find the patch for the changes to this point along with a necessary jar file for the JSON serialization. Also below you will find my notes on how to use what is finished with the interface to this point.

This patch is based off of jira issues: 
HBASE-814 and HBASE-815

I am interested on gaining feedback on:
-what you guys think works
-what doesn't work for the project
-anything that may need to be added
-code style
-anything else...


Finished components:
-framework around parsing json/xml input
-framework around serialzing xml/json output
-changes to exception handing
-changes to the response object to better handle the serializing of output data
-table CRUD calls
-Full table fetching
-creating/fetching scanners

TODO:
-fix up the filtering with scanners
-row insert/delete operations
-individual row fetching
-cell fetching interface
-scanner use interface


Here are the wiki(ish) notes for what is done to this point:
REST Service for HBASE Notes:

GET / 
-retrieves a list of all the tables with their meta data in HBase
curl -v -H ""Accept: text/xml"" -X GET -T - http://localhost:60050/

curl -v -H ""Accept: application/json"" -X GET -T - http://localhost:60050/

POST / 
-Create a table
curl -H ""Content-Type: text/xml"" -H ""Accept: text/xml"" -v -X POST -T - http://localhost:60050/newTable
<table>
<name>test14</name>
<columnfamilies>
<columnfamily>
<name>subscription</name>
<max-versions>2</max-versions>
<compression>NONE</compression>
<in-memory>false</in-memory>
<block-cache>true</block-cache>
</columnfamily>
</columnfamilies>
</table>

Response:
<status><code>200</code><message>success</message></status>

JSON:
curl -H ""Content-Type: application/json"" -H ""Accept: application/json"" -v -X POST -T - http://localhost:60050/newTable
{""name"":""test5"", ""column_families"":[{
""name"":""columnfam1"",
""bloomfilter"":true,
""time_to_live"":10,
""in_memory"":false,
""max_versions"":2,
""compression"":"""", 
""max_value_length"":50,
""block_cache_enabled"":true
}
]}

*NOTE* this is an enum defined in class HColumnDescriptor.CompressionType

GET /[table_name]
-returns all records for the table
curl -v -H ""Accept: text/xml"" -X GET -T - http://localhost:60050/tablename
curl -v -H ""Accept: application/json"" -X GET -T - http://localhost:60050/tablename

GET /[table_name]

-Parameter Action 
metadata - returns the metadata for this table.
regions - returns the regions for this table

curl -v -H ""Accept: text/xml"" -X GET -T - http://localhost:60050/pricing1?action=metadata


Update Table
PUT /[table_name]
-updates a table 
curl -v -H ""Content-Type: text/xml"" -H ""Accept: text/xml"" -X PUT -T - http://localhost:60050/pricing1
<columnfamilies>
<columnfamily>
<name>subscription</name>
<max-versions>3</max-versions>
<compression>NONE</compression>
<in-memory>false</in-memory>
<block-cache>true</block-cache>
</columnfamily>
<columnfamily>
<name>subscription1</name>
<max-versions>3</max-versions>
<compression>NONE</compression>
<in-memory>false</in-memory>
<block-cache>true</block-cache>
</columnfamily>
</columnfamilies>

curl -v -H ""Content-Type: application/json"" -H ""Accept: application/json"" -X PUT -T - http://localhost:60050/pricing1
{""column_families"":[{
""name"":""columnfam1"",
""bloomfilter"":true,
""time_to_live"":10,
""in_memory"":false,
""max_versions"":2,
""compression"":"""", 
""max_value_length"":50,
""block_cache_enabled"":true
}, 
{
""name"":""columnfam2"",
""bloomfilter"":true,
""time_to_live"":10,
""in_memory"":false,
""max_versions"":2,
""compression"":"""", 
""max_value_length"":50,
""block_cache_enabled"":true
}
]}

Delete Table
curl -v -H ""Content-Type: text/xml"" -H ""Accept: text/xml"" -X DELETE -T - http://localhost:60050/TEST16


creating a scanner
curl -v -H ""Content-Type: application/json"" -H ""Accept: application/json"" -X POST -T - http://localhost:60050/TEST16?action=newscanner

//TODO fix up the scanner filters.

response:
xml:
<scanner>
<id>
2
</id>
</scanner>

json:
{""id"":1}

Using a scanner
curl -v -H ""Content-Type: application/json"" -H ""Accept: application/json"" -X POST -T - ""http://localhost:60050/TEST16?action=scan&scannerId=<scannerID>&numrows=<num rows to return>""



This would be my first submission to an open source project of this size, so please, give it to me rough. =)

Thanks.
","Duplicated Code, Long Method, , , , , "
"   Rename Method,Extract Method,Inline Method,","DBE encode path improvements Here 1st we write KVs (Cells) into a buffer and then passed to DBE encoder. Encoder again reads kvs one by one from the buffer and encodes and creates a new buffer.
There is no need to have this model now. Previously we had option of no encode in disk and encode only in cache. At that time the read buffer from a HFile block was passed to this and encodes.
So encode cell by cell can be done now. Making this change will need us to have a NoOp DBE impl which just do the write of a cell as it is with out any encoding.
","Duplicated Code, Long Method, , , "
"   Rename Class,Extract Superclass,","Extend ByteRange to create Mutable and Immutable ByteRange We would need APIs that would 
setLimit(int limit)
getLimt()
asReadOnly()
These APIs would help in implementations that have Buffers offheap (for now BRs backed by DBB).
If anything more is needed could be added when needed.",", Duplicated Code, Large Class, "
"   Extract Method,Pull Up Attribute,","Control number of regions assigned to backup masters By default, a backup master is treated just like another regionserver. So it can host as many regions as other regionserver does. When the backup master becomes the active one, region balancer needs to move those user regions on this master to other region servers. To minimize the impact, it's better not to assign too many regions on backup masters. It may not be good to leave the backup masters idle and not host any region either.

We should make this adjustable so that users can control how many regions to assign to each backup master.","Duplicated Code, Long Method, , Duplicated Code, "
"   Rename Method,","Support visibility expressions on Deletes Accumulo can specify visibility expressions for delete markers. During compaction the cells covered by the tombstone are determined in part by matching the visibility expression. This is useful for the use case of data set coalescing, where entries from multiple data sets carrying different labels are combined into one common large table. Later, a subset of entries can be conveniently removed using visibility expressions.

Currently doing the same in HBase would only be possible with a custom coprocessor. Otherwise, a Delete will affect all cells covered by the tombstone regardless of any visibility expression scoping. This is correct behavior in that no data spill is possible, but certainly could be surprising, and is only meant to be transitional. We decided not to support visibility expressions on Deletes to control the complexity of the initial implementation.",", "
"   Rename Class,Extract Method,",Extend traces through FSHLog#sync Changes introduced in HBASE-8755 decouple wal append from wal sync. A gap was left in the tracing of these requests. I believe this means are spans are decoupled from the work happening over on HDFS-5274. This ticket is to close the air-gap between threads.,"Duplicated Code, Long Method, , "
"   Rename Method,","Remove TimeoutMontior With HBASE-8002, the TimeoutMonitor is disabled by default. Lately, we haven't see much problem of region assignments during integration testing with CM. I was thinking it may be time to remove the timeout monitor now?",", "
"   Extract Superclass,Move Method,Extract Method,Move Attribute,",Add mapred.TableSnapshotInputFormat We should have feature parity between mapreduce and mapred implementations. This is important for Hive.,"Duplicated Code, Long Method, , , , Duplicated Code, Large Class, "
"   Rename Method,","Enable HBaseAdmin.execProcedure to return data from procedure execution HBASE-11201 enables global procedure members to return data to procedure master.
HBASE-9426 lets user invoke procedure from client via HBaseAdmin.execProcedure.
This JIRA is to fill the gap to enable client to get return data from master after procedure execution.",", "
"   Rename Method,Extract Method,","[blockcache] lazy block decompression Maintaining data in its compressed form in the block cache will greatly increase our effective blockcache size and should show a meaning improvement in cache hit rates in well designed applications. The idea here is to lazily decompress/decrypt blocks when they're consumed, rather than as soon as they're pulled off of disk.

This is related to but less invasive than HBASE-8894.","Duplicated Code, Long Method, , "
"   Move Class,Move Method,Extract Method,Inline Method,Move Attribute,","[Thrift] support authentication/impersonation Thrift server can access HBase as a fixed authenticated user. However, we don't authenticate thrift clients. It will be great if the thrift server can authenticate clients, and support impersonation.","Duplicated Code, Long Method, , , , , "
"   Move Method,Extract Method,Move Attribute,","a couple of callQueue related improvements In one of my in-memory read only testing(100% get requests), one of the top scalibility bottleneck came from the single callQueue. A tentative sharing this callQueue according to the rpc handler number showed a big throughput improvement(the original get() qps is around 60k, after this one and other hotspot tunning, i got 220k get() qps in the same single region server) in a YCSB read only scenario.
Another stuff we can do is seperating the queue into read call queue and write call queue, we had done it in our internal branch, it would helpful in some outages, to avoid all read or all write requests ran out of all handler threads.
One more stuff is changing the current blocking behevior once the callQueue is full, considering the full callQueue almost means the backend processing is slow somehow, so a fail-fast here should be more reasonable if we using HBase as a low latency processing system. see ""callQueue.put(call)""","Duplicated Code, Long Method, , , , "
"   Rename Method,","MetaTableAccessor shouldn't use ZooKeeeper After committing patch for HBASE-4495, there's an further improvement which can be made (discussed originally on review board to that jira).

We have MetaTableAccessor and MetaTableLocator classes. First one is used to access information stored in hbase:meta table. Second one is used to deal with ZooKeeper state to find out region server hosting hbase:meta, wait for it to become available and so on.

MetaTableAccessor, in turn, should only operate on the meta table content, so shouldn't need ZK. The only reason why MetaTableAccessor is using ZK - when callers request assignment information, they can request location of meta table itself, which we can't read from meta, so in that case MetaTableAccessor relays the call to MetaTableLocator. May be the solution here is to declare that clients of MetaTableAccessor shall not use it to work with meta table itself (not it's content).",", "
"   Rename Class,Rename Method,Push Down Method,Move Method,Extract Method,Inline Method,Move Attribute,","Abstract visibility label related services into an interface - storage and retrieval of label dictionary and authentication sets 
- marshalling and unmarshalling of visibility expression representations in operation attributes and cell tags
- management of assignment of authorizations to principals
This will allow us to introduce additional serde implementations for visibility expressions, for example storing as strings in some places and compressed/tokenized representation in others in order to support additional use cases.
","Duplicated Code, Long Method, , , , , , "
"   Rename Method,Extract Method,Inline Method,","Clean up ZK-based region assignment We can clean up the ZK-based region assignment code and use the ZK-less one in the master branch, to make the code easier to understand and maintain.","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,Inline Method,","Tighten up region state transition When a regionserver reports to master a region transition, we should check the current region state to be exactly what we expect.","Duplicated Code, Long Method, , , "
"   Rename Class,Extract Interface,Rename Method,","Find a way to set sequenceId on Cells on the server Over in HBASE-11591 there was a need to set the sequenceId of the HFile to the bulk loaded KVs. Since we are trying to use the concept of Cells in the read path if we need to use setSequenceId(), then the Cell has to be converted to KV and only KeyValue impl has the accessor setSequenceId().
[~anoop.hbase] suggested if we can use a Server side impl of Cell and have these accessors in them.
This JIRA aims to solve this and see the related code changes that needs to be carried out for this.",", Large Class, "
"   Extract Interface,Rename Method,Move Method,Extract Method,Move Attribute,","[ImportTSV] Abstract labels tags creation into pluggable Interface HBASE-11553 abstracted out the VisibilityLabelService and using which one can go with String based labels tags instead of default ordinal based. In MapReduce side, ImportTSV also should support such an abstraction layer. This is applicable for HFileOutputFormat where mappers create Cells to be written to HFiles. 

Also every Mapper (custom written also) dealing with such low level HBase things (Creation of tags and Cells) is not a good idea. This issue plan to provide a public audience Facade class to facilitate such Cell creation so that Mappers can just make use of them.
","Duplicated Code, Long Method, , , , Large Class, "
"   Extract Interface,Extract Superclass,","Create Connection and ConnectionManager This is further cleanup of the HBase interface for 1.0 after implementing the new Table and Admin interfaces. Following Enis's guidelines in HBASE-10602, this JIRA will generate a new ConnectionManager to replace HCM and Connection to replace HConnection.

For more detail, this JIRA intends to implement this portion:

{code}
interface Connection extends Closeable{
Table getTable(), and rest of HConnection methods 
getAdmin()
// no deprecated methods (cache related etc)
}

@Deprecated
interface HConnection extends Connection {
@Deprecated
HTableInterface getTable()
// users are encouraged to use Connection
}

class ConnectionManager {
createConnection(Configuration) // not sure whether we want a static factory method to create connections or a ctor
}

@Deprecated
class HCM extends ConnectionManager {
// users are encouraged to use ConnectionManager
}
{code}
",", Duplicated Code, Large Class, Large Class, "
"   Rename Method,","Get rid of Writables in HTableDescriptor, HColumnDescriptor Currently we have protobuf for encoding this structures. Existence of Writable is misleading and need to be removed.",", "
"   Pull Up Method,Extract Method,Pull Up Attribute,","Make setting the start and stop row for a specific prefix easier If you want to set a scan from your application to scan for a specific row prefix this is actually quite hard.
As described in several places you can set the startRow to the prefix; yet the stopRow should be set to the prefix '+1'
If the prefix 'ASCII' put into a byte[] then this is easy because you can simply increment the last byte of the array. 
But if your application uses real binary rowids you may run into the scenario that your prefix is something like 
{code}{ 0x12, 0x23, 0xFF, 0xFF }{code} Then the increment should be {code}{ 0x12, 0x24 }{code}

I have prepared a proposed patch that makes setting these values correctly a lot easier.","Duplicated Code, Long Method, , Duplicated Code, Duplicated Code, "
"   Rename Method,Extract Method,Inline Method,","Keep table state in META HBASE-7767 moved table enabled|disabled state to be kept in hdfs instead of zookeeper. isTableDisabled() which is used in HConnectionImplementation.relocateRegion() now became a master RPC call rather than a zookeeper client call. Since we do relocateRegion() calls everytime we want to relocate a region (region moved, RS down, etc) this implies that when the master is down, the some of the clients for uncached regions will be affected. 

See HBASE-7767 and HBASE-11974 for some more background.","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Bytes: more Unsafe, more Faster  Additional optimizations to *org.apache.hadoop.hbase.util.Bytes*:

* New version of compareTo method.
* New versions for primitive converters : putXXX/toXXX.","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Change HBase StoreKey format HBASE-859 cleaned up keys removing the need of HRegionInfo being in the context comparing keys. This issue is about changing the format. Work done in HBASE-859 means changes have been localized to HStoreKey, in particular to comparators and parse routines. We should do this now since 0.20.0 will require rewriting all data.

Things to consider:

<row> <columnfamily> <columnqualifier> <timestamp> <keytype>

Or leave off columnfamily altogether and just write it once into the hfile metadata (All key compares are done in the Store context so columnfamily can be safely left out of the equation; its only when the key rises above Store that the columnfamily needs appending).

keytype is probably a byte. Types are delete cell, delete row, delete family, delete column? What else? Where should we put it? At the end? How should type sort? Or should it not be part of sort so its just the order at which we encounter the key?

How are we going to support keys that go in out of chronological order?","Duplicated Code, Long Method, , , , , "
"   Move Class,Extract Interface,Move Attribute,","WAL accounting by Store HBASE-10201 has made flush decisions per Store, but has not done enough work on HLog, so there are two problems:
1. We record minSeqId both in HRegion and FSHLog, which is a duplication.
2. There maybe holes in WAL accounting.
For example, assume family A with sequence id 1 and 3, family B with seqId 2. If we flush family A, we can only record that WAL before sequence id 1 can be removed safely. If we do a replay at this point, sequence id 3 will also be replayed which is unnecessary.",", , Large Class, "
"   Move Class,Pull Up Method,Pull Up Attribute,Move Attribute,",Use ThreadLocalRandom for RandomQueueBalancer looks like the Random in RandomQueueBalancer is now the main cause of contention in the rpc queue insertion. we should replace that Random with a ThreadLocalRandom.,", , Duplicated Code, Duplicated Code, "
"   Move Class,Push Down Method,Move Method,Push Down Attribute,Move Attribute,","Add RpcClient interface and enable changing of RpcClient implementation Currently HConnectionImplementation works with the included RpcClient which is a direct implementation and not defined by an interface.

It would be great to be able to swap out the default RpcClient with another implementation which can also be controlled by the default HConnectionImplementation. 

Suggested changes:
- Create a RpcClient interface which defines all the ways HConnectionImplementation interacts with an RPC client. Like getting a blocking protobuf service interface or closing the client.
- Define which RpcClient implementation to construct by setting a configuration variable which defaults to the current RpcClient.
- Possibly create an abstract RpcClient class to only load all the basic Rpc layer configurations to be used in an implementation.

Why? It enables experimentation with RpcClients which could enable new features or could be more performant than the included client. 
I created a new RpcClient implementation based on Netty which can also be called asynchronously. It would be great to also be able to use this RpcClient in all the default ways and tests to see if there are any issues with it. 

https://github.com/jurmous/async-hbase-client/
https://github.com/jurmous/async-hbase-client/blob/master/src/main/java/org/apache/hadoop/hbase/ipc/AsyncRpcClient.java",", , , , , "
"   Move Class,Pull Up Method,","Adapt PayloadCarryingRpcController so it can also be used in async way With the changes in HBASE-12597 it is possible to create a new RPC client. But in all places the BlockingRpcChannel is called with a PayloadCarryingRpcController. This controller is not usable in Async context because some methods are not supported at the moment. (See TimeLimitedRpcController for the methods that throw UnsupportedOperationException)

This issue is about implementing these methods so PayloadCarryingRpcController can also be used in an async context and work the same in a sync context.",", Duplicated Code, "
"   Rename Method,Extract Method,","Support multiple port numbers in ZK quorum string HBase does not allow the zk quorum string to contain port numbers in this format:

{noformat}
hostname1:port1,hostname2:port2,hostname3:port3
{noformat}

Instead it expects the string to be in this format:

{noformat}
hostname1,hostname2,hostname3:port3
{noformat}

And port 3 is used for all the client ports. We should flex the parsing so that both forms are accepted.

A sample exception:
{code}
java.io.IOException: Cluster key passed host1:2181,host2:2181,host3:2181,host4:2181,host5:2181:2181:/hbase is invalid, the format should be:hbase.zookeeper.quorum:hbase.zookeeper.client.port:zookeeper.znode.parent
at org.apache.hadoop.hbase.zookeeper.ZKUtil.transformClusterKey(ZKUtil.java:403)
at org.apache.hadoop.hbase.zookeeper.ZKUtil.applyClusterKeyToConf(ZKUtil.java:386)
at org.apache.hadoop.hbase.replication.ReplicationPeersZKImpl.getPeerConf(ReplicationPeersZKImpl.java:304)
at org.apache.hadoop.hbase.replication.ReplicationPeersZKImpl.createPeer(ReplicationPeersZKImpl.java:435)
{code}","Duplicated Code, Long Method, , "
"   Rename Method,","Let MetaScanner recycle a given Connection It is very heavy to create a Connection on each meta scan. Connections create RpcClients, RpcClients create RPC connections and all cannot be recycled.

Tests with a lot of metascans are very heavy with the async client.

This issue is to make anything that uses metaScans reuse the same connection.",", "
"   Rename Method,Move Method,","Visibility Labels:  support visibility labels for user groups. The thinking is that we should support visibility labels to be associated with user groups.
We will then be able grant visibility labels to a group in addition to individual users, which provides convenience and usability.
We will use '@group' to denote a group name, as similarly done in AcccessController.
For example, 
{code}
set_auths '@group1', ['SECRET','PRIVATE']
{code}
{code}
get_auth '@group1'
{code}
A user belonging to 'group1' will have all the visibility labels granted to 'group1'

We'll also support super user groups as specified in hbase-site.xml.

The code update will mainly be on the server side VisibilityLabelService implementation.",", , "
"   Rename Method,","Don't transfer all the queued hlogs of a dead server to the same alive server When a region server is down(or the cluster restart), all the hlog queues will be transferred by the same alive region server. In a shared cluster, we might create several peers replicating data to different peer clusters. There might be lots of hlogs queued for these peers caused by several reasons, such as some peers might be disabled, or errors from peer cluster might prevent the replication, or the replication sources may fail to read some hlog because of hdfs problem. Then, if the server is down or restarted, another alive server will take all the replication jobs of the dead server, this might bring a big pressure to resources(network/disk read) of the alive server and also is not fast enough to replicate the queued hlogs. And if the alive server is down, all the replication jobs including that takes from other dead servers will once again be totally transferred to another alive server, this might cause a server have a large number of queued hlogs(in our shared cluster, we find one server might have thousands of queued hlogs for replication). As an optional way, is it reasonable that the alive server only transfer one peer's hlogs from the dead server one time? Then, other alive region servers might have the opportunity to transfer the hlogs of rest peers. This may also help the queued hlogs be processed more fast. Any discussion is welcome.",", "
"   Move Class,Rename Class,Extract Interface,Rename Method,Extract Method,Inline Method,","Region, a supportable public/evolving subset of HRegion On HBASE-12566, [~lhofhansl] proposed:
{quote}
Maybe we can have a {{Region}} interface that is to {{HRegion}} is what {{Store}} is to {{HStore}}. Store marked with {{@InterfaceAudience.Private}} but used in some coprocessor hooks.
{quote}

By example, now coprocessors have to reach into HRegion in order to participate in row and region locking protocols, this is one area where the functionality is legitimate for coprocessors but not for users, so an in-between interface make sense.

In addition we should promote {{Store}}'s interface audience to LimitedPrivate(COPROC).","Duplicated Code, Long Method, , , Large Class, "
"   Move Class,Rename Class,Push Down Method,Push Down Attribute,","Supportable SplitTransaction and RegionMergeTransaction interfaces Making SplitTransaction, RegionMergeTransaction limited private is required to support local indexing feature in Phoenix to ensure regions colocation. 

We can ensure region split, regions merge in the coprocessors in few method calls without touching internals like creating zk's, file layout changes or assignments.
1) stepsBeforePONR, stepsAfterPONR we can ensure split.
2) meta entries can pass through coprocessors to atomically update with the normal split/merge.
3) rollback on failure.",", , , "
"   Move And Rename Class,Rename Method,Move Method,Extract Method,Inline Method,","MetaScanner should be replaced by MetaTableAccessor MetaScanner and MetaTableAccessor do similar things, but seems they tend to diverge. Let's have only one thing to enquiry META.","Duplicated Code, Long Method, , , , "
"   Rename Method,Extract Method,","Hbase Streaming Scan Feature A scan operation iterates over all rows of a table or a subrange of the table. The synchronous nature in which the data is served at the client side hinders the speed the application traverses the data: it increases the overall processing time, and may cause a great variance in the times the application waits for the next piece of data.

The scanner next() method at the client side invokes an RPC to the regionserver and then stores the results in a cache. The application can specify how many rows will be transmitted per RPC; by default this is set to 100 rows. 
The cache can be considered as a producer-consumer queue, where the hbase client pushes the data to the queue and the application consumes it. Currently this queue is synchronous, i.e., blocking. More specifically, when the application consumed all the data from the cache --- so the cache is empty --- the hbase client retrieves additional data from the server and re-fills the cache with new data. During this time the application is blocked.

Under the assumption that the application processing time can be balanced by the time it takes to retrieve the data, an asynchronous approach can reduce the time the application is waiting for data.

We attach a design document.
We also have a patch that is based on a private branch, and some evaluation results of this code.
","Duplicated Code, Long Method, , "
"   Move Class,Extract Superclass,Move Method,Extract Method,Move Attribute,","HBase should provide an InputFormat supporting multiple scans in mapreduce jobs over snapshots Currently, HBase supports the pushing of multiple scans to mapreduce jobs over live tables (via MultiTableInputFormat) but only supports a single scan for mapreduce jobs over table snapshots. It would be handy to support multiple scans over snapshots as well, probably through another input format (MultiTableSnapshotInputFormat?). To mimic the functionality present in MultiTableInputFormat, the new input format would likely have to take in the names of all snapshots used in addition to the scans.","Duplicated Code, Long Method, , , , Duplicated Code, Large Class, "
"   Pull Up Method,Extract Method,Inline Method,Pull Up Attribute,","Improvements to Stochastic load balancer There are two things this jira tries to address:

1. The locality picker in the stochastic balancer does not pick regions with least locality as candidates for swap/move. So when any user configures locality cost in the configs, the balancer does not always seems to move regions with bad locality. 

2. When a cluster has equal number of loaded regions, it always picks the first one. It should pick a random region on one of the equally loaded servers. This improves a chance of finding a good candidate, when load picker is invoked several times.","Duplicated Code, Long Method, , , Duplicated Code, Duplicated Code, "
"   Rename Method,Move Method,Inline Method,",Remove distributed mode from MiniZooKeeper MiniZooKeeper currently has a standalone and a distributed mode. For HBase testing we only need (and only use) the standalone mode. We should remove all of the distributed logic.,", , , "
"   Rename Method,Extract Method,","[performance] Distributed splitting of regionserver commit logs HBASE-1008 has some improvements to our log splitting on regionserver crash; but it needs to run even faster.

(Below is from HBASE-1008)

In bigtable paper, the split is distributed. If we're going to have 1000 logs, we need to distribute or at least multithread the splitting.

1. As is, regions starting up expect to find one reconstruction log only. Need to make it so pick up a bunch of edit logs and it should be fine that logs are elsewhere in hdfs in an output directory written by all split participants whether multithreaded or a mapreduce-like distributed process (Lets write our distributed sort first as a MR so we learn whats involved; distributed sort, as much as possible should use MR framework pieces). On startup, regions go to this directory and pick up the files written by split participants deleting and clearing the dir when all have been read in. Making it so can take multiple logs for input, can also make the split process more robust rather than current tenuous process which loses all edits if it doesn't make it to the end without error.
2. Each column family rereads the reconstruction log to find its edits. Need to fix that. Split can sort the edits by column family so store only reads its edits.","Duplicated Code, Long Method, , "
"   Rename Method,","Rename *column methods in MasterObserver to *columnFamily This being an interface makes it a bit harder on implementors. It'd be easier with Java8 and default implementations.

We could either

# add new *columnFamily methods and deprecate the old ones or
# rename the existing ones without doing a deprecation first.

Implementors would need to change their code in each of those cases. But because we have the {{BaseMasterObserver}} and {{BaseMasterAndRegionObserver}} it'd make things easier for people using those classes if we go with option 1. So that's my preference.

The plan would be to add these methods in 2.0.0 and remove the old ones in 3.0.0.",", "
"   Rename Method,",Add RegionLocator methods to Thrift2 proxy. Thrift2 doesn't provide the same functionality as the java client for getting region locations. We should change that.,", "
"   Rename Method,Move Method,Extract Method,Move Attribute,",Provide single super user check implementation Followup for HBASE-13375.,"Duplicated Code, Long Method, , , , "
"   Rename Class,Rename Method,","remove HLogEdit From https://issues.apache.org/jira/browse/HBASE-1403?focusedCommentId=12708553&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12708553

@stack: 
I'm thinking of the HLog and HCD. HLog values are a HLogEdit which is now just the new KeyValue class + a THBase flag. In interests of performance, I'd like to not have to create a HLogEdit at all and just shove in the KeyValue. 
",", "
"   Rename Class,Rename Method,Move Method,Extract Method,","HBase Backup/Restore Phase 3: Merge backup images User can merge incremental backup images into single incremental backup image.

# Merge supports only incremental images
# Merge supports only images for the same backup destinations

Command:
{code}
hbase backup merge image1,image2,..imageK
{code}

Example:
{code}
hbase backup merge backup_143126764557,backup_143126764456 
{code}

When operation is complete, only the most recent backup image will be kept (in above example - backup_143126764557) as a merged backup image, all other images will be deleted from both: file system and backup system tables, corresponding backup manifest for the merged backup image will be updated to remove dependencies from deleted images. Merged backup image will contains all the data from original image and from deleted images.


","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","HBase Backup/Restore Phase 3: Filter WALs on backup to include only edits from backed up tables h2. High level design overview

* When incremental backup request comes for tables {t} we select all the tables already registered in a backup system - {T} and union them with {t}, which results in a new table set - U(t, T)
* For every table K from U(t,T) we perform the following:
** Convert new WAL files into HFile applying table filter K (only edits for table T will pass the filter)
** Move these HFile(s) to backup destination

During restore (incremental):

* We run full restore first
* Then collect all HFiles from intermediate incremental images and run them through HFileSplitterJob, which splits files into a current tables region boundaries
* Load these files using LoadIncrementalHFiles tool


","Duplicated Code, Long Method, , "
"   Rename Class,Move Attribute,","Refine RegionGroupingProvider: fix issues and make it more scalable There're multiple issues in RegionGroupingProvider, including:
* The provider cache in it is using byte array as the key of ConcurrentHashMap, which is not right (the reason is [here|http://stackoverflow.com/questions/1058149/using-a-byte-array-as-hashmap-key-java])

* It's using IdentityGroupingStrategy to get group and use it as key of the cache, which means the cache will include an entry for each region. This is especially unnecessary when using BoundedRegionGroupingProvider

Besides fixing the above issues, I suggest to change BoundedRegionGroupingProvider from a *provider* to a pluggable *strategy*, which will make the whole picture much more clear.

For more details, please refer to the patch",", , "
"   Rename Method,","Consolidate printUsage in IntegrationTestLoadAndVerify Investigating the use of {{itlav}} is a little screwy. Subclasses are not overriding the {{printUsage()}} methods correctly, so you have to pass {{--help}} to get some info and no arguments to get the rest.

{noformat}
[hbase@ndimiduk-112rc2-7 ~]$ hbase org.apache.hadoop.hbase.test.IntegrationTestLoadAndVerify --help
usage: bin/hbase org.apache.hadoop.hbase.test.IntegrationTestLoadAndVerify <options>
Options:
-h,--help Show usage
-m,--monkey <arg> Which chaos monkey to run
-monkeyProps <arg> The properties file for specifying chaos monkey properties.
-ncc,--noClusterCleanUp Don't clean up the cluster at the end
[hbase@ndimiduk-112rc2-7 ~]$ hbase org.apache.hadoop.hbase.test.IntegrationTestLoadAndVerify
IntegrationTestLoadAndVerify [-Doptions] <load|verify|loadAndVerify>
Loads a table with row dependencies and verifies the dependency chains
Options
-Dloadmapper.table=<name> Table to write/verify (default autogen)
-Dloadmapper.backrefs=<n> Number of backreferences per row (default 50)
-Dloadmapper.num_to_write=<n> Number of rows per mapper (default 100,000 per mapper)
-Dloadmapper.deleteAfter=<bool> Delete after a successful verify (default true)
-Dloadmapper.numPresplits=<n> Number of presplit regions to start with (default 40)
-Dloadmapper.map.tasks=<n> Number of map tasks for load (default 200)
-Dverify.reduce.tasks=<n> Number of reduce tasks for verify (default 35)
-Dverify.scannercaching=<n> Number hbase scanner caching rows to read (default 50)
{noformat}",", "
"   Move Class,Move Attribute,","Show regionserver's version in master status page In production env, regionservers may be removed from the cluster for hardware problems and rejoined the cluster after the repair. There is a potential risk that the version of rejoined regionserver may diff from others because the cluster has been upgraded through many versions. 

To solve this, we can show the all regionservers' version in the server list of master's status page, and highlight the regionserver when its version is different from the master's version, similar to HDFS-3245

Suggestions are welcome~
",", , "
"   Rename Method,Move Method,","Concurrent LRU Block Cache The LRU-based block cache that will be committed in HBASE-1192 is thread-safe but contains a big lock on the hash map. Under high load, the block cache will be hit very heavily from a number of threads, so it needs to be built to handle massive concurrency.

This issue aims to implement a new block cache with LRU eviction, but backed by a ConcurrentHashMap and a separate eviction thread. Influence will be drawn from Solr's ConcurrentLRUCache, however there are major differences because solr treats all cached elements as equal size whereas we are dependent on our HeapSize interface with realistic (though approximate) heap usage.",", , "
"   Rename Method,",Remove deprecated HBaseTestCase dependency from TestHFile Remove dependency on long deprecated HBaseTestCase (0.90). modified references to tfiles which haven't been part of any modern hbase.,", "
"   Extract Interface,Rename Method,Pull Up Method,Move Method,Extract Method,","HTable.mutateRow does not collect stats We are trying to fix the stats implementation, by moving it out of the Result object and into an Rpc payload (but not the 'cell payload', just as part of the values returned from the request). This change will also us use easily switch to AsyncProcess as the executor, and support stats, for nearly all the rpc calls. However, that means when you upgrade the client or server, you will lose stats visibility until the other side is upgraded. We could keep around the Result based stats storage to accommodate the old api and send both stats back from the server (in each result and in the rpc payload).
Note that we will still be wire compatible - protobufs mean we can just ride over the lack of information.
The other tricky part of this is that Result has a non-InterfaceAudience.Private getStatistics() method (along with two InterfaceAudience.Private addResults and setStatistics methods), so we might need a release to deprecate the getStats() method before throwing it out?

","Duplicated Code, Long Method, , , Duplicated Code, Large Class, "
"   Extract Method,Inline Method,",some cleanup to snapshot code move out some common code and use helpers,"Duplicated Code, Long Method, , , "
"   Rename Class,Move Method,Extract Method,Move Attribute,","Support passing multiple QOPs to SaslClient/Server via hbase.rpc.protection Currently, we can set the value of hbase.rpc.protection to one of authentication/integrity/privacy. It is the used to set {{javax.security.sasl.qop}} in SaslUtil.java.
The problem is, if a cluster wants to switch from one qop to another, it'll have to take a downtime. Rolling upgrade will create a situation where some nodes have old value and some have new, which'll prevent any communication between them. There will be similar issue when clients will try to connect.

{{javax.security.sasl.qop}} can take in a list of QOP in preferences order. So a transition from qop1 to qop2 can be easily done like this
""qop1"" --> ""qop2,qop1"" --> rolling restart --> ""qop2"" --> rolling restart

Need to change hbase.rpc.protection to accept a list too.","Duplicated Code, Long Method, , , , "
"   Rename Method,Move Method,Extract Method,Move Attribute,","VerifyReplication should use peer configuration in peer connection VerifyReplication uses the replication peer's configuration to construct the ZooKeeper quorum address for the peer connection. However, other configuration properties in the peer's configuration are dropped. It should merge all configuration properties from the {{ReplicationPeerConfig}} when creating the peer connection and obtaining a credentials for the peer cluster.","Duplicated Code, Long Method, , , , "
"   Rename Method,","Don't allow multi's to over run the max result size. If a user puts a list of tons of different gets into a table we will then send them along in a multi. The server un-wraps each get in the multi. While no single get may be over the size limit the total might be.

We should protect the server from this. 
We should batch up on the server side so each RPC is smaller.",", "
"   Move Class,Move And Rename Class,Move Method,Move Attribute,","Add throughput controller for flush In HBASE-8329 we added a throughput controller for compaction, to avoid spike caused by huge IO pressure like network/disk overflow. However, even with this control on, we are still observing disk utils near 100%, and by analysis we think this is caused by flush, especially when we increase the setting of {{hbase.hstore.flusher.count}}

In this JIRA, we propose to add throughput control feature for flush, as a supplement of HBASE-8329 to better control IO pressure.",", , , "
"   Move Method,Move Attribute,",Cull TestHFileWriterV2 and HFileWriterFactory 2.x removes hfile v2. This patch cleans up remnants from unit tests associated with it.,", , , "
"   Move Method,Extract Method,","A simple implementation of date based tiered compaction This is a simple implementation of date-based tiered compaction similar to Cassandra's for the following benefits:
1. Improve date-range-based scan by structuring store files in date-based tiered layout.
2. Reduce compaction overhead.
3. Improve TTL efficiency.

Perfect fit for the use cases that:
1. has mostly date-based date write and scan and a focus on the most recent data. 
2. never or rarely deletes data.

Out-of-order writes are handled gracefully. Time range overlapping among store files is tolerated and the performance impact is minimized.

Configuration can be set at hbase-site.xml or overriden at per-table or per-column-famly level by hbase shell.

Design spec is at https://docs.google.com/document/d/1_AmlNb2N8Us1xICsTeGDLKIqL6T-oHoRLZ323MG_uy8/edit?usp=sharing
Results in our production is at https://docs.google.com/document/d/1GqRtQZMMkTEWOijZc8UCTqhACNmdxBSjtAQSYIWsmGU/edit#
","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Use less contended classes for metrics Running the benchmarks now, but it looks like the results are pretty extreme. The locking in our histograms is pretty extreme.","Duplicated Code, Long Method, , "
"   Move And Rename Class,","Break out writer and reader from StoreFile StoreFile.java is trending to become a monolithic class, it's ~1800 lines. Would it make sense to break out reader and writer (~500 lines each) into separate files.

We are doing so many different things in a single class: comparators, reader, writer, other stuff; and it hurts readability a lot, to the point that just reading through a piece of code require scrolling up and down to see which level (reader/writer/base class level) it belongs to. These small-small things really don't help while trying to understanding the code. There are good reasons we don't do these often (affects existing patches, needs to be done for all branches, etc). But this and a few other classes can really use a single iteration of refactoring to make things a lot better.",", "
"   Rename Method,","HBase should manage multiple node ZooKeeper quorum I thought there was already a JIRA for this, but I cannot seem to find it.

We need to manage multiple node ZooKeeper quorums (required for fully distributed option) in HBase to make things easier for users.

Here's relevant IRC conversation with Ryan and Andrew:

{code}
Jun 17 18:14:39 <dj_ryan> right now we include our client deps in hbase/lib
Jun 17 18:14:47 <dj_ryan> so removing zookeeper would be problematic
Jun 17 18:14:56 <dj_ryan> but hbase does put up a private zk quorum
Jun 17 18:15:02 <dj_ryan> it just doesnt bother with q>1
Jun 17 18:15:05 <apurtell> dj_ryan, nitay: agreed, so that's why i wonder about a private zk quorum managed by hbase
Jun 17 18:15:12 <apurtell> q ~= 5
Jun 17 18:15:22 <dj_ryan> so maybe we should ship tools to manage it
Jun 17 18:15:23 <apurtell> if possible
Jun 17 18:15:29 <dj_ryan> i can agree with that
Jun 17 18:15:39 <nitay> apurtell, ok, i'd be happy to bump the priority of hbase managing full cluster and work on that
Jun 17 18:15:47 * iand (n=iand@205.158.58.226.ptr.us.xo.net) has joined #hbase
Jun 17 18:15:48 <apurtell> nitay: that would be awesome
Jun 17 18:15:57 <apurtell> then i can skip discussions with cloudera about including zk also
Jun 17 18:16:12 <apurtell> and we can use some private ports that won't conflict with a typical zk install
Jun 17 18:16:15 <nitay> but i also think that users should be able to point at existing clusters, so as long as your rpms are compatible, it should be fine
Jun 17 18:16:23 <nitay> apurtell, isn't hadoop going to start using ZK
Jun 17 18:16:31 <apurtell> nitay: agree, but this is the cloudera-autoconfig-rpm (and deb) case
Jun 17 18:16:34 <nitay> the cloudera dude was working on using it for namenode whatnot like we do for master
Jun 17 18:16:35 <dj_ryan> so there are only 2 things
Jun 17 18:16:38 <dj_ryan> - set up myids
Jun 17 18:16:38 <nitay> what are they doing for that
Jun 17 18:16:40 <dj_ryan> - start zk
Jun 17 18:16:42 <dj_ryan> - stop zk
Jun 17 18:16:50 <dj_ryan> we dont want to start/stop zk just when we are doing a cluster bounce
Jun 17 18:16:51 <nitay> ye stupid myids
Jun 17 18:16:52 <dj_ryan> you start it once
Jun 17 18:16:54 <dj_ryan> and be done with ti
Jun 17 18:16:58 * iand (n=iand@205.158.58.226.ptr.us.xo.net) has left #hbase (""Leaving."")
Jun 17 18:17:13 <apurtell> dj_ryan: yes, start it once. that's what i do. works fine through many hbase restarts...
Jun 17 18:17:28 <nitay> so then we need a separate shell cmd or something to stop zk
Jun 17 18:17:35 <nitay> and start on start-hbase if not already running type thing
Jun 17 18:17:43 <dj_ryan> yes
Jun 17 18:17:58 <nitay> ok
Jun 17 18:18:19 <apurtell> with quorum peers started on nodes in conf/regionservers, up to ~5 if possible
Jun 17 18:18:37 <apurtell> but what about zoo.cfg?
Jun 17 18:18:51 <nitay> oh i was thinking of having separate conf/zookeepers
Jun 17 18:18:58 <apurtell> nitay: even better
Jun 17 18:18:59 <nitay> but we can use first five RS too
Jun 17 18:19:26 <nitay> apurtell, yeah so really there wouldnt be a conf/zookeepers, i would rip out hostnames from zoo.cfg
Jun 17 18:19:38 <nitay> or go the other way, generate zoo.cfg from conf/zookeepers
Jun 17 18:19:42 <nitay> gotta do one or the other
Jun 17 18:19:49 <nitay> dont want to have to edit both
Jun 17 18:19:54 <apurtell> nitay: right
Jun 17 18:20:21 <apurtell> well...
Jun 17 18:20:29 <nitay> zoo.cfg has the right info right now, cause u need things other than just hostnames, i.e. client and quorum ports
Jun 17 18:20:31 <apurtell> we can leave out servers from our default zoo.cfg
Jun 17 18:20:39 <apurtell> and consider a conf/zookeepers
Jun 17 18:20:47 <dj_ryan> i call it conf/zoos
Jun 17 18:20:54 <dj_ryan> in my zookeeper config
Jun 17 18:20:54 <dj_ryan> dir
Jun 17 18:20:57 <nitay> and then have our parsing of zoo.cfg insert them
Jun 17 18:21:08 <nitay> cause right now its all off java Properties anyways
Jun 17 18:21:12 <apurtell> and let the zk wrapper parse the files if they exist and otherwise build the list of quorum peers like it does already
Jun 17 18:21:34 <apurtell> so someone could edit either and it would dtrt
Jun 17 18:21:48 <nitay> apurtell, yeah, makes sense
Jun 17 18:21:58 <nitay> we can discuss getting rid of zoo.cfg completely
Jun 17 18:22:12 <nitay> put it all in XML and just create a Properties for ZK off the right props
Jun 17 18:22:14 <apurtell> for my purposes, i just need some files available for a post install script to lay down a static hbase cluster config based on what it discovers about the hadoop installation
Jun 17 18:23:56 <apurtell> then i need to hook sysvinit and use chkconfig to enable/disable services on the cluster nodes according to their roles defined by hadoop/conf/masters and hadoop/conf/regionservers
Jun 17 18:24:13 <apurtell> so we put the hmaster on the namenode
Jun 17 18:24:17 <apurtell> and the region servers on the datanodes
Jun 17 18:24:35 <apurtell> hadoop/conf/slaves i mean
Jun 17 18:24:44 <apurtell> and pick N hosts out of slaves to host the zk quorum
Jun 17 18:24:50 <apurtell> make sense?
Jun 17 18:25:33 <nitay> yes i think so, and u'll be auto generating the hbase configs for what servers run what then?
Jun 17 18:25:50 <apurtell> nitay: yes
Jun 17 18:25:51 <nitay> which is why a simple line by line conf/zookeepers type file is clean and easy
Jun 17 18:25:57 <apurtell> nitay: agree
Jun 17 18:25:59 <apurtell> so i think my initial question has been answered, hbase will manage a private zk ensemble
Jun 17 18:26:07 <apurtell> ... somehow
Jun 17 18:26:10 <nitay> right :)
Jun 17 18:26:15 <apurtell> ok, thanks
{code}",", "
"   Rename Method,","Make SnapshotManager accessible through MasterServices See this comment for background:

https://issues.apache.org/jira/browse/HBASE-15411?focusedCommentId=15209640&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15209640

When procedure executes on master and performs snapshot, the procedure needs to access SnapshotManager.

This JIRA is to add accessor to MasterServices.",", "
"   Move Class,Extract Method,",Remove PB references from Admin for 2.0 This is a sub-task for HBASE-15174.,"Duplicated Code, Long Method, , "
"   Inline Method,Move Attribute,",Remove deprecated HConnection for 2.0 thus removing all PB references for 2.0 This is sub-task for HBASE-15174.,", , , "
"   Extract Superclass,Extract Interface,Push Down Method,Move Method,Push Down Attribute,Move Attribute,","Refactor RPC classes to better accept async changes. The class layout needs to be changed to better accept Async versions of the same classes. This is a partly commit of the patch in HBASE-13784

Classes turned interface:
* AsyncRpcChannel with AsyncRpcChannelImpl as implementation
* CoprocessorRpcChannel with SyncCoprocessorRpcChannel for implementation

New lower-level abstractions:
* ProtoRetryingCallable below RetryingCallable
* AbstractRegionServerCallable below RegionServerCallable",", , , Duplicated Code, Large Class, , , Large Class, "
"   Rename Method,","Move memcache to ConcurrentSkipListMap from ConcurrentSkipListSet The CSLM will replace old entry with a new when you put. The CSLS will NOT replace if existent key making for a test, and if present, remove semantic which to be safe needs synchronizing (Replacement is a Ryan Rawson suggestion).",", "
"   Rename Class,Move And Rename Class,Rename Method,","Change the name of the in-memory updates from 'memcache' to 'memtable' or.... Our map of sorted edits kept per Store is currently called memcache. Memcache is name of a well-known app that does something else. Also, our 'memcache' is not a cache. In BT its called 'memtable'. Lets rename it. It'll make it easier explaining hbase. What'll we call it? 'updates', 'inmemory-store-edits'....",", "
"   Rename Method,Extract Method,Pull Up Attribute,","Add Async RpcChannels to all RpcClients The RpcClients all need to expose an async protobuf RpcChannel and our own custom AsyncRpcChannel (without protobuf overhead) so an Async table implementation can be made.

","Duplicated Code, Long Method, , Duplicated Code, "
"   Move Method,Extract Method,Move Attribute,","An endpoint-based export tool The time for exporting table can be reduced, if we use the endpoint technique to export the hdfs files by the region server rather than by hbase client.

In my experiments, the elapsed time of endpoint-based export can be less than half of current export tool (enable the hdfs compression)

But the shortcomings is we need to alter table for deploying the endpoint

any comments about this? thanks","Duplicated Code, Long Method, , , , "
"   Rename Method,","Better documentation of ReplicationPeers Some of the ReplicationPeers interface's methods are not documented and are tied to a ZooKeeper implementation of ReplicationPeers. Also some method names are a little confusing.

Review board at: https://reviews.apache.org/r/48696/",", "
"   Move Method,Extract Method,","Improve HBaseFsck Scalability There are some problems with HBaseFsck that make it unnecessarily slow especially for large tables or clusters with many regions. 

This patch tries to fix the biggest bottlenecks and also include a couple of bug fixes for some of the race conditions caused by gathering and holding state about a live cluster that is no longer true by the time you use that state in Fsck processing. These race conditions cause Fsck to crash and become unusable on large clusters with lots of region splits/merges.

Here are some scalability/performance problems in HBaseFsck and the changes the patch makes:
- Unnecessary I/O and RPCs caused by fetching an array of FileStatuses and then discarding everything but the Paths, then passing the Paths to a PathFilter, and then having the filter look up the (previously discarded) FileStatuses of the paths again. This is actually worse than double I/O because the first lookup obtains a batch of FileStatuses while all the other lookups are individual RPCs performed sequentially.
-- Avoid this by adding a FileStatusFilter so that filtering can happen directly on FileStatuses
-- This performance bug affects more than Fsck, but also to some extent things like snapshots, hfile archival, etc. I didn't have time to look too deep into other things affected and didn't want to increase the scope of this ticket so I focus mostly on Fsck and make only a few improvements to other codepaths. The changes in this patch though should make it fairly easy to fix other code paths in later jiras if we feel there are some other features strongly impacted by this problem. 
- OfflineReferenceFileRepair is the most expensive part of Fsck (often 50% of Fsck runtime) and the running time scales with the number of store files, yet the function is completely serial
-- Make offlineReferenceFileRepair multithreaded
- LoadHdfsRegionDirs() uses table-level concurrency, which is a big bottleneck if you have 1 large cluster with 1 very large table that has nearly all the regions
-- Change loadHdfsRegionDirs() to region-level parallelism instead of table-level parallelism for operations.

The changes benefit all clusters but are especially noticeable for large clusters with a few very large tables. On our version of 0.98 with the original patch we had a moderately sized production cluster with 2 (user) tables and ~160k regions where HBaseFsck went from taking 18 min to 5 minutes.
","Duplicated Code, Long Method, , , "
"   Extract Method,Move Attribute,","TableCfWALEntryFilter and ScopeWALEntryFilter should not redundantly iterate over cells. TableCfWALEntryFilter and ScopeWALEntryFilter both filter by iterating over cells. Since the filters are chained we do this work twice. Instead iterate over cells once and apply the ""cell filtering"" logic to these cells.","Duplicated Code, Long Method, , , "
"   Rename Method,",Add comments to ProcedureStoreTracker 0,", "
"   Rename Method,","Improve performance for RPC encryption with Apache Common Crypto Hbase RPC encryption is enabled by setting “hbase.rpc.protection” to ""privacy"". With the token authentication, it utilized DIGEST-MD5 mechanisms for secure authentication and data protection. For DIGEST-MD5, it uses DES, 3DES or RC4 to do encryption and it is very slow, especially for Scan. This will become the bottleneck of the RPC throughput.
Apache Commons Crypto is a cryptographic library optimized with AES-NI. It provides Java API for both cipher level and Java stream level. Developers can use it to implement high performance AES encryption/decryption with the minimum code and effort. Compare with the current implementation of org.apache.hadoop.hbase.io.crypto.aes.AES, Crypto supports both JCE Cipher and OpenSSL Cipher which is better performance than JCE Cipher. User can configure the cipher type and the default is JCE Cipher.",", "
"   Rename Class,Rename Method,","Replication by namespaces config in peer Now we only config table cfs in peer. But in our production cluster, there are a dozen of namespace and every namespace has dozens of tables. It was complicated to config all table cfs in peer. For some namespace, it need replication all tables to other slave cluster. It will be easy to config if we support replication by namespace. Suggestions and discussions are welcomed.

Review board: https://reviews.apache.org/r/51521/",", "
"   Inline Method,Move Attribute,","Move znode path configs to a separated class Which makes it easier to use curator at client in the future.

And also, will try to fix the bad practise that declares masterMaintZNode as static but update it in a non-static method.",", , , "
"   Move Method,Move Attribute,","Make ByteBufferUtils#equals safer and correct ByteBufferUtils.equals(HConstants.EMPTY_BYTE_BUFFER, 0, 0, HConstants.EMPTY_BYTE_ARRAY, 0, 0) will throw java.lang.ArrayIndexOutOfBoundsException: -1, i think it should return true.

",", , , "
"   Rename Class,Rename Method,Extract Method,","Add a replicate_all flag to avoid misuse the namespaces and table-cfs config of replication peer First add a new peer by shell cmd.
{code}
add_peer '1', CLUSTER_KEY => ""server1.cie.com:2181:/hbase"".
{code}
If we don't set namespaces and table cfs in peer config. It means replicate all tables to the peer cluster.

Then append a table to the peer config.
{code}
append_peer_tableCFs '1', {""table1"" => []}
{code}
Then this peer will only replicate table1 to the peer cluster. It changes to replicate only one table from replicate all tables in the cluster. It is very easy to misuse in production cluster. So we should avoid appending table to a peer which replicates all table.
","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Create more than 1 split per region, generalize HBASE-12590 A common request from users is to be able to better control how many map tasks are created per region. Right now, it is always 1 region = 1 input split = 1 map task. Same goes for Spark since it uses the TIF. With region sizes as large as 50 GBs, it is desirable to be able to create more than 1 split per region.

HBASE-12590 adds a config property for MR jobs to be able to handle skew in region sizes. The algorithm is roughly: 
{code}
If (region size >= average size*ratio) : cut the region into two MR input splits
If (average size <= region size < average size*ratio) : one region as one MR input split
If (sum of several continuous regions size < average size * ratio): combine these regions into one MR input split.
{code}

Although we can set data skew ratio to be 0.5 or something to abuse HBASE-12590 into creating more than 1 split task per region, it is not ideal. But there is no way to create more with the patch as it is. For example we cannot create more than 2 tasks per region. 

If we want to fix this properly, we should extend the approach in HBASE-12590, and make it so that the client can specify the desired num of mappers, or desired split size, and the TIF generates the splits based on the current region sizes very similar to the algorithm in HBASE-12590, but a more generic way. This also would eliminate the hand tuning of data skew ratio.

We also can think about the guidepost approach that Phoenix has in the stats table which is used for exactly this purpose. Right now, the region can be split into powers of two assuming uniform distribution within the region. 

","Duplicated Code, Long Method, , "
"   Move Class,Pull Up Method,Move Method,Extract Method,Move Attribute,","Refactor RWQueueRpcExecutor/BalancedQueueRpcExecutor/RpcExecutor 1. The RWQueueRpcExecutor has eight constructor method and the longest one has ten parameters. But It is only used in SimpleRpcScheduler and easy to confused when read the code.
2. There are duplicate method implement in RWQueueRpcExecutor and BalancedQueueRpcExecutor. They can be implemented in their parent class RpcExecutor.
3. SimpleRpcScheduler read many configs to new RpcExecutor. But the CALL_QUEUE_SCAN_SHARE_CONF_KEY is only needed by RWQueueRpcExecutor. And CALL_QUEUE_CODEL_TARGET_DELAY, CALL_QUEUE_CODEL_INTERVAL and CALL_QUEUE_CODEL_LIFO_THRESHOLD are only needed by AdaptiveLifoCoDelCallQueue.

So I thought we can refactor it. Suggestions are welcome.

Review board: https://reviews.apache.org/r/53726/","Duplicated Code, Long Method, , , , Duplicated Code, "
"   Rename Method,Extract Method,","Improve SimpleLoadBalancer to always take server-level balance into account Currently with bytable strategy there might still be server-level imbalance and we will improve this in this JIRA.

Some more background:
When operating large scale clusters(our case), some companies still prefer to use {{SimpleLoadBalancer}} due to its simplicity, quick balance plan generation, etc. Current SimpleLoadBalancer has two modes: 
1. byTable, which only guarantees that the regions of one table could be uniformly distributed. 
2. byCluster, which ignores the distribution within tables and balance the regions all together.
If the pressures on different tables are different, the first byTable option is the preferable one in most case. Yet, this choice sacrifice the cluster level balance and would cause some servers to have significantly higher load, e.g. 242 regions on server A but 417 regions on server B.(real world stats)
Consider this case, a cluster has 3 tables and 4 servers:
{noformat}
server A has 3 regions: table1:1, table2:1, table3:1
server B has 3 regions: table1:2, table2:2, table3:2
server C has 3 regions: table1:3, table2:3, table3:3
server D has 0 regions.
{noformat}
From the byTable strategy's perspective, the cluster has already been perfectly balanced on table level. But a perfect status should be like:
{noformat}
server A has 2 regions: table2:1, table3:1
server B has 2 regions: table1:2, table3:2
server C has 3 regions: table1:3, table2:3, table3:3
server D has 2 regions: table1:1, table2:2
{noformat}
We can see the server loads change from 3,3,3,0 to 2,2,3,2, while the table1, table2 and table3 still keep balanced. And this is the goal this JIRA tries to achieve.

Two UTs will be added as well with the last one demonstrating advantage of the new strategy. Also, a onConfigurationChange method will be implemented to hot control the ""slop"" variable.

We have been using the strategy on our largest cluster for several months, so the effect could be assured to some extent.



","Duplicated Code, Long Method, , "
"   Move Method,Move Attribute,","Expose KeyValue#checkParameters() and checkForTagsLength() to be used by other Cell implementations KeyValue has 2 useful but private functions to check input parameters, as
- checkParameters()
- checkForTagsLength()

It would be great if their's access could be not private, so that they can be used by other Cell or ExtendedCell implementations.

",", , , "
"   Rename Method,Extract Method,","MOB : Make ref cell creation more efficient When we flush MOB data, ref cells are created per actual MOB cell. This creates lots of garbage
Refer MobUtils#createMobRefCell
We need to add 2 tags into the ref cell. An ArrayList is created with default size creating ref array. Call to CellUtil.getTags will create a new ArrayList even if the original cell is having no tags.
A new KV is created which will create a new backing byte[] and do copy. Also along with each of the flush/compaction op, a fresh Tag object is created for TableName tag.
Fixes include
1. A table name tag is not going to change per HStore. Even both the new tags to be added. Will keep a byte[] of these 2 tags at MobHStore level so that all flush and compactions in this store can use it.
2. Create a new MobRefCell just like TagRewriteCell where only value and tags part will be diff from the original cell.

","Duplicated Code, Long Method, , "
"   Rename Method,","Add retry to LoadIncrementalHFiles tool As using the LoadIncrementalHFiles tool with S3 as the filesystem is prone to failing due to FileNotFoundExceptions due to inconsistency, simple, configurable retry logic was added.",", "
"   Rename Method,Extract Method,Inline Method,","Optimize mob compaction with _del files Today, when there is a _del file in mobdir, with major mob compaction, every mob file will be recompacted, this causes lots of IO and slow down major mob compaction (may take months to finish). This needs to be improved. A few ideas are: 

1) Do not compact all _del files into one, instead, compact them based on groups with startKey as the key. Then use firstKey/startKey to make each mob file to see if the _del file needs to be included for this partition.

2). Based on the timerange of the _del file, compaction for files after that timerange does not need to include the _del file as these are newer files.","Duplicated Code, Long Method, , , "
"   Rename Method,","Refactor the AsyncProcess, BufferedMutatorImpl, and HTable The following are reasons of refactoring.
# A update-heavy application, for example, loader, creates many BufferedMutator for batch updates. But these BufferedMutators can’t share a large threadpool because the shutdown() method will be called when closing any BufferedMutator. This patch adds a flag into BufferedMutatorParams for preventing calling the shutdown() method in BufferedMutatorImpl#close
# The AsyncProcess has the powerful traffic control, but the control is against the single table currently. We should allow alternate traffic control implementation for advanced user who want more control.

All suggestions are welcome.
",", "
"   Move Class,Extract Interface,Rename Method,Move Method,Extract Method,Move Attribute,","Refactor the AsyncProcess, BufferedMutatorImpl, and HTable The following are reasons of refactoring.
# A update-heavy application, for example, loader, creates many BufferedMutator for batch updates. But these BufferedMutators can’t share a large threadpool because the shutdown() method will be called when closing any BufferedMutator. This patch adds a flag into BufferedMutatorParams for preventing calling the shutdown() method in BufferedMutatorImpl#close
# The AsyncProcess has the powerful traffic control, but the control is against the single table currently. We should allow alternate traffic control implementation for advanced user who want more control.

All suggestions are welcome.
","Duplicated Code, Long Method, , , , Large Class, "
"   Move Method,Move Attribute,",hold a reference to the HRegion in Store instead of only the HRegionInfo A generally useful and low impact modification. Enabled my fixup patch for HBASE-1715. Lets one potentially take actions on the region out of a store. We're already associating a HRI. Seems like a binding of region to store without the benefits of a reference to the region object. Result passes all local tests.,", , , "
"   Extract Method,Inline Method,","Abstract out an interface for RpcServer.Call RpcServer.Call is a concrete class, but it is marked as:
{noformat}
@InterfaceAudience.LimitedPrivate({HBaseInterfaceAudience.COPROC, HBaseInterfaceAudience.PHOENIX})
{noformat}

Let's abstract out an interface out of it for potential consumers that want to pass it around.","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,",Remove ImmutableSegment#getKeyValueScanner This is based on a discussion over [~anastas]'s patch. The MemstoreSnapshot uses a KeyValueScanner which actually seems redundant considering we already have a SegmentScanner. The idea is that the snapshot scanner should be a simple iterator type of scanner but it lacks the capability to do the reference counting on that segment that is now used in snapshot. With snapshot having mulitple segments in the latest impl it is better we hold on to the segment by doing ref counting.,"Duplicated Code, Long Method, , "
"   Rename Method,","Add observer notification before bulk loaded hfile is moved to region directory Currently the postBulkLoadHFile() hook notifies the locations of bulk loaded hfiles.
However, if bulk load fails after hfile is moved to region directory but before postBulkLoadHFile() hook is called, there is no way for pluggable components (replication - see HBASE-17290, backup / restore) to know which hfile(s) have been moved to region directory.

Even if postBulkLoadHFile() is called in finally block, the write (to backup table or zookeeper) issued from postBulkLoadHFile() may fail, ending up with same situation.

This issue adds a preCommitStoreFile() hook which notifies path of to be committed hfile before bulk loaded hfile is moved to region directory.

With preCommitStoreFile() hook, write (to backup table or zookeeper) can be issued before the movement of hfile.
If write fails, IOException would make bulk load fail, not leaving hfile in region directory.",", "
"   Rename Method,","Near-instantaneous online schema and table state updates We should not need to take a table offline to update HCD or HTD. 

One option for that is putting HTDs and HCDs up into ZK, with mirror on disk catalog tables to be used only for cold init scenarios, as discussed on IRC. In this scheme, regionservers hosting regions of a table would watch permanent nodes in ZK associated with that table for schema updates and take appropriate actions out of the watcher. In effect, schema updates become another item in the ToDo list.

{{/hbase/tables/<table-name>/schema}}

Must be associated with a write locking scheme also handled with ZK primitives to avoid situations where one concurrent update clobbers another.",", "
"   Rename Class,Rename Method,Extract Method,","Introduce per request limit by number of mutations HBASE-16224 introduced hbase.client.max.perrequest.heapsize to limit the amount of data sent from client.

We should consider adding per request limit through the number of mutations in a batch.

In recent troubleshooting sessions, customer had to do this in their application code to avoid OOME on the server side.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Support specifying a WAL directory outside of the root directory Currently, the WAL and the StoreFiles need to be on the same FileSystem. Some FileSystems (such as Amazon S3) don’t support append or consistent writes. These two properties are imperative for the WAL in order to avoid loss of writes. However, StoreFiles don’t necessarily need the same consistency guarantees (since writes are cached locally and if writes fail, they can always be replayed from the WAL).

This JIRA aims to allow users to configure a log directory (for WALs) that is outside of the root directory or even in a different FileSystem. The default value will still put the log directory under the root directory.","Duplicated Code, Long Method, , "
"   Move And Rename Class,Move Method,Inline Method,Move Attribute,","Refactor procedure framework code Changes
- Moved locks out of MasterProcedureScheduler#Queue. One Queue object is used for each namespace/table, which aren't more than 100. So we don't need complexity arising from all functionalities being in one place. SchemaLocking now owns locks and locking implementaion has been moved to procedure2 package.
- Removed NamespaceQueue because it wasn't being used as Queue (add,peek,poll,etc functions threw UnsupportedOperationException). It's was only used for locks on namespaces. Now that locks have been moved out of Queue class, it's not needed anymore.
- Remoed RegionEvent which was there only for locking on regions. Tables/namespaces used locking from Queue class and regions couldn't (there are no separate proc queue at region level), hence the redundance. Now that locking is separate, we can use the same for regions too.
- Removed QueueInterface class. No declarations, except one implementaion, which makes the point of having an interface moot.
- Removed QueueImpl, which was the only concrete implementation of abstract Queue class. Moved functions to Queue class itself to avoid unnecessary level in inheritance hierarchy.
- Removed ProcedureEventQueue class which was just a wrapper around ArrayDeque class. But we now have ProcedureWaitQueue as 'Type class'.
- Encapsulated table priority related stuff in a single class.
- Removed some unused functions.

*Perf using MasterProcedureSchedulerPerformanceEvaluation*
10 threads, 10M ops, 5 tables

Without patch:
10 regions/table : #yield 584980, addBack time 4.1s, poll time 10s
1M regions/table: #yield 16, addBack time 5.9s, poll time 12.9s

With patch:
10 regions/table : #yield 86413, addBack time 4.1s, poll time 8.2s
1M regions/table: #yield 9, addBack time 6s, poll time 13s

*Memory footprint and CPU* (don't compare GC as that depends on life of objects which will be much longer in real-world scenarios)
Without patch
!without-patch.png|width=800!

With patch
!with-patch.png|width=800!
",", , , , "
"   Rename Method,Extract Method,","Reuse the bytes array when building the hfile block There are two improvements.
# The onDiskBlockBytesWithHeader should maintain a bytes array which can be reused when building the hfile.
# The onDiskBlockBytesWithHeader is copied to an new bytes array only when we need to cache the block.
# If no block need to be cached, the uncompressedBlockBytesWithHeader will never be created.

{code:title=HFileBlock.java|borderStyle=solid}
private void finishBlock() throws IOException {
if (blockType == BlockType.DATA) {
this.dataBlockEncoder.endBlockEncoding(dataBlockEncodingCtx, userDataStream,
baosInMemory.getBuffer(), blockType);
blockType = dataBlockEncodingCtx.getBlockType();
}
userDataStream.flush();
// This does an array copy, so it is safe to cache this byte array when cache-on-write.
// Header is still the empty, 'dummy' header that is yet to be filled out.
uncompressedBlockBytesWithHeader = baosInMemory.toByteArray();
prevOffset = prevOffsetByType[blockType.getId()];

// We need to set state before we can package the block up for cache-on-write. In a way, the
// block is ready, but not yet encoded or compressed.
state = State.BLOCK_READY;
if (blockType == BlockType.DATA || blockType == BlockType.ENCODED_DATA) {
onDiskBlockBytesWithHeader = dataBlockEncodingCtx.
compressAndEncrypt(uncompressedBlockBytesWithHeader);
} else {
onDiskBlockBytesWithHeader = defaultBlockEncodingCtx.
compressAndEncrypt(uncompressedBlockBytesWithHeader);
}
// Calculate how many bytes we need for checksum on the tail of the block.
int numBytes = (int) ChecksumUtil.numBytes(
onDiskBlockBytesWithHeader.length,
fileContext.getBytesPerChecksum());

// Put the header for the on disk bytes; header currently is unfilled-out
putHeader(onDiskBlockBytesWithHeader, 0,
onDiskBlockBytesWithHeader.length + numBytes,
uncompressedBlockBytesWithHeader.length, onDiskBlockBytesWithHeader.length);
// Set the header for the uncompressed bytes (for cache-on-write) -- IFF different from
// onDiskBlockBytesWithHeader array.
if (onDiskBlockBytesWithHeader != uncompressedBlockBytesWithHeader) {
putHeader(uncompressedBlockBytesWithHeader, 0,
onDiskBlockBytesWithHeader.length + numBytes,
uncompressedBlockBytesWithHeader.length, onDiskBlockBytesWithHeader.length);
}
if (onDiskChecksum.length != numBytes) {
onDiskChecksum = new byte[numBytes];
}
ChecksumUtil.generateChecksums(
onDiskBlockBytesWithHeader, 0, onDiskBlockBytesWithHeader.length,
onDiskChecksum, 0, fileContext.getChecksumType(), fileContext.getBytesPerChecksum());
}{code}
","Duplicated Code, Long Method, , "
"   Rename Method,","Put writeToWAL methods do not have proper getter/setter names Put.writeToWAL() is the getter, and Put.writeToWAL(boolean) is the setter.

Should be: Put.getWriteToWAL() returning boolean and Put.setWriteToWAL(boolean)",", "
"   Rename Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","RSGroup code refactoring - Making rsGroupInfoManager non-static in RSGroupAdminEndpoint
- Encapsulate RSGroupAdminService into an internal class in RSGroupAdminEndpoint (on need of inheritence).
- Make RSGroupAdminEndpoint extend BaseMasterObserver, so got rid of unwanted empty implementations.
- Change two internal classes in RSGroupAdminServer to non-static (so outer classes' variables can be shared).
- Rename RSGroupSerDe to RSGroupProtobufUtil('ProtobufUtil' is what we use in other places). Moved 2 functions to RSGroupManagerImpl because they are only used there.
- Javadoc comments
- Improving variable names
- Maybe other misc refactoring","Duplicated Code, Long Method, , , , , "
"   Extract Method,Inline Method,","Removing MemStoreScanner and SnapshotScanner With CompactingMemstore becoming the new default, a store comprises multiple memory segments and not just 1-2. MemStoreScanner encapsulates the scanning of segments in the memory part of the store. SnapshotScanner is used to scan the snapshot segment upon flush to disk.
Having the logic of scanners scattered in multiple classes (StoreScanner, SegmentScanner, MemStoreScanner, SnapshotScanner) makes maintainance and debugging challenging tasks, not always for a good reason.
For example, MemStoreScanner has a KeyValueHeap (KVH). When creating the store scanner which also has a KVH, this makes a KVH inside a KVH. Reasoning about the correctness of the methods supported by the scanner (seek, next, hasNext, peek, etc.) is hard and debugging them is cumbersome. 
In addition, by removing the MemStoreScanner layer we allow store scanner to filter out each one of the memory scanners instead of either taking them all (in most cases) or discarding them all (rarely).
SnapshotScanner is a simplified version of SegmentScanner as it is used only in a specific context. However it is an additional implementation of the same logic with no real advantage of improved performance.

Therefore, I suggest removing both MemStoreScanner and SnapshotScanner. The code is adjusted to handle the list of segment scanners they encapsulate.
This fits well with the current code since in most cases at some point a list of scanner is expected, so passing the actual list of segment scanners is more natural than wrapping a single (high level) scanner with Collections.singeltonList(...).","Duplicated Code, Long Method, , , "
"   Rename Class,Move And Rename Class,Rename Method,Pull Up Method,Move Method,Extract Method,Move Attribute,","Coprocessor Design Improvements The two main changes are:
* *Adding template for coprocessor type to CoprocessorEnvironment i.e. {{interface CoprocessorEnvironment<C extends Coprocessor>}}*
** Enables us to load only relevant coprocessors in hosts. Right now each type of host loads all types of coprocs and it's only during execOperation that it checks if the coproc is of correct type i.e. XCoprocessorHost will load XObserver, YObserver, and all others, and will check in execOperation if {{coproc instanceOf XObserver}} and ignore the rest.
** Allow sharing of a bunch functions/classes which are currently duplicated in each host. For eg. CoprocessorOperations, CoprocessorOperationWithResult, execOperations().
* *Introduce 4 coprocessor classes and use composition between these new classes and and old observers*
** The real gold here is, moving forward, we'll be able to break down giant everything-in-one observers (masterobserver has 100+ functions) into smaller, more focused observers. These smaller observer can then have different compat guarantees!!

Here's a more detailed design doc: https://docs.google.com/document/d/1mPkM1CRRvBMZL4dBQzrus8obyvNnHhR5it2yyhiFXTg/edit?usp=sharing","Duplicated Code, Long Method, , , , Duplicated Code, "
"   Move Class,Rename Class,Extract Superclass,","Support both weak and soft object pool During YCSB testing on embedded mode after HBASE-17744, we found that under high read load GC is quite severe even with offheap L2 cache. After some investigation, we found it's caused by using weak reference in {{IdReadWriteLock}}. In embedded mode the read is so quick that the lock might already get promoted to the old generation when the weak reference is cleared, which causes dirty card table (old reference get removed and new lock object set into {{referenceCache}}, see {{WeakObjectPool#get}}) thus slowing YGC. In distributed mode there'll also be more lock object created with weak reference than soft reference that slowing down the processing.

So we proposed to use soft reference for this {{IdReadWriteLock}} used in cache, which won't get cleared until JVM memory is not enough, and could resolve the issue mentioned above. What's more, we propose to extend the {{WeakObjectPool}} to be more generate to support both weak and soft reference.

Note that the GC issue only emerges under embedded mode with DirectOperator, in which case all costs on the wire is removed thus produces extremely high concurrency.",", Duplicated Code, Large Class, "
"   Push Down Method,Extract Method,Push Down Attribute,",Remove the testing code in the AsyncRequestFutureImpl HBASE-16224 left some testing code in the AsyncRequestFutureImpl. It iterates all mutations in order to get the data size. The cost is directly proportional to the size of batch. We should remove it.,"Duplicated Code, Long Method, , , , "
"   Rename Method,","Improve PerformanceEvaluation Current PerformanceEvaluation class have two problems:
- It is not updated for hadoop-0.20.0. 
- The approach to split maps is not strict. Need to provide correct InputSplit and InputFormat classes. Current code uses TextInputFormat and FileSplit, it is not reasonable.

We will fix these problems.",", "
"   Rename Method,","Add generic methods for updating metrics on start and end of a procedure execution For all procedures in Procv2 framework, Procedure class can have generic methods to update metrics on start and end of a procedure execution. Specific procedure can override these and implement/ update respective metrics. Default implementation needs to be provided so override and implementation is optional.",", "
"   Move Method,Extract Method,Move Attribute,","Too many ZK connections Currently we open tons of new connections to Zookeeper, like every time we instantiate a new HTable. There is a maximum number of client connections as described here:

{code}
<property>
<name>hbase.zookeeper.property.maxClientCnxns</name>
<value>30</value>
<description>Property from ZooKeeper's config zoo.cfg.
Limit on number of concurrent connections (at the socket level) that a
single client, identified by IP address, may make to a single member of
the ZooKeeper ensemble. Set high to avoid zk connection issues running
standalone and pseudo-distributed.
</description>
</property>
{code}

If you hit that max number, ZK will just refuse your connections. Suppose you have 4 maps running on a server hosting a RS, you may actually lose your connection in the RS and eventually hit a session timeout. Maybe we should singleton ZKW?","Duplicated Code, Long Method, , , , "
"   Rename Method,Move Method,Extract Method,Move Attribute,",Refactor ReplicationSource One basic idea is move the code about recovered queue to a new subclass RecoveredReplicationSource. Then ReplicationSource will don't need call isQueueRecovered many times. This will make the code more clearly.,"Duplicated Code, Long Method, , , , "
"   Rename Method,Move Method,Extract Method,","Low-latency space quota size reports Presently space quota enforcement relies on RegionServers sending reports to the master about each Region that they host. This is done by periodically, reading the cached size of each HFile in each Region (which was ultimately computed from HDFS).

This means that the Master is unaware of Region size growth until the the next time this chore in a RegionServer fires which is a fair amount of latency (a few minutes, by default). Operations like flushes, compactions, and bulk-loads are delayed even though the RegionServer is running those operations locally.

Instead, we can create an API which these operations could invoke that would automatically update the size of the Region being operated on. For example, a successful flush can report that the size of a Region increased by the size of the flush. A compaction can subtract the size of the input files of the compaction and add in the size of the resulting file.

This de-couples the computation of a Region's size from sending the Region sizes to the Master, allowing us to send reports more frequently, increasing the responsiveness of the cluster to size changes.","Duplicated Code, Long Method, , , "
"   Move Class,Rename Method,Move Method,Extract Method,","Track file archival for low latency space quota with snapshots Related to the work proposed on HBASE-17748 and building on the same idea as HBASE-18133, we can make the space quota tracking for HBase snapshots faster to respond.

When snapshots are in play, the location of a file (whether in the {{data}} or {{archive}} directory) plays a factor in the realized size of a table. Like flushes, compactions, etc, moving files from the data directory to the archive directory is done by the RegionServer. We can hook into this call and send the necessary information to the Master so that it can more quickly update the size of a table when there are snapshots in play.

This will require the RegionServer to report the full coordinates of the file being moved (table+region+family+file) so that the SnapshotQuotaObserverChore running in the master can avoid HDFS lookups in partial or total to compute the location of a Region's hfiles.

This may also require some refactoring of the SnapshotQuotaObserverChore to de-couple the receipt of these file archival reports from RegionServers (e.g. {{HRegionFileSystem.removeStoreFiles(..)}}, and the Master processing the sizes of snapshots.","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Incremental Load support for Multiple-Table HFileOutputFormat h2. Introduction

MapReduce currently supports the ability to write HBase records in bulk to HFiles for a single table. The file(s) can then be uploaded to the relevant RegionServers information with reasonable latency. This feature is useful to make a large set of data available for queries at the same time as well as provides a way to efficiently process very large input into HBase without affecting query latencies.

There is, however, no support to write variations of the same record key to HFiles belonging to multiple HBase tables from within the same MapReduce job. 

h2. Goal

The goal of this JIRA is to extend HFileOutputFormat2 to support writing to HFiles for different tables within the same MapReduce job while single-table HFile features backwards-compatible. 

For our use case, we needed to write a record key to a smaller HBase table for quicker access, and the same record key with a date appended to a larger table for longer term storage with chronological access. Each of these tables would have different TTL and other settings to support their respective access patterns. We also needed to be able to bulk write records to multiple tables with different subsets of very large input as efficiently as possible. Rather than run the MapReduce job multiple times (one for each table or record structure), it would be useful to be able to parse the input a single time and write to multiple tables simultaneously.

Additionally, we'd like to maintain backwards compatibility with the existing heavily-used HFileOutputFormat2 interface to allow benefits such as locality sensitivity (that was introduced long after we implemented support for multiple tables) to support both single table and multi table hfile writes. 

h2. Proposal
* Backwards compatibility for existing single table support in HFileOutputFormat2 will be maintained and in this case, mappers will need to emit the table rowkey as before. However, a new class - MultiHFileOutputFormat - will provide a helper function to generate a rowkey for mappers that prefixes the desired tablename to the existing rowkey as well as provides configureIncrementalLoad support for multiple tables.
* HFileOutputFormat2 will be updated in the following way:
** configureIncrementalLoad will now accept multiple table descriptor and region locator pairs, analogous to the single pair currently accepted by HFileOutputFormat2. 
** Compression, Block Size, Bloom Type and Datablock settings PER column family that are set in the Configuration object are now indexed and retrieved by tablename AND column family
** getRegionStartKeys will now support multiple regionlocators and calculate split points and therefore partitions collectively for all tables. Similarly, now the eventual number of Reducers will be equal to the total number of partitions across all tables. 
** The RecordWriter class will be able to process rowkeys either with or without the tablename prepended depending on how configureIncrementalLoad was configured with MultiHFileOutputFormat or HFileOutputFormat2.
* The use of MultiHFileOutputFormat will write the output into HFiles which will match the output format of HFileOutputFormat2. However, while the default use case will keep the existing directory structure with column family name as the directory and HFiles within that directory, in the case of MultiHFileOutputFormat, it will output HFiles in the output directory with the following relative paths: 
{noformat}
--table1 
--family1 
--HFiles 
--table2 
--family1 
--family2 
--HFiles
{noformat}

This aims to be a comprehensive solution to the original tickets - HBASE-3727 and HBASE-16261. Thanks to [~clayb] for his support. This is a contribution from Bloomberg developers.

The patch will be attached shortly.","Duplicated Code, Long Method, , "
"   Move Method,Extract Method,Move Attribute,","Much faster locality cost function and candidate generator We noticed that during the stochastic load balancer was not scaling well with cluster size. That is to say that on our smaller clusters (~17 tables, ~12 region servers, ~5k regions), the balancer considers ~100,000 cluster configurations in 60s per balancer run, but only ~5,000 per 60s on our bigger clusters (~82 tables, ~160 region servers, ~13k regions) .

Because of this, our bigger clusters are not able to converge on balance as quickly for things like table skew, region load, etc. because the balancer does not have enough time to ""think"".

We have re-written the locality cost function to be incremental, meaning it only recomputes cost based on the most recent region move proposed by the balancer, rather than recomputing the cost across all regions/servers every iteration.

Further, we also cache the locality of every region on every server at the beginning of the balancer's execution for both the LocalityBasedCostFunction and the LocalityCandidateGenerator to reference. This way, they need not collect all HDFS blocks of every region at each iteration of the balancer.

The changes have been running in all 6 of our production clusters and all 4 QA clusters without issue. The speed improvements we noticed are massive. Our big clusters now consider 20x more cluster configurations.

One design decision I made is to consider locality cost as the difference between the best locality that is possible given the current cluster state, and the currently measured locality. The old locality computation would measure the locality cost as the difference from the current locality and 100% locality, but this new computation instead takes the difference between the current locality for a given region and the best locality for that region in the cluster.","Duplicated Code, Long Method, , , , "
"   Rename Class,Rename Method,Extract Method,","Refactor ReplicationSourceWALReaderThread HBASE-18130 add some get* method to ReplicationSource. So ReplicationSourceWALReaderThread doesn't need so many parameter to initialize. And the WALEntryFilter only used by ReplicationSourceWALReaderThread, so we don't need to new it for every ReplicationSourceWALReaderThread. Meanwhile, we can separate a new RecoveredReplicationSourceWALReaderThread for recovered replication source. Thanks.","Duplicated Code, Long Method, , "
"   Extract Method,Inline Method,",Remove the deprecated APIs Remove all the deprecated stuff in client and mapred.,"Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Reduce global heap pressure: flush based on heap occupancy A region is flushed if its memory component exceed a threshold (default size is 128MB).
A flush policy decides whether to flush a store by comparing the size of the store to another threshold (that can be configured with hbase.hregion.percolumnfamilyflush.size.lower.bound).
Currently the implementation (in both cases) compares the data size (key-value only) to the threshold where it should compare the heap size (which includes index size, and metadata).
","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Remove byte[] from formal parameter of sizeOf() of ClassSize, ClassSize.MemoryLayout and ClassSize.UnsafeLayout In ClassSize class and its internal static class, sizeOf() function has 2 formal parameters: byte[] b and int len. But the function's internal logic does not use or refer to byte[] b. Could be removed.

{code:title=hbase-common/src/main/java/org/apache/hadoop/hbase/util/ClassSize.java|borderStyle=solid}
// Class of ClassSize
public static long sizeOf(byte[] b, int len) {
return memoryLayout.sizeOf(b, len);
}

// Class of ClassSize.MemoryLayout
long sizeOf(byte[] b, int len) {
return align(arrayHeaderSize() + len);
}

// Class of ClassSize.UnsafeLayout
long sizeOf(byte[] b, int len) {
return align(arrayHeaderSize() + len * UnsafeAccess.theUnsafe.ARRAY_BYTE_INDEX_SCALE);
}
{code}","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","MultiGet, MultiDelete, and MultiPut - batched to the appropriate region servers I've started to create a general interface for doing these batch/multi calls and would like to get some input and thoughts about how we should handle this and what the protocol should
look like. 

First naive patch, coming soon.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,Move Attribute,","MultiGet, MultiDelete, and MultiPut - batched to the appropriate region servers I've started to create a general interface for doing these batch/multi calls and would like to get some input and thoughts about how we should handle this and what the protocol should
look like. 

First naive patch, coming soon.","Duplicated Code, Long Method, , , "
"   Rename Method,","Expose BucketCache values to be configured BucketCache always uses the default values for all cache configuration. However, this doesn't work for all use cases. In particular, users want to be able to configure the percentage of the cache that is single access, multi access, and in-memory access.",", "
"   Rename Method,Extract Method,","Always overwrite the TS for Append/Increment unless no existing cells are found We don't pass the custom timestamp for Increment, and the increment's timestamp always be rewrite. Hence, user can't increment a cell with custom timestamp.
{code:title=ProtobufUtil.java}
if (values != null && values.size() > 0) {
for (Cell cell: values) {
valueBuilder.clear();
valueBuilder.setQualifier(UnsafeByteOperations.unsafeWrap(
cell.getQualifierArray(), cell.getQualifierOffset(), cell.getQualifierLength()));
valueBuilder.setValue(UnsafeByteOperations.unsafeWrap(
cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
if (cell.getTagsLength() > 0) {
valueBuilder.setTags(UnsafeByteOperations.unsafeWrap(cell.getTagsArray(),
cell.getTagsOffset(), cell.getTagsLength()));
}
columnBuilder.addQualifierValue(valueBuilder.build());
}
}
{code}
In contrast to Increment, user can append the cell with custom timestamp. It would be better that make their behavior consistent.","Duplicated Code, Long Method, , "
"   Rename Method,","Refactor ClusterOptions before applying to code base So far, ClusterStatus.Options is not so clean that can be applied to code base.
Refactoring it before next move.",", "
"   Rename Class,Move And Rename Class,",Deprecate KV Usage in MR to move to Cells in 3.0 0,", "
"   Move Class,Extract Superclass,Extract Method,",Copy LoadIncrementalHFiles to another package and mark the old one as deprecated LoadIncrementalHFiles does not depend on map reduce.,"Duplicated Code, Long Method, , Duplicated Code, Large Class, "
"   Extract Superclass,Move Method,Move Attribute,","Use of filesystem that requires hflush / hsync / append / etc should query outputstream capabilities In places where we rely on the underlying filesystem holding up the promises of hflush/hsync (most importantly the WAL), we should use the new interfaces provided by HDFS-11644 to fail loudly when they are not present (e.g. on S3, on EC mounts, etc).",", , , Duplicated Code, Large Class, "
"   Rename Method,Extract Method,","VerifyRep by snapshot need not to restore snapshot for each mapper In following method stack, seems like each mapper task will restore the snapshot. If we verify replication by a snapshot which has many hfiles, then we will take long time to restore snapshot. In our cluster, we took ~30min for the snapshot restoring when verify a big table.

{code}

Verifier.map
|--------> replicatedScanner = new TableSnapshotScanner(...)
|--------> TableSnapshotScanner.init()
|-----> RestoreSnapshotHelper.copySnapshotForScanner
{code}","Duplicated Code, Long Method, , "
"   Rename Method,","Coprocessor Design Improvements follow up of HBASE-17732 Creating new jira to track suggestions that came in review (https://reviews.apache.org/r/62141/) but are not blocker and can be done separately.

Suggestions by [~apurtell]
- Change {{Service Coprocessor#getService()}} to {{List<Service> Coprocessor#getServices()}}
- I think we overstepped by offering [table resource management via this interface|https://github.com/apache/hbase/blob/master/hbase-client/src/main/java/org/apache/hadoop/hbase/CoprocessorEnvironment.java#L57]. There are a lot of other internal resource types which could/should be managed this way but they are all left up to the implementor. Perhaps we should remove the table ref management and leave it up to them as well.
----
- Checkin the finalized design doc into repo (https://docs.google.com/document/d/1mPkM1CRRvBMZL4dBQzrus8obyvNnHhR5it2yyhiFXTg/edit) (fyi: [~stack])
- Added example to javadoc of Coprocessor base interface on how to implement one in the new design",", "
"   Move Method,Extract Method,","Bulk incremental load into an existing table hbase-48 is about bulk load of a new table,maybe it's more practicable to bulk load aganist a existing table.
","Duplicated Code, Long Method, , , "
"   Rename Method,","Move the transform logic of FilterList into transformCell() method to avoid extra ref to question cell  As [~anoop.hbase] and I discussed, we can implement the filterKeyValue () and transformCell() methods as following to avoid saving transformedCell & referenceCell state in FilterList, and we can avoid the costly cell clone. 

{code} 
ReturnCode filterKeyValue(Cell c){ 
ReturnCode rc = null; 
for(Filter filter: sub-filters){ 
// ... 
rc = mergeReturnCode(rc, filter.filterKeyValue(c)); 
// ... 
} 
return rc; 
} 

Cell transformCell(Cell c) throws IOException { 
Cell transformed = c; 
for(Filter filter: sub-filters){ 
if(filter.filterKeyValue(c) is INCLUDE*) { // ----> line#1 
transformed = filter.transformCell(transformed); 
} 
} 
return transformed; 
} 
{code} 

For line #1, we need to remember the return code of the sub-filter for its filterKeyValue(). because only INCLUDE* ReturnCode, we need to transformCell for sub-filter. 

A new boolean array will be introduced in FilterList. and the cost of maintaining the boolean array will be less than the cost of maintaining the two ref of question cell. 

",", "
"   Rename Method,Extract Method,","Reduce zk request when doing split log We observe once the cluster has 1000+ nodes and when hundreds of nodes abort and doing split log, the split is very very slow, and we find the regionserver and master wait on the zookeeper response, so we need to reduce zookeeper request and pressure for big cluster. 
(1) Reduce request to rsZNode, every time calculateAvailableSplitters will get rsZNode's children from zookeeper, when cluster is huge, this is heavy. This patch reduce the request. 
(2) When the regionserver has max split tasks running, it may still trying to grab task and issue zookeeper request, we should sleep and wait until we can grab tasks again.","Duplicated Code, Long Method, , "
"   Move Class,Rename Method,Move Method,Inline Method,Move Attribute,",Promote TestAcidGuarantees to LargeTests and start mini cluster once to make it faster 0,", , , , "
"   Rename Method,Inline Method,","Refactoring in RegionStates, and RSProcedureDispatcher Working on a bug fix, was in these parts for first time to understand new AM and trying to make sense of things. Did a few improvements on the way. 

- Adding javadoc comments 
- Bug: ServerStateNode#regions is HashSet but there's no synchronization to prevent concurrent addRegion/removeRegion. Let's use concurrent set instead. 
- Use getRegionsInTransitionCount() directly to avoid instead of getRegionsInTransition().size() because the latter copies everything into a new array - what a waste for just the size. 
- There's mixed use of getRegionNode and getRegionStateNode for same return type - RegionStateNode. Changing everything to getRegionStateNode. Similarly rename other *RegionNode() fns to *RegionStateNode(). 
- RegionStateNode#transitionState() return value is useless since it always returns it's first param. 
- Other minor improvements",", , "
"   Extract Method,Inline Method,",Remove the Span object in SyncFuture as it is useless now 0,"Duplicated Code, Long Method, , , "
"   Rename Method,","Make CPEnv#getConnection return a facade that throws Unsupported if CP calls #close Follows from HBASE-19301, a suggestion by [~zghaobac]. 

To prevent a CP accidentally closing the connection returned by CpEnv#getConnection -- which returns the hosting Servers Connection -- we should throw UnsupportedException if the CP calls #close.... Do it.",", "
"   Rename Method,Move Method,Extract Method,","Add proper privilege check for rsgroup commands Currently list_rsgroups command can be executed by any user. 

This is inconsistent with other list commands such as list_peers and list_peer_configs. 

We should add proper privilege check for list_rsgroups command. 

privilege check should be added for get_table_rsgroup / get_server_rsgroup / get_rsgroup commands.","Duplicated Code, Long Method, , , "
"   Rename Method,Move Method,Extract Method,Inline Method,","Add proper privilege check for rsgroup commands Currently list_rsgroups command can be executed by any user. 

This is inconsistent with other list commands such as list_peers and list_peer_configs. 

We should add proper privilege check for list_rsgroups command. 

privilege check should be added for get_table_rsgroup / get_server_rsgroup / get_rsgroup commands.","Duplicated Code, Long Method, , , , "
"   Rename Method,Move Method,Inline Method,","Add proper privilege check for rsgroup commands Currently list_rsgroups command can be executed by any user. 

This is inconsistent with other list commands such as list_peers and list_peer_configs. 

We should add proper privilege check for list_rsgroups command. 

privilege check should be added for get_table_rsgroup / get_server_rsgroup / get_rsgroup commands.",", , , "
"   Rename Method,Extract Method,",Add TimeRange support into checkAndMutate 0,"Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,Inline Method,","BufferedMutatorImpl#mutate should wait the result from AP in order to throw the failed mutations Currently, BMI#mutate doesn't wait the result from AP so the errors are stored in AP. The only way which can return the errors to user is, calling the flush to catch the exception. That is non-intuitive. 

I feel BMI#mutate should wait the result. That is to say, user can parse the exception thrown by BM#mutate to get the failed mutations. Also, we can remove the global error from AP. 




","Duplicated Code, Long Method, , , "
"   Move Class,Rename Method,Extract Method,",Unit test the full WAL replay cycle Currently we test log splitting (master's current role) but we never try it with Store.runReconstructionLog(). Now is a good time to unit test the whole cycle.,"Duplicated Code, Long Method, , "
"   Rename Method,Inline Method,","Remove BaseLogCleanerDelegate deprecated #isLogDeletable(FileStatus) and use #isFileDeletable(FileStatus) instead Mark #isLogDeletable(FileStatus) deprecated, and update server-side codes to use #isFileDeletable(FileStatus).",", , "
"   Rename Method,Extract Method,","Use RegionInfo directly instead of an identifier and a namespace when getting WAL This is needed for synchronous replication as we need to determine whether a region should use the special WAL which writes to two FileSystems. 

But I think it is a general simplification so we can apply it to master and branch-2 first.","Duplicated Code, Long Method, , "
"   Move Class,Rename Method,Move Method,Move Attribute,","Split TestHCM to several smaller tests It takes too much time to finish, especially the timeout related tests. We should move these tests out.",", , , "
"   Rename Method,","RowMutations should follow the fluent pattern Other row ops, including {{Put}}, {{Delete}}, {{Get}}, {{Scan}}, do have the fluent interface. Also, Changing the return type from {{Void}} to {{RowMutations}} won't break the API BC (unless someone has interest in {{Void}} object...)",", "
"   Rename Class,Rename Method,Move Method,Inline Method,","Break dependency of WAL constructor on Replication When implementing synchronous replication, I found that we need to depend more on replication in WAL so it is even more pain...",", , , "
"   Rename Method,",Redesign single instance pool in CleanerChore 0,", "
"   Rename Method,","Improve snapshot manifest copy in ExportSnapshot ExportSnapshot need to copy snapshot manifest to destination cluster first, then setOwner and setPermission for those paths. But it's done with one thread, which lead to a long time to submit the job if your snapshot is big. I tried to make them processing in parallel, which can reduce the total time of submitting dramatically.",", "
"   Move Class,Rename Class,Extract Method,Move Attribute,","Break out WAL reader and writer impl from HLog Use Configuration and reflection to bind HLog reader and writer at runtime, to allow users to chose between different HLog reader/writer implementation pairs.","Duplicated Code, Long Method, , , "
"   Move Method,Move Attribute,","Validate pre-2.0 coprocessors against HBase 2.0+ We have co-processors for a while, but the API has been changed recently. We should give some tooling for our users to determine if they can use the previous co-processors safely or not. 

The tool should: 
- try to load the co-processors on our current classpath for ensuring class references are on our classpath, 
- should check for previously removed co-processor methods. 

In this version we check only method signatures. Code references should be checked in further versions.",", , , "
"   Rename Method,Extract Method,","Colocate recovered edits directory with hbase.wal.dir During investigation of HBASE-20723, I realized that we wouldn't get the best performance when hbase.wal.dir is configured to be on different (fast) media than hbase rootdir w.r.t. recovered edits since recovered edits directory is currently under rootdir. 

Such setup may not result in fast recovery when there is region server failover. 

This issue is to find proper (hopefully backward compatible) way in colocating recovered edits directory with hbase.wal.dir .","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,Inline Method,",Modify pre-upgrade coprocessor validator to support table level coprocessors 0,"Duplicated Code, Long Method, , , "
"   Rename Method,","Improve Snapshot Performance with Temporary Snapshot Directory when rootDir on S3 When using Apache HBase, the snapshot feature can be used to make a point in time recovery. To do this, HBase creates a manifest of all the files in all of the Regions so that those files can be referenced again when a user restores a snapshot. With HBase's S3 storage mode, developers can store their data off-cluster on Amazon S3. However, utilizing S3 as a file system is inefficient in some operations, namely renames. Most Hadoop ecosystem applications use an atomic rename as a method of committing data. However, with S3, a rename is a separate copy and then a delete of every file which is no longer atomic and, in fact, quite costly. In addition, puts and deletes on S3 have latency issues that traditional filesystems do not encounter when manipulating the region snapshots to consolidate into a single manifest. When HBase on S3 users have a significant amount of regions, puts, deletes, and renames (the final commit stage of the snapshot) become the bottleneck causing snapshots to take many minutes or even hours to complete. 

The purpose of this patch is to increase the overall performance of snapshots while utilizing HBase on S3 through the use of a temporary directory for the snapshots that exists on a traditional filesystem like HDFS to circumvent the bottlenecks.",", "
"   Rename Method,",Save on a few log strings and some churn in wal splitter by skipping out early if no logs in dir Trivial change to splitlogmanager that saves us a log line at least per WAL dir when it goes to split. Also saves some not-needed churn in SLM.,", "
"   Rename Class,Move Method,","Split TableInputFormatScan to individual tests We have done a split in HBASE-8326, which split the test to two parts. But it is still a bit slow, split it into several tests can increase the parallelism and make the 'mvn test' run faster.",", , "
"   Extract Superclass,Extract Method,","Make RefreshHFilesClient runnable Other than when user enables hbase.coprocessor.region.classes with RefreshHFilesEndPoint, user can also run this client as tool runner class/CLI and calls refresh HFiles directly.","Duplicated Code, Long Method, , Duplicated Code, Large Class, "
"   Extract Method,Move Attribute,","Compaction events should be written to HLog The sequence for a compaction should look like this:
# Compact region to ""new"" files
# Write a ""Compacted Region"" entry to the HLog
# Delete ""old"" files

This deals with a case where the RS has paused between step 1 and 2 and the regions have since been reassigned.","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Support both Hadoop 0.20, 0.21, and 0.22 Since Hadoop 0.21 isn't going to be well supported and that a lot of users may wish to stick on 0.20, the next HBase major release should support both 0.20 and 0.21. 

HDFS-265 support will be swapped for HDFS-200 if running on HDFS 0.20. A cluster without that patchset shouldn't be supported.","Duplicated Code, Long Method, , "
"   Rename Method,","HFile and Memstore should maintain minimum and maximum timestamps In order to fix HBASE-1485 and HBASE-29, it would be very helpful to have HFile and Memstore track their maximum and minimum timestamps. This has the following nice properties:

- for a straight Get, if an entry has been already been found with timestamp X, and X >= HFile.maxTimestamp, the HFile doesn't need to be checked. Thus, the current fast behavior of get can be maintained for those who use strictly increasing timestamps, but ""correct"" behavior for those who sometimes write out-of-order.
- for a scan, the ""latest timestamp"" of the storage can be used to decide which cell wins, even if the timestamp of the cells is equal. In essence, rather than comparing timestamps, instead you are able to compare tuples of (row timestamp, storage.max_timestamp)
- in general, min_timestamp(storage A) >= max_timestamp(storage B) if storage A was flushed after storage B.",", "
"   Rename Method,Extract Method,","[Transactional Contrib] Correctly handle or avoid cases where writes occur in same millisecond This patch fixes a few issues where puts/deletes occur with the same timestamp.

In the indexing layer, we avoid a Delete followed by a Put to the same row for the index update. When the row is the same, we can just do the put.

In the transactional layer, we correctly handled put, put, scan. This way the scan will see the last put, even if they have the same timestamp.

Remove the sleep to fix the putPutScan transactional test, and run it many times to be sure we hit the case where they are in the same millisecond.

Also some small cleanup, null handling, and fail-fast changes.","Duplicated Code, Long Method, , "
"   Move And Rename Class,Move Method,","Refactoring of TableRecordReader (mapred / mapreduce) for reuse outside the scope of InputSplit / RecordReader  For the storing of tf-idf in hbase ( lucene-hbase project) we need to scan the keys across the table and retrieve columnar values. Quite an amount of logic can be reused from TableRecordReader for the purpose. 

- Refactored TableRecordReader ( from being a protected inner class to a public class outside ) 
- Created an impl class , that does the actual work, without the dependency on hadop.mapreduce.* packages ( RecordReader) while retaining the implementation that can be reused across libraries. 


Do the same thing for .mapred. and .mapreduce. packages. 

Let me know the thoughts on the same.",", , "
"   Move Class,Extract Method,","[stargate] review Jersey and JSON dependencies From Thomas Koch over on HBASE-2383:

{quote}
The stargate lib folder contains:
asm-3.1.jar, jackson-asl-0.9.4.jar, jersey-core-1.1.0-ea.jar, jersey-server-1.1.0-ea.jar, persistence-api-1.0.jar, commons-codec-1.3.jar, jaxb-impl-2.1.10.jar, jersey-json-1.1.0-ea.jar, jsr311-api-1.1.jar, protobuf-java-2.1.0.jar

It seems, that the following jars are either not used or only used for the tests:
asm-3.1.jar, jackson-asl-0.9.4.jar, jaxb-impl-2.1.10.jar

The following are already in Debian:
commons-codec-1.3.jar
persistence-api-1.0.jar (libgeronimo-jpa-3.0-spec-java 1.1.1-1)
protobuf-java-2.1.0.jar (libprotobuf-java 2.3.0-1)

Which leaves the following to be packaged:

jersey-core-1.1.0-ea.jar, jersey-server-1.1.0-ea.jar, jersey-json-1.1.0-ea.jar ( https://jersey.dev.java.net )

jsr311-api-1.1.jar ( https://jsr311.dev.java.net )

Upstream version of jersey is 1.1.5.1. Would stargate work with this version?

java.net doesn't seem to release tarballs, so I could obtain the sources only from these jars:

http://download.java.net/maven/2/com/sun/jersey/jersey-bundle/1.1.5.1/jersey-bundle-1.1.5.1-sources.jar
http://download.java.net/maven/2/javax/ws/rs/jsr311-api/1.1.1/jsr311-api-1.1.1-sources.jar
{quote}

The jars in the Stargate lib directory were added per the getting started recipe up on the Jersey wiki.

Tasks:

- Update Jersey to 1.1.5.1.

- Remove json.org JSON dependencies and substitute as required. 

- Prune unnecessary jars.
","Duplicated Code, Long Method, , "
"   Rename Method,","Concurrent flushers in HLog sync using HDFS-895 HDFS-895 changes hflush() to be able to run concurrently from multiple threads, where flushes can be concurrent with further writes to the same file.

We need to rip out/amend the group commit code a bit to take advantage of this.",", "
"   Rename Method,Move Method,Extract Method,","Refactor StoreFile Code Currently, the StoreFile code is a thin wrapper around an HFile.Reader. With the addition of BloomFilters and other features that operate at the HFile layer, we need to clarify the difference between a StoreFile & HFile. To that end, we need to refactor the StoreFile.Reader code and the code that inter-operates with it.","Duplicated Code, Long Method, , , "
"   Rename Class,Rename Method,","REST module tests use deprecated foundation The REST module unit tests are based on HBaseClusterTestCase, which is deprecated. Update to use junit4 and HBaseTestingUtility.",", "
"   Rename Method,Extract Method,","Add atomic checkAndDelete support Currently HBase has support for atomic checkAndPut operations on individual rows. It would be very useful to also support atomic checkAndDelete. I have added support for atomic checkAndDelete to trunk and will provide the corresponding patch tonight after I merge in any upstream changes from the last few days.

","Duplicated Code, Long Method, , "
"   Rename Method,","speed up REST tests In the meantime before HBASE-2564, the REST tests could be a lot faster than they are currently.",", "
"   Move Class,Move Method,Extract Method,","Refactor HLog splitLog, hbase-2437 continued; break out split code as new classes A suggestion made during a review of hbase-2437 is that we further untangle hlog split code by moving split code and support out of hlog into distinct classes. See early Todd Lipcon comment over in http://review.hbase.org/r/74/","Duplicated Code, Long Method, , , "
"   Move Class,Rename Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Master rewrite and cleanup for 0.90 This is the parent issue for master changes targeted at 0.90 release.

Changes done as part of this issue grew out of work done over in HBASE-2485 to move region transitions into ZK.

In addition to that work, this issue will include general HMaster and ZooKeeper refactorings and cleanups.","Duplicated Code, Long Method, , , , , "
"   Rename Method,Extract Method,","Provide strong authentication with a secure RPC engine The HBase RPC code (org.apache.hadoop.hbase.ipc.*) was originally forked off of Hadoop RPC classes, with some performance tweaks added. Those optimizations have come at a cost in keeping up with Hadoop RPC changes however, both bug fixes and improvements/new features. 

In particular, this impacts how we implement security features in HBase (see HBASE-1697 and HBASE-2016). The secure Hadoop implementation (HADOOP-4487) relies heavily on RPC changes to support client authentication via kerberos and securing and mutual authentication of client/server connections via SASL. Making use of the built-in Hadoop RPC classes will gain us these pieces for free in a secure HBase.

So, I'm proposing that we drop the HBase forked version of RPC and convert to direct use of Hadoop RPC, while working to contribute important fixes back upstream to Hadoop core. Based on a review of the HBase RPC changes, the key divergences seem to be:

HBaseClient:
- added use of TCP keepalive (HBASE-1754)
- made connection retries and sleep configurable (HBASE-1815)
- prevent NPE if socket == null due to creation failure (HBASE-2443)

HBaseRPC:
- mapping of method names <-> codes (removed in HBASE-2219)

HBaseServer:
- use of TCP keep alives (HBASE-1754)
- OOME in server does not trigger abort (HBASE-1198)

HbaseObjectWritable:
- allows List<> serialization
- includes it's own class <-> code mapping (HBASE-328)


Proposed process is:

1. open issues with patches on Hadoop core for important fixes/adjustments from HBase RPC (HBASE-1198, HBASE-1815, HBASE-1754, HBASE-2443, plus a pluggable ObjectWritable implementation in RPC.Invocation to allow use of HbaseObjectWritable).

2. ship a Hadoop version with RPC patches applied -- ideally we should avoid another copy-n-paste code fork, subject to ability to isolate changes from impacting Hadoop internal RPC wire formats

3. if all Hadoop core patches are applied we can drop back to a plain vanilla Hadoop version


I realize there are many different opinions on how to proceed with HBase RPC, so I'm hoping this issue will kick off a discussion on what the best approach might be. My own motivation is maximizing re-use of the authentication and connection security work that's already gone into Hadoop core. I'll put together a set of patches around #1 and #2, but obviously we need some consensus around this to move forward. If I'm missing other differences between HBase and Hadoop RPC, please list as well. Discuss!","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,Extract Method,","Create a better way to chain log cleaners From Stack's review of HBASE-2223:

{quote}
Why this implementation have to know about other implementations? Can't we do a chain of decision classes? Any class can say no? As soon as any decision class says no, we exit the chain.... So in this case, first on the chain would be the ttl decision... then would be this one... and third would be the snapshotting decision. You don't have to do the chain as part of this patch but please open an issue to implement.
{quote}","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Do some small cleanups in org.apache.hadoop.hbase.regionserver.wal Since i am touching this area its probably better to leave it in a cleaner state. Non deprecated ,etc","Duplicated Code, Long Method, , "
"   Rename Method,","Do some small cleanups in org.apache.hadoop.hbase.regionserver.wal Since i am touching this area its probably better to leave it in a cleaner state. Non deprecated ,etc",", "
"   Rename Method,","Do some small cleanups in org.apache.hadoop.hbase.regionserver.wal Since i am touching this area its probably better to leave it in a cleaner state. Non deprecated ,etc",", "
"   Rename Method,Extract Method,","Retain assignment information between cluster shutdown/startup Over in HBASE-57 we want to consider block locations for region assignment. This is most important during cluster startup where you currently lose all locality because regions are assignment randomly.

This jira is about a shot-term solution to the cluster startup problem by retaining assignment information after a cluster shutdown and using it on the next cluster startup.","Duplicated Code, Long Method, , "
"   Extract Method,Inline Method,","Refactor zk logging in trunk; do less and and identifier to log messages to help debugging Currently all logging is done by zookeeperwatcher so can add a zkw identifier on the front of the log message. This happens for logging done by other classes -- zkutil, zkconfig -- which makes it so can't turn off logging using log4j config (its all or nothing).

Also, zk gives connections a sessionid. To tie zk logs to hbase logs, we should include sessionid in the zkw identifier. It helps debugging especially when using minihbasecluster where clients, master, rs, and zk server are all logging to the same file.","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","unify code for major/minor compactions Today minor compactions do not process deletes, purge old versions, etc. Only major compactions do. The rationale was probably to save CPU (?). We should evaluate if major compaction logic indeed runs significantly slower.

Unifying minor compactions to do the same thing as major compactions has other advantages:

* If the same keys are deleted/updated repeatedly, the fact that deletes/overwrites are not processed during minor compaction makes each subsequent minor compaction more expensive as the total amount of data keeps growing.

* We'll have fewer bugs if the logic is as symmetric as possible. Any bugs in TTL enforcement, version enforcement, etc. could cause behavior to be different after a major compaction. Keeping the same logic means these bugs will get caught earlier.

-

Note: There will still need to be one difference in the two schemes, and that has to do with delete markers. Any compaction which doesn't compact all files will still need to leave delete markers.
","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Add ability to have multiple ZK servers in a quorum in MiniZooKeeperCluster for test writing Interesting things can happen when you have a ZK quorum of multiple servers and one of them dies. Doing testing here on clusters, this has turned up some bugs with HBase interaction with ZK.

Would be good to add the ability to have multiple ZK servers in unit tests and be able to kill them individually.","Duplicated Code, Long Method, , "
"   Move Method,Extract Method,","Add ability to have multiple Masters LocalHBaseCluster for test writing To really be able to unit test the new master properly, we need to be able to have multiple masters running at once within a single logical cluster.

Should expose some methods like getActiveMaster() and isActiveMaster() as well as a simple way to kill an individual master / kill the active master.","Duplicated Code, Long Method, , , "
"   Rename Class,Rename Method,Inline Method,","Drop ROOT and instead store META location(s) directly in ZooKeeper Rather than storing the ROOT region location in ZooKeeper, going to ROOT, and reading the META location, we should just store the META location directly in ZooKeeper.

The purpose of the root region from the bigtable paper was to support multiple meta regions. Currently, we explicitly only support a single meta region, so the translation from our current code of a single root location to a single meta location will be very simple. Long-term, it seems reasonable that we could store several meta region locations in ZK. There's been some discussion in HBASE-1755 about actually moving META into ZK, but I think this jira is a good step towards taking some of the complexity out of how we have to deal with catalog tables everywhere.

As-is, a new client already requires ZK to get the root location, so this would not change those requirements in any way.

The primary motivation for this is to simplify things like CatalogTracker. The way we can handle root in that class is really simple but the tracking of meta is difficulty and a bit hacky. This hack on tracking of the meta location is what caused one of the bugs over in HBASE-3159.",", , "
"   Rename Method,Extract Method,","HBASE-1921 for the new master HBASE-1921 was lost when writing the new master code. I guess it's going to be much harder to implement now, but I think it's a critical feature to have considering the reasons that brought me do it in the old master. There's already a test in TestZooKeeper which has been disabled a while ago.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Max Compaction Size Add ability to specify a maximum storefile size for compaction. After this limit, we will not include this file in compactions. This is useful for large object stores and clusters that pre-split regions.","Duplicated Code, Long Method, , "
"   Extract Method,Inline Method,","hbck should pause after fixing before re-checking state Right now when run with the -fix option, hbck tries to fix up the issue and then immediately re-runs itself to see if the fix worked. However most of the fixes require some other nodes in the cluster to take some action, which will take a couple of seconds (eg for them to notice a change in ZK and pick up the fixed region).

So, hbck should pause for some amount of time in between fixing and re-running.","Duplicated Code, Long Method, , , "
"   Rename Method,","Remove the KV copy of every KV in Scan; introduced by HBASE-3232 Here is offending code from inside in StoreScanner#next:

{code}
// kv is no longer immutable due to KeyOnlyFilter! use copy for safety
KeyValue copyKv = new KeyValue(kv.getBuffer(), kv.getOffset(), kv.getLength());
{code}

This looks wrong given philosophy up to this has been avoidance of garbage-making copies.

Maybe this has been looked into before and this is the only thing to be done but why is KeyOnlyFilter not making copies rather than mutating originals?

Making this critical against 0.92.",", "
"   Rename Method,Extract Method,Inline Method,","[hbase] Make BatchUpdate public in the API Today, when you want to interact with a row in HBase, you start an update, make changes, and then commit the lock. This is fine for very simple applications. However, when you try to do things like support table operations as part of a MapReduce job, it becomes more difficult to support.

I propose that we create a new class, RowMutation (a la the Bigtable paper), which encapsulates a group of actions on a row, and make this available to API consumers. It might look something like:

{code}

RowMutation r = table.getMutation(row_key);
r.setTimestamp(1111);
r.put(new Text(""colfam1:name"", value));
r.delete(new Text(""colfam2:deleted""));
table.commit(r);

{code}

This syntax would supercede the existing startUpdate/commit format, which could be deprecated and mapped to a RowMutation behind the scenes.","Duplicated Code, Long Method, , , "
"   Move Method,Move Attribute,","Speedup HFile.Writer append Remove double writes when block cache is specified, by using, only, the ByteArrayDataStream.
baos is flushed with the compress stream on finishBlock.

On my machines HFilePerformanceEvaluation SequentialWriteBenchmark passes from 4000ms to 2500ms.

Running SequentialWriteBenchmark for 1000000 rows took 4247ms.
Running SequentialWriteBenchmark for 1000000 rows took 4512ms.
Running SequentialWriteBenchmark for 1000000 rows took 4498ms.

Running SequentialWriteBenchmark for 1000000 rows took 2697ms.
Running SequentialWriteBenchmark for 1000000 rows took 2770ms.
Running SequentialWriteBenchmark for 1000000 rows took 2721ms.",", , , "
"   Move Method,Extract Method,Move Attribute,","Speedup HFile.Writer append Remove double writes when block cache is specified, by using, only, the ByteArrayDataStream.
baos is flushed with the compress stream on finishBlock.

On my machines HFilePerformanceEvaluation SequentialWriteBenchmark passes from 4000ms to 2500ms.

Running SequentialWriteBenchmark for 1000000 rows took 4247ms.
Running SequentialWriteBenchmark for 1000000 rows took 4512ms.
Running SequentialWriteBenchmark for 1000000 rows took 4498ms.

Running SequentialWriteBenchmark for 1000000 rows took 2697ms.
Running SequentialWriteBenchmark for 1000000 rows took 2770ms.
Running SequentialWriteBenchmark for 1000000 rows took 2721ms.","Duplicated Code, Long Method, , , , "
"   Move Method,Extract Method,Move Attribute,","Speedup HFile.Writer append Remove double writes when block cache is specified, by using, only, the ByteArrayDataStream.
baos is flushed with the compress stream on finishBlock.

On my machines HFilePerformanceEvaluation SequentialWriteBenchmark passes from 4000ms to 2500ms.

Running SequentialWriteBenchmark for 1000000 rows took 4247ms.
Running SequentialWriteBenchmark for 1000000 rows took 4512ms.
Running SequentialWriteBenchmark for 1000000 rows took 4498ms.

Running SequentialWriteBenchmark for 1000000 rows took 2697ms.
Running SequentialWriteBenchmark for 1000000 rows took 2770ms.
Running SequentialWriteBenchmark for 1000000 rows took 2721ms.","Duplicated Code, Long Method, , , , "
"   Rename Method,","Distinguish read and write request count in region Distinguishing read and write request counts, on top of HBASE-3507, would benefit load balancer.
The action for balancing read vs. write load should be different. For read load, region movement should be low (to keep scanner happy). For write load, region movement is allowed.
Now that we have cheap(er) counters, it should not be too burdensome keeping up the extra count.",", "
"   Rename Method,","Distinguish read and write request count in region Distinguishing read and write request counts, on top of HBASE-3507, would benefit load balancer.
The action for balancing read vs. write load should be different. For read load, region movement should be low (to keep scanner happy). For write load, region movement is allowed.
Now that we have cheap(er) counters, it should not be too burdensome keeping up the extra count.",", "
"   Rename Method,","Distinguish read and write request count in region Distinguishing read and write request counts, on top of HBASE-3507, would benefit load balancer.
The action for balancing read vs. write load should be different. For read load, region movement should be low (to keep scanner happy). For write load, region movement is allowed.
Now that we have cheap(er) counters, it should not be too burdensome keeping up the extra count.",", "
"   Rename Method,Extract Method,","Speedup LoadIncrementalHFiles From Adam Phelps:
from the logs it looks like <1% of the hfiles we're loading have to be split. Looking at the code for LoadIncrementHFiles (hbase v0.90.1), I'm actually thinking our problem is that this code loads the hfiles sequentially. Our largest table has over 2500 regions and the data being loaded is fairly well distributed across them, so there end up being around 2500 HFiles for each load period. At 1-2 seconds per HFile that means the loading process is very time consuming.

Currently server.bulkLoadHFile() is a blocking call.
We can utilize ExecutorService to achieve better parallelism on multi-core computer.

New configuration parameter ""hbase.loadincremental.threads.max"" is introduced which sets the maximum number of threads for parallel bulk load.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Cleanup the locking contention in the master The new master uses a lot of synchronized blocks to be safe, but it only takes a few jstacks to see that there's multiple layers of lock contention when a bunch of regions are moving (like when the balancer runs). The main culprits are regionInTransition in AssignmentManager, ZKAssign that uses ZKW.getZNnodes (basically another set of region in transitions), and locking at the RegionState level. 

My understanding is that even tho we have multiple threads to handle regions in transition, everything is actually serialized. Most of the time, lock holders are talking to ZK or a region server, which can take a few milliseconds.

A simple example is when AssignmentManager wants to update the timers for all the regions on a RS, it will usually be waiting on another thread that's holding the lock while talking to ZK.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,Inline Method,","StoreFile Level Compaction Locking Multithreaded compactions (HBASE-1476) will solve the problem of major compactions clogging high-priority minor compactions. However, there is still a problem here. Since compactions are store-level, the store undergoing major compaction will have it's storefile count increase during the major. We really need a way to allow multiple outstanding compactions per store. compactSelection() should lock/reserve the files being used for compaction. This will also allow us to know what we're going to compact when inserting into the CompactSplitThread and make more informed priority queueing decisions.","Duplicated Code, Long Method, , , "
"   Rename Class,Rename Method,","Tidy up naming consistency and documentation in coprocessor framework We have a few naming inconsistencies in the coprocessor API and some stale javadocs that have been spotted by Lars George as he digs through it. We should clean these up before we have an official release and are forced to go through a round of deprecation to make any changes.

Current items on the list:
* rename BaseRegionObserverCoprocessor -> BaseRegionObserver
* in BaseMasterObserver, rename ObserverContext parameter variable from ""env"" to ""c"" or ""ctx""
* unnecessary public modifier for methods in RegionObserver interface

As part of this, we should take a pass through the javadocs and verify they are up to date with what is currently implemented.

Please tack on other cosmetic changes or inconsistencies as you find them.",", "
"   Rename Method,","Refactor Coprocessor Compaction API After HBASE-3797, the compaction logic flow has been significantly altered. Because of this, the current compaction coprocessor API is insufficient for gaining full insight into compaction requests/results. Refactor coprocessor API after HBASE-3797 is committed to be more extensible and increase visibility.",", "
"   Rename Class,Rename Method,Extract Method,Inline Method,","Change the HFile Format In order to support HBASE-3763 and HBASE-3856, we need to change the format of the HFile. The new format proposal is attached here. Thanks for Mikhail Bautin for the documentation.","Duplicated Code, Long Method, , , "
"   Move And Rename Class,Extract Method,","Make mapper function in ImportTSV plug-able It would be really useful to allow the ability to specify a different Mapper for the {{ImportTsv}} class to use than the current {{TsvImporter}}. This would allow transformations to be made on the input data before being added to HBase. One suggestion is to add a new command line option to specify a user defined mapper (UDM?). Or maybe instead we just refactor it to be extended where a subclass can specify a new mapper.

The mapper is statically defined and bound to the job though, so I'm not sure of the best way to make it dynamically plug-able. Suggestions welcome.","Duplicated Code, Long Method, , "
"   Extract Method,Move Attribute,","enhance HBase RPC to support free-ing up server handler threads even if response is not ready In the current implementation, the server handler thread picks up an item from the incoming callqueue, processes it and then wraps the response as a Writable and sends it back to the IPC server module. This wastes thread-resources when the thread is blocked for disk IO (transaction logging, read into block cache, etc).

It would be nice if we can make the RPC Server Handler threads pick up a call from the IPC queue, hand it over to the application (e.g. HRegion), the application can queue it to be processed asynchronously and send a response back to the IPC server module saying that the response is not ready. The RPC Server Handler thread is now ready to pick up another request from the incoming callqueue. When the queued call is processed by the application, it indicates to the IPC module that the response is now ready to be sent back to the client.

The RPC client continues to experience the same behaviour as before. A RPC client is synchronous and blocks till the response arrives.

This RPC enhancement allows us to do very powerful things with the RegionServer. In future, we can make enhance the RegionServer's threading model to a message-passing model for better performance. We will not be limited by the number of threads in the RegionServer.

","Duplicated Code, Long Method, , , "
"   Rename Method,Inline Method,","hbase version command line should print version info Hadoop has the handy feature where you can dump the version info. eg:
{code}
# hadoop version
Hadoop 0.20.2-cdh3u1-SNAPSHOT
Subversion -r d94813ecd0d4b3f63f4d30baa8a22a59dc76d5a8
Compiled by root on Wed May 25 03:15:04 EDT 2011
From source with checksum 72d8d076770d2afa1f16f06d31d2b58a
{code}

We should do the same with hbase",", , "
"   Rename Method,Extract Method,","hbase version command line should print version info Hadoop has the handy feature where you can dump the version info. eg:
{code}
# hadoop version
Hadoop 0.20.2-cdh3u1-SNAPSHOT
Subversion -r d94813ecd0d4b3f63f4d30baa8a22a59dc76d5a8
Compiled by root on Wed May 25 03:15:04 EDT 2011
From source with checksum 72d8d076770d2afa1f16f06d31d2b58a
{code}

We should do the same with hbase","Duplicated Code, Long Method, , "
"   Rename Method,","HLog Pretty Printer We currently have a rudimentary way to print HLog data, but it is limited and currently prints key-only information. We need extend this functionality, similar to how we developed HFile's pretty printer. Ideas for functionality:

- filter by sequence_id
- filter by row / region
- option to print values in addition to key info
- option to print output in JSON format (so scripts can easily parse for analysis)
",", "
"   Rename Method,","HLog Pretty Printer We currently have a rudimentary way to print HLog data, but it is limited and currently prints key-only information. We need extend this functionality, similar to how we developed HFile's pretty printer. Ideas for functionality:

- filter by sequence_id
- filter by row / region
- option to print values in addition to key info
- option to print output in JSON format (so scripts can easily parse for analysis)
",", "
"   Extract Method,Inline Method,","[hbase] Add a method of getting multiple (but not all) cells for a row at once We should have the ability to return some but not all cells from a row at once. There are likely to be a number of situations when getFull will return much more data than needed, but using individual get calls would likely be too small. This method should support returning a specific list of columns all at once.

{code}
Map<Text, byte[]> results = table.getMulti(new Text[]{cellA, cellB, cellC}, timestamp);
{code}","Duplicated Code, Long Method, , , "
"   Rename Class,Move And Rename Class,Extract Interface,","Update HBase metrics framework to metrics2 framework Metrics Framework has been marked deprecated in Hadoop 0.20.203+ and 0.22+, and it might get removed in future Hadoop release. Hence, HBase needs to revise the dependency of MetricsContext to use Metrics2 framework.",", Large Class, "
"   Move Method,Extract Method,Move Attribute,","Data GC: Remove all versions > TTL EXCEPT the last written version We were chatting today about our backup cluster. What we want is to be able to restore the dataset from any point of time but only within a limited timeframe -- say one week. Thereafter, if the versions are older than one week, rather than as we do with TTL where we let go of all versions older than TTL, instead, let go of all versions EXCEPT the last one written. So, its like versions==1 when TTL > one week. We want to allow that if an error is caught within a week of its happening -- user mistakenly removes a critical table -- then we'll be able to restore up the the moment just before catastrophe hit otherwise, we keep one version only.","Duplicated Code, Long Method, , , , "
"   Rename Class,Move Class,Rename Method,Move Method,Move Attribute,","blockCache contents report Summarized block-cache report for a RegionServer would be helpful. For example ...

table1
cf1 100 blocks, totalBytes=yyyyy, averageTimeInCache=XXXX hours
cf2 200 blocks, totalBytes=zzzzz, averageTimeInCache=XXXX hours

table2
cf1 75 blocks, totalBytes=yyyyy, averageTimeInCache=XXXX hours
cf2 150 blocks, totalBytes=zzzzz, averageTimeInCache=XXXX hours

... Etc.

The current metrics list blockCacheSize and blockCacheFree, but there is no way to know what's in there. Any single block isn't really important, but the patterns of what CF/Table they came from, how big are they, and how long (on average) they've been in the cache, are important.

No such interface exists in HRegionInterface. But I think it would be helpful from an operational perspective.

Updated (7-29): Removing suggestion for UI. I would be happy just to get this report on a configured interval dumped to a log file.

",", , , "
"   Rename Class,Pull Up Method,","atomicAppend: A put that appends to the latest version of a cell; i.e. reads current value then adds the bytes offered by the client to the tail and writes out a new entry Its come up a few times that clients want to add to an existing cell rather than make a new cell each time. At our place, the frontend keeps a list of urls a user has visited -- their md5s -- and updates it as user progresses. Rather than read, modify client-side, then write new value back to hbase, it would be sweet if could do it all in one operation in hbase server. TSDB aims to be space efficient. Rather than pay the cost of the KV wrapper per metric, it would rather have a KV for an interval an in this KV have a value that is all the metrics for the period.

It could be done as a coprocessor but this feels more like a fundamental feature.

Benoît suggests that atomicAppend take a flag to indicate whether or not the client wants to see the resulting cell; often a client won't want to see the result and in this case, why pay the price formulating and delivering a response that client just drops.",", Duplicated Code, "
"   Move Class,Move Method,Extract Method,","Metrics for HFile HDFS block locality Normally, when we put hbase and HDFS in the same cluster ( e.g., region server runs on the datenode ), we have a reasonably good data locality, as explained by Lars. Also Work has been done by Jonathan to address the startup situation.

There are scenarios where regions can be on a different machine from the machines that hold the underlying HFile blocks, at least for some period of time. This will have performance impact on whole table scan operation and map reduce job during that time.

1. After load balancer moves the region and before compaction (thus generate HFile on the new region server ) on that region, HDFS block can be remote.
2. When a new machine is added, or removed, Hbase's region assignment policy is different from HDFS's block reassignment policy.
3. Even if there is no much hbase activity, HDFS can load balance HFile blocks as other non-hbase applications push other data to HDFS.

Lots has been or will be done in load balancer, as summarized by Ted. I am curious if HFile HDFS block locality should be used as another factor here.

I have done some experiments on how HDFS block locality can impact map reduce latency. First we need to define a metrics to measure HFile data locality.

Metrics defintion:

For a given table, or a region server, or a region, we can define the following. The higher the value, the more local HFile is from region server's point of view.

HFile locality index = ( Total number of HDFS blocks that can be retrieved locally by the region server ) / ( Total number of HDFS blocks for all HFiles )

Test Results:
This is to show how HFile locality can impact the latency. It is based on a table with 1M rows, 36KB per row; regions are distributed in balance. The map job is RowCounter.

HFile Locality Index Map job latency ( in sec )
28% 157
36% 150
47% 142
61% 133
73% 122
89% 103
99% 95

So the first suggestion is to expose HFile locality index as a new region server metrics. It will be ideal if we can somehow measure HFile locality index on a per map job level.

Regarding if/when we should include that as another factor for load balancer, that will be a different work item. It is unclear how load balancer can take various factors into account to come up with the best load balancer strategy.","Duplicated Code, Long Method, , , "
"   Extract Interface,Rename Method,Extract Method,",Make the Replication Service pluggable via a standard interface definition The current HBase code supports a replication service that can be used to sync data from from one hbase cluster to another. It would be nice to make it a pluggable interface so that other cross-data-center replication services can be used in conjuction with HBase.,"Duplicated Code, Long Method, , Large Class, "
"   Rename Method,Inline Method,",Make the Replication Service pluggable via a standard interface definition The current HBase code supports a replication service that can be used to sync data from from one hbase cluster to another. It would be nice to make it a pluggable interface so that other cross-data-center replication services can be used in conjuction with HBase.,", , "
"   Extract Interface,Rename Method,Extract Method,",Make the Replication Service pluggable via a standard interface definition The current HBase code supports a replication service that can be used to sync data from from one hbase cluster to another. It would be nice to make it a pluggable interface so that other cross-data-center replication services can be used in conjuction with HBase.,"Duplicated Code, Long Method, , Large Class, "
"   Rename Method,",Extend the WALActionsListener API to accomodate log archival The WALObserver interface exposes the log roll events. It would be nice to extend it to accomodate log archival events as well.,", "
"   Rename Method,","Exposing HBase Filters to the Thrift API Currently, to use any of the filters, one has to explicitly add a scanner for the filter in the Thrift API making it messy and long. With this patch, I am trying to add support for all the filters in a clean way. The user specifies a filter via a string. The string is parsed on the server to construct the filter. More information can be found in the attached document named Filter Language

This patch is trying to extend and further the progress made by the patches in the HBASE-1744 JIRA (https://issues.apache.org/jira/browse/HBASE-1744)",", "
"   Rename Method,Move Method,Extract Method,","Support for fault tolerant, instant schema updates with out master's intervention (i.e with out enable/disable and bulk assign/unassign) through ZK. This Jira is a slight variation in approach to what is being done as part of 
https://issues.apache.org/jira/browse/HBASE-1730

Support instant schema updates such as Modify Table, Add Column, Modify Column operations:
1. With out enable/disabling the table.
2. With out bulk unassign/assign of regions.","Duplicated Code, Long Method, , , "
"   Rename Method,Push Down Method,Move Method,Inline Method,Move Attribute,","Data Block Encoding of KeyValues  (aka delta encoding / prefix compression A compression for keys. Keys are sorted in HFile and they are usually very similar. Because of that, it is possible to design better compression than general purpose algorithms,

It is an additional step designed to be used in memory. It aims to save memory in cache as well as speeding seeks within HFileBlocks. It should improve performance a lot, if key lengths are larger than value lengths. For example, it makes a lot of sense to use it when value is a counter.

Initial tests on real data (key length = ~ 90 bytes , value length = 8 bytes) shows that I could achieve decent level of compression:
key compression ratio: 92%
total compression ratio: 85%
LZO on the same data: 85%
LZO after delta encoding: 91%
While having much better performance (20-80% faster decompression ratio than LZO). Moreover, it should allow far more efficient seeking which should improve performance a bit.

It seems that a simple compression algorithms are good enough. Most of the savings are due to prefix compression, int128 encoding, timestamp diffs and bitfields to avoid duplication. That way, comparisons of compressed data can be much faster than a byte comparator (thanks to prefix compression and bitfields).

In order to implement it in HBase two important changes in design will be needed:
-solidify interface to HFileBlock / HFileReader Scanner to provide seeking and iterating; access to uncompressed buffer in HFileBlock will have bad performance
-extend comparators to support comparison assuming that N first bytes are equal (or some fields are equal)

Link to a discussion about something similar:
http://search-hadoop.com/m/5aqGXJEnaD1/hbase+windows&subj=Re+prefix+compression",", , , , , "
"   Extract Superclass,Rename Method,Pull Up Method,Extract Method,","Data Block Encoding of KeyValues  (aka delta encoding / prefix compression A compression for keys. Keys are sorted in HFile and they are usually very similar. Because of that, it is possible to design better compression than general purpose algorithms,

It is an additional step designed to be used in memory. It aims to save memory in cache as well as speeding seeks within HFileBlocks. It should improve performance a lot, if key lengths are larger than value lengths. For example, it makes a lot of sense to use it when value is a counter.

Initial tests on real data (key length = ~ 90 bytes , value length = 8 bytes) shows that I could achieve decent level of compression:
key compression ratio: 92%
total compression ratio: 85%
LZO on the same data: 85%
LZO after delta encoding: 91%
While having much better performance (20-80% faster decompression ratio than LZO). Moreover, it should allow far more efficient seeking which should improve performance a bit.

It seems that a simple compression algorithms are good enough. Most of the savings are due to prefix compression, int128 encoding, timestamp diffs and bitfields to avoid duplication. That way, comparisons of compressed data can be much faster than a byte comparator (thanks to prefix compression and bitfields).

In order to implement it in HBase two important changes in design will be needed:
-solidify interface to HFileBlock / HFileReader Scanner to provide seeking and iterating; access to uncompressed buffer in HFileBlock will have bad performance
-extend comparators to support comparison assuming that N first bytes are equal (or some fields are equal)

Link to a discussion about something similar:
http://search-hadoop.com/m/5aqGXJEnaD1/hbase+windows&subj=Re+prefix+compression","Duplicated Code, Long Method, , Duplicated Code, Large Class, Duplicated Code, "
"   Extract Superclass,Rename Method,Pull Up Method,Extract Method,Inline Method,","Data Block Encoding of KeyValues  (aka delta encoding / prefix compression A compression for keys. Keys are sorted in HFile and they are usually very similar. Because of that, it is possible to design better compression than general purpose algorithms,

It is an additional step designed to be used in memory. It aims to save memory in cache as well as speeding seeks within HFileBlocks. It should improve performance a lot, if key lengths are larger than value lengths. For example, it makes a lot of sense to use it when value is a counter.

Initial tests on real data (key length = ~ 90 bytes , value length = 8 bytes) shows that I could achieve decent level of compression:
key compression ratio: 92%
total compression ratio: 85%
LZO on the same data: 85%
LZO after delta encoding: 91%
While having much better performance (20-80% faster decompression ratio than LZO). Moreover, it should allow far more efficient seeking which should improve performance a bit.

It seems that a simple compression algorithms are good enough. Most of the savings are due to prefix compression, int128 encoding, timestamp diffs and bitfields to avoid duplication. That way, comparisons of compressed data can be much faster than a byte comparator (thanks to prefix compression and bitfields).

In order to implement it in HBase two important changes in design will be needed:
-solidify interface to HFileBlock / HFileReader Scanner to provide seeking and iterating; access to uncompressed buffer in HFileBlock will have bad performance
-extend comparators to support comparison assuming that N first bytes are equal (or some fields are equal)

Link to a discussion about something similar:
http://search-hadoop.com/m/5aqGXJEnaD1/hbase+windows&subj=Re+prefix+compression","Duplicated Code, Long Method, , Duplicated Code, Large Class, , Duplicated Code, "
"   Rename Method,Extract Method,","Make HLog more resilient to write pipeline failures The current implementation of HLog rolling to recover from transient errors in the write pipeline seems to have two problems:

# When {{HLog.LogSyncer}} triggers an {{IOException}} during time-based sync operations, it triggers a log rolling request in the corresponding catch block, but only after escaping from the internal while loop. As a result, the {{LogSyncer}} thread will exit and never be restarted from what I can tell, even if the log rolling was successful.
# Log rolling requests triggered by an {{IOException}} in {{sync()}} or {{append()}} never happen if no entries have yet been written to the log. This means that write errors are not immediately recovered, which extends the exposure to more errors occurring in the pipeline.

In addition, it seems like we should be able to better handle transient problems, like a rolling restart of DataNodes while the HBase RegionServers are running. Currently this will reliably cause RegionServer aborts during log rolling: either an append or time-based sync triggers an initial {{IOException}}, initiating a log rolling request. However the log rolling then fails in closing the current writer (""All datanodes are bad""), causing a RegionServer abort. In this case, it seems like we should at least allow you an option to continue with the new writer and only abort on subsequent errors.","Duplicated Code, Long Method, , "
"   Extract Interface,Extract Method,","Update protobuf dependency to 2.4.0a Hadoop trunk is using this version of protobufs, so the incompatibility makes it impossible for HBase to coexist. We need to regenerate the code with 2.4.0a and update the pom.","Duplicated Code, Long Method, , Large Class, "
"   Rename Class,Move Class,Push Down Method,Push Down Attribute,","Allow Loadbalancer to be pluggable. Everyone seems to want something different from a load balancer. People want low latency, simplicity, and total control. It seems like at some point the load balancer can't be all things to all people. Something akin to what hadoop JT's pluggable scheduler seems like it will enable all solutions without making the code much more complex.",", , , "
"   Pull Up Method,Pull Up Attribute,","Optimize flushing of the Store cache for max versions and (new) min versions As discussed with with Jon, there is room for improvement in how the memstore is flushed to disk.
Currently only expired KVs are pruned before flushing, but we can also prune versions if we find at least maxVersions versions in the memstore.
The same holds for the new minversion feature: If we find at least minVersion versions in the store we can remove all further versions that are expired.

Generally we should use the same mechanism here that is used for Compaction. I.e. StoreScanner. We only need to add a scanner to Memstore that can scan along the current snapshot.",", Duplicated Code, Duplicated Code, "
"   Rename Method,",Update Thrift to 0.7.0 The new version of Thrift is 0.7.0 and it has features and bug fixes that could be useful to include in the next release of HBase.,", "
"   Extract Superclass,Pull Up Method,Extract Method,Pull Up Attribute,","Remove duplicated code from Put, Delete, Get, Scan, MultiPut This came from discussion with Stack w.r.t. HBASE-2195.

There is currently a lot of duplicated code especially between Put and Delete, and also between all Operations.
For example all of Put/Delete/Get/Scan have attributes with exactly the same code in all classes.
Put and Delete also have the familyMap, Row, Rowlock, Timestamp, etc.

One way to do this is to introduce ""OperationWithAttributes"" which extends Operation, and have Put/Delete/Get/Scan extend that rather than Operation.
In addition Put and Delete could extends from Mutation (which itself would extend OperationWithAttributes).

If a static inheritance hierarchy is not desired here, we can use delegation.
","Duplicated Code, Long Method, , Duplicated Code, Large Class, Duplicated Code, Duplicated Code, "
"   Rename Method,","Provide access to RpcServer instance from RegionServerServices In some cases, RegionObserver coprocessors may want to directly access the running RpcServer instance on the region server. For token based authentication, for example, this is needed for a coprocessor to interact with the SecretManager that validates authentication tokens in the secure RPC engine. With the addition of async call handling on the server-side, this becomes additionally important if coprocessors want to send back delayed responses to clients. In this case, the coprocessor would need to be able to call RpcServer.getCurrentCall() to send back the response.

So I propose we add access to the RpcServer in RegionServerServices:
{code}
/**
* Returns a reference to the region server's RPC server
*/
public RpcServer getRpcServer();
{code}

We can simultaneously drop the existing RegionServerServices.getRpcMetrics() method, since this could then be accessed via RpcServer.getRpcMetrics().",", "
"   Move Method,Move Attribute,","Move ClientScanner out of HTable See HBASE-1935 for motivation.
ClientScanner should be able to exist outside of HTable.
While we're at it, we can also add an abstract client scanner to easy development of new client side scanners (such as parallel scanners, or per region scanners).
",", , , "
"   Extract Method,Move Attribute,",Run more aggressive compactions during off peak hours The number of iops on the disk and the top of the rack bandwidth utilization at off peak hours is much lower than at peak hours depending on the application usage pattern. We can utilize this knowledge to improve the performance of the HBase cluster by increasing the compact selection ratio to a much larger value during off-peak hours than otherwise - increasing hbase.hstore.compaction.ratio (1.2 default) to hbase.hstore.compaction.ratio.offpeak (5 default). This will help reduce the average number of files per store.,"Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Lazy-seek optimization for StoreFile scanners Previously, if we had several StoreFiles for a column family in a region, we would seek in each of them and only then merge the results, even though the row/column we are looking for might only be in the most recent (and the smallest) file. Now we prioritize our reads from those files so that we check the most recent file first. This is done by doing a ""lazy seek"" which pretends that the next value in the StoreFile is (seekRow, seekColumn, lastTimestampInStoreFile), which is earlier in the KV order than anything that might actually occur in the file. So if we don't find the result in earlier files, that fake KV will bubble up to the top of the KV heap and a real seek will be done. This is expected to significantly reduce the amount of disk IO (as of 09/22/2011 we are doing dark launch testing and measurement).

This is joint work with Liyin Tang -- huge thanks to him for many helpful discussions on this and the idea of putting fake KVs with the highest timestamp of the StoreFile in the scanner priority queue.
","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Ability for an application to store metadata into the transaction log mySQL allows an application to store an arbitrary blob along with each transaction in its transaction logs. This JIRA is to have a similar feature request for HBASE.

The use case is as follows: An application on one data center A stores a blob of data along with each transaction. A replication software picks up these blobs from the transaction logs in A and hands it to another instance of the same application running on a remote data center B. The application in B is responsible for applying this to the remote Hbase cluster (and also handle conflict resolution if any).","Duplicated Code, Long Method, , "
"   Rename Class,Extract Method,","Better key splitting in RegionSplitter The RegionSplitter utility allows users to create a pre-split table from the command line or do a rolling split on an existing table. It supports pluggable split algorithms that implement the SplitAlgorithm interface. The only/default SplitAlgorithm is one that assumes keys fall in the range from ASCII string ""00000000"" to ASCII string ""7FFFFFFF"". This is not a sane default, and seems useless to most users. Users are likely to be surprised by the fact that all the region splits occur in in the byte range of ASCII characters.

A better default split algorithm would be one that evenly divides the space of all bytes, which is what this patch does. Making a table with five regions would split at \x33\x33..., \x66\x66...., \x99\x99..., \xCC\xCC..., and \xFF\xFF.","Duplicated Code, Long Method, , "
"   Rename Method,Move Method,Inline Method,","CatalogTracker has an identity crisis; needs to be cut-back in scope CT needs a good reworking. I'd suggest its scope be cut way down to only deal in zk transactions rather than zk and reading meta location in hbase (over an HConnection) and being a purveyor of HRegionInterfaces on meta and root servers and being an Abortable and a verifier of catalog locations. Once this is done, I would suggest it then better belongs over under the zk package and that the Meta* classes then move to client package.

Here's some messy notes I added to head of CT class in hbase-3446 where I spent some time trying to make out what it was CT did.

{code}
// TODO: This class needs a rethink. The original intent was that it would be
// the one-stop-shop for root and meta locations and that it would get this
// info from reading and watching zk state. The class was to be used by
// servers when they needed to know of root and meta movement but also by
// client-side (inside in HTable) so rather than figure root and meta
// locations on fault, the client would instead get notifications out of zk.
// 
// But this original intent is frustrated by the fact that this class has to
// read an hbase table, the -ROOT- table, to figure out the .META. region
// location which means we depend on an HConnection. HConnection will do
// retrying but also, it has its own mechanism for finding root and meta
// locations (and for 'verifying'; it tries the location and if it fails, does
// new lookup, etc.). So, at least for now, HConnection (or HTable) can't
// have a CT since CT needs a HConnection (Even then, do want HT to have a CT?
// For HT keep up a session with ZK? Rather, shouldn't we do like asynchbase
// where we'd open a connection to zk, read what we need then let the
// connection go?). The 'fix' is make it so both root and meta addresses
// are wholey up in zk -- not in zk (root) -- and in an hbase table (meta).
//
// But even then, this class does 'verification' of the location and it does
// this by making a call over an HConnection (which will do its own root
// and meta lookups). Isn't this verification 'useless' since when we
// return, whatever is dependent on the result of this call then needs to
// use HConnection; what we have verified may change in meantime (HConnection
// uses the CT primitives, the root and meta trackers finding root locations).
//
// When meta is moved to zk, this class may make more sense. In the
// meantime, it does not cohere. It should just watch meta and root and
// NOT do verification -- let that be out in HConnection since its going to
// be done there ultimately anyways.
//
// This class has spread throughout the codebase. It needs to be reigned in.
// This class should be used server-side only, even if we move meta location
// up into zk. Currently its used over in the client package. Its used in
// MetaReader and MetaEditor classes usually just to get the Configuration
// its using (It does this indirectly by asking its HConnection for its
// Configuration and even then this is just used to get an HConnection out on
// the other end). St.Ack 10/23/2011.
//
{code}",", , , "
"   Rename Method,Move Method,Inline Method,","Remove HTableDescriptor from HRegionInfo There is an HRegionInfo for every region in HBase. Currently HRegionInfo also contains the HTableDescriptor (the schema). That means we store the schema n times where n is the number of regions in the table.

Additionally, for every region of the same table that the region server has open, there is a copy of the schema. Thus it is stored in memory once for each open region.

If HRegionInfo merely contained the table name the HTableDescriptor could be stored in a separate file and easily found.",", , , "
"   Rename Method,Extract Method,Move Attribute,","Remove HTableDescriptor from HRegionInfo There is an HRegionInfo for every region in HBase. Currently HRegionInfo also contains the HTableDescriptor (the schema). That means we store the schema n times where n is the number of regions in the table.

Additionally, for every region of the same table that the region server has open, there is a copy of the schema. Thus it is stored in memory once for each open region.

If HRegionInfo merely contained the table name the HTableDescriptor could be stored in a separate file and easily found.","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Avoid top row seek by dedicated bloom filter for delete family bloom filter The previous jira, HBASE-4469, is to avoid the top row seek operation if row-col bloom filter is enabled. 
This jira tries to avoid top row seek for all the cases by creating a dedicated bloom filter only for delete family

The only subtle use case is when we are interested in the top row with empty column.

For example, 
we are interested in row1/cf1:/1/put.
So we seek to the top row: row1/cf1:/MAX_TS/MAXIMUM. And the delete family bloom filter will say there is NO delete family.
Then it will avoid the top row seek and return a fake kv, which is the last kv for this row (createLastOnRowCol).
In this way, we have already missed the real kv we are interested in.


The solution for the above problem is to disable this optimization if we are trying to GET/SCAN a row with empty column.


Evaluation from TestSeekOptimization:
Previously:
For bloom=NONE, compr=NONE total seeks without optimization: 2506, with optimization: 1714 (68.40%), savings: 31.60%
For bloom=ROW, compr=NONE total seeks without optimization: 2506, with optimization: 1714 (68.40%), savings: 31.60%
For bloom=ROWCOL, compr=NONE total seeks without optimization: 2506, with optimization: 1458 (58.18%), savings: 41.82%

For bloom=NONE, compr=GZ total seeks without optimization: 2506, with optimization: 1714 (68.40%), savings: 31.60%
For bloom=ROW, compr=GZ total seeks without optimization: 2506, with optimization: 1714 (68.40%), savings: 31.60%
For bloom=ROWCOL, compr=GZ total seeks without optimization: 2506, with optimization: 1458 (58.18%), savings: 41.82%

So we can get about 10% more seek savings ONLY if the ROWCOL bloom filter is enabled.[HBASE-4469]

================================================

After this change:
For bloom=NONE, compr=NONE total seeks without optimization: 2506, with optimization: 1458 (58.18%), savings: 41.82%
For bloom=ROW, compr=NONE total seeks without optimization: 2506, with optimization: 1458 (58.18%), savings: 41.82%
For bloom=ROWCOL, compr=NONE total seeks without optimization: 2506, with optimization: 1458 (58.18%), savings: 41.82%

For bloom=NONE, compr=GZ total seeks without optimization: 2506, with optimization: 1458 (58.18%), savings: 41.82%
For bloom=ROW, compr=GZ total seeks without optimization: 2506, with optimization: 1458 (58.18%), savings: 41.82%
For bloom=ROWCOL, compr=GZ total seeks without optimization: 2506, with optimization: 1458 (58.18%), savings: 41.82%

So we can get about 10% more seek savings for ALL kinds of bloom filter.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Allow CF to retain deleted rows Parent allows for a cluster to retain rows for a TTL or keep a minimum number of versions.
However, if a client deletes a row all version older than the delete tomb stone will be remove at the next major compaction (and even at memstore flush - see HBASE-4241).
There should be a way to retain those version to guard against software error.

I see two options here:
1. Add a new flag HColumnDescriptor. Something like ""RETAIN_DELETED"".
2. Folds this into the parent change. I.e. keep minimum-number-of-versions of versions even past the delete marker.

#1 would allow for more flexibility. #2 comes somewhat naturally with parent (from a user viewpoint)

Comments? Any other options?","Duplicated Code, Long Method, , "
"   Rename Method,","Always cache index and bloom blocks This would add a new boolean config option: hfile.block.cache.datablocks
Default would be true.

Setting this to false allows HBase in a mode where only index blocks are cached, which is useful for analytical scenarios where a useful working set of the data cannot be expected to fit into the (aggregate) cache.
This is the equivalent of setting cacheBlocks to false on all scans (including scans on behalf of gets).

I would like to get a general feeling about what folks think about this.
The change itself would be simple.

Update (Mikhail): we probably don't need a new conf option. Instead, we will make index blocks cached by default.
",", "
"   Rename Method,Move Method,Extract Method,Inline Method,",Streamline HStore startup and compactions Several useful patches that streamline HStore startup and compactions that were a part of the abandoned changes in HBASE-69 should be incorporated.,"Duplicated Code, Long Method, , , , "
"   Rename Method,","Improvements in tests Global:
- when possible, make the test using the default cluster configuration for the number of region (1 instead of 2 or 3). This allows a faster stop/start, and is a step toward a shared cluster configuration.
- 'sleep': lower or remove the sleep based synchronisation in the tests (in HBaseTestingUtility, TestGlobalMemStoreSize, TestAdmin, TestCoprocessorEndpoint, TestHFileOutputFormat, TestLruBlockCache, TestServerCustomProtocol, TestReplicationSink)
- Optimize 'put' by setting setWriteToWAL to false, when the 'put' is big or in a loop. Not done for tests that rely on the WAL.

Local issues:
- TestTableInputFormatScan fully deletes the hadoop.tmp.dir directory on tearDown, that makes it impossible to use in // with another test using this directory
- TestIdLock logs too much (9000 lines per seconds). Test time lowered to 15 seconds to make it a part of the small subset
- TestMemoryBoundedLogMessageBuffer useless System.out.println
- io.hfile.TestReseekTo useless System.out.println
- TestTableInputFormat does not shutdown the cluster
- testGlobalMemStore does not shutdown the cluster
- rest.client.TestRemoteAdmin: simplified, does not use local admin, single test instead of two.
- HBaseTestingUtility#ensureSomeRegionServersAvailable starts only one server, should start the number of missing server instead.
- TestMergeTool should starts/stops the dfs cluster with HBaseTestingUtility",", "
"   Rename Method,Extract Method,","Use a random ZK client port in unit tests so we can run them in parallel The hard-coded ZK client port has long been a problem for running HBase test suite in parallel. The mini ZK cluster should run on a random free port, and that port should be passed to all parts of the unit tests that need to talk to the mini cluster. In fact, randomizing the port exposes a lot of places in the code where a new configuration is instantiated, and as a result the client tries to talk to the default ZK client port and times out.

The initial fix is for 0.89-fb, where it already allows to run unit tests in parallel in 10 minutes. A fix for the trunk will follow.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Per-(table, columnFamily) metrics with configurable table name inclusion As we kept adding more granular block read and block cache usage statistics, a combinatorial explosion of various cases to monitor started to happen, especially when we wanted both per-table/column family/block type statistics and aggregate statistics on various subsets of these dimensions. Here, we un-clutters HFile readers, LruBlockCache, StoreFile, etc. by creating a centralized class that knows how to update all kinds of per-table/CF/block type counters. 

Table name and column family configuration have been pushed to a base class, SchemaConfigured. This is convenient as many of existing classes that have these properties (HFile readers/writers, HFile blocks, etc.) did not have a base class. Whether to collect per-(table, columnFamily) or per-columnFamily only metrics can be configured with the hbase.metrics.showTableName configuration key. We don't expect this configuration to change at runtime, so we cache the setting statically and log a warning when an attempt is made to flip it once already set. This way we don't have to pass configuration to a lot more places, e.g. everywhere an HFile reader is instantiated.

Thanks to Liyin for his initial version of per-table metrics patch and a lot of valuable feedback.
","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Distributed log splitting coding enhancement to make it easier to understand, no semantics change In reviewing distributed log splitting feature, we found some cosmetic issues. They make the code hard to understand.
It will be great to fix them. For this issue, there should be no semantic change.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Make Thrift server thread pool bounded and add a command-line UI test This started as an internal hotfix where we found out that the Thrift server spawned 15000 threads. To bound the thread pool size I added a custom thread pool server implementation called HBaseThreadPoolServer into HBase codebase, and made the following parameters configurable from both command line and as config settings: minWorkerThreads, maxWorkerThreads, and maxQueuedRequests. Under an increasing load, the server creates new threads for every connection before the pool size reaches minWorkerThreads. After that, the server puts new connections into the queue and only creates a new thread when the queue is full. If an attempt to create a new thread fails, the server drops connection. The default TThreadPoolServer would crash in that case, but it never happened because the thread pool was unbounded, so the server would hang indefinitely, consume a lot of memory, and cause huge latency spikes on the client side.

Another part of this fix is refactoring and unit testing of the command-line part of the Thrift server. The logic there is sufficiently complicated, and the existing ThriftServer class does not test that part at all. The new TestThriftServerCmdLine test starts the Thrift server on a random port with various combinations of options and talks to it through the client API from another thread.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Improve the performance of block cache keys Doing a pure random read test on data that's 100% block cache, I see that we are spending quite some time in getBlockCacheKey:

{quote}
""IPC Server handler 19 on 62023"" daemon prio=10 tid=0x00007fe0501ff800 nid=0x6c87 runnable [0x00007fe0577f6000]
java.lang.Thread.State: RUNNABLE
at java.util.Arrays.copyOf(Arrays.java:2882)
at java.lang.AbstractStringBuilder.expandCapacity(AbstractStringBuilder.java:100)
at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:390)
at java.lang.StringBuilder.append(StringBuilder.java:119)
at org.apache.hadoop.hbase.io.hfile.HFile.getBlockCacheKey(HFile.java:457)
at org.apache.hadoop.hbase.io.hfile.HFileReaderV2.readBlock(HFileReaderV2.java:249)
at org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexReader.seekToDataBlock(HFileBlockIndex.java:209)
at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$ScannerV2.seekTo(HFileReaderV2.java:521)
at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$ScannerV2.seekTo(HFileReaderV2.java:536)
at org.apache.hadoop.hbase.regionserver.StoreFileScanner.seekAtOrAfter(StoreFileScanner.java:178)
at org.apache.hadoop.hbase.regionserver.StoreFileScanner.seek(StoreFileScanner.java:111)
at org.apache.hadoop.hbase.regionserver.StoreFileScanner.seekExactly(StoreFileScanner.java:219)
at org.apache.hadoop.hbase.regionserver.StoreScanner.<init>(StoreScanner.java:80)
at org.apache.hadoop.hbase.regionserver.Store.getScanner(Store.java:1689)
at org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.<init>(HRegion.java:2857)
{quote}

Since the HFile name size is known and the offset is a long, it should be possible to allocate exactly what we need. Maybe use byte[] as the key and drop the separator too.","Duplicated Code, Long Method, , "
"   Rename Method,Move Method,Extract Method,","support checksums in HBase block cache The current implementation of HDFS stores the data in one block file and the metadata(checksum) in another block file. This means that every read into the HBase block cache actually consumes two disk iops, one to the datafile and one to the checksum file. This is a major problem for scaling HBase, because HBase is usually bottlenecked on the number of random disk iops that the storage-hardware offers.","Duplicated Code, Long Method, , , "
"   Rename Class,Rename Method,Extract Method,","[uber hbck] Online automated repair of table integrity and region consistency problems The current (0.90.5, 0.92.0rc2) versions of hbck detects most of region consistency and table integrity invariant violations. However with '-fix' it can only automatically repair region consistency cases having to do with deployment problems. This updated version should be able to handle all cases (including a new orphan regiondir case). When complete will likely deprecate the OfflineMetaRepair tool and subsume several open META-hole related issue.

Here's the approach (from the comment of at the top of the new version of the file).
{code}
/**
* HBaseFsck (hbck) is a tool for checking and repairing region consistency and
* table integrity. 
* 
* Region consistency checks verify that META, region deployment on
* region servers and the state of data in HDFS (.regioninfo files) all are in
* accordance. 
* 
* Table integrity checks verify that that all possible row keys can resolve to
* exactly one region of a table. This means there are no individual degenerate
* or backwards regions; no holes between regions; and that there no overlapping
* regions. 
* 
* The general repair strategy works in these steps.
* 1) Repair Table Integrity on HDFS. (merge or fabricate regions)
* 2) Repair Region Consistency with META and assignments
* 
* For table integrity repairs, the tables their region directories are scanned
* for .regioninfo files. Each table's integrity is then verified. If there 
* are any orphan regions (regions with no .regioninfo files), or holes, new 
* regions are fabricated. Backwards regions are sidelined as well as empty
* degenerate (endkey==startkey) regions. If there are any overlapping regions,
* a new region is created and all data is merged into the new region. 
* 
* Table integrity repairs deal solely with HDFS and can be done offline -- the
* hbase region servers or master do not need to be running. These phase can be
* use to completely reconstruct the META table in an offline fashion. 
* 
* Region consistency requires three conditions -- 1) valid .regioninfo file 
* present in an hdfs region dir, 2) valid row with .regioninfo data in META,
* and 3) a region is deployed only at the regionserver that is was assigned to.
* 
* Region consistency requires hbck to contact the HBase master and region
* servers, so the connect() must first be called successfully. Much of the
* region consistency information is transient and less risky to repair.
*/
{code}

","Duplicated Code, Long Method, , "
"   Move Method,Extract Method,","Remove getRegionServerWithoutRetries and getRegionServerWithRetries from HConnection Interface Its broke having these meta methods in HConnection. They take ServerCallables which themselves have HConnections inevitably. It makes for a tangle in the model and frustrates being able to do mocked implemenations of HConnection. These methods better belong in something like HConnectionManager, or elsewhere altogether.","Duplicated Code, Long Method, , , "
"   Rename Method,","Basic client pushback mechanism The current blocking we do when we are close to some limits (memstores over the multiplier factor, too many store files, global memstore memory) is bad, too coarse and confusing. After hitting HBASE-5161, it really becomes obvious that we need something better.

I did a little brainstorm with Stack, we came up quickly with two solutions:

- Send some exception to the client, like OverloadedException, that's thrown when some situation happens like getting past the low memory barrier. It would be thrown when the client gets a handler and does some check while putting or deleting. The client would treat this a retryable exception but ideally wouldn't check .META. for a new location. It could be fancy and have multiple levels of pushback, like send the exception to 25% of the clients, and then go up if the situation persists. Should be ""easy"" to implement but we'll be using a lot more IO to send the payload over and over again (but at least it wouldn't sit in the RS's memory).
- Send a message alongside a successful put or delete to tell the client to slow down a little, this way we don't have to do back and forth with the payload between the client and the server. It's a cleaner (I think) but more involved solution.

In every case the RS should do very obvious things to notify the operators of this situation, through logs, web UI, metrics, etc.

Other ideas?",", "
"   Push Down Method,Extract Method,Push Down Attribute,","Basic client pushback mechanism The current blocking we do when we are close to some limits (memstores over the multiplier factor, too many store files, global memstore memory) is bad, too coarse and confusing. After hitting HBASE-5161, it really becomes obvious that we need something better.

I did a little brainstorm with Stack, we came up quickly with two solutions:

- Send some exception to the client, like OverloadedException, that's thrown when some situation happens like getting past the low memory barrier. It would be thrown when the client gets a handler and does some check while putting or deleting. The client would treat this a retryable exception but ideally wouldn't check .META. for a new location. It could be fancy and have multiple levels of pushback, like send the exception to 25% of the clients, and then go up if the situation persists. Should be ""easy"" to implement but we'll be using a lot more IO to send the payload over and over again (but at least it wouldn't sit in the RS's memory).
- Send a message alongside a successful put or delete to tell the client to slow down a little, this way we don't have to do back and forth with the payload between the client and the server. It's a cleaner (I think) but more involved solution.

In every case the RS should do very obvious things to notify the operators of this situation, through logs, web UI, metrics, etc.

Other ideas?","Duplicated Code, Long Method, , , , "
"   Rename Method,","Basic client pushback mechanism The current blocking we do when we are close to some limits (memstores over the multiplier factor, too many store files, global memstore memory) is bad, too coarse and confusing. After hitting HBASE-5161, it really becomes obvious that we need something better.

I did a little brainstorm with Stack, we came up quickly with two solutions:

- Send some exception to the client, like OverloadedException, that's thrown when some situation happens like getting past the low memory barrier. It would be thrown when the client gets a handler and does some check while putting or deleting. The client would treat this a retryable exception but ideally wouldn't check .META. for a new location. It could be fancy and have multiple levels of pushback, like send the exception to 25% of the clients, and then go up if the situation persists. Should be ""easy"" to implement but we'll be using a lot more IO to send the payload over and over again (but at least it wouldn't sit in the RS's memory).
- Send a message alongside a successful put or delete to tell the client to slow down a little, this way we don't have to do back and forth with the payload between the client and the server. It's a cleaner (I think) but more involved solution.

In every case the RS should do very obvious things to notify the operators of this situation, through logs, web UI, metrics, etc.

Other ideas?",", "
"   Rename Method,","Basic client pushback mechanism The current blocking we do when we are close to some limits (memstores over the multiplier factor, too many store files, global memstore memory) is bad, too coarse and confusing. After hitting HBASE-5161, it really becomes obvious that we need something better.

I did a little brainstorm with Stack, we came up quickly with two solutions:

- Send some exception to the client, like OverloadedException, that's thrown when some situation happens like getting past the low memory barrier. It would be thrown when the client gets a handler and does some check while putting or deleting. The client would treat this a retryable exception but ideally wouldn't check .META. for a new location. It could be fancy and have multiple levels of pushback, like send the exception to 25% of the clients, and then go up if the situation persists. Should be ""easy"" to implement but we'll be using a lot more IO to send the payload over and over again (but at least it wouldn't sit in the RS's memory).
- Send a message alongside a successful put or delete to tell the client to slow down a little, this way we don't have to do back and forth with the payload between the client and the server. It's a cleaner (I think) but more involved solution.

In every case the RS should do very obvious things to notify the operators of this situation, through logs, web UI, metrics, etc.

Other ideas?",", "
"   Rename Method,Extract Method,","Delete out of TTL store files before compaction selection Currently, HBase deletes the out of TTL store files after compaction. We can change the sequence to delete the out of TTL store files before selecting store files for compactions. 
In this way, HBase can keep deleting the old invalid store files without compaction, and also prevent from unnecessary compactions since the out of TTL store files will be deleted before the compaction selection.","Duplicated Code, Long Method, , "
"   Move And Rename Class,Extract Interface,Rename Method,Extract Method,","Improve client scanner interface The current client scanner interface is pretty ugly. You need to instantiate an HStoreKey and SortedMap<Text, byte[]> externally and then pass them into next. This is pretty bad, because for starters, the client has to choose the implementation of the map when they create it, so it's extra brain cycles to figure that out. HStoreKey doesn't show up anywhere else in the entire client side API, but here it bubbles out of next as a way to get the row and presumably the timestamp of the columns.

I propose that we supplant HScannerInterface with Scanner, an easier-to-use version for clients. Its next method would look something like:

{code}
public RowResult next() throws IOException;
{code}

This packs the data up much more cleanly, including using Cells as values instead of raw byte[], meaning you have much more granular timestamp information. You also don't need HStoreKey anymore.

By breaking Scanner away from HScannerInterface, we can leave the internal scanning code completely alone (keep using HStoreKeys and such) but make the client cleaner.","Duplicated Code, Long Method, , Large Class, "
"   Rename Method,","Provide basic building blocks for ""multi-row"" local transactions. In the final iteration, this issue provides a generalized, public mutateRowsWithLocks method on HRegion, that can be used by coprocessors to implement atomic operations efficiently.
Coprocessors are already region aware, which makes this is a good pairing of APIs. This feature is by design not available to the client via the HTable API.

It took a long time to arrive at this and I apologize for the public exposure of my (erratic in retrospect) thought processes.

Was:
HBase should provide basic building blocks for multi-row local transactions. Local means that we do this by co-locating the data. Global (cross region) transactions are not discussed here.

After a bit of discussion two solutions have emerged:
1. Keep the row-key for determining grouping and location and allow efficient intra-row scanning. A client application would then model tables as HBase-rows.
2. Define a prefix-length in HTableDescriptor that defines a grouping of rows. Regions will then never be split inside a grouping prefix.

#1 is true to the current storage paradigm of HBase.
#2 is true to the current client side API.

I will explore these two with sample patches here.

--------------------
Was:
As discussed (at length) on the dev mailing list with the HBASE-3584 and HBASE-5203 committed, supporting atomic cross row transactions within a region becomes simple.
I am aware of the hesitation about the usefulness of this feature, but we have to start somewhere.

Let's use this jira for discussion, I'll attach a patch (with tests) momentarily to make this concrete.",", "
"   Rename Method,Extract Method,","Ensure compactions do not cache-on-write data blocks Create a unit test for HBASE-3976 (making sure we don't cache data blocks on write during compactions even if cache-on-write is enabled generally enabled). This is because we have very different implementations of HBASE-3976 without HBASE-4422 CacheConfig (on top of 89-fb, created by Liyin) and with CacheConfig (presumably it's there but not sure if it even works, since the patch in HBASE-3976 may not have been committed). We need to create a unit test to verify that we don't cache data blocks on write during compactions, and resolve HBASE-3976 so that this new unit test does not fail.","Duplicated Code, Long Method, , "
"   Rename Method,","Filter out the expired store file scanner during the compaction During the compaction time, HBase will generate a store scanner which will scan a list of store files. And it would be more efficient to filer out the expired store file since there is no need to read any key values from these store files.

This optimization has been already implemented on 89-fb and this is the building block for HBASE-5199 as well. It is supposed to be no-ops to compact the expired store files.",", "
"   Rename Method,Extract Method,","Region Historian Whenever we try to debug region splitting, assignment, compaction, etc. issues, we always end up having to look in 1-20 different log files for cryptic names of regions and try to piece together the chain of events ourselves. This is a challenging at best effort most of the time.

What would be very useful would be a new utility I've nicknamed the Region Historian. You give it the text name of a region, and it will track down the log messages relevant to it in the master and regionserver logs. Then, it will interleave the messages in such a way that the timestamps correctly list the order of events. The result is a log summary that accurately describes what happened to a region during it's lifetime, making it much easier to try and figure out where something went wrong.

Other things it could do would be replace cryptic log messages with simple events like ""the region was split into a and b"", ""the region was assigned to server x"", and trace the lineage of a region backwards to its parent before it came into existence.

I'm sure there are other things we would think up that would be useful as well.","Duplicated Code, Long Method, , "
"   Extract Interface,Extract Method,","Automagically tweak global memstore and block cache sizes based on workload Hypertable does a neat thing where it changes the size given to the CellCache (our MemStores) and Block Cache based on the workload. If you need an image, scroll down at the bottom of this link: http://www.hypertable.com/documentation/architecture/

That'd be one less thing to configure.","Duplicated Code, Long Method, , Large Class, "
"   Rename Method,Move Method,Extract Method,Move Attribute,","Remove MiniDFS startup from MiniHBaseCluster Change MiniHBaseCluster to always require a MiniDfs started elsewhere. It will decide where to read and write based on the config passed in to its constructor. 

HBaseClusterTestCase will be updated to do the MiniDFS spinup and shutdown as part of its normal process.

Completing this issue will prime us for solving the truly external DFS issue for making the test suite faster.","Duplicated Code, Long Method, , , , "
"   Move Method,Extract Method,Move Attribute,","Improve exceptions that come out on client-side Client-side exceptions should contain regionserver and region client was going against. Looking at an UnknownScannerException that came out of a client, it looks like something happened over on the regionserver but my cluster has a 20-plus machines and I don't know which I should be looking at.","Duplicated Code, Long Method, , , , "
"   Rename Method,Extract Method,Inline Method,","Add baseline compression efficiency to DataBlockEncodingTool DataBlockEncodingTool currently does not provide baseline compression efficiency, e.g. Hadoop compression codec applied to unencoded data. E.g. if we are using LZO to compress blocks, we would like to have the following columns in the report (possibly as percentages of raw data size).

Baseline K+V in blockcache | Baseline K + V on disk (LZO compressed) | K + V DataBlockEncoded in block cache | K + V DataBlockEncoded + LZOCompressed (on disk)

Background: we never store compressed blocks in cache, but we always store encoded data blocks in cache if data block encoding is enabled for the column family.
","Duplicated Code, Long Method, , , "
"   Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Move compression/decompression to an encoder specific encoding context As part of working on HBASE-5313, we want to add a new columnar encoder/decoder. It makes sense to move compression to be part of encoder/decoder:
1) a scanner for a columnar encoded block can do lazy decompression to a specific part of a key value object
2) avoid an extra bytes copy from encoder to hblock-writer. 

If there is no encoder specified for a writer, the HBlock.Writer will use a default compression-context to do something very similar to today's code.
","Duplicated Code, Long Method, , , , , "
"   Rename Method,","Unify HRegion.mutateRowsWithLocks() and HRegion.processRow() mutateRowsWithLocks() does atomic mutations on multiple rows.
processRow() does atomic read-modify-writes on a single row.

It will be useful to generalize both and have a
processRowsWithLocks() that does atomic read-modify-writes on multiple rows.

This also helps reduce some redundancy in the codes.",", "
"   Rename Class,Rename Method,Move Method,Inline Method,Move Attribute,","Unify HRegion.mutateRowsWithLocks() and HRegion.processRow() mutateRowsWithLocks() does atomic mutations on multiple rows.
processRow() does atomic read-modify-writes on a single row.

It will be useful to generalize both and have a
processRowsWithLocks() that does atomic read-modify-writes on multiple rows.

This also helps reduce some redundancy in the codes.",", , , , "
"   Rename Class,Rename Method,Move Method,Extract Method,Move Attribute,","Unify HRegion.mutateRowsWithLocks() and HRegion.processRow() mutateRowsWithLocks() does atomic mutations on multiple rows.
processRow() does atomic read-modify-writes on a single row.

It will be useful to generalize both and have a
processRowsWithLocks() that does atomic read-modify-writes on multiple rows.

This also helps reduce some redundancy in the codes.","Duplicated Code, Long Method, , , , "
"   Rename Class,Rename Method,Move Method,Extract Method,Move Attribute,","Unify HRegion.mutateRowsWithLocks() and HRegion.processRow() mutateRowsWithLocks() does atomic mutations on multiple rows.
processRow() does atomic read-modify-writes on a single row.

It will be useful to generalize both and have a
processRowsWithLocks() that does atomic read-modify-writes on multiple rows.

This also helps reduce some redundancy in the codes.","Duplicated Code, Long Method, , , , "
"   Move Class,Extract Superclass,Move Method,Extract Method,Move Attribute,","Don't delete HFiles when in ""backup mode"" This came up in a discussion I had with Stack.
It would be nice if HBase could be notified that a backup is in progress (via a znode for example) and in that case either:
1. rename HFiles to be delete to <file>.bck
2. rename the HFiles into a special directory
3. rename them to a general trash directory (which would not need to be tied to backup mode).

That way it should be able to get a consistent backup based on HFiles (HDFS snapshots or hard links would be better options here, but we do not have those).

#1 makes cleanup a bit harder.
","Duplicated Code, Long Method, , , , Duplicated Code, Large Class, "
"   Rename Method,Extract Method,","Avoid byte buffer allocations when reading a value from a Result object When calling Result.getValue(), an extra dummy KeyValue and its associated underlying byte array are allocated, as well as a persistent buffer that will contain the returned value.

These can be avoided by reusing a static array for the dummy object and by passing a ByteBuffer object as a value destination buffer to the read method.

The current functionality is maintained, and we have added a separate method call stack that employs the described changes. I will provide more details with the patch.

Running tests with a profiler, the reduction of read time seems to be of up to 40%.","Duplicated Code, Long Method, , "
"   Rename Method,",hbck should disable the balancer using synchronousBalanceSwitch. hbck disable the balancer using admin.balanceSwith(bool) when it would be preferable to use the newer synchronusBalanceSwitch method found in 0.94 and trunk branches.,", "
"   Rename Method,",hbck should handle case where .tableinfo file is missing. 0.92+ branches have a .tableinfo file which could be missing from hdfs. hbck should be able to detect and repair this properly.,", "
"   Rename Method,Extract Method,Inline Method,",Make ProcessBasedLocalHBaseCluster run HDFS and make it more robust Currently ProcessBasedLocalHBaseCluster runs on top of raw local filesystem. We need it to start a process-based HDFS cluster as well. We also need to make the whole thing more stable so we can use it in unit tests.,"Duplicated Code, Long Method, , , "
"   Rename Method,","Parallelize load of .regioninfo files in diagnostic/repair portion of hbck. On heavily loaded hdfs's some dfs nodes may not respond quickly and backs off for 60s before attempting to read data from another datanode. Portions of the information gathered from hdfs (.regioninfo files) are loaded serially. With HBase with clusters with 100's, or 1000's, or 10000's regions encountering these 60s delay blocks progress and can be very painful. 

There is already some parallelization of portions of the hdfs information load operations and the goal here is move the reading of .regioninfos into the parallelized sections..",", "
"   Move Method,Extract Method,","Fix HLog compression's incompatibilities I ran some tests to verify if WAL compression should be turned on by default.

For a use case where it's not very useful (values two order of magnitude bigger than the keys), the insert time wasn't different and the CPU usage 15% higher (150% CPU usage VS 130% when not compressing the WAL).

When values are smaller than the keys, I saw a 38% improvement for the insert run time and CPU usage was 33% higher (600% CPU usage VS 450%). I'm not sure WAL compression accounts for all the additional CPU usage, it might just be that we're able to insert faster and we spend more time in the MemStore per second (because our MemStores are bad when they contain tens of thousands of values).

Those are two extremes, but it shows that for the price of some CPU we can save a lot. My machines have 2 quads with HT, so I still had a lot of idle CPUs.","Duplicated Code, Long Method, , , "
"   Move Method,Move Attribute,","Move Dynamic Metrics storage off of HRegion. HRegion right now has the responsibility of storing static counts and latency numbers for use by the metrics package. Since these maps are incremented and set from lots of places it makes adding functionality hard.

So move the metrics functionality into SchemaMetrics making it more than just a class for naming. The next step will be to simplify the api exposed so that using it will be easier.",", , , "
"   Rename Method,",Add more to verification step in HLogPerformanceEvaluation Verify the wal has expected count of edits.,", "
"   Pull Up Method,Move Method,","Allow adding filters to TableInputFormat (At same time, ensure TIF is subclassable) 0",", , Duplicated Code, "
"   Rename Method,","Names in the filter interface are confusing I don't like the names of the filter methods in RowFilterInterface. They don't really tell how the methods are being used in the implementation of scanners.

I'd like to change:
- filter(Text) to filterRow(...)
- filter(Text, Text, byte[]) to filterColumn(...)
and the worst one is
- filterNotNull(SortedMap<Text, byte[]>). This should be filterRow(Text, SortedMap<Text, byte[]>) (so we add the row key/).

It may be nice to have timestamps in the methods as well? 

Also the java doc could be cleaned and improved to tell how the filtering is implemented (check rows keys first, then check each individual columns, finally check the assembled row)

Upon positive feedback, and I'll create a patch.",", "
"   Move Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Remove HRegionInterface As a step to move internals to PB, so as to avoid the conversion for performance reason, we should remove the HRegionInterface. 
Therefore region server only supports ClientProtocol and AdminProtocol. Later on, HRegion can work with PB messages directly.","Duplicated Code, Long Method, , , , , "
"   Rename Method,","[hbck] Refactor parallel WorkItem* to Futures. This would convert WorkItem* logic (with low level notifies, and rough exception handling) into a more canonical Futures pattern.

Currently there are two instances of this pattern (for loading hdfs dirs, for contacting regionservers for assignments, and soon -- for loading hdfs .regioninfo files).

",", "
"   Move Class,Rename Method,Move Method,Extract Method,Move Attribute,",Add other load balancers Now that balancers are pluggable we should give some options.,"Duplicated Code, Long Method, , , , "
"   Move Method,Move Attribute,","Scanner responses from RS should include metrics on rows/KVs filtered Currently it's difficult to know, when issuing a filter, what percentage of rows were skipped by that filter. We should expose some basic counters back to the client scanner object. For example:
- number of rows filtered by row key alone (filterRowKey())
- number of times each filter response was returned by filterKeyValue() - corresponding to Filter.ReturnCode

What would be slickest is if this could actually return a tree of counters for cases where FilterList or other combining filters are used. But a top-level is a good start.",", , , "
"   Pull Up Method,Pull Up Attribute,","Scanner responses from RS should include metrics on rows/KVs filtered Currently it's difficult to know, when issuing a filter, what percentage of rows were skipped by that filter. We should expose some basic counters back to the client scanner object. For example:
- number of rows filtered by row key alone (filterRowKey())
- number of times each filter response was returned by filterKeyValue() - corresponding to Filter.ReturnCode

What would be slickest is if this could actually return a tree of counters for cases where FilterList or other combining filters are used. But a top-level is a good start.",", Duplicated Code, Duplicated Code, "
"   Pull Up Method,Pull Up Attribute,","Scanner responses from RS should include metrics on rows/KVs filtered Currently it's difficult to know, when issuing a filter, what percentage of rows were skipped by that filter. We should expose some basic counters back to the client scanner object. For example:
- number of rows filtered by row key alone (filterRowKey())
- number of times each filter response was returned by filterKeyValue() - corresponding to Filter.ReturnCode

What would be slickest is if this could actually return a tree of counters for cases where FilterList or other combining filters are used. But a top-level is a good start.",", Duplicated Code, Duplicated Code, "
"   Rename Method,","Offline Snapshots in HBase 0.96 Continuation of HBASE-50 for the current trunk. Since the implementation has drastically changed, opening as a new ticket.",", "
"   Rename Method,Extract Method,","[hbase] Create an HBase-specific MapFile implementation Today, HBase uses the Hadoop MapFile class to store data persistently to disk. This is convenient, as it's already done (and maintained by other people :). However, it's beginning to look like there might be possible performance benefits to be had from doing an HBase-specific implementation of MapFile that incorporated some precise features.

This issue should serve as a place to track discussion about what features might be included in such an implementation.","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,Move Method,Extract Method,Move Attribute,","[hbase] Create an HBase-specific MapFile implementation Today, HBase uses the Hadoop MapFile class to store data persistently to disk. This is convenient, as it's already done (and maintained by other people :). However, it's beginning to look like there might be possible performance benefits to be had from doing an HBase-specific implementation of MapFile that incorporated some precise features.

This issue should serve as a place to track discussion about what features might be included in such an implementation.","Duplicated Code, Long Method, , , , "
"   Rename Method,Extract Method,","hbck check specified tables only Currently hbck can fix specified tables so that we can fix one table each time.
However, it doesn't check the health of the specified tables only. It still
check the health of the whole system.

If tables are specified, we can check the health of these tables only.","Duplicated Code, Long Method, , "
"   Move Method,Move Attribute,","move DataBlockEncoding and related classes to hbase-common module In order to isolate the implementation details of HBASE-4676 (PrefixTrie encoding) and other DataBlockEncoders by putting them in modules, this pulls up the DataBlockEncoding related interfaces into hbase-common.

No tests are moved in this patch. The only notable change was trimming a few dependencies on HFileBlock which adds dependencies to much of the regionserver.

The test suite passes locally for me.

I tried to keep it as simple as possible... let me know if there are any concerns.",", , , "
"   Move Class,Extract Method,","Possible performance improvement in client batch operations: presplit and send in background today batch algo is:
{noformat}
for Operation o: List<Op>{
add o to todolist
if todolist > maxsize or o last in list
split todolist per location
send split lists to region servers
clear todolist
wait
}
{noformat}

We could:
- create immediately the final object instead of an intermediate array
- split per location immediately
- instead of sending when the list as a whole is full, send it when there is enough data for a single location

It would be:
{noformat}
for Operation o: List<Op>{
get location
add o to todo location.todolist
if (location.todolist > maxLocationSize)
send location.todolist to region server 
clear location.todolist
// don't wait, continue the loop
}
send remaining
wait
{noformat}

It's not trivial to write if you add error management: retried list must be shared with the operations added in the todolist. But it's doable.
It's interesting mainly for 'big' writes","Duplicated Code, Long Method, , "
"   Move Method,Inline Method,","Enable multi-thread for memstore flush If the KV is large or Hlog is closed with high-pressure putting, we found memstore is often above the high water mark and block the putting.

So should we enable multi-thread for Memstore Flush?

Some performance test data for reference,

1.test environment ： 
random writting；upper memstore limit 5.6GB;lower memstore limit 4.8GB;400 regions per regionserver；row len=50 bytes, value len=1024 bytes;5 regionserver, 300 ipc handler per regionserver;5 client, 50 thread handler per client for writing

2.test results:

one cacheFlush handler, tps: 7.8k/s per regionserver, Flush:10.1MB/s per regionserver, appears many aboveGlobalMemstoreLimit blocking

two cacheFlush handlers, tps: 10.7k/s per regionserver, Flush:12.46MB/s per regionserver,

200 thread handler per client & two cacheFlush handlers, tps:16.1k/s per regionserver, Flush:18.6MB/s per regionserver",", , , "
"   Move Method,Extract Method,","Enable multi-thread for memstore flush If the KV is large or Hlog is closed with high-pressure putting, we found memstore is often above the high water mark and block the putting.

So should we enable multi-thread for Memstore Flush?

Some performance test data for reference,

1.test environment ： 
random writting；upper memstore limit 5.6GB;lower memstore limit 4.8GB;400 regions per regionserver；row len=50 bytes, value len=1024 bytes;5 regionserver, 300 ipc handler per regionserver;5 client, 50 thread handler per client for writing

2.test results:

one cacheFlush handler, tps: 7.8k/s per regionserver, Flush:10.1MB/s per regionserver, appears many aboveGlobalMemstoreLimit blocking

two cacheFlush handlers, tps: 10.7k/s per regionserver, Flush:12.46MB/s per regionserver,

200 thread handler per client & two cacheFlush handlers, tps:16.1k/s per regionserver, Flush:18.6MB/s per regionserver","Duplicated Code, Long Method, , , "
"   Move Method,Extract Method,","Enable multi-thread for memstore flush If the KV is large or Hlog is closed with high-pressure putting, we found memstore is often above the high water mark and block the putting.

So should we enable multi-thread for Memstore Flush?

Some performance test data for reference,

1.test environment ： 
random writting；upper memstore limit 5.6GB;lower memstore limit 4.8GB;400 regions per regionserver；row len=50 bytes, value len=1024 bytes;5 regionserver, 300 ipc handler per regionserver;5 client, 50 thread handler per client for writing

2.test results:

one cacheFlush handler, tps: 7.8k/s per regionserver, Flush:10.1MB/s per regionserver, appears many aboveGlobalMemstoreLimit blocking

two cacheFlush handlers, tps: 10.7k/s per regionserver, Flush:12.46MB/s per regionserver,

200 thread handler per client & two cacheFlush handlers, tps:16.1k/s per regionserver, Flush:18.6MB/s per regionserver","Duplicated Code, Long Method, , , "
"   Extract Superclass,Extract Method,","Quarantine Corrupted HFiles with hbck We've encountered a few upgrades from 0.90 hbases + 20.2/1.x hdfs to 0.92 hbases + hdfs 2.x that get stuck. I haven't been able to duplicate the problem in my dev environment but we suspect this may be related to HDFS-3731. On the HBase side, it seems reasonable to quarantine what are most likely truncated hfiles, so that can could later be recovered.

Here's an example of the exception we've encountered:

{code}
2012-07-18 05:55:01,152 ERROR handler.OpenRegionHandler (OpenRegionHandler.java:openRegion(346)) - Failed open of region=user_mappings,080112102AA76EF98197605D341B9E6C5824D2BC|1001,1317824890618.eaed0e7abc6d27d28ff0e5a9b49c4c
0d. 
java.io.IOException: java.lang.IllegalArgumentException: Invalid HFile version: 842220600 (expected to be between 1 and 2) 
at org.apache.hadoop.hbase.io.hfile.FixedFileTrailer.readFromStream(FixedFileTrailer.java:306) 
at org.apache.hadoop.hbase.io.hfile.HFile.pickReaderVersion(HFile.java:371) 
at org.apache.hadoop.hbase.io.hfile.HFile.createReader(HFile.java:387) 
at org.apache.hadoop.hbase.regionserver.StoreFile$Reader.<init>(StoreFile.java:1026) 
at org.apache.hadoop.hbase.regionserver.StoreFile.open(StoreFile.java:485) 
at org.apache.hadoop.hbase.regionserver.StoreFile.createReader(StoreFile.java:566) 
at org.apache.hadoop.hbase.regionserver.Store.loadStoreFiles(Store.java:286) 
at org.apache.hadoop.hbase.regionserver.Store.<init>(Store.java:223) 
at org.apache.hadoop.hbase.regionserver.HRegion.instantiateHStore(HRegion.java:2534) 
at org.apache.hadoop.hbase.regionserver.HRegion.initialize(HRegion.java:454) 
at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:3282) 
at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:3230) 
at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.openRegion(OpenRegionHandler.java:331)
at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.process(OpenRegionHandler.java:107)
at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:169) 
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) 
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) 
at java.lang.Thread.run(Thread.java:619) 
Caused by: java.lang.IllegalArgumentException: Invalid HFile version: 842220600 (expected to be between 1 and 2) 
at org.apache.hadoop.hbase.io.hfile.HFile.checkFormatVersion(HFile.java:515) 
at org.apache.hadoop.hbase.io.hfile.FixedFileTrailer.readFromStream(FixedFileTrailer.java:303) 
... 17 more
{code}

Specifically -- the FixedFileTrailer are incorrect, and seemingly missing.
","Duplicated Code, Long Method, , Duplicated Code, Large Class, "
"   Rename Method,Extract Method,Push Down Attribute,",[Migration] addColumn/deleteColumn functionality in MetaUtils Needed by HBASE-533,"Duplicated Code, Long Method, , , "
"   Rename Method,Move Method,Extract Method,Move Attribute,","ReplicationSourceManager should be able to track multiple WAL paths Currently ReplicationSourceManager uses logRolled() to receive notification about new HLog and remembers it in latestPath.
When region server has multiple WAL support, we need to keep track of multiple Path's in ReplicationSourceManager","Duplicated Code, Long Method, , , , "
"   Rename Method,Extract Method,","MultiRegion transactions with Optimistic Concurrency Control We have a need for ACID transactions across tables. This issue is about adding transactions which span multiple regions. We do not envision many competing writes, and will be read-dominated in general. This makes Optimistic Concurrency Control (OCC) seem like the way to go.","Duplicated Code, Long Method, , "
"   Move Method,Move Attribute,","ResourceChecker refinement This was based on some discussion from HBASE-6234.

The ResourceChecker was added by N. Keywal to help resolve some hadoop qa issues, but has since not be widely utilized. Further, with modularization we have had to drop the ResourceChecker from the tests that are moved into the hbase-common module because bringing the ResourceChecker up to hbase-common would involved bringing all its dependencies (which are quite far reaching).

The question then is, what should we do with it? Get rid of it? Refactor and resuse?",", , , "
"   Rename Method,","Allow the master info server to be started in a read only mode. There are some cases that a user could want a web ui to be accessible but might not want the split and compact functionality to be usable.

Allowing the web ui to start in a readOnly mode would be good.",", "
"   Rename Method,Move Method,Extract Method,Move Attribute,","Make bloomfilter true/false and self-sizing Remove bloomfilter options. Only one bloomfilter type makes sense in hbase context. Also, make bloomfilter self-sizing; you know size when flushing.

Putting in 0.2 for now because its API change (for the simpler). We can punt later.","Duplicated Code, Long Method, , , , "
"   Extract Superclass,Rename Method,Push Down Method,Move Method,Extract Method,","[MTTR] Improve Region Server Recovery Time - Distributed Log Replay Just saw interesting issue where a cluster went down hard and 30 nodes had 1700 WALs to replay. Replay took almost an hour. It looks like it could run faster that much of the time is spent zk'ing and nn'ing.

Putting in 0.96 so it gets a look at least. Can always punt.
","Duplicated Code, Long Method, , , Duplicated Code, Large Class, , "
"   Rename Method,Extract Method,","[hbase] Master should rebalance region assignments periodically The master currently only does region assignments at startup or when there are splits or dead regionservers. This means that if you join a new regionserver to the cluster after startup, it does not get assigned a fair share of the already-served regions as you would expect. It only gets a share of new regions being served.

The master should periodically check the balance of regions, based on whatever assignment function, instead of in reaction to the above listed events.","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,Move Method,Extract Method,",refactor the compaction selection and config code similarly to 0.89-fb changes Separate JIRA for refactoring changes from HBASE-7055 (and further ones after code review),"Duplicated Code, Long Method, , , "
"   Rename Method,Move Method,Inline Method,",refactor the compaction selection and config code similarly to 0.89-fb changes Separate JIRA for refactoring changes from HBASE-7055 (and further ones after code review),", , , "
"   Rename Method,Move Method,Extract Method,",refactor the compaction selection and config code similarly to 0.89-fb changes Separate JIRA for refactoring changes from HBASE-7055 (and further ones after code review),"Duplicated Code, Long Method, , , "
"   Rename Method,","Add Data Block Encoding and -D opts to Performance Evaluation Add the ability to specify Data Block Encoding and other configuration options.

--blockEncoding=TYPE
-D <property=value>

Example:
hbase org.apache.hadoop.hbase.PerformanceEvaluation -D mapreduce.task.timeout=60000 --blockEncoding=DIFF sequentialWrite 1",", "
"   Move Class,Move Method,Extract Method,Move Attribute,",Prefix Compression - Trie data block encoding; hbase-common and hbase-server changes These are the hbase-common and hbase-server changes for hbase-4676 Prefix Compression - Trie data block encoding.,"Duplicated Code, Long Method, , , , "
"   Push Down Method,Extract Method,","Have HLog files for .META. and -ROOT- edits only Over on HBASE-6774, there is a discussion on separating out the edits for .META. regions from the other regions' edits w.r.t where the edits are written. This jira is to track an implementation of that.","Duplicated Code, Long Method, , , "
"   Push Down Method,Extract Method,","Have HLog files for .META. and -ROOT- edits only Over on HBASE-6774, there is a discussion on separating out the edits for .META. regions from the other regions' edits w.r.t where the edits are written. This jira is to track an implementation of that.","Duplicated Code, Long Method, , , "
"   Pull Up Method,Inline Method,","Have HLog files for .META. and -ROOT- edits only Over on HBASE-6774, there is a discussion on separating out the edits for .META. regions from the other regions' edits w.r.t where the edits are written. This jira is to track an implementation of that.",", , Duplicated Code, "
"   Move Method,Inline Method,","Compaction Tool In HBASE-5616, as part of the compaction code refactor, a CompactionTool was added.

but there are some issues:
* The tool is under test/
* mockito is required, so the ""test"" scope should be removed from the pom.xml, otherwise the tool doesn't start
* The mock, used by the tool, is mocking HRegion.getRegionInfo() but some code (Store) uses HRegion.regionInfo directly HStore.java#L2021, HStore.java#L1389, HStore.java#L1402 and you end up with a NPE in the tool.
* The Mocked Store uses a dummy family and the compacted files doesn't get the same family properties specified (compression, encoding, ...)
* at the end of compaction CompactionTool.java#L155, on by default, the compaction file is removed (note that the compacted one are already removed inside the store.compact()... and you end up with an empty dir, if you compact everything.

I've fixed some stuff and added support to:
* Run the compaction as a MR Job
* Specify a Table (compact each region/family)
* Specify a Region (compact each family)
* Specify a Family (as before)",", , , "
"   Rename Method,",Supporting for HLog appends I thank we should open a ticket to track what needs changed to support appends when the coding is done on HADOOP-1700.,", "
"   Extract Method,Inline Method,","Add verbose logging option to HConnectionManager In the course of HBASE-7250 I found that client-side errors (as well as server-side errors, but that's another question) are hard to debug.
I have some local commits with useful, not-that-hacky HConnectionManager logging added.
Need to ""productionize"" it to be off by default but easy-to-enable for debugging.","Duplicated Code, Long Method, , , "
"   Rename Method,Inline Method,","remove flush-related records from WAL and make locking more granular Comments from many people in HBASE-6466 and HBASE-6980 indicate that flush records in WAL are not useful. If so, they should be removed.",", , "
"   Rename Method,Extract Method,","remove flush-related records from WAL and make locking more granular Comments from many people in HBASE-6466 and HBASE-6980 indicate that flush records in WAL are not useful. If so, they should be removed.","Duplicated Code, Long Method, , "
"   Rename Method,Inline Method,","remove flush-related records from WAL and make locking more granular Comments from many people in HBASE-6466 and HBASE-6980 indicate that flush records in WAL are not useful. If so, they should be removed.",", , "
"   Rename Method,","Online Merge Support executing region merge transaction on Regionserver, similar with split transaction

Process of merging two regions:
a.client sends RPC (dispatch merging regions) to master
b.master moves the regions together (on the same regionserver where the more heavily loaded region resided)
c.master sends RPC (merge regions) to this regionserver
d.Regionserver executes the region merge transaction in the thread pool
e.the above b,c,d run asynchronously

Process of region merge transaction:
a.Construct a new region merge transaction.

b.prepare for the merge transaction, the transaction will be canceled if it is unavailable, 
e.g. two regions don't belong to same table; two regions are not adjacent in a non-compulsory merge; region is closed or has reference

c.execute the transaction as the following:
/**
* Set region as in transition, set it into MERGING state.
*/
SET_MERGING_IN_ZK,
/**
* We created the temporary merge data directory.
*/
CREATED_MERGE_DIR,
/**
* Closed the merging region A.
*/
CLOSED_REGION_A,
/**
* The merging region A has been taken out of the server's online regions list.
*/
OFFLINED_REGION_A,
/**
* Closed the merging region B.
*/
CLOSED_REGION_B,
/**
* The merging region B has been taken out of the server's online regions list.
*/
OFFLINED_REGION_B,
/**
* Started in on creation of the merged region.
*/
STARTED_MERGED_REGION_CREATION,
/**
* Point of no return. If we got here, then transaction is not recoverable
* other than by crashing out the regionserver.
*/
PONR

d.roll back if step c throws exception


Usage:

HBaseAdmin#mergeRegions


See more details from the patch",", "
"   Rename Method,Inline Method,","Cleanup client connection layers This issue originated from a discussion over in HBASE-7442. We currently have a broken abstraction with {{HBaseClient}}, where it is bound to a single {{Configuration}} instance at time of construction, but then reused for all connections to all clusters. This is combined with multiple, overlapping layers of connection caching.

Going through this code, it seems like we have a lot of mismatch between the higher layers and the lower layers, with too much abstraction in between. At the lower layers, most of the {{ClientCache}} stuff seems completely unused. We currently effectively have an {{HBaseClient}} singleton (for {{SecureClient}} as well in 0.92/0.94) in the client code, as I don't see anything that calls the constructor or {{RpcEngine.getProxy()}} versions with a non-default socket factory. So a lot of the code around this seems like built up waste.

The fact that a single Configuration is fixed in the {{HBaseClient}} seems like a broken abstraction as it currently stands. In addition to cluster ID, other configuration parameters (max retries, retry sleep) are fixed at time of construction. The more I look at the code, the more it looks like the {{ClientCache}} and sharing the {{HBaseClient}} instance is an unnecessary complication. Why cache the {{HBaseClient}} instances at all? In {{HConnectionManager}}, we already have a mapping from {{Configuration}} to {{HConnection}}. It seems to me like each {{HConnection(Implementation)}} instance should have it's own {{HBaseClient}} instance, doing away with the {{ClientCache}} mapping. This would keep each {{HBaseClient}} associated with a single cluster/configuration and fix the current breakage from reusing the same {{HBaseClient}} against different clusters.

We need a refactoring of some of the interactions of {{HConnection(Implementation)}}, {{HBaseRPC/RpcEngine}}, and {{HBaseClient}}. Off hand, we might want to expose a separate {{RpcEngine.getClient()}} method that returns a new {{RpcClient}} interface (implemented by {{HBaseClient}}) and move the {{RpcEngine.getProxy()}}/{{stopProxy()}} implementations into the client. So all proxy invocations can go through the same client, without requiring the static client cache. I haven't fully thought this through, so I could be missing other important aspects. But that approach at least seems like a step in the right direction for fixing the client abstractions.",", , "
"   Move Method,Extract Method,","A canary monitoring program specifically for regionserver *Motivation*
This ticket is to provide a canary monitoring tool specifically for HRegionserver, details as follows
1. This tool is required by operation team due to they thought that the canary for each region of a HBase is too many for them, so I implemented this coarse-granular one based on the original o.a.h.h.tool.Canary for them
2. And this tool is implemented by multi-threading, which means the each Get request sent by a thread. the reason I use this way is due to we suffered the region server hung issue by now the root cause is still not clear. so this tool can help operation team to detect hung region server if any.

*example*
1. the tool docs
./bin/hbase org.apache.hadoop.hbase.tool.RegionServerCanary -help
Usage: [opts] [regionServerName 1 [regionServrName 2...]]
regionServerName - FQDN serverName, can use linux command:hostname -f to check your serverName
where [-opts] are:
-help Show this help and exit.
-e Use regionServerName as regular expression
which means the regionServerName is regular expression pattern
-f <B> stop whole program if first error occurs, default is true
-t <N> timeout for a check, default is 600000 (milisecs)
-daemon Continuous check at defined intervals.
-interval <N> Interval between checks (sec)

2. Will send a request to each regionserver in a HBase cluster
./bin/hbase org.apache.hadoop.hbase.tool.RegionServerCanary

3. Will send a request to a regionserver by given name
./bin/hbase org.apache.hadoop.hbase.tool.RegionServerCanary rs1.domainname

4. Will send a request to regionserver(s) by given regular-expression
/opt/trend/circus-opstool/bin/hbase-canary-monitor-each-regionserver.sh -e rs1.domainname.pattern
// another example
./bin/hbase org.apache.hadoop.hbase.tool.RegionServerCanary -e tw-poc-tm-puppet-hdn[0-9]\{1,2\}.client.tw.trendnet.org


5. Will send a request to a regionserver and also set a timeout limit for this test
// query regionserver:rs1.domainname with timeout limit 10sec
// -f false, means that will not exit this program even test failed
./bin/hbase org.apache.hadoop.hbase.tool.RegionServerCanary -f false -t 10000 rs1.domainname
// echo ""1"" if timeout
echo ""$?""

6. Will run as daemon mode, which means it will send request to each regionserver periodically
./bin/hbase org.apache.hadoop.hbase.tool.RegionServerCanary -daemon






","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Convert all tests that use HBaseTestingUtility.createMultiRegions to HBA.createTable Like I discussed in HBASE-7534, {{HBaseTestingUtility.createMultiRegions}} should disappear and not come back. There's about 25 different places in the code that rely on it that need to be changed the same way I changed TestReplication.

Perfect for someone that wants to get started with HBase dev :)","Duplicated Code, Long Method, , "
"   Pull Up Method,Move Method,Pull Up Attribute,Move Attribute,",[replication] Create an interface for replication peers 0,", , , Duplicated Code, Duplicated Code, "
"   Rename Method,Move Method,Extract Method,",[replication] Create an interface for replication queues 0,"Duplicated Code, Long Method, , , "
"   Rename Method,","Add a costless notifications mechanism from master to regionservers & clients t would be very useful to add a mechanism to distribute some information to the clients and regionservers. Especially It would be useful to know globally (regionservers + clients apps) that some regionservers are dead. This would allow:
- to lower the load on the system, without clients using staled information and going on dead machines
- to make the recovery faster from a client point of view. It's common to use large timeouts on the client side, so the client may need a lot of time before declaring a region server dead and trying another one. If the client receives the information separatly about a region server states, it can take the right decision, and continue/stop to wait accordingly.

We can also send more information, for example instructions like 'slow down' to instruct the client to increase the retries delay and so on.

Technically, the master could send this information. To lower the load on the system, we should:
- have a multicast communication (i.e. the master does not have to connect to all servers by tcp), with once packet every 10 seconds or so.
- receivers should not depend on this: if the information is available great. If not, it should not break anything.
- it should be optional.

So at the end we would have a thread in the master sending a protobuf message about the dead servers on a multicast socket. If the socket is not configured, it does not do anything. On the client side, when we receive an information that a node is dead, we refresh the cache about it.",", "
"   Move Class,Rename Method,","Use the Store interface instead of HStore (coprocessor, compaction, ...) We have a Store interface that is almost never used, coprocessors and compactions are using the HStore.

Replace the HStore with Store to have the ability to create custom Store for testing, instead of injecting stuff in HStore.",", "
"   Move Method,Extract Method,Move Attribute,","clean up compactionrequest and compactselection - part 1 Certain parts of CompactionRequest are unnecessary.
Off-peak hour management is part way in selection, part way in Store and part way in policy.
Needs to be cleaned up in preparation of having CompactionPolicy return CompactionRequest.","Duplicated Code, Long Method, , , , "
"   Rename Method,Move Method,Extract Method,Inline Method,","enable encapsulating compaction policy/compactor/store file manager interaction shennanigans To avoid massive casting and/or deciphering of structures traveling between SFM, compaction policy, and compactor in non-trivial compaction schemes like stripe or level, we need to make interaction between themselves hidden.
Elsewhere, the changes are being made to coprocessor for compactions that will make CompactionRequest a limited-visibility class for users, with coprocessors being able to subclass and return it. -This seems like a viable solution for the problem at hand too. Policy will (optionally) subclass compaction request and return it. Instead of calling something.compact(req), req.compact() will be called (with ""something"" already stored inside req as of now), after which, magic will happen.-
After merging that code I actually see that subclassing compactionrequest in both will break the policy, or require bunch of ugly code in coprocessors (a subclass for every policy, and telling them apart.
","Duplicated Code, Long Method, , , , "
"   Rename Method,","make policy and compactor in default store engine separately pluggable (for things like tier-based, and default policy experiments with permutations) Technically, StoreEngine can be used to achieve any permutations of things, but to make it more convenient to replace compaction policy/compator in standard schemes like tier-based, we can add separate hooks in DefaultStoreEngine (as long as custom ones conform to its default expectations e.g. flat list of sorted files, etc.)",", "
"   Rename Method,","More Table operation in TableHandler for REST interface In the current implementation, only the metadata query of table allowed. It's not convinent to use the REST interface to write a third-party client library. I think the functionality of REST interface should be similar with the hbase shell. So user can create/show/update/delate/enable/disable the table.",", "
"   Rename Class,Rename Method,Move Method,Extract Method,Move Attribute,","Snapshot Manifest file instead of multiple empty files Currently taking a snapshot means creating one empty file for each file in the source table directory, plus copying the .regioninfo file for each region, the table descriptor file and a snapshotInfo file.

during the restore or snapshot verification we traverse the filesystem (fs.listStatus()) to find the snapshot files, and we open the .regioninfo files to get the information.

to avoid hammering the NameNode and having lots of empty files, we can use a manifest file that contains the list of files and information that we need.
To keep the RS parallelism that we have, each RS can write its own manifest.

{code}
message SnapshotDescriptor {
required string name;
optional string table;
optional int64 creationTime;
optional Type type;
optional int32 version;
}

message SnapshotRegionManifest {
optional int32 version;
required RegionInfo regionInfo;
repeated FamilyFiles familyFiles;

message StoreFile {
required string name;
optional Reference reference;
}

message FamilyFiles {
required bytes familyName;
repeated StoreFile storeFiles;
}
}
{code}

{code}
/hbase/.snapshot/<snapshotName>
/hbase/.snapshot/<snapshotName>/snapshotInfo
/hbase/.snapshot/<snapshotName>/<tableName>
/hbase/.snapshot/<snapshotName>/<tableName>/tableInfo
/hbase/.snapshot/<snapshotName>/<tableName>/regionManifest(.n)
{code}
","Duplicated Code, Long Method, , , , "
"   Extract Interface,Extract Method,",refactor default compactor to make its parts easier to reuse Refactor default compactor to make its parts easier to reuse. To make eventual HBASE-7967 patch smaller.,"Duplicated Code, Long Method, , Large Class, "
"   Rename Method,Extract Method,","row keys should be array of bytes I have heard from several people that row keys in HBase should be less restricted than hadoop.io.Text.

What do you think?

At the very least, a row key has to be a WritableComparable. This would lead to the most general case being either hadoop.io.BytesWritable or hbase.io.ImmutableBytesWritable. The primary difference between these two classes is that hadoop.io.BytesWritable by default allocates 100 bytes and if you do not pay attention to the length, (BytesWritable.getSize()), converting a String to a BytesWritable and vice versa can become problematic. 

hbase.io.ImmutableBytesWritable, in contrast only allocates as many bytes as you pass in and then does not allow the size to be changed.

If we were to change from Text to a non-text key, my preference would be for ImmutableBytesWritable, because it has a fixed size once set, and operations like get, etc do not have to something like System.arrayCopy where you specify the number of bytes to copy.

Your comments, questions are welcome on this issue. If we receive enough feedback that Text is too restrictive, we are willing to change it, but we need to hear what would be the most useful thing to change it to as well.

","Duplicated Code, Long Method, , "
"   Rename Method,Pull Up Method,Extract Method,","HBCK support for table locks Table locks have been introduced in HBASE-7305, HBASE-7546, and others (see the design doc at HBASE-7305). 

This issue adds support in HBCK to report and fix possible conditions about table locks. Namely, if due to some bug, the table lock remains not-released, then HBCK should be able to report it, and remove the lock, so that normal table operations will continue. 

Also see the comments in HBASE-7977.","Duplicated Code, Long Method, , Duplicated Code, "
"   Move Method,Extract Method,","Improve LoadTest extensibility This patch rolls up related changes:

- Allow classes extending TestMiniClusterLoadSequential to override how reader and writer threads are set up, including extending the reader and writers themselves.

- Make it so classes extending MultiThreadedWriter can cleanly override how Puts are constructed.

- Provide an option for passing in a custom table descriptor to HBaseTestingUtility#createPreSplitLoadTestTable.

- HBaseTestingUtility#waitUntilAllRegionsAssigned does not check if the region it is counting belongs to the table created by the test and will not return if it accidentally counts ""too many"" regions, for example the regions of the ACL table when security is enabled. (Some class based on TestMiniClusterLoadSequential may want to enable security.)","Duplicated Code, Long Method, , , "
"   Pull Up Method,Extract Method,","MapReduce over snapshot files The idea is to add an InputFormat, which can run the mapreduce job over snapshot files directly bypassing hbase server layer. The IF is similar in usage to TableInputFormat, taking a Scan object from the user, but instead of running from an online table, it runs from a table snapshot. We do one split per region in the snapshot, and open an HRegion inside the RecordReader. A RegionScanner is used internally for doing the scan without any HRegionServer bits. 

Users have been asking and searching for ways to run MR jobs by reading directly from hfiles, so this allows new use cases if reading from stale data is ok:
- Take snapshots periodically, and run MR jobs only on snapshots.
- Export snapshots to remote hdfs cluster, run the MR jobs at that cluster without HBase cluster.
- (Future use case) Combine snapshot data with online hbase data: Scan from yesterday's snapshot, but read today's data from online hbase cluster. 
","Duplicated Code, Long Method, , Duplicated Code, "
"   Extract Method,Move Attribute,",Port  HBASE-6874  Implement prefetching for scanners from 0.89-fb This should help scanner performance. We should have it in trunk.,"Duplicated Code, Long Method, , , "
"   Move Method,Extract Method,Inline Method,Move Attribute,","[replication] Remove ReplicationZookeeper class Once all of the logic in ReplicationZookeeper has been refactored into three interfaces (for status, queues, and peers), there is almost no logic in ReplicationZookeeper. It can now be removed and classes that call it should be refactored to call the state interfaces directly.","Duplicated Code, Long Method, , , , , "
"   Rename Method,Extract Method,Inline Method,","Allow parallel snapshot of different tables currently only one snapshot at the time is allowed.
Like for the restore, we should allow taking snapshot of different tables in parallel.","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Server-side, remove convertion from pb type to client type before we call method In the regionserver, when the rpc receives a call, the call is described using protobufs. Before we make the server-side invocation, we do a transform on the pb param objects to make a native pojo -- e.g. from a pb Puts into an hbase o.a.h.h.client.Put -- and only then do we make the call against the server.

On the way out, similar, before putting the result on the wire, we will do a convertion from o.a.h.h.client.Result into pb Result.

This issue is about our first INVESTIGATING if it is possible to do away w/ this marshalling/unmarshalling serverside especially given the pb objects themselves are rich in accessor and getters, etc. If it is possible to do w/ pbs alone serverside, then we should go ahead and rip out all the serverside convertions.","Duplicated Code, Long Method, , "
"   Rename Method,Move Method,Extract Method,","Implement tags and the internals of how a tag should look like The intent of this JIRA comes from HBASE-7897.
This would help us to decide on the structure and format of how the tags should look like.","Duplicated Code, Long Method, , , "
"   Rename Method,Inline Method,","Store last flushed sequence id for each store of region for Distributed Log Replay HBASE-7006 stores last flushed sequence id of the region in zookeeper.

To prevent deleted data from appearing again, we should store last flushed sequence id for each store of region in zookeeper.

See discussion here:
https://issues.apache.org/jira/browse/HBASE-7006?focusedCommentId=13660428&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13660428",", , "
"   Rename Class,Rename Method,Extract Method,Inline Method,","Region assigments scan table directory making them slow for huge tables On a table with 130k regions it takes about 3 seconds for a region server to open a region once it has been assigned.

Watching the threads for a region server running 0.94.5 that is opening many such regions shows the thread opening the reigon in code like this:
{noformat}
""PRI IPC Server handler 4 on 60020"" daemon prio=10 tid=0x00002aaac07e9000 nid=0x6566 runnable [0x000000004c46d000]
java.lang.Thread.State: RUNNABLE
at java.lang.String.indexOf(String.java:1521)
at java.net.URI$Parser.scan(URI.java:2912)
at java.net.URI$Parser.parse(URI.java:3004)
at java.net.URI.<init>(URI.java:736)
at org.apache.hadoop.fs.Path.initialize(Path.java:145)
at org.apache.hadoop.fs.Path.<init>(Path.java:126)
at org.apache.hadoop.fs.Path.<init>(Path.java:50)
at org.apache.hadoop.hdfs.protocol.HdfsFileStatus.getFullPath(HdfsFileStatus.java:215)
at org.apache.hadoop.hdfs.DistributedFileSystem.makeQualified(DistributedFileSystem.java:252)
at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:311)
at org.apache.hadoop.fs.FilterFileSystem.listStatus(FilterFileSystem.java:159)
at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:842)
at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:867)
at org.apache.hadoop.hbase.util.FSUtils.listStatus(FSUtils.java:1168)
at org.apache.hadoop.hbase.util.FSTableDescriptors.getTableInfoPath(FSTableDescriptors.java:269)
at org.apache.hadoop.hbase.util.FSTableDescriptors.getTableInfoPath(FSTableDescriptors.java:255)
at org.apache.hadoop.hbase.util.FSTableDescriptors.getTableInfoModtime(FSTableDescriptors.java:368)
at org.apache.hadoop.hbase.util.FSTableDescriptors.get(FSTableDescriptors.java:155)
at org.apache.hadoop.hbase.util.FSTableDescriptors.get(FSTableDescriptors.java:126)
at org.apache.hadoop.hbase.regionserver.HRegionServer.openRegion(HRegionServer.java:2834)
at org.apache.hadoop.hbase.regionserver.HRegionServer.openRegion(HRegionServer.java:2807)
at sun.reflect.GeneratedMethodAccessor64.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:320)
at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1426)
{noformat}

To open the region, the region server first loads the latest HTableDescriptor. Since HBASE-4553 HTableDescriptor's are stored in the file system at ""/hbase/<tableDir>/.tableinfo.<sequenceNum>"". The file with the largest sequenceNum is the current descriptor. This is done so that the current descirptor is updated atomically. However, since the filename is not known in advance FSTableDescriptors it has to do a FileSystem.listStatus operation which has to list all files in the directory to find it. The directory also contains all the region directories, so in our case it has to load 130k FileStatus objects. Even using a globStatus matching function still transfers all the objects to the client before performing the pattern matching. Furthermore HDFS uses a default of transferring 1000 directory entries in each RPC call, so it requires 130 roundtrips to the namenode to fetch all the directory entries.

Consequently, to reassign all the regions of a table (or a constant fraction thereof) requires time proportional to the square of the number of regions.

In our case, if a region server fails with 200 such regions, it takes 10+ minutes for them all to be reassigned, after the zk expiration and log splitting.
","Duplicated Code, Long Method, , , "
"   Rename Class,Move Method,Move Attribute,","Pluggable RpcScheduler Today, the RPC scheduling mechanism is pretty simple: it execute requests in isolated thread-pools based on their priority. In the current implementation, all normal get/put requests are using the same pool. We'd like to add some per-user or per-region level isolation, so that a misbehaved user/region will not saturate the thread-pool and cause DoS to others easily. The idea is similar to FairScheduler in MR. The current scheduling code is not standalone and is mixed with others (Connection#processRequest). The issue is the first step to extract it to an interface, so that people are free to write and test their own implementations.

This patch doesn't make it completely pluggable yet, as some parameters are pass from constructor. This is because HMaster and HRegionServer both use RpcServer and they have different thread-pool size config. Let me know if you have a solution to this.",", , , "
"   Rename Method,","protobuf message style Google's protobuf style guide (https://developers.google.com/protocol-buffers/docs/style) lays out the convention that message field names should be underscore_separated and services should be MixedCase. The .proto's in trunk do not follow this style; instead they follow Java naming conventions. The protobuf compiler will automatically change the style of names to match the language it is compiling for, but it is unable to do so if the protobuf style is not used. As a result, using the HBase proto files from languages with different naming conventions than Java is a little bit more painful.

Since a core feature of moving to protobufs is opening the door to wire compatible implementations in other languages, I think this may want to be addressed. This patch changes the naming convention in the protos. The resulting .java files that the protobuf compiler puts out are functionally the same (with the same correct Java naming style).",", "
"   Rename Class,Rename Method,","Addendum to pluggable RpcScheduler This patch fixes the review comments from [~stack] and a small fix:
- Make RpcScheduler fully pluggable. One can write his/her own implementation and add it to classpath and specify it by config ""hbase.region.server.rpc.scheduler.factory.class"".
- Add unit tests and fix that RpcScheduler.stop is not called (discovered by tests)
",", "
"   Rename Method,","Add a view/edit tool for favored node mappings for regions Add a tool that one can run offline to view the favored node mappings for regions, and also fix the mappings if needed. Such a tool exists in the 0.89-fb branch. Will port it over to trunk/0.95.",", "
"   Rename Method,Extract Method,","ReplicationLogCleaner slow at large scale At a large scale the ReplicationLogCleaner fails to clean up .oldlogs as fast as the cluster is producing them. For each old HLog file that has been replicated and should be deleted the ReplicationLogCleaner checks every replication queue in ZooKeeper before removing it. This means that as a cluster scales up the number of files to delete scales as well as the time to delete each file so the cleanup chore scales quadratically. In our case it reached the point where the oldlogs were growing faster than they were being cleaned up.

We're now running with a patch that allows the ReplicationLogCleaner to refresh its list of files in the replication queues from ZooKeeper just once for each batch of files the CleanerChore wants to evaluate.

I'd propose updating FileCleanerDelegate to take a List<FileStatus> rather than a single one at a time. This would allow file cleaners that check an external resource for references such as ZooKeeper (for ReplicationLogCleaner) or HDFS (for SnapshotLogCleaner which looks like it may also have similar trouble at scale) to load those references once per batch rather than for every log.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,",Thrift getRow does not support specifying columns Thrift interface has a getRow function but it does not support asking for specific columns.,"Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Make custom distributed barrier procedure pluggable  
Currently if one wants to implement a custom distributed barrier procedure (e.g., distributed log roll or distributed table flush), the HBase core code needs to be modified in order for the procedure to work.

Looking into the snapshot code (especially on region server side), most of the code to enable the procedure are generic life-cycle management (i.e., init, start, stop). We can make this part pluggable.

Here is the proposal. Following the coprocessor example, we define two properties:

{code}
hbase.procedure.regionserver.classes
hbase.procedure.master.classes
{code}

The values for both are comma delimited list of classes. On region server side, the classes implements the following interface:

{code}
public interface RegionServerProcedureManager {
public void initialize(RegionServerServices rss) throws KeeperException;
public void start();
public void stop(boolean force) throws IOException;
public String getProcedureName();
}
{code}

While on Master side, the classes implement the interface:

{code}
public interface MasterProcedureManager {
public void initialize(MasterServices master) throws KeeperException, IOException, UnsupportedOperationException;
public void stop(String why);
public String getProcedureName();
public void execProcedure(ProcedureDescription desc) throws IOException;
IOException;
}
{code}

Where the ProcedureDescription is defined as

{code}
message ProcedureDescription {
required string name = 1;
required string instance = 2;
optional int64 creationTime = 3 [default = 0];
message Property {
required string tag = 1;
optional string value = 2;
}
repeated Property props = 4;
}
{code}

A generic API can be defined on HMaster to trigger a procedure:

{code}
public boolean execProcedure(ProcedureDescription desc) throws IOException;
{code}

_SnapshotManager_ and _RegionServerSnapshotManager_ are special examples of _MasterProcedureManager_ and _RegionServerProcedureManager_. They will be automatically included (users don't need to specify them in the conf file).","Duplicated Code, Long Method, , "
"   Rename Class,Move And Rename Class,Rename Method,Move Method,Move Attribute,","Cleanup inconsistencies in protobuf message and rpc names This is a minor cleanup of some inconsistencies in names of protobuf messages and rpcs. I ran into these while dynamically working with hbase-protocol, where it is nice to have message and rpc names as consistent as possible. Specifically, most of these changes are to have the rpc name match the message names plus [Request, Response]. Changes:

Message renames:
AccessControlProtos.UserPermissions[Request, Response] -> GetUserPermissions[Request, Response]
AggregateProtos.AggregateArgument -> AggregateRequest
AuthenticationProtos.Token[Request, Response] -> GetAuthenticationToken[Request, Response]
AuthenticationProtos.Token[Request, Response] -> GetAuthenticationToken[Request, Response]
MultiRowMutation.MultiMutate[Request, Response] -> MutateRows[Request, Response]
RowProcessorProtos.RowProcessor[Request, Response] -> ProcessRowRequest[Request, Response]
SecureBulkLoadProtos.DelegationTokenProto -> DelegationToken

RPC renames:
MasterAdminProtos.RunCatalogScan -> CatalogScan
MasterAdminProtos.Snapshot -> TakeSnapshot
MasterAdminProtos.GetCompletedSnapshots -> ListCompletedSnapshots
RowProcessorProtos.Process -> RowProcessorProtos.ProcessRow

Outer classname changes:
MultiRowMutation -> MultiRowMutationProtos
Tracing -> TracingProtos

File renames:
hbase.proto -> HBase.proto",", , , "
"   Rename Class,Extract Superclass,Rename Method,Extract Method,","Multi row get does not return any results even if any one of the rows specified in the query is missing and improve exception handling When a client tries to retrieve multiple rows using REST API, even if one of the specified rows does not exist, 404 is returned. The correct way should be to return the result for the found rows and ignore the non-existent ones. Also, in the current code base, only some exceptions are handled, if some exception like Access denied or no column found exception is throws by the APIs, 500 ( server not found) is returned to user. This is leaves the end user wondering what caused the rest command to fail.","Duplicated Code, Long Method, , Duplicated Code, Large Class, "
"   Rename Method,Extract Method,Inline Method,","More temporary test files are being left in /tmp/hbase-<user> There is currently about 160MB of stuff being left behind in after a unit test run in our jenkins setup.

Table names left behind indicate that it is due to these classes.

TestCompaction
Either TestForceCacheImporatntBlocks or TestScannerSelectionUingTTL or TestScannerSelectionUsingKeyRange
TestHRegion

","Duplicated Code, Long Method, , , "
"   Move Method,Extract Method,","Alow more than one log splitter per RS IIRC, this is an idea that came from the lads at Xiaomi.

I have a small cluster of 6 RSs and one went down. It had a few WALs. I see this in logs:

2013-10-09 05:47:27,890 DEBUG org.apache.hadoop.hbase.master.SplitLogManager: total tasks = 25 unassigned = 21

WAL splitting is held up for want of slots out on the cluster to split WALs.

We need to be careful we don't overwhelm the foreground regionservers but more splitters should help get all back online faster.","Duplicated Code, Long Method, , , "
"   Move Class,Move Method,Extract Method,Move Attribute,","HBase native metrics and metric collection for coprocessors It would help provide better visibility into what coprocessors are doing if we provided a way for coprocessors to export their own metrics. The general idea is to:

* extend access to the HBase ""metrics bus"" down into the coprocessor environments
* coprocessors can then register and increment custom metrics
* coprocessor metrics are then reported along with all others through normal mechanisms","Duplicated Code, Long Method, , , , "
"   Extract Method,Inline Method,","make the compaction logging less confusing 1) One of the most popular question from HBase users has got to be ""I have scheduled major compactions to run once per week, why are there so many"".
We need to somehow tell the user, wherever we log that there is a ""major"" compaction, whether it's a major compaction because that's what was in the request (from regular major compaction or user request), or was it just promoted because it took all files. Esp. the latter should be clear.
2) small vs large compaction threads and minor vs major compactions is confusing. Maybe the threads can be named short and long compactions.

We","Duplicated Code, Long Method, , , "
"   Push Down Method,Push Down Attribute,","Start of JSR-88 Console Deployer Hook up the JSR-88 DDBeans and DConfigBeans, and start a console-based deployer. Right now it just starts navigating the EJB DD and printing info to the screen, but it's enough to show it working.

This includes:
- fixes to DDBeans and DConfigBeans to make them work well together
- a new tool implementation of DeployableObject for EJB JAR (with subclass)
- a new CLI console package, which right now consists solely of a command-line deployer. I assume this will be expanded in the future.
",", , , "
"   Rename Method,Extract Method,","webservice deployment with ews this patch has code to deploy the webservice in the geronimo.
there is a
1) WebServiceDeployerGbean and by calling deploy GBean method one can
deploy a webservice. right now Axis service keep a seperate config-store
but that can be changed when we decide on the final thing

2)the Deployer accept a webservice jar file and generate the required code
using ews and deploy them. for time been the Axis GBean need to restart to
get the new deploymnet.

3)the complete senario is shown in the WebServiceTest.java test. But to
run this code need the JAVA_HOME/lib/tools.jar in the classpth. Since
still can not get it in the classpath of maven I exclude the tests. If
somebody using IDE and add the tools.jar in the classpath they should work
fine :)

4) let me add some doc to the wiki about how this works.
","Duplicated Code, Long Method, , "
"   Rename Method,",Add support for ejb-ref resolution by matching on the ejb interfaces Some applications assume that ejb-refs can be resolved by matching on interface types.,", "
"   Rename Method,Move Method,Move Attribute,","TransactionContext usage is confused and confusing Currently there is the TransactionContext with static accessors for the current thread's TransactionContext and a TransactionContextManager object/gbean with non-static accessors and helper methods to create the various types of TC and associate them with the current thread. Some components use the TransactionContext static methods and some use the gbean methods. 

We have a problem that new threads don't automatically get a TC. For threads we create, we can add code to create the TC, but for threads created e.g. by a servlet we have no direct access to do so.

Our TC cannot safely be shared between threads. Therefore I don't think that using an InheritableThreadLocal in TC is a good solution.

My proposal, which I will implement in the absence of other suggestions, is to eliminate the static methods on TC and just use the gbean accessors. The getTransactionContext method will never return null: if there is no TC for the current thread it will create a new UnspecifiedTransactionContext and associate it with the thread before returning. I may create an additional method for this, getNonNullTransactionContext, so the existing method can be used where it would be an error for no TransactionContext to be associated with the current thread.

The advantage I see in using a gbean rather that static methods is that is will simplify plugging in a different set of TransactionContext implementations should we wish to do so.",", , , "
"   Rename Class,Rename Method,Move Method,Extract Method,Move Attribute,","Improved structure for xxx-refs in geronimo plans In geronimo, jndi component environment references are resolved by turning some information supplied in an ejb-link or geronimo plan element into a complete object name. Resolving the reference usually consists of calling some method on the named gbean.

Currently the ""easy ways"" are ejb-link in the spec dds or target-name for a resource-ref to a connection factory. non-link ejb-refs require you to supply the complete object name, which is difficult and error prone.

All the target object names are jsr77 compliant names. 

This proposal is to have tags in the geronimo plan ref element for each component of the jsr77 name. Any that are not supplied can be filled in from the context. The elements are:
domain
server
application
module
type
name

I'm not entirely clear on whether the server attribute will allow you to specify a server in a different vm. If not something along these lines will be needed as well.

I'm proposing to keep the current target-name element in case you want to specify the entire name yourself.

Among other things this should provide geronimo plan ""ejb-link"" like functionality and simplify access to javamail and other non-connectionfactory resources (if any)","Duplicated Code, Long Method, , , , "
"   Rename Method,","SocketProtocolStressTest  fails SocketProtocolStressTest testConcurrentRequests() constantly fails during a maven build (cf. TIMEOUT problem, GERONIMO-160) but succeeds after a 'Run | JUnit Test' from within Eclipse v. 3.0.1. 

I'd like to have it excluded from unitTest in the file modules/network/project.xml like DatagramProtocolTest (patch follows). 

BTW: Could someone with Admin rights for jira add the network module to the Component/s list. Thanks.

-- Ralf",", "
"   Rename Method,Pull Up Method,Extract Method,Pull Up Attribute,","More runtime control over connection pools needed The connection pools should be more visible and configurable at runtime (and deployment time).

read-only:
current size
current idle connections
partition count

read-write:
maxSize
minSize (needs initial implementation)
blockingTimeoutMilliseconds
idleTimeoutMinutes (needs initial implementation)

","Duplicated Code, Long Method, , Duplicated Code, Duplicated Code, "
"   Rename Method,Extract Method,","ConfigurationEntry support for multiple LoginModules The abstract class ConfigurationEntry has support for returning multiple LoginModules (or more accurately, an array of AppConfigurationEntry's). However, none of the concrete implementations allow this.

It's a required feature in order for the CallerIdentityUserPasswordRealmBridge to work, because that needs the password to be put in the private credential set. Currently we have one set of login modules that actually authenticate you, and a different LoginModule that populates the private credential set. In order to be both behaviors, you need to load both LoginModules, but currently the available ConfigurationEntries can't be configured for that.

A problem is that the ConfigurationEntry gets its data from a SecurityRealm, and the SecurityRealm can only return a single AppConfigurationEntry (or LoginModule). It doesn't make sense to me to make the new ""multiple configuration entry"" take multiple security realms as its input. In concept, you want one security realm with two login modules.

So I think the change has to start by allowing a SecurityRealm to return multiple AppConfgurationEntry values.

Then we need the configuration syntax for the standard security realm GBeans to change so that they can take multiple login modules, including the options and control flags for each. Like, you might want to use a vanilla SQLSecurityRealm, but have it add a GeroinmoPasswordCredentialLoginModule (or a hypothetical AuditTrailLoginModule) in addition to its standard LoginModule.
","Duplicated Code, Long Method, , "
"   Rename Method,","Connection factories extracted from conceptually wrong gbean Currently connection factories/datasources (or their proxies) are obtained from the JCAManagedConnectionFactory gbean. Since there is a ConnectionFactory/Datasource gbean for the jsr-77 requirements, it would make more sense to obtain the connection factory/datasource from there. This would have the additional feature of allowing one to set up several connection factories under different names that all use the same ConnectionManager and ManagedConnectionFactory. This would for instance let you set up separately named QueueConnectionFactory and TopicConnectionFactory that share the same connections: named appropriately, this can let you leave out resource-refs in plans for apps that call the factories different names.",", "
"   Rename Class,Move Method,Extract Method,","GBeans should use jsr-77 naming conventions and these names should have mostly default components Currently the usage of object names for non-j2ee-wrapping gbeans is more or less random and confusing. We should adopt as much of jsr-77 naming as possible for our gbeans. Furthermore, as little as possible of the names should be specified in the gbean xml descriptor. Here's a proposal:

1. A service module that has no parent must specify domain and server name. This domain and server name will be inherited by all children recursively.

2. All gbeans deployed from a service dd will have J2EEApplication=null and GeronimoModule=<configId>

3. All gbeans deployed from a j2ee module or application will have J2EEApplication set from the application and GeronimoModule=<configId>

4. A gbean xml descriptor will have attributes for j2eeType and name. We will invent more j2eeType names as needed and prefix them with Ger or Geronimo.

I'm inclined to remove the possibility of directly specifying the entire object name. If it is really needed I'd suggest the attribute be called target-name in analogy to the usage in refs.","Duplicated Code, Long Method, , , "
"   Rename Class,Extract Method,Pull Up Attribute,","GBeans should use jsr-77 naming conventions and these names should have mostly default components Currently the usage of object names for non-j2ee-wrapping gbeans is more or less random and confusing. We should adopt as much of jsr-77 naming as possible for our gbeans. Furthermore, as little as possible of the names should be specified in the gbean xml descriptor. Here's a proposal:

1. A service module that has no parent must specify domain and server name. This domain and server name will be inherited by all children recursively.

2. All gbeans deployed from a service dd will have J2EEApplication=null and GeronimoModule=<configId>

3. All gbeans deployed from a j2ee module or application will have J2EEApplication set from the application and GeronimoModule=<configId>

4. A gbean xml descriptor will have attributes for j2eeType and name. We will invent more j2eeType names as needed and prefix them with Ger or Geronimo.

I'm inclined to remove the possibility of directly specifying the entire object name. If it is really needed I'd suggest the attribute be called target-name in analogy to the usage in refs.","Duplicated Code, Long Method, , Duplicated Code, "
"   Rename Method,Move Method,Extract Method,Move Attribute,","Connector deployment should convert 1.0 dds to 1.5 dds before processing them. Just as with the other module builders, the connector module builder should convert 1.0 spec dds to 1.5 spec dds before processing them.","Duplicated Code, Long Method, , , , "
"   Extract Superclass,Extract Method,","Put the gbeandatas in a deployment context in a queriable container and use queries to resolve links GBeans in a DeploymentContext should be kept in a queriable registry like the BasicGBeanRegistry, but for GBeanData rather thand GBeanInstance. ejb-links, resource-links, gbean-links, etc should be resolved by querying this registry rather than more special purpose tracking. This should simplify RefContext considerably. This feature is needed so that if the mail gbean is deployed in a j2ee module (such as an app client) it can be used in that same module.","Duplicated Code, Long Method, , Duplicated Code, Large Class, "
"   Rename Method,Extract Method,","Put the gbeandatas in a deployment context in a queriable container and use queries to resolve links GBeans in a DeploymentContext should be kept in a queriable registry like the BasicGBeanRegistry, but for GBeanData rather thand GBeanInstance. ejb-links, resource-links, gbean-links, etc should be resolved by querying this registry rather than more special purpose tracking. This should simplify RefContext considerably. This feature is needed so that if the mail gbean is deployed in a j2ee module (such as an app client) it can be used in that same module.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,Inline Method,","Put the gbeandatas in a deployment context in a queriable container and use queries to resolve links GBeans in a DeploymentContext should be kept in a queriable registry like the BasicGBeanRegistry, but for GBeanData rather thand GBeanInstance. ejb-links, resource-links, gbean-links, etc should be resolved by querying this registry rather than more special purpose tracking. This should simplify RefContext considerably. This feature is needed so that if the mail gbean is deployed in a j2ee module (such as an app client) it can be used in that same module.","Duplicated Code, Long Method, , , "
"   Extract Superclass,Rename Method,Pull Up Method,Move Method,Extract Method,Inline Method,","Numerous enhancements / fixes to interop - Numerious changes have been made to the code to make it more Geronimo code friendly.
- Cleaned up the stub / skel generator
- Add java method overloading
- Removed some un-necessary files (iiop name stubs, repository, standalone server startup)
- Fixeda bunch of IIOP marshalling errors that showed up while running the rmi.iiop code against the J2EE CS 1.3 test harness.
- Not all the rmi.iiop variable names and code style was changed. This wasn't done to make the previous point easier.
- Made a number of GBeans to support various elements of the server
- Started to use the existing NetworkServices idea of the EJBContainer and WebServiceContainer.
- More things that I can not remember doing...

Still todo:

- Add interceptor support to plug in security and transactions
- Add code for inserting custom data into the object ref to store references to statefull objects on the server side.
","Duplicated Code, Long Method, , , Duplicated Code, Large Class, , Duplicated Code, "
"   Extract Superclass,Rename Method,Pull Up Method,Extract Method,Inline Method,","Numerous enhancements / fixes to interop - Numerious changes have been made to the code to make it more Geronimo code friendly.
- Cleaned up the stub / skel generator
- Add java method overloading
- Removed some un-necessary files (iiop name stubs, repository, standalone server startup)
- Fixeda bunch of IIOP marshalling errors that showed up while running the rmi.iiop code against the J2EE CS 1.3 test harness.
- Not all the rmi.iiop variable names and code style was changed. This wasn't done to make the previous point easier.
- Made a number of GBeans to support various elements of the server
- Started to use the existing NetworkServices idea of the EJBContainer and WebServiceContainer.
- More things that I can not remember doing...

Still todo:

- Add interceptor support to plug in security and transactions
- Add code for inserting custom data into the object ref to store references to statefull objects on the server side.
","Duplicated Code, Long Method, , Duplicated Code, Large Class, , Duplicated Code, "
"   Rename Method,","Remaining EJB JAR JavaBeans The initial checkin of JavaBeans for the XML DDs earlier today included a partial implementation of EJB-JAR, but it was pretty rough because we hadn't combined it with the J2EE objects yet. Now the EJB JAR JavaBeans are complete. I also added minimal JavaDoc to all the JavaBeans.

There's a loader for the EJB JAR based on DOM, that's about 1/2 complete (it handles everything under enterprise-beans, but nothing under relationships or assembly-descriptor). Nevertheless, it's enough to begin to work with. This involved a couple minor enhancements to the loader utility classes that Jeremy put together.

This also includes the validator, which uses the new beans to process META-INF/ejb-jar.xml. The actual tests so far are minimal, but it should be pretty easy to extend in parallel.",", "
"   Rename Method,","Provide a reset command to reset an option to its default value Within a session, currently we set configuration options and it would be very useful to have a 'reset' command to reset the value of an option to its default system value: 
ALTER SESSION RESET <option name> 

If we don't want to add a new keyword for RESET, we could potentially overload the SET command and allow the user to set to the 'default' value.
",", "
"   Move Class,Move Method,Extract Method,Inline Method,Move Attribute,","Multiplex BitData, adding application-layer back pressure 0","Duplicated Code, Long Method, , , , , "
"   Rename Class,Move And Rename Class,Rename Method,",Rename RandomReceiver to UnorderedReceiver to decrease confusion 0,", "
"   Move And Rename Class,Move Method,","Senders report misleading/incomplete metrics * Screen, BroadcastSender, and SingleSender are populating metrics in the queryProfile with 0.
* waitTime is not being reported.
* # of bytes sent should be reported.
* N_SENDERS should be named N_RECEIVERS",", , "
"   Rename Method,Pull Up Method,Pull Up Attribute,",Add configuration in DrillClient to encode complex/repeated types as JSON string 0,", Duplicated Code, Duplicated Code, "
"   Rename Class,Rename Method,Move Attribute,","Memory estimation for planning * Optionally allow limiting query memory
* Compute memory usage of a query and re-plan without hash operations if usage exceeds the limit",", , "
"   Rename Method,Move Method,","Hive Scalar UDFs: Add data type support for Date, TimeStamp and Decimal Currently passing data of type (Date, TimeStamp and Decimal) to Hive UDFs is not supported. This task is to add support for these types.",", , "
"   Move Method,Extract Method,","Implement support for Fixed Binary type in parquet reader Currently we can red out of a fixed binary column in parquet only if it is one of the decimal converted types. This is only a subset of the use cases of the fixed binary table as it can be used to store any arbitrary binary data that is byte aligned and the same length for all values. Unfortunately we do not have the fixed binary vector implemented in Drill currently, so the data will be read into the varbinary vectors, but this cam be optimized later.","Duplicated Code, Long Method, , , "
"   Pull Up Method,Move Method,Extract Method,Pull Up Attribute,","Add support for Local Exchange nodes Exchange nodes currently assume remote communication. In certain situations, it is useful to consolidate separate minor fragments together before pushing to remote nodes. This can reduce the communication overhead depending on the size and nature of the data.","Duplicated Code, Long Method, , , Duplicated Code, Duplicated Code, "
"   Extract Method,Move Attribute,","Fast Schema Return Right now, the first schema for a query set is not returned until all blocking operators complete. In many cases, it would be useful for an empty record set to be returned prior to completing blocking operators so that tools can understand schema sooner.","Duplicated Code, Long Method, , , "
"   Rename Method,Move Method,",Support scalar replacement of ValueHolder expressions to improve performance 0,", , "
"   Move Class,Move Method,Extract Method,Move Attribute,",Create unit test framework to ease creating unit tests with full result verification 0,"Duplicated Code, Long Method, , , , "
"   Rename Method,",Avro Record Reader Record reader implementation for Avro data files.,", "
"   Rename Method,","Update HiveRecordReader to read Hive decimal fields with scale and precision Currently HiveRecordReader reads decimal data in Hive and converts to VarChar. The reason is: Hive-0.12 doesn't enforce same precision and scale for all records. Drill may get records with different precision and scale within a column which Drill can't handle it currently. In Hive-0.13 scale and precision are enforced for all records in a column. As DRILL-1347 upgrades Hive storage plugin to work with Hive-0.13, this JIRA is created to track changes to HiveRecordReader to read Hive decimal data as decimal and not as VarChar.",", "
"   Push Down Method,Move Method,Extract Method,",Generalize CodeGenerator and FunctionHolder to support other types of functions CodeGenerator has too much embedded business logic. This should be abstracted out so we don't have to copy and past code multiple times.,"Duplicated Code, Long Method, , , , "
"   Extract Interface,Rename Method,Move Attribute,","Enable full engine in JDBC Layer You can get to the full exec engine now but too many things are implicitly defined. Instead, we need to add explicit options.

My proposal is to define a set of jdbc connection string properties:

engine=ref means that the Reference interpreter's capabilities and schemas are available. engine=full (default) means that the full execution engines tools should be available. engine=both means that both types are available.

For engine=full.

We will have the additional property: zk=[connection string]

This also requires the implementation of a smarter Schema type that provides automatic schema for all types recognized by the execution engine.",", , Large Class, "
"   Rename Class,Rename Method,Pull Up Method,Extract Method,Inline Method,Pull Up Attribute,","More prudent use of ZkStore When using Zk, we need to be more prudent about how much data we store. We should update things so that the ZkStore is cache/watcher based and that profiles are not stored there, but rather in local or dfs.","Duplicated Code, Long Method, , , Duplicated Code, Duplicated Code, "
"   Rename Method,Extract Method,","Merge Join Operator Implement the merge join physical operator with support for left, right and inner joins.","Duplicated Code, Long Method, , "
"   Rename Method,","Merge Join Operator Implement the merge join physical operator with support for left, right and inner joins.",", "
"   Extract Superclass,Pull Up Method,Move Method,Extract Method,","Implement filter pushdown for Parquet The parquet reader currently supports project pushdown, for limiting the number of columns read, however it does not use filter pushdown to read a subset of the requested columns. This is particularly useful with parquet files that contain statistics, most importantly min and max values on pages. Evaluating predicates against these values could save some major reading and decoding time.

The largest barrier to implementing this is the current design of the reader. Firstly, we currently have two separate parquet readers, one for reading flat files very quickly and another or reading complex data. There are enhancements we can make the the flat reader, to make it support nested data in a much more efficient manner. However the speed of the flat file reader currently comes from being able to make vectorized copies out the the parquet file. This design is somewhat at odds with filter pushdown, as we will only can make useful vectorized copies if the filter matches a large run of values within the file. This might not be too rare a case, assuming files are often somewhat sorted on a primary field like date or a numeric key, and these are often fields used to limit the query to a subset of the data. However for cases where we are filter out a few records here and there, we should just make individual copies.

We need to do more design work on the best way to balance performance with these use cases in mind.","Duplicated Code, Long Method, , , Duplicated Code, Large Class, Duplicated Code, "
"   Rename Method,","Implement filter pushdown for Parquet The parquet reader currently supports project pushdown, for limiting the number of columns read, however it does not use filter pushdown to read a subset of the requested columns. This is particularly useful with parquet files that contain statistics, most importantly min and max values on pages. Evaluating predicates against these values could save some major reading and decoding time.

The largest barrier to implementing this is the current design of the reader. Firstly, we currently have two separate parquet readers, one for reading flat files very quickly and another or reading complex data. There are enhancements we can make the the flat reader, to make it support nested data in a much more efficient manner. However the speed of the flat file reader currently comes from being able to make vectorized copies out the the parquet file. This design is somewhat at odds with filter pushdown, as we will only can make useful vectorized copies if the filter matches a large run of values within the file. This might not be too rare a case, assuming files are often somewhat sorted on a primary field like date or a numeric key, and these are often fields used to limit the query to a subset of the data. However for cases where we are filter out a few records here and there, we should just make individual copies.

We need to do more design work on the best way to balance performance with these use cases in mind.",", "
"   Rename Method,","Add peak memory allocation in a operator to OperatorStats. Currently we have ""localMemoryAllocated"" which is always set to zero as we try to fill the stats at the end of fragment execution by calling allocator.getAllocatedMemory() which at that point has already released the allocated memory. Instead if we have a stat for peak memory an allocator has seen in the lifetime of the operator execution will be useful (each operator has its own allocator). 

Example query on query profile: To find aggregate of peak memory of each operator across all minor fragments in a major fragment and list them in descending order of peak memory usage
{code:sql}
SELECT
majorFragmentId,
opProfile['operatorType'] opType,
sum(opProfile['peakLocalMemoryAllocated']) aggPeakMemoryAcrossAllMinorFragments 
FROM 
(SELECT
majorFragmentId,
flatten(minorFragProfile['operatorProfile']) opProfile
FROM 
(SELECT
majorFragment['majorFragmentId'] majorFragmentId, 
flatten(majorFragment['minorFragmentProfile']) minorFragProfile
FROM
(SELECT flatten(fragmentProfile) as majorFragment from dfs.`/tmp/a.json`)
)
)
-- WHERE opProfile['operatorType'] = 6 -- If want to filter to particular operator
GROUP BY 
majorFragmentId,
opProfile['operatorType']
ORDER BY
aggPeakMemoryAcrossAllMinorFragments DESC;
{code}

{code}
+-----------------+------------+--------------------------------------+
| majorFragmentId | opType | aggPeakMemoryAcrossAllMinorFragments |
+-----------------+------------+--------------------------------------+
| 1 | 4 | 115065856 |
| 1 | 3 | 10027008 |
| 0 | 3 | 1671168 |
| 3 | 6 | 1536000 |
| 2 | 6 | 901120 |
| 1 | 6 | 606208 |
| 3 | 28 | 393216 |
| 2 | 28 | 229376 |
| 3 | 10 | 122880 |
| 2 | 10 | 81920 |
| 0 | 11 | 0 |
| 0 | 10 | 0 |
| 0 | 13 | 0 |
| 1 | 10 | 0 |
| 1 | 11 | 0 |
+-----------------+------------+--------------------------------------+
{code}",", "
"   Move And Rename Class,","Support Basic parallelization insertion with SQL aggregate plan Add additional hash functions and xor to support hash combinations. 

Update BasicOptimizer to support parallel SQL aggregation plans when there is no upstream user requested sorts.",", "
"   Move Method,Extract Method,","Add IO wait time stats for Parquet and Json input files Currently time spent in IO read time is included as part of the total processing time. This JIRA is to measure the IO read time separately and add it to OperatorStats. 

Implementation details:
Add a FileSystem implementation called DrillFileSystem which takes an existing FileSystem instance and OperatorStats. Whenever a file is opened using DrillFileSystem, it returns an instance of DrillFSDataInputStream which is a facade to actual FSDataInputStream. DrillFSDataInputStream adds the IO read time stats whenever a read request is issued. 

IO Stats work only when DrillFileSystem is used. This patch modified JSON and Parquet readers to use DrillFileSystem. Text reader is not included.","Duplicated Code, Long Method, , , "
"   Move Class,Move And Rename Class,Rename Method,Move Method,","Split JDBC implementation out of org.apache.drill.jdbc, so that pkg. is place for doc. The JDBC implementation classes and interfaces that are not part of Drill's published JDBC interface should be moved out of package org.apache.drill.jdbc.

This will support using Javadoc to produce end-user documentation of Drill-specific JDBC API behavior (e.g., what's implemented or not, plus any extensions), and keep clear what is part of Drill's published JDBC interface vs. what is not (i.e., items that are technically accessible (public or protected) but _not_ meant to be used by Drill users).

Parts:

1. Move most classes and packages in {{org.apache.drill.jdbc}} (e.g., {{DrillHandler}}, {{DrillConnectionImpl}}) to an implementation package (e.g., {{org.apache.drill.jdbc.impl}}).

2. Split the current {{org.apache.drill.jdbc.Driver}} into a published-interface portion still at {{org.apache.drill.jdbc.Driver}} plus an implementation portion at {{org.apache.drill.jdbc.impl.DriverImpl}}.

({{org.apache.drill.jdbc.Driver}} would expose only the published interface (e.g., its constructor and methods from {{java.sql.Driver}}). 

{{org.apache.drill.jdbc.impl.DriverImpl}} would contain methods that are not part of Drill's published JDBC interface (including methods that need to be public or protected because of using Avatica but which shouldn't be used by Drill users).)

3. As needed (for Drill extensions and for documentation), create Drill-specific interfaces extending standard JDBC interfaces.

For example, to create a place for documenting Drill-specific behavior of methods defined in {{java.sql.Connection}}, create an interface, e.g., {{org.apache.drill.jdbc.DrillConnection}}, that extends interface {{java.sql.Connection}}, adjust the internal implementation class in {{org.apache.drill.jdbc.impl}} to implement that Drill-specified interface rather than directly implementing {{java.sql.Connection}}, and then add a method declaration with the Drill-specific documentation to the Drill-specific subinterface.

4. In Drill-specific interfaces created per part 3, _consider_ using co-variant return types to narrow return types to the Drill-specific interfaces.

For example: {{java.sql.Connection}}'s {{createStatement()}} method returns type {{java.sql.Statement}}. Drill's implementation of that method will always return a Drill-specific implementation of {{java.sql.Statement}}, which will also be an implementation of the Drill-specific interface that extends {{java.sql.Statement}}. Therefore, the Drill-specific {{Connection}} interface can re-declare {{createStatement()}} as returning the Drill-specific {{Statement}} interface type (because the Drill-specific {{Statement}} type is a subtype of {{java.sql.Statement}}).

That would likely make it easier for client code to access any Drill extension methods: Although the client might have to cast or do something else special to get to the first Drill-specific interface or class, it could traverse to other objects (e.g., from connection to statement, from statement to result set, etc.) still using Drill-specific types, not needing casts or whatever as each step.

Note: Steps 1 and 2 have already been prototyped.
",", , "
"   Move Class,Move Method,Move Attribute,",Improve profile page in web ui to show additional memory stats and fragment status 0,", , , "
"   Rename Method,","Make JDBC connection honor a configuration option for disabling embedded web server Current implementation of Drill's jdbc connection does not honor nor handle parameters passed from jdbc connection string(except the local mode flag). The proposal is to make the implementation recognize a bag of configuration options passed by user(specifically the one for disabling the embedded web server). 

A typical use case for this feature would be in unit tests. Some jdbc unit tests instantiates an individual DrillBit per test case in local mode that in turn starts an embedded web server. This prevents us from parallelizing the tests due to the fact that port assigned for the web server is currently in use by the former bit. This also impairs the test runtime as starting the web server takes from 1-2 seconds/test case.",", "
"   Extract Interface,Rename Method,Extract Method,","Build a sampling range partitioner Create a new operator that caches a number of record batches and then coordinates across the cluster on the distribution of partitioning keys to try to determine a reasonable set of range partitions. The outgoing stream should include a partition key that is equal to the width of the receiving fragment.

- histogram or similar should be held in the distributed cache
- need to figure out the logic for how long to wait before the partitioning estimate is good enough. 
- need to update the partitioning sender so that we can drop the partitioning column rather than sending it onward.


","Duplicated Code, Long Method, , Large Class, "
"   Rename Class,Rename Method,Extract Method,","Build a sampling range partitioner Create a new operator that caches a number of record batches and then coordinates across the cluster on the distribution of partitioning keys to try to determine a reasonable set of range partitions. The outgoing stream should include a partition key that is equal to the width of the receiving fragment.

- histogram or similar should be held in the distributed cache
- need to figure out the logic for how long to wait before the partitioning estimate is good enough. 
- need to update the partitioning sender so that we can drop the partitioning column rather than sending it onward.


","Duplicated Code, Long Method, , "
"   Rename Method,Move Method,Extract Method,Move Attribute,","add exception and pause injections for testing drillbit stability Use the exception injection mechanism to add exception injections to test a variety of distributed failure scenarios.

Here are some scenarios we've worked out before:
1. Cancellation:
TC1: cancel before any result set is returned
TC2: cancel in the middle of fetching result set
TC3: cancel after all result set are produced but not all are fetched
TC4: cancel after everything is completed and fetched

As test setup, we need:
- query dataset large enough to be sent to different drillbits, e.g., TPCH 100
- queries that force multiple drillbits to work on them; e.g., count ... group by

2. Completed (in each case check all drillbits are still up and running):
TC1: success
TC2: failed query - before query is executed - while sql parsing
TC3: failed query - before query is executed - while sending fragments to other drillbits for execution
TC4: failed query - during query execution

It is currently not possible to create a scenario in which a query may hang.

To check all drillbits up and running and in a clean state, run:

-select count(*) from sys.drillbits;-
{code}
select count(*) from sys.memory;
{code}","Duplicated Code, Long Method, , , , "
"   Rename Method,Extract Method,","Current method of combining hash values can produce skew The current method of combining hash values of multiple columns can produce skew in some cases even though each individual hash function does not produce skew. The combining function is XOR: 
{code}
hash(a, b) = XOR (hash(a), hash(b))
{code}
The above result will be 0 for all rows where a = b, so hash(a) = hash(b). This will clearly create severe skew and affects the performance of queries that do HashAggregate based group-by on {a, b} or a HashJoin .on both columns.","Duplicated Code, Long Method, , "
"   Extract Method,Move Attribute,","Fix expression interpreter to allow executing expressions at planning time The expression interpreter currently available in Drill cannot be used at planning time, as it does not have a means to connect to the direct memory allocator stored at the DrillbitContext level. To implement new rules based on evaluating expressions on constants, or small datasets, such as partition information this limitation must be addressed.","Duplicated Code, Long Method, , , "
"   Move Class,Rename Class,","Fix expression interpreter to allow executing expressions at planning time The expression interpreter currently available in Drill cannot be used at planning time, as it does not have a means to connect to the direct memory allocator stored at the DrillbitContext level. To implement new rules based on evaluating expressions on constants, or small datasets, such as partition information this limitation must be addressed.",", "
"   Rename Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Separate QueryResult into two messages QueryResult and QueryData Currently, both the Foreman and the ScreenRoot use QueryResult as part of a QueryWritableBatch to send data and/or query status. We should split this into two separate messages one for the data that will be sent from the ScreenRoot and another for the queryStatus that will be sent from the Foreman","Duplicated Code, Long Method, , , , , "
"   Rename Method,","Implement Diagnostic Operator Support inserting a diagnostic physical operator into a physical plan that records the record batches output of a preceding operator. Additionally, provide a tool to view this data.",", "
"   Move Method,Move Attribute,","Proposal for CAST mechanism *Proposal for implementing a CAST mechanism in Drill.* 
===============================================

The casting mechanism would be of two types
- Implicit type casting
- Explicit type casting

*Details:*

* The Implicit type cast would take care of the casting of the lower datatype holders to the higher datatype holders automatically. 
bq. Eg. IntHolder would be casted to Float4Holder/Float8Holder directly.

* The explicit type casting would enable the user to use a CAST() function to cast some value to another datatype by specifying the type. The cast function would be a function exposed with syntax similar to standard SQL format. 
bq. Eg. SELECT CAST (somevalue AS INT) FROM sometable;

*Type conversion rules:*
Conversion rules have to be similar to SQL standards.

_Implicit type conversion:_
* For arithmetic & comparison operators (+, -, *, /, <, >, =, etc) - 
** If both operands types are different, Strings would be converted to Double, and then both the operands would be converted to the same type by choosing the type with higher precision.
* For values passed to a Function/UDF - 
** The values would be converted to the parameter accepted by the Function. 
** In case of multiple overloaded functions are present, the function with least number of conversions would be selected.
** In case there are multiple functions with least number of conversions, there would be an error returned to user for ambiguous function.

_Explicit Type Conversion_
* User would use the CAST Function for converting types to another specified type.
* For nonconvertible types user gets an error back.",", , , "
"   Rename Class,Rename Method,Move Method,Extract Method,Inline Method,",Support Large in list 0,"Duplicated Code, Long Method, , , , "
"   Rename Class,Rename Method,Move Method,Extract Method,Move Attribute,","Pause injections should pause indefinitely until signalled Currently injected pauses make threads sleep for a specified time. This can be an enhanced to stop the thread indefinitely using a CountDownLatch. It is quite similar to how cancellation works. 

Tasks: 
(a) Add another message to RPC layer to signal paused remote threads to resume (through ControlHandler) by counting down. Complications if the thread has not reached the pause site yet.
(b) Add resume signal (like ctrl-c) to sqlline 
(further enhancement: another signal to trigger pause from sqlline)
","Duplicated Code, Long Method, , , , "
"   Move Class,Extract Method,","Handle counting and status of sent batches by FragmentContext The tracking of sent batches is currently done by the senders, and much of the code is duplicated and unnecessary. We want to move the tracking of sent batches and the handling of their status to the FragmentContext.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Make dfs_test.tmp schema location on local fs exclusive to test JVM fork. Currently {{dfs_test.tmp}} workspace location on local filesystem is hardcoded to {{/tmp/drilltest}}. If a test creates a view or new table, it is created under this location. Problem is when two or more test forks are running in parallel, sharing the {{dfs_test.tmp}} workspace location causes synchronization issues.

For example: {{TestViewSupport#view1}} creates a view {{testView1}} in workspace {{dfs_test.tmp}}. This causes a new view file under {{/tmp/drilltest}} directory. At this point parallel running test {{TestInfoSchema#showTables}} makes a call to list tables in {{dfs_test.tmp}} workspace. Show tables returns {{testView1}} as one of the tables in {{dfs_test.tmp}} workspace which it is not expecting causing the {{TestInfoSchema#showTables}} to fail. 

Proposed solution is:
When setting up Drill test cluster in {{BaseTestQuery}} (root class for most tests), modify workspace {{dfs_test.tmp}} location to point to a temp directory created using {{Files.createTempDir}}. If two or more processes call {{Files.createTempDir}} at the same time each one guarateed to get a exclusive directory as long as there are no more than 1000 calls per millisecond.

For JDBC, we rely on property {{drillJDBCUnitTests}} in connection properties to setup exclusive directory for {{dfs_test.tmp}} workspace.","Duplicated Code, Long Method, , "
"   Rename Method,","Use PojoRecordReader for all system tables The current implementation uses SystemRecordReader to populate SystemRecords, which is not required. PojoRecordReader can be used to accomplish the same task.",", "
"   Rename Method,Extract Method,","Add simplified activity log Create a simple log that tracks time submitted, time completed, user, query, queryid and outcome.","Duplicated Code, Long Method, , "
"   Rename Method,",TextReader should support multibyte line delimiters lineDelimiter in the TextFormatConfig doesn't support \r\n for record delimiters.,", "
"   Extract Method,Push Down Attribute,",Reading only select columns from a parquet file 0,"Duplicated Code, Long Method, , , "
"   Move Method,Move Attribute,","Apache Drill JDBC storage plugin to query rdbms systems such as MySQL and Netezza from Apache Drill I have developed the base code for a JDBC storage-plugin for Apache Drill. The code is primitive but consitutes a good starting point for further coding. Today it provides primitive support for SELECT against RDBMS with JDBC. 

The goal is to provide complete SELECT support against RDBMS with push down capabilities.

Currently the code is using standard JDBC classes.",", , , "
"   Rename Class,Rename Method,Move Method,Extract Method,Move Attribute,","Add Window functions: ROW_NUMBER, RANK, PERCENT_RANK, DENSE_RANK and CUME_DIST add support for the following window functions:
- ROW_NUMBER
- RANK
- DENSE_RANK
- PERCENT_RANK
- CUME_DIST","Duplicated Code, Long Method, , , , "
"   Rename Method,Extract Method,","Enhance RPC layer to offload all request work onto a separate thread. Right now, the app is responsible for ensuring that very small amounts of work are done on the RPC thread. In some cases, the app doesn't do this correctly. Additionally, in high load situations these small amounts of work become no trivial. As such, we need to make RPC layer protect itself from slow requests/responses.","Duplicated Code, Long Method, , "
"   Move Method,Move Attribute,",UserExceptions should be logged from the right class Currently system errors are logged using the UserException#logger. Classes that throws UserException should delegate logging to UserException#build method.,", , , "
"   Rename Method,","Add named metrics and named operators in OperatorProfile + Useful when reading JSON query profile.
+ Rename FragmentStats#getOperatorStats to FragmentStats#newOperatorStats",", "
"   Rename Class,Rename Method,","Add named metrics and named operators in OperatorProfile + Useful when reading JSON query profile.
+ Rename FragmentStats#getOperatorStats to FragmentStats#newOperatorStats",", "
"   Rename Method,Inline Method,","Move OperatorWrapper list and FragmentWrapper list creation to ProfileWrapper ctor + avoid re-computation in some cases
+ consistent comparator names",", , "
"   Rename Method,Extract Method,Inline Method,","Extend off heap memory manager to support growing vectors Add the following interfaces to the bufferl memory manager:
ptr = alloc(size, min, max) - allocate a block with given size, but
with a total capacity between min+max.
trim(ptr) - free up extra capacity, so current size is the capacity

Rationale: It isn't always possible to anticipate the size of a vector. When creating a new vector, one strategy is to over-allocate the vector and then trim the size once the vector is complete. These routines allow us to implement this strategy.
","Duplicated Code, Long Method, , , "
"   Rename Class,Rename Method,Inline Method,Move Attribute,","Rename NonRootStatusReporter to FragmentStatusReporter After DRILL-3072, there is no need for the StatusReporter interface or AbstractStatusReporter because there is only one implementation: NonRootStatusReporter. This should be renamed to FragmentStatusReporter because root and non-root fragments use this to report status to the Foreman.",", , , "
"   Rename Method,Extract Method,","Add implicit file columns support I could not find another ticket which talks about this ...

The file name should be a column which can be selected or filtered when querying a directory just like dir0, dir1 are available.","Duplicated Code, Long Method, , "
"   Move And Rename Class,","Bson Record Reader for Mongo storage plugin Improve the mongo query performance.
We are considering the suggestions provided by [~dragoncurve] and [~hgunes] in drill mailing chain.",", "
"   Rename Method,Extract Method,","Add support for encoding of Drill data types into byte ordered format The following JIRA added this functionality in HBase: https://issues.apache.org/jira/browse/HBASE-8201

We need to port this functionality in Drill so as to allow filtering and pruning of rows during scans.","Duplicated Code, Long Method, , "
"   Rename Method,","Add ANSI_QUOTES option so that Drill's SQL Parser will recognize ANSI_SQL identifiers  *Added a possibility of changing characters for quoting identifiers by setting QUOTING_IDENTIFIERS system/session option:*
{code}planner.parser.quoting_identifiers{code}
There are three modes for quoting identifiers:
1. ""BACK TICKS"" (default quoting mode):
Unicode U+0060; ""GRAVE ACCENT"" {code}`{code}
The character is used for setting system/session option and for quoting identifiers;
2. ""DOUBLE QUOTES"" Unicode U+0022; 'QUOTATION MARK' 
{code}""{code}
The character is used for setting system/session option and for quoting identifiers;
3. ""BRACKETS"" 
Unicode U+005B; 'LEFT SQUARE BRACKET' 
{code}[{code}
The character is used for setting system/session option and for quoting identifiers as left quote character. The right quote character for quoting identifiers with this mode is Unicode U+005D; 'RIGHT SQUARE BRACKET'
{code}]{code}
Examples of using QUOTING_IDENTIFIERS option:
{code}
0: jdbc:drill:zk=local> select * from sys.options where name = 'planner.parser.quoting_identifiers';
+-------------------------------------+---------+---------+----------+----------+-------------+-----------+------------+
| name | kind | type | status | num_val | string_val | bool_val | float_val |
+-------------------------------------+---------+---------+----------+----------+-------------+-----------+------------+
| planner.parser.quoting_identifiers | STRING | SYSTEM | DEFAULT | null | ` | null | null |
+-------------------------------------+---------+---------+----------+----------+-------------+-----------+------------+
1 row selected (0.189 seconds)
0: jdbc:drill:zk=local> select `employee_id`, `full_name` from cp.`employee.json` limit 1;
+--------------+---------------+
| employee_id | full_name |
+--------------+---------------+
| 1 | Sheri Nowmer |
+--------------+---------------+
1 row selected (0.148 seconds)
0: jdbc:drill:zk=local> ALTER SESSION SET planner.parser.quoting_identifiers = '""';
+-------+----------------------------------------------+
| ok | summary |
+-------+----------------------------------------------+
| true | planner.parser.quoting_identifiers updated. |
+-------+----------------------------------------------+
1 row selected (0.107 seconds)
0: jdbc:drill:zk=local> select ""employee_id"", ""full_name"" from cp.""employee.json"" limit 1;
+--------------+---------------+
| employee_id | full_name |
+--------------+---------------+
| 1 | Sheri Nowmer |
+--------------+---------------+
1 row selected (0.129 seconds)
0: jdbc:drill:zk=local> ALTER SESSION SET planner.parser.quoting_identifiers = '[';
+-------+----------------------------------------------+
| ok | summary |
+-------+----------------------------------------------+
| true | planner.parser.quoting_identifiers updated. |
+-------+----------------------------------------------+
1 row selected (0.102 seconds)
0: jdbc:drill:zk=local> select [employee_id], [full_name] from cp.[employee.json] limit 1;
+--------------+---------------+
| employee_id | full_name |
+--------------+---------------+
| 1 | Sheri Nowmer |
+--------------+---------------+
1 row selected (0.14 seconds)
0: jdbc:drill:zk=local> ALTER SESSION SET planner.parser.quoting_identifiers = '`';
+-------+----------------------------------------------+
| ok | summary |
+-------+----------------------------------------------+
| true | planner.parser.quoting_identifiers updated. |
+-------+----------------------------------------------+
1 row selected (0.1 seconds)
0: jdbc:drill:zk=local> select `employee_id`, `full_name` from cp.`employee.json` limit 1;
+--------------+---------------+
| employee_id | full_name |
+--------------+---------------+
| 1 | Sheri Nowmer |
+--------------+---------------+
1 row selected (0.139 seconds)
{code}

*Other quoting characters are not acceptable while particular one is chosen:*
{code}
0: jdbc:drill:zk=local> ALTER SESSION SET planner.parser.quoting_identifiers = '""';
+-------+----------------------------------------------+
| ok | summary |
+-------+----------------------------------------------+
| true | planner.parser.quoting_identifiers updated. |
+-------+----------------------------------------------+
1 row selected (0.561 seconds)
0: jdbc:drill:zk=local> select `employee_id`, `full_name` from cp.`employee.json` limit 1;
Error: PARSE ERROR: Lexical error at line 1, column 8. Encountered: ""`"" (96), after : """"

SQL Query select `employee_id`, `full_name` from cp.`employee.json` limit 1
^
[Error Id: 9bfcb6b7-7d9d-46d7-8ea0-78d1d88b5c3b on vitalii-pc:31010] (state=,code=0)
{code}

*There is a possibility of setting QUOTING_IDENTIFIERS by using the ""quoting_identifiers"" property in the jdbc connection URL string.* 
For example:
{code}
jdbc:drill:zk=local;quoting_identifiers=[
{code}



",", "
"   Move Class,Move Method,Extract Method,Move Attribute,","Add support for inner classes in code generator In some situations, it would be helpful to support doing code generation of inner classes. Expand the CodeGenerator and surrounding classes to support this case.","Duplicated Code, Long Method, , , , "
"   Rename Method,","Drop table support Umbrella JIRA to track support for ""Drop table"" feature.",", "
"   Move Method,Extract Method,","Add support for LEAD, LAG, NTILE, FIRST_VALUE and LAST_VALUE window functions This JIRA will track the progress on the following window functions (no particular order):
- LEAD
- LAG
- NTILE
- FIRST_VALUE
- LAST_VALUE","Duplicated Code, Long Method, , , "
"   Move Class,Move Method,Extract Method,Move Attribute,","Improve classpath scanning to reduce the time it takes classpath scanning and function registry take a long time (seconds every time).
We'd want to avoid loading the classes (use bytecode inspection instead) and have a build time cache to avoid doing the scanning at startup.","Duplicated Code, Long Method, , , , "
"   Rename Method,Extract Method,",Rewrite MergeJoinBatch using record batch iterator. Current implementation of merge join operator is convoluted as it has to handle duplicates between record batches. MergeJoin rewrite should use record batch iterator which hides the complexity of managing multiple record batches and iterating through them.,"Duplicated Code, Long Method, , "
"   Extract Method,Move Attribute,","Implement External Sort operator This operator will allow sorting data that is larger than the size of memory by spilling to disk when necessary.

Also as part of this jira, I will implement a new merge sort algorithm that will hopefully better utilize cluster resources than our current sort, which is based on Quicksort. The problem with quicksort is that we can't do any sorting until all of the batches have arrived. But this will often result in very low CPU utilization while the data is read from disk, followed by a period of very high CPU usage.

The external sort will include sorting each batch individually when it arrives. In the case where no spills occur, it makes sense to take advantage of the fact that the batches are already sorted, but doing the an N-way merge, as is done when there are spills, is not the most effective way to do this, (according to some tests I have done). What will be done in this case, rather than an N-way merge using a heap, we will do a variation of natural merge sort, which amounts to essentially a staged, 2-way merge of the incoming (sorted) batches.","Duplicated Code, Long Method, , , "
"   Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Hive UDFs in Drill For now the plan is to support only scalar hive functions (UDF and GenericUDF). Aggregate and table functions will come later.

Design document:
https://docs.google.com/document/d/19hd4fcQHO8gKTYeiXHykKdG1kowLhb1811Hl1-c4u8E/edit?usp=sharing

Review:
https://reviews.apache.org/r/18372/diff/?page=1","Duplicated Code, Long Method, , , , , "
"   Pull Up Method,Extract Method,","Read raw key value bytes from sequence files Sequence files store list of key-value pairs. Keys/values are of type hadoop writable.
Provide a format plugin that reads raw bytes out of sequence files which can be further deserialized by a udf(from hadoop writable -> drill type)","Duplicated Code, Long Method, , Duplicated Code, "
"   Rename Method,Extract Method,","Select with options Add a mechanism to pass parameters down to the StoragePlugin when writing a Select statement.
Some discussion here:
http://mail-archives.apache.org/mod_mbox/drill-dev/201510.mbox/%3CCAO%2Bvc4AcGK3%2B3QYvQV1-xPPdpG3Tc%2BfG%3D0xDGEUPrhd6ktHv5Q%40mail.gmail.com%3E
http://mail-archives.apache.org/mod_mbox/drill-dev/201511.mbox/%3CCAO+vc4CLzyLvJEviSFJQtcyxb-ZSmFy4bQrM-jHBiDWzGqFVJg@mail.gmail.com%3E","Duplicated Code, Long Method, , "
"   Rename Method,Push Down Method,Push Down Attribute,","Reduce metadata cache file size The parquet metadata cache file has fair amount of redundant metadata that causes the size of the cache file to bloat. Two things that we can reduce are :
1) Schema is repeated for every row group. We can keep a merged schema (similar to what was discussed for insert into functionality) 2) The max and min value in the stats are used for partition pruning when the values are the same. We can keep the maxValue only and that too only if it is the same as the minValue.
",", , , "
"   Extract Method,Inline Method,",Make all uses of AutoCloseables use addSuppressed exceptions to avoid noise in logs 0,"Duplicated Code, Long Method, , , "
"   Move Method,Extract Method,","Ability to submit simple type of physical plan directly to EndPoint DrillBit for execution Today Drill Query execution is optimistic and stateful (at least due to data exchanges) - if any of the stages of query execution fails whole query fails. If query is just simple scan, filter push down and project where no data exchange happens between DrillBits there is no need to fail whole query when one DrillBit fails, as minor fragments running on that DrillBit can be rerun on the other DrillBit. There are probably multiple ways to achieve this. This JIRA is to open discussion on: 
1. agreement that we need to support above use case 
2. means of achieving it.","Duplicated Code, Long Method, , , "
"   Extract Method,Pull Up Attribute,","UNION ALL involving empty directory on any side of union all results in Failed query UNION ALL query that involves an empty directory on either side of UNION ALL operator results in FAILED query. We should return the results for the non-empty side (input) of UNION ALL. 
Note that empty_DIR is an empty directory, the directory exists, but it has no files in it. 

Drill 1.4 git.commit.id=b9068117 
4 node cluster on CentOS 
{code:java} 
0: jdbc:drill:schema=dfs.tmp> select columns[0] from empty_DIR UNION ALL select cast(columns[0] as int) c1 from `testWindow.csv`; 
Error: VALIDATION ERROR: From line 1, column 24 to line 1, column 32: Table 'empty_DIR' not found 


[Error Id: 5c024786-6703-4107-8a4a-16c96097be08 on centos-01.qa.lab:31010] (state=,code=0) 

0: jdbc:drill:schema=dfs.tmp> select cast(columns[0] as int) c1 from `testWindow.csv` UNION ALL select columns[0] from empty_DIR; 
Error: VALIDATION ERROR: From line 1, column 90 to line 1, column 98: Table 'empty_DIR' not found 


[Error Id: 58c98bc4-99df-425c-aa07-c8c5faec4748 on centos-01.qa.lab:31010] (state=,code=0) 
{code} 
*Solution overview:* 
After resolving the current issue Drill can query an empty directory. It is a schemaless Drill table for now. 
User can query empty directory and use it for queries with any JOIN and UNION (UNION ALL) operators. 
Empty directory with parquet metadata cache files is schemaless Drill table as well. 
It works similar to empty files: 
- The query with star will return empty result. 
- If some fields are indicated in select statement, that fields will be returned as INT-OPTIONAL types. 
- The empty directory in the query with UNION operator will not change the result as if the statement with UNION is absent in the query. 
- The query with joins will return an empty result except the cases of using outer join clauses, when the outer table for ""right join"" or derived table for ""left join"" has a data. In that case the data from a non-empty table is returned. 
- The empty directory table can be used in complex queries. 

*Code changes:* 
Internally empty directory interprets as DynamicDrillTable with null selection. SchemalessScan, SchemalessBatchCreator and SchemalessBatch are introduced and used on execution state for interactions with other operators and batches. 
If empty directory contain parquet metadata cache files, the ParquetGroupScan for such table is not valid and SchemalessScan is used instead of that.","Duplicated Code, Long Method, , Duplicated Code, "
"   Rename Method,",Enhance StoragePlugin interface to expose logical space rules for planning purpose Currently StoragePlugins can only expose rules that are executed in physical space. Add an interface method to StoragePlugin to expose logical space rules to planner.,", "
"   Move Method,Move Attribute,",Add Support for HBase 1.X Is there any Road map to upgrade the Hbase version to 1.x series. Currently drill supports Hbase 0.98 version.,", , , "
"   Rename Method,",Add Experimental Kudu plugin Merge the work done here into Drill master so others can utilize the plugin: https://github.com/dremio/drill-storage-kudu,", "
"   Move Method,Extract Method,","Adding support for some custom window frames Current implementation of window functions (<1.6) only supports the default frame. We want to add support for the FRAME clause. 

This is an umbrella task to track the progress while adding all remaining frames.
","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,Inline Method,","Allow field names to include dots If you have some json data like this...
{code:javascript}
{
""0.0.1"":{
""version"":""0.0.1"",
""date_created"":""2014-03-15""
},
""0.1.2"":{
""version"":""0.1.2"",
""date_created"":""2014-05-21""
}
}
{code}
... there is no way to select any of the rows since their identifiers contain dots and when trying to select them, Drill throws the following error:

Error: SYSTEM ERROR: UnsupportedOperationException: Unhandled field reference ""0.0.1""; a field reference identifier must not have the form of a qualified name

This must be fixed since there are many json data files containing dots in some of the keys (e.g. when specifying version numbers etc)
","Duplicated Code, Long Method, , , "
"   Move Class,Rename Class,Push Down Method,Push Down Attribute,","Kerberos Authentication Drill should support Kerberos based authentication from clients. This means that both the ODBC and JDBC drivers as well as the web/REST interfaces should support inbound Kerberos. For Web this would most likely be SPNEGO while for ODBC and JDBC this will be more generic Kerberos.

Since Hive and much of Hadoop supports Kerberos there is a potential for a lot of reuse of ideas if not implementation.

Note that this is related to but not the same as https://issues.apache.org/jira/browse/DRILL-3584",", , , "
"   Move Method,Move Attribute,","Kerberos Authentication Drill should support Kerberos based authentication from clients. This means that both the ODBC and JDBC drivers as well as the web/REST interfaces should support inbound Kerberos. For Web this would most likely be SPNEGO while for ODBC and JDBC this will be more generic Kerberos.

Since Hive and much of Hadoop supports Kerberos there is a potential for a lot of reuse of ideas if not implementation.

Note that this is related to but not the same as https://issues.apache.org/jira/browse/DRILL-3584",", , , "
"   Move Class,Extract Method,","Kerberos Authentication Drill should support Kerberos based authentication from clients. This means that both the ODBC and JDBC drivers as well as the web/REST interfaces should support inbound Kerberos. For Web this would most likely be SPNEGO while for ODBC and JDBC this will be more generic Kerberos.

Since Hive and much of Hadoop supports Kerberos there is a potential for a lot of reuse of ideas if not implementation.

Note that this is related to but not the same as https://issues.apache.org/jira/browse/DRILL-3584","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,Move Method,Extract Method,","Kerberos Authentication Drill should support Kerberos based authentication from clients. This means that both the ODBC and JDBC drivers as well as the web/REST interfaces should support inbound Kerberos. For Web this would most likely be SPNEGO while for ODBC and JDBC this will be more generic Kerberos.

Since Hive and much of Hadoop supports Kerberos there is a potential for a lot of reuse of ideas if not implementation.

Note that this is related to but not the same as https://issues.apache.org/jira/browse/DRILL-3584","Duplicated Code, Long Method, , , "
"   Rename Method,","Kerberos Authentication Drill should support Kerberos based authentication from clients. This means that both the ODBC and JDBC drivers as well as the web/REST interfaces should support inbound Kerberos. For Web this would most likely be SPNEGO while for ODBC and JDBC this will be more generic Kerberos.

Since Hive and much of Hadoop supports Kerberos there is a potential for a lot of reuse of ideas if not implementation.

Note that this is related to but not the same as https://issues.apache.org/jira/browse/DRILL-3584",", "
"   Rename Method,","Kerberos Authentication Drill should support Kerberos based authentication from clients. This means that both the ODBC and JDBC drivers as well as the web/REST interfaces should support inbound Kerberos. For Web this would most likely be SPNEGO while for ODBC and JDBC this will be more generic Kerberos.

Since Hive and much of Hadoop supports Kerberos there is a potential for a lot of reuse of ideas if not implementation.

Note that this is related to but not the same as https://issues.apache.org/jira/browse/DRILL-3584",", "
"   Rename Method,Move Method,Extract Method,Move Attribute,","Apache Drill should support network encryption - SASL encryption between Drill Client to Drillbit This is clearly related to Drill-291 but wanted to make explicit that this needs to include network level encryption and not just authentication. This is particularly important for the client connection to Drill which will often be sending passwords in the clear until there is encryption.

","Duplicated Code, Long Method, , , , "
"   Rename Method,","Drill and Hive have incompatible timestamp representations in parquet git.commit.id.abbrev=83d460c

I created a parquet file with a timestamp type using Drill. Now if I define a hive table on top of the parquet file and use ""timestamp"" as the column type, drill fails to read the hive table through the hive storage plugin

Implementation: 

Added int96 to timestamp converter for both parquet readers and controling it by system / session option ""store.parquet.int96_as_timestamp"".
The value of the option is false by default for the proper work of the old query scripts with the ""convert_from TIMESTAMP_IMPALA"" function.

When the option is true using of that function is unnesessary and can lead to the query fail.
",", "
"   Rename Class,Rename Method,","Implement text format plugin Implement a format plugin to read text files, e.g. csv, tsv. This will be able to read compressed and uncompressed data.

The record reader will return a single column of type RepeatedVarChar. Compression and delimiter type will be determined from the file extensions.",", "
"   Move Method,Move Attribute,","Improve current fragment parallelization module Current fragment parallelizer {{SimpleParallelizer.java}} can’t handle correctly the case where an operator has mandatory scheduling requirement for a set of DrillbitEndpoints and affinity for each DrillbitEndpoint (i.e how much portion of the total tasks to be scheduled on each DrillbitEndpoint). It assumes that scheduling requirements are soft (except one case where Mux and DeMux case where mandatory parallelization requirement of 1 unit). 

An example is:
Cluster has 3 nodes running Drillbits and storage service on each. Data for a table is only present at storage services in two nodes. So a GroupScan needs to be scheduled on these two nodes in order to read the data. Storage service doesn't support (or costly) reading data from remote node.

Inserting the mandatory scheduling requirements within existing SimpleParallelizer is not sufficient as you may end up with a plan that has a fragment with two GroupScans each having its own hard parallelization requirements.

Proposal is:
Add a property to each operator which tells what parallelization implementation to use. Most operators don't have any particular strategy (such as Project or Filter), they depend on incoming operator. Current existing operators which have requirements (all existing GroupScans) default to current parallelizer {{SimpleParallelizer}}. {{Screen}} defaults to new mandatory assignment parallelizer. It is possible that PhysicalPlan generated can have a fragment with operators having different parallelization strategies. In that case an exchange is inserted in between operators where a change in parallelization strategy is required.

Will send a detailed design doc.",", , , "
"   Rename Method,Extract Method,","Improve metadata cache performance for queries with single partition  Consider two types of queries which are run with Parquet metadata caching: 
{noformat}
query 1:
SELECT col FROM `A/B/C`;

query 2:
SELECT col FROM `A` WHERE dir0 = 'B' AND dir1 = 'C';
{noformat}

For a certain dataset, the query1 elapsed time is 1 sec whereas query2 elapsed time is 9 sec even though both are accessing the same amount of data. The user expectation is that they should perform roughly the same. The main difference comes from reading the bigger metadata cache file at the root level 'A' for query2 and then applying the partitioning filter. query1 reads a much smaller metadata cache file at the subdirectory level. 
","Duplicated Code, Long Method, , "
"   Rename Method,","Generate warning on Web UI if drillbits version mismatch is detected Display drillbit version on web UI. If any of drillbits version doesn't match with current drillbit, generate warning.
Screenshots - NEW_matching_drillbits.JPG, NEW_mismatching_drillbits.JPG",", "
"   Rename Method,","Expose New System Metrics + Add more metrics to the DrillMetrics registry (exposed through web UI and jconsole, through JMX): pending queries, running queries, completed queries, current memory usage (root allocator)
+ Clean up and document metric registration API
-+ Deprecate getMetrics() method in contextual objects; use DrillMetrics.getRegistry() directly-
+ Make JMX reporting and log reporting configurable through system properties (since config file is not meant to be used in common module)",", "
"   Move And Rename Class,Pull Up Method,Extract Method,","Dynamic UDFs support Allow register UDFs without restart of Drillbits.
Design is described in document below:

https://docs.google.com/document/d/1FfyJtWae5TLuyheHCfldYUpCdeIezR2RlNsrOTYyAB4/edit?usp=sharing 

Gist - https://gist.github.com/arina-ielchiieva/a1c4cfa3890145c5ecb1b70a39cbff55#file-dynamicudfssupport-md","Duplicated Code, Long Method, , Duplicated Code, "
"   Rename Method,","Improve metadata cache performance for queries with multiple partitions Consider queries of the following type run against Parquet data with metadata caching: 

{noformat}
SELECT col FROM `A` WHERE dir0 = 'B`' AND dir1 IN ('1', '2', '3')
{noformat}

For such queries, Drill will read the metadata cache file from the top level directory 'A', which is not very efficient since we are only interested in the files from some subdirectories of 'B'. DRILL-4530 improves the performance of such queries when the leaf level directory is a single partition. Here, there are 3 subpartitions due to the IN list. We can build upon the DRILL-4530 enhancement by at least reading the cache file from the immediate parent level `/A/B` instead of the top level. 

The goal of this JIRA is to improve performance for such types of queries.",", "
"   Rename Method,Extract Method,","Improve parquet reader performance Reported by a user in the field - 

We're generally getting read speeds of about 100-150 MB/s/node on PARQUET scan operator. This seems a little low given the number of drives on the node - 24. We're looking for options we can improve the performance of this operator as most of our queries are I/O bound. 

","Duplicated Code, Long Method, , "
"   Move And Rename Class,Rename Method,","Add support for Null Equality Joins Join with an equality condition which allows null=null fails. For example, if we use some of this queries:

{code:sql}
select ... FROM t1, t2 WHERE t1.c1 = t2.c2 OR (t1.c1 IS NULL AND t2.c2 IS NULL);

select ... FROM t1 INNER JOIN t2 ON t1.c1 = t2.c2 OR (t1.c1 IS NULL AND t2.c2 IS NULL);
{code}

we got ""UNSUPPORTED_OPERATION ERROR"". We should add support for this option.",", "
"   Rename Class,Extract Method,","Function that returns a unique id per session/connection similar to MySQL's CONNECTION_ID() Design and implement a function that returns a unique id per session/connection similar to MySQL's CONNECTION_ID().

*Implementation details*
function *session_id* will be added. Function returns current session unique id represented as string. Parameter {code:java} boolean isNiladic{code} will be added to UDF FunctionTemplate to indicate if a function is niladic (a function to be called without any parameters and parentheses)

Please note, this function will override columns that have the same name. Table alias should be used to retrieve column value from table.
Example:
{code:sql}select session_id from <table> // returns the value of niladic function session_id {code} 
{code:sql}select t1.session_id from <table> t1 // returns session_id column value from table {code}
","Duplicated Code, Long Method, , "
"   Move Class,Rename Method,Extract Method,Move Attribute,","Option to debug generated Java code using an IDE Drill makes extensive use of Java code generation to implement its operators. Drill uses sophisticated techniques to blend generated code with pre-compiled template code. An unfortunate side-effect of this behavior is that it is very difficult to visualize and debug the generated code.

As it turns out, Drill's code-merge facility is, in essence, a do-it-yourself version of subclassing. The Drill ""template"" is the parent class, the generated code is the subclass. But, rather than using plain-old subclassing, Drill combines the code from the two classes into a single ""artificial"" packet of byte codes for which no source exists.

Modify the code generation path to optionally allow ""plain-old Java"" compilation: the generated code is a subclass of the template. Compile the generated code as a plain-old Java class with no byte-code fix-up. Write the code to a known location that the IDE can search when looking for source files.

With this change, developers can turn on the above feature, set a breakpoint in a template, then step directly into the generated Java code called from the template.

This feature should be an option, enabled by developers when needed. The existing byte-code technique should be used for production code generation.","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Skip initializing all enabled storage plugins for every query In a query's lifecycle, at attempt is made to initialize each enabled storage plugin, while building the schema tree. This is done regardless of the actual plugins involved within a query. 

Sometimes, when one or more of the enabled storage plugins have issues - either due to misconfiguration or the underlying datasource being slow or being down, the overall query time taken increases drastically. Most likely due the attempt being made to register schemas from a faulty plugin.

For example, when a jdbc plugin is configured with SQL Server, and at one point the underlying SQL Server db goes down, any Drill query starting to execute at that point and beyond begin to slow down drastically. 

We must skip registering unrelated schemas (& workspaces) for a query.","Duplicated Code, Long Method, , "
"   Rename Method,Inline Method,","Skip initializing all enabled storage plugins for every query In a query's lifecycle, at attempt is made to initialize each enabled storage plugin, while building the schema tree. This is done regardless of the actual plugins involved within a query. 

Sometimes, when one or more of the enabled storage plugins have issues - either due to misconfiguration or the underlying datasource being slow or being down, the overall query time taken increases drastically. Most likely due the attempt being made to register schemas from a faulty plugin.

For example, when a jdbc plugin is configured with SQL Server, and at one point the underlying SQL Server db goes down, any Drill query starting to execute at that point and beyond begin to slow down drastically. 

We must skip registering unrelated schemas (& workspaces) for a query.",", , "
"   Rename Method,Extract Method,","Skip initializing all enabled storage plugins for every query In a query's lifecycle, at attempt is made to initialize each enabled storage plugin, while building the schema tree. This is done regardless of the actual plugins involved within a query. 

Sometimes, when one or more of the enabled storage plugins have issues - either due to misconfiguration or the underlying datasource being slow or being down, the overall query time taken increases drastically. Most likely due the attempt being made to register schemas from a faulty plugin.

For example, when a jdbc plugin is configured with SQL Server, and at one point the underlying SQL Server db goes down, any Drill query starting to execute at that point and beyond begin to slow down drastically. 

We must skip registering unrelated schemas (& workspaces) for a query.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,Move Attribute,","Enable generated code debugging in each Drill operator DRILL-5052 adds the ability to debug generated code. Some of the code generated by Drill's operators has minor problems when compiled directly using the new technique. These issues are ignore by the byte-code-merge technique uses in production. This ticket asks to try the DRILL-5052 feature in each operator, clean up any minor problems, and ensure each operator generates code suitable for debugging. Use the new {{CodeGenerator.plainOldJavaCapable()}} method to mark each generated class as ready for ""plain-old Java"" code gen.

The advantages of this feature are two:

1. Ability to step through the generated code to increase understanding of existing operators and to ease development of improvements to existing operators and of any new operators we choose to create.
2. Open the door to experimenting with how to improve performance of the generated code.","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,Move Attribute,","Provide simplified, unified ""cluster fixture"" for tests Drill provides a robust selection of test frameworks that have evolved to satisfy the needs of a variety of test cases. For newbies, however, the result is a bewildering array of ways to do basically the same thing: set up an embedded Drill cluster, run queries and check results.

Further, some key test settings are distributed: some are in the pom.xml file, some in config files stored as resources, some in hard-coded settings in base test classes.

Also, some test base classes helpfully set up a test cluster, but then individual tests need a different config, so they immediately tear down the default cluster and create a new one.

This ticket proposes a new test framework, available for new tests, that combines the best of the existing test frameworks into a single, easy-to-use package.

* Builder for the cluster
* Accept config-time options
* Accept run-time session and system options
* Specify number of Drillbits
* Simplified API for the most common options
* AutoCloseable for use in try-with-resources statements
* Integration with existing test builder classes

And so on.","Duplicated Code, Long Method, , , "
"   Move And Rename Class,Move Method,Extract Method,Move Attribute,","Allow ""extended"" mock tables access from SQL queries DRILL-5152 provided a simple way to generate sample data in SQL using a new, simplified version of the mock data generator. This approach is very convenient, but is inherently limited. For example, the limited syntax available in SQL does not encoding much information about columns such as repeat count, data generator or so on. The simple SQL approach does not allow generating multiple groups of data.

However, all these features are present in the original mock data source via a special JSON configuration file. Previously, only physical plans could access that extended syntax.

This ticket requests a SQL interface to the extended mock data source:

{code}
SELECT * FROM `mock`.`example/mock-options.json`
{code}

Mock data source options are always stored as a JSON file. Since the existing mock data generator for SQL never uses JSON files, a simple rule is that if the table name ends in "".json"" then it is a specification, else the information is encoded in table and column names.

The format of the data generation syntax is documented in the mock data source classes.","Duplicated Code, Long Method, , , , "
"   Rename Method,Move Method,Extract Method,Move Attribute,","Refinements to new ""Cluster Fixture"" test framework Roll-up of a number of enhancements to the cluster fixture framework.

* Config option to suppress printing of CSV and other output. (Allows printing for single tests, not printing when running from Maven.)
* Parsing of query profiles to extract plan and run time information.
* Fix bug in log fixture when enabling logging for a package.
* Improved ZK support.
* Set up the new CTTAS default temporary workspace for tests.
* Revise TestDrillbitResiliance to use the new framework.
* Revise TestWindowFrame to to use the new framework.
* Revise TestMergeJoinWithSchemaChanges to use the new framework.","Duplicated Code, Long Method, , , , "
"   Move And Rename Class,Rename Method,","Add server metadata API JDBC and ODBC clients exposes lots of metadata regarding server version and support of various parts of the SQL standard.

Currently the returned information is hardcoded in both clients/drivers which means that the infomation returned is support as of the client version, not the server version.

Instead, a new method should be provided to the clients to query the actual server support. Support on the client or the server should be optional (for example a client should not use this API if the server doesn't support it and fallback to default values).",", "
"   Extract Superclass,Extract Method,Inline Method,","Create a sub-operator test framework Drill provides two unit test frameworks for whole-server, SQL-based testing: the original {{BaseTestQuery}} and the newer {{ClusterFixture}}. Both use the {{TestBuilder}} mechanism to build system-level functional tests that run queries and check results.

Jason provided an operator-level test framework based, in part on mocks: 

As Drill operators become more complex, we have a crying need for true unit-level tests at a level below the whole system and below operators. That is, we need to test the individual pieces that, together, form the operator.

This umbrella ticket includes a number of tasks needed to create the sub-operator framework. Our intention is that, over time, as we find the need to revisit existing operators, or create new ones, we can employ the sub-operator test framework to exercise code at a finer granularity than is possible prior to this framework.","Duplicated Code, Long Method, , Duplicated Code, Large Class, , "
"   Rename Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Implement sub-operator unit tests for managed external sort Validate the proposed sub-operator test framework, by creating low-level unit tests for the managed version of the external sort.

The external sort has a small number of existing tests, but those tests are quite superficial; the ""managed sort"" project found many bugs. The managed sort itself was tested with ad-hoc system-level tests created using the new ""cluster fixture"" framework. But, again, such tests could not reach deep inside the sort code to exercise very specific conditions.

As a result, we spent far too much time using QA functional tests to identify specific code issues.

Using the sub-opeator unit test framework, we can instead test each bit of functionality at the unit test level.

If doing so works, and is practical, it can serve as a model for other operator testing projects.","Duplicated Code, Long Method, , , , , "
"   Rename Class,Extract Method,","Extend test framework profile parser printer for multi-fragment queries The recently added test framework has a tool called the {{ProfileParser}} which started as a tool for analyzing run times of single-fragment queries. Over time, it evolved to compare planned and actual cost for multi-fragment queries.

This ticket requests that multi-fagment support be added to the printing of run times.

If a query is single-thread, print the query as in the prior version:

{code}
Op: 0 Screen
Setup: 0 - 0%, 0%
Process: 35 - 0%, 0%
Wait: 16
Memory: 10
Op: 1 Project
Setup: 22 - 1%, 0%
Process: 41 - 0%, 0%
Memory: 5
...
{code}

If the query is multi-fragment and forms a tree, use the format used to display planning vs. actual info:

{code}
03-09 . . Project
Setup: 0 ms - 0%, 0%
Process: 0 ms - 0%, 0%
03-10 . . HashJoin (HASH JOIN)
Setup: 0 ms - 0%, 0%
Process: 5,097,619 ms - 326770%, 73%
03-12 . . . . Project
Setup: 36 ms - 2%, 0%
Process: 180 ms - 11%, 0%
{code}","Duplicated Code, Long Method, , "
"   Rename Method,Move Method,Extract Method,Move Attribute,","Refactor Parquet Record Reader The Parquet record reader class is a key part of Drill that has evolved over time to become somewhat hard to follow.

A number of us are working on Parquet-related tasks and find we have to spend an uncomfortable amount of time trying to understand the code. In particular, this writer needs to figure out how to convince the reader to provide higher-density record batches.

Rather than continue to decypher the complex code multiple times, this ticket requests to refactor the code to make it functionally identical, but structurally cleaner. The result will be faster time to value when working with this code.

This is a lower-priority change and will be coordinated with others working on this code base. This ticket is only for the record reader class itself; it does not include the various readers and writers that Parquet uses since another project is actively modifying those classes.","Duplicated Code, Long Method, , , , "
"   Pull Up Method,Move Method,Extract Method,Pull Up Attribute,Move Attribute,","Refactor ScanBatch to allow unit testing record readers A recent set of PRs refactored some of the ""context"" code to allow easier unit testing of operator internals.

A recent attempt to help a community user revealed that it would be helpful to refactor parts of {{ScanBatch}} and {{OperatorContext}} to allow unit testing of reader code. In particular:

* Make {{ScanBatch.Mutator}} into a static class that can be created in a unit test separate from a {{ScanBatch}} (and the rest of Drill.)
* Split {{OperatorContext}} into a execution-only {{OperatorExecContext}} interface that can be used in testing. Leave in {{OperatorContext}} the methods that require all of Drill to be present.

Doing the above requires a bit of implementation work:

* Change {{OperatorContext}} from an abstract class to an interface so that it can extend {OperatorExceContext}}.
* {{OperatorContext}} appears to be a class so that it can hold a static method. (Java 8 allows static methods on interfaces, but Drill uses Java 7 which does not have such support.) Move this method to a new {{OperatorUtilities}} class and fix up references.
* Split the {{OperatorContextImpl}} class into two parts. Move into a new {{AbstractOperatorContext}} class the code which implements the low-level runtime methods. Leave in the original class the functionality that depends on the rest of Drill (such as references to the {{DrillbitContext}}.)
* Add a method to the new {{OperatorFixture}} class to create a test-time version of the {{OperatorExecContext}} interface.","Duplicated Code, Long Method, , , , Duplicated Code, Duplicated Code, "
"   Extract Method,Move Attribute,","Support HTTP Kerberos auth using SPNEGO DRILL-4280 supports Kerberos through JDBC and ODBC API. This ticket requests to add Kerberos (using [SPENGO|https://en.wikipedia.org/wiki/SPNEGO]) for HTTP connections.

This requires creating ""direct"" web sessions; currently web sessions are sessions over Java client sessions.","Duplicated Code, Long Method, , , "
"   Rename Method,Inline Method,","Support Spill to Disk for the Hash Aggregate Operator Support gradual spilling memory to disk as the available memory gets too small to allow in memory work for the Hash Aggregate Operator.
",", , "
"   Extract Method,Inline Method,","Extend physical operator test framework to test mini plans consisting of multiple operators DRILL-4437 introduced a unit test framework to test a non-scan physical operator. A JSON reader is implicitly used to specify the inputs to the physical operator under test. 

There are needs to extend such unit test framework for two scenarios.

1. We need a way to test scan operator with different record readers. Drill supports a variety of data source, and it's important to make sure every record reader work properly according to the protocol defined.

2. We need a way to test a so-called mini-plan (aka plan fragment) consisting of multiple non-scan operators. 

For the 2nd need, an alternative is to leverage SQL statement and query planner. However, such approach has a direct dependency on query planner; 1) any planner change may impact the testcase and lead to a different plan, 2) it's not always easy job to force the planner to get a desired plan fragment for testing.

In particular, it would be good to have a relatively easy way to specify a mini-plan with a couple of targeted physical operators. 

This JIRA is created to track the work to extend the unit test framework in DRILL-4437.

Related work: DRILL-5318 introduced a sub-operator test fixture, which mainly targeted to test at sub-operator level. The framework in DRILL-4437 and the extension would focus on operator level, or multiple operator levels, where execution would go through RecordBatch's API call. 

Same as DRILL-4437, we are going to use mockit to mock required objects such fragment context, operator context etc. 
","Duplicated Code, Long Method, , , "
"   Move Class,Pull Up Method,Move Method,Pull Up Attribute,Move Attribute,","Remove WebServer dependency on DrillClient With encryption support using SASL, client's won't be able to authenticate using PLAIN mechanism when encryption is enabled on the cluster. Today WebServer which is embedded inside Drillbit creates a DrillClient instance for each WebClient session. And the WebUser is authenticated as part of authentication between DrillClient instance and Drillbit using PLAIN mechanism. But with encryption enabled this will fail since encryption doesn't support authentication using PLAN mechanism, hence no WebClient can connect to a Drillbit. There are below issues as well with this approach:
1) Since DrillClient is used per WebUser session this is expensive as it has heavyweight RPC layer for DrillClient and all it's dependencies. 
2) If the Foreman for a WebUser is also selected to be a different node then there will be extra hop of transferring data back to WebClient.
To resolve all the above issue it would be better to authenticate the WebUser locally using the Drillbit on which WebServer is running without creating DrillClient instance. We can use the local PAMAuthenticator to authenticate the user. After authentication is successful the local Drillbit can also serve as the Foreman for all the queries submitted by WebUser. This can be achieved by submitting the query to the local Drillbit Foreman work queue. This will also remove the requirement to encrypt the channel opened between WebServer (DrillClient) and selected Drillbit since with this approach there won't be any physical channel opened between them.",", , , Duplicated Code, Duplicated Code, "
"   Rename Class,Move Method,Move Attribute,","Implement functions for date and interval data types Below is the list of functions to be supported as part of this task:

Date & Interval Arithmetic Functions:

date +/- integer

date + interval 
time + interval 
timestamp + interval 
timestamptz + interval
date + intervalday 
time + intervalday 
timestamp + intervalday
timestamptz + intervalday
date + intervalyear 
time + intervalyear
timestamp + intervalyear 
timestamptz + intervalyear

date + time

date - date
time - time
timestamp - timestamp
timestamptz - timestamptz

interval +/- interval
intervalday +/- intervalday
intervalyear +/- intervalyear

interval *//(div) integer or float or double
intervalday *//(div) integer or float or double
intervalyear *//(div) integer or float or double

-interval
-intervalday
-intervalyear

Date Utility Functions:

CURRENT_DATE
CURRENT_TIME
CURRENT_TIMESTAMP
LOCALTIME
LOCALTIMESTAMP
now()
timeofday()
clock_timestamp()

// For each of the below functions, the 'text' parameter can be one of the following {year, month, day, hour, minute, second}
date_part(text, date)
date_part(text, time)
date_part(text, timestamp)
date_part(text, timestamptz)
date_part(text, interval)
date_part(text, intervalday)
date_part(text, intervalyear)

// Extract functions similar to date_part
extract(field from date)
extract(field from time)
extract(field from timestamp)
extract(field from timestamptz)
extract(field from interval)
extract(field from intervalday)
extract(field from intervalyear) 

Date Formatting Functions: 
// 'text' parameter represents the desired output format
to_char(date, text)
to_char(time, text)
to_char(timestamp, text)
to_char(timestamptz, text)

// In the below functions first 'text' param represents the string to be converted to date type
// Second 'text' param represents the format its in
to_date(text, text) 
to_time(text, text) 
to_timestamp(text, text) 
to_timestamptz(text, text) 

// Input is long milliseconds from epoch
to_date(long)
to_time(long)
to_timestamp(long)
to_timestamptz(long)",", , , "
"   Rename Method,Extract Method,Pull Up Attribute,","Roll-up of a number of test framework enhancements Recent development work identified a number of minor enhancements to the ""sub-operator"" unit tests:

* Create a {{SubOperatorTest}} base class to do routine setup and shutdown.
* Additional methods to simplify creating complex schemas with field widths.
* Define a test workspace with plugin-specific options (as for the CSV storage plugin)
* When verifying row sets, add methods to verify and release just the ""actual"" batch in addition to the existing method for verify and free both the actual and expected batches.


","Duplicated Code, Long Method, , Duplicated Code, "
"   Extract Interface,Move Method,Extract Method,Move Attribute,","Predicate push down into HBase scan With DRILL-494 HBase storage plugin could transform an HBase scan followed by a qualified filter, for example a row_key equality into a single HBase scan operation which an HBase (RowKeyFilter) filter.","Duplicated Code, Long Method, , , , Large Class, "
"   Rename Method,Pull Up Method,Move Method,Extract Method,Inline Method,","Support System/Session Internal Options And Additional Option System Fixes This is a feature proposed by [~ben-zvi].

Currently all the options are accessible by the user in sys.options. We would like to add internal options which can be altered, but are not visible in the sys.options table. These internal options could be seen by another alias select * from internal.options. The intention would be to put new options we weren't comfortable with exposing to the end user in this table.

After the options and their corresponding features are considered stable they could be changed to appear in the sys.option table.

A bunch of other fixes to the Option system have been clubbed into this:

* OptionValidators no longer hold default values. Default values are contained in the SystemOptionManager
* Options have an OptionDefinition. The option definition includes:
* A validator
* Metadata about the options visibility, required permissions, and the scope in which it can be set.
* The Option Manager interface has been cleaned up so that a Type is not required to be passed in in order to set and delete options
","Duplicated Code, Long Method, , , , Duplicated Code, "
"   Move Class,Rename Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Make code generation in the TopN operator more modular and test it The work for this PR has had several other PRs batched together with it. The full description of work is the following: 

DRILL-5783 

* A unit test is created for the priority queue in the TopN operator 
* The code generation classes passed around a completely unused function registry reference in some places so I removed it. 
* The priority queue had unused parameters for some of its methods so I removed them. 

DRILL-5841 

* There were many many ways in which temporary folders were created in unit tests. I have unified the way these folders are created with the DirTestWatcher, SubDirTestWatcher, and BaseDirTestWatcher. All the unit tests have been updated to use these. The test watchers create temp directories in ./target//. So all the files generated and used in the context of a test can easily be found in the same consistent location. 
* This change should fix the sporadic hashagg test failures, as well as failures caused by stray files in /tmp 

DRILL-5894 

* dfs_test is used as a storage plugin throughout the unit tests. This is highly confusing and we can just use dfs instead. 

*Misc* 

* General code cleanup. 
* There are many places where String.format is used unnecessarily. The test builder methods already use String.format for you when you pass them args. I cleaned some of these up.","Duplicated Code, Long Method, , , , , "
"   Rename Method,",#N/A,", "
"   Rename Method,Extract Method,","Use more often the new parquet reader The choice of using the regular parquet reader of the optimized one is based of what type of columns is in the file. But the columns that are read by the query doesn't matter. We can increase a little bit the cases where the optimized reader is used by checking is the projected column are simple or not.
This is an optimization waiting for the fast parquet reader to handle complex structure.","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,Move Method,Extract Method,","Migrate OperatorFixture to use SystemOptionManager rather than mock The {{OperatorFixture}} provides structure for testing individual operators and other ""sub-operator"" bits of code. To do that, the framework provides mock network-free and server-free versions of the fragment context and operator context.

As part of the mock, the {{OperatorFixture}} provides a mock version of the system option manager that provides a simple test-only implementation of an option set.

With the recent major changes to the system option manager, this mock implementation has drifted out of sync with the system option manager. Rather than upgrading the mock implementation, this ticket asks to use the system option manager directly -- but configured for no ZK or file persistence of options.

The key reason for this change is that the system option manager has implemented a sophisticated way to handle option defaults; it is better to leverage that than to provide a mock implementation.
","Duplicated Code, Long Method, , , "
"   Rename Class,Rename Method,Pull Up Method,Move Method,Pull Up Attribute,Move Attribute,","Refactor and simplify the fragment, operator contexts for testing Drill's execution engine has a ""fragment context"" that provides state for a fragment as a whole, and an ""operator context"" which provides state for a single operator. Historically, these have both been concrete classes that make generous references to the Drillbit context, and hence need a full Drill server in order to operate.

Drill has historically made extensive use of system-level testing: build the entire server and fire queries at it to test each component. Over time, we are augmenting that approach with unit tests: the ability to test each operator (or parts of an operator) in isolation.

Since each operator requires access to both the operator and fragment context, the fact that the contexts depend on the overall server creates a large barrier to unit testing. An earlier checkin started down the path of defining the contexts as interfaces that can have different run-time and test-time implementations to enable testing.

This ticket asks to refactor those interfaces: simplifying the operator context and introducing an interface for the fragment context. New code will use these new interfaces, while older code continues to use the concrete implementations. Over time, as operators are enhanced, they can be modified to allow unit-level testing.",", , , Duplicated Code, Duplicated Code, "
"   Rename Method,Move Method,","Simple pattern matchers can work with DrillBuf directly For the 4 simple patterns we have i.e. startsWith, endsWith, contains and constant,, we do not need the overhead of charSequenceWrapper. We can work with DrillBuf directly. This will save us from doing isAscii check and UTF8 decoding for each row. 
UTF-8 encoding ensures that no UTF-8 character is a prefix of any other valid character. So, instead of decoding varChar from each row we are processing, encode the patternString once during setup and do raw byte comparison. Instead of bounds checking and reading one byte at a time, we get the whole buffer in one shot and use that for comparison. 
This improved overall performance for filter operator by around 20%. 
",", , "
"   Rename Method,","Avoid the strong check introduced by DRILL-5582 for PLAIN mechanism For PLAIN mechanism we will weaken the strong check introduced with DRILL-5582 to keep the forward compatibility between Drill 1.12 client and Drill 1.9 server. This is fine since with and without this strong check PLAIN mechanism is still vulnerable to MITM during handshake itself unlike mutual authentication protocols like Kerberos. 

Also for keeping forward compatibility with respect to SASL we will treat UNKNOWN_SASL_SUPPORT as valid value. For handshake message received from a client which is running on later version (let say 1.13) then Drillbit (1.12) and having a new value for SaslSupport field which is unknown to server, this field will be decoded as UNKNOWN_SASL_SUPPORT. In this scenario client will be treated as one aware about SASL protocol but server doesn't know exact capabilities of client. Hence the SASL handshake will still be required from server side.",", "
"   Rename Method,","Implement ""CREATE TABLE IF NOT EXISTS"" Currently, if a table/view with the same name exists CREATE TABLE fails with VALIDATION ERROR 

Having ""IF NOT EXISTS"" support for CREATE TABLE will ensure that query succeeds 

Also the same functionality was added for views.",", "
"   Rename Method,","Updating of Apache and MapR Hive libraries to 2.3.2 and 2.1.1-mapr-1710 versions respectively Currently Drill uses [Hive version 1.2.1 libraries|https://github.com/apache/drill/blob/master/pom.xml#L53] to perform queries on Hive. This version of library can be used for Hive1.x versions and Hive2.x versions too, but some features of Hive2.x are broken (for example using of ORC transactional tables). To fix that it will be good to update drill-hive library version to 2.1 or newer. 
Tasks which should be done: 
- resolving dependency conflicts; 
- investigating backward compatibility of newer drill-hive library with older Hive versions (1.x); 
- updating drill-hive version for [MapR|https://github.com/apache/drill/blob/master/pom.xml#L1777] profile too. 



Starting from this commit: 
* Drill Hive client is updated to 2.3.2 and 2.1.1-mapr-1710 versions for default and MapR profiles respectively; 
* Drill supports of querying Hive transactional ORC bucketed tables [https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions]. 
Note: Updated Drill Hive client preserves backward compatibility with older 1.2.1 Hive server/metastore version.",", "
"   Rename Method,Move Method,Extract Method,Pull Up Attribute,","Improve Performance of Copiers used by SV Remover, Top N etc. Currently the copier can only copy record from an incoming batch to the beginning of an outgoing batch. We need to be able to copy a record and append it to the end of the outgoing batch. Also Paul's Generic copiers are more performant and simpler and should be added.","Duplicated Code, Long Method, , , Duplicated Code, "
"   Extract Method,Inline Method,",Avoid memory copy from direct buffer to heap while spilling to local disk When spilling to a local disk or to any file system that supports WritableByteChannel it is preferable to avoid copy from off-heap to java heap as WritableByteChannel can work directly with the off-heap memory.,"Duplicated Code, Long Method, , , "
"   Extract Method,Inline Method,","Direct buffer bounds checking should be disabled by default Direct buffer bounds checking is enabled either when assertions are enabled (see DRILL-6001) or when {{drill.enable_unsafe_memory_access}} property is not set to true, so it is enabled in production as by default {{drill.enable_unsafe_memory_access}} is not set.","Duplicated Code, Long Method, , , "
"   Rename Class,Move Class,Rename Method,Move Method,Extract Method,Move Attribute,","Implement spill to disk for the Hash Join Implement the spill memory to disk (as needed) feature for the Hash Join operator (similar to the prior work on the Hash Aggregate). 

A design draft document has been published: 

[https://docs.google.com/document/d/1-c_oGQY4E5d58qJYv_zc7ka834hSaB3wDQwqKcMoSAI/edit?usp=sharing] 

Functional spec is available: [https://docs.google.com/document/d/1bPAddVCRxKHxi2RjqUvISIWXNqLdbRzZT9CWmanh4h0/edit]","Duplicated Code, Long Method, , , , "
"   Move Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Implement spill to disk for the Hash Join Implement the spill memory to disk (as needed) feature for the Hash Join operator (similar to the prior work on the Hash Aggregate). 

A design draft document has been published: 

[https://docs.google.com/document/d/1-c_oGQY4E5d58qJYv_zc7ka834hSaB3wDQwqKcMoSAI/edit?usp=sharing] 

Functional spec is available: [https://docs.google.com/document/d/1bPAddVCRxKHxi2RjqUvISIWXNqLdbRzZT9CWmanh4h0/edit]","Duplicated Code, Long Method, , , , , "
"   Extract Method,Inline Method,","Allow splitting generated code in ChainedHashTable into blocks to avoid ""code too large"" error Allow splitting generated code in ChainedHashTable into blocks to avoid ""code too large"" error. 

*REPRODUCE* 
File {{1200_columns.csv}} 
{noformat} 
0,1,2,3...1200 
0,1,2,3...1200 
{noformat} 

Query 
{noformat} 
select columns[0], column[1]...columns[1200] from dfs.`1200_columns.csv` 
union 
select columns[0], column[1]...columns[1200] from dfs.`1200_columns.csv` 
{noformat} 

Error 
{noformat} 
Error: SYSTEM ERROR: CompileException: File 'org.apache.drill.exec.compile.DrillJavaFileObject[HashTableGen10.java]', Line -7886, Column 24: HashTableGen10.java:57650: error: code too large 
public boolean isKeyMatchInternalBuild(int incomingRowIdx, int htRowIdx) 
^ (compiler.err.limit.code) 
{noformat} 

*ROOT CAUSE* 
DRILL-4715 added ability to ensure that methods size won't go beyond the 64k limit imposed by JVM. {{BlkCreateMode.TRUE_IF_BOUND}} was added to create new block only if # of expressions added hit upper-bound defined by {{exec.java.compiler.exp_in_method_size}}. Once number of expressions in methods hits upper bound we create from call inner method. 
Example: 
{noformat} 
public void doSetup(RecordBatch incomingBuild, RecordBatch incomingProbe) throws SchemaChangeException { 
// some logic 

return doSetup0(incomingBuild, incomingProbe); 
} 
{noformat} 

During code generation {{ChainedHashTable}} added all code in its methods in one block (using {{BlkCreateMode.FALSE}}) since {{getHashBuild}} and {{getHashProbe}} methods contained state and thus could not be split. In these methods hash was generated for each key expression. For the first key seed was 0, subsequent keys hash was generated based on seed from previous key. 
To allow splitting for there methods the following was done: 
1. Method signatures was changed: added new parameter {{seedValue}}. Initially starting seed value was hard-coded during code generation (set to 0), now it is passed as method parameter. 
2. Initially hash function call for all keys was transformed into one logical expression which did not allow splitting. Now we create logical expression for each key and thus splitting is possible. New {{seedValue}} parameter is used as seed holder to pass seed value for the next key. 
3. {{ParameterExpression}} was added to generate reference to method parameter during code generation. 

Code example: 
{noformat} 
public int getHashBuild(int incomingRowIdx, int seedValue) 
throws SchemaChangeException 
{ 
{ 
NullableVarCharHolder out3 = new NullableVarCharHolder(); 
{ 
out3 .isSet = vv0 .getAccessor().isSet((incomingRowIdx)); 
if (out3 .isSet == 1) { 
out3 .buffer = vv0 .getBuffer(); 
long startEnd = vv0 .getAccessor().getStartEnd((incomingRowIdx)); 
out3 .start = ((int) startEnd); 
out3 .end = ((int)(startEnd >> 32)); 
} 
} 
IntHolder seedValue4 = new IntHolder(); 
seedValue4 .value = seedValue; 
//---- start of eval portion of hash32 function. ----// 
IntHolder out5 = new IntHolder(); 
{ 
final IntHolder out = new IntHolder(); 
NullableVarCharHolder in = out3; 
IntHolder seed = seedValue4; 

Hash32FunctionsWithSeed$NullableVarCharHash_eval: { 
if (in.isSet == 0) { 
out.value = seed.value; 
} else 
{ 
out.value = org.apache.drill.exec.expr.fn.impl.HashHelper.hash32(in.start, in.end, in.buffer, seed.value); 
} 
} 

out5 = out; 
} 
//---- end of eval portion of hash32 function. ----// 
seedValue = out5 .value; 

return getHashBuild0((incomingRowIdx), (seedValue)); 
} 
{noformat} 

Examples of code generation: 
{{HashTableGen5_for_40_columns_BEFORE.java}} - code compiles 
{{HashTableGen5_for_40_columns_AFTER.java}} - code compiles 

{{HashTableGen5_for_1200_columns_BEFORE.java}} - error during compilation, method too large 
{{HashTableGen5_for_1200_columns_AFTER.java}} - code compiles since methods were split","Duplicated Code, Long Method, , , "
"   Extract Interface,Move Method,Extract Method,Inline Method,Move Attribute,","Avoid excessive locking in LocalPersistentStore When query profiles are written to LocalPersistentStore, the write is unnecessary serialized due to read/write lock that was introduced for versioned PersistentStore. Only versioned access needs to be protected by read/write lock.","Duplicated Code, Long Method, , , , , Large Class, "
"   Move Class,Rename Method,Move Method,Move Attribute,","Complete internal metadata layer for improved batch handling Slice of the [""batch handling"" project.|https://github.com/paul-rogers/drill/wiki/Batch-Handling-Upgrades] that includes enhancements to the internal metadata system.",", , , "
"   Move Class,Extract Method,","SingleMergeExchange is not scaling up when many minor fragments are allocated for a query. SingleMergeExchange is created when a global order is required in the output. The following query produces the SingleMergeExchange. 
{code:java} 
0: jdbc:drill:zk=local> explain plan for select L_LINENUMBER from dfs.`/drill/tables/lineitem` order by L_LINENUMBER; 
+------+------+ 
| text | json | 
+------+------+ 
| 00-00 Screen 
00-01 Project(L_LINENUMBER=[$0]) 
00-02 SingleMergeExchange(sort0=[0]) 
01-01 SelectionVectorRemover 
01-02 Sort(sort0=[$0], dir0=[ASC]) 
01-03 HashToRandomExchange(dist0=[[$0]]) 
02-01 Scan(table=[[dfs, /drill/tables/lineitem]], groupscan=[JsonTableGroupScan [ScanSpec=JsonScanSpec [tableName=maprfs:///drill/tables/lineitem, condition=null], columns=[`L_LINENUMBER`], maxwidth=15]]) 
{code} 

On a 10 node cluster if the table is huge then DRILL can spawn many minor fragments which are all merged on a single node with one merge receiver. Doing so will create lot of memory pressure on the receiver node and also execution bottleneck. To address this issue, merge receiver should be multiphase merge receiver. 

Ideally for large cluster one can introduce tree merges so that merging can be done parallel. But as a first step I think it is better to use the existing infrastructure for multiplexing operators to generate an OrderedMux so that all the minor fragments pertaining to one DRILLBIT should be merged and the merged data can be sent across to the receiver operator. 

On a 10 node cluster if each node processes 14 minor fragments. 

Current version of code merges 140 minor fragments 
the proposed version has two level merges 1 - 14 merge in each drillbit which is parallel 
and 10 minorfragments are merged at the receiver node. 

","Duplicated Code, Long Method, , "
"   Move And Rename Class,Rename Method,Move Method,Extract Method,Move Attribute,","Handle item star columns during project  /  filter push down and directory pruning Project push down, filter push down and partition pruning does not work with dynamically expanded column with is represented as star in ITEM operator: _ITEM($0, 'column_name')_ where $0 is a star. 
This often occurs when view, sub-select or cte with star is issued. 
To solve this issue we can create {{DrillFilterItemStarReWriterRule}} which will rewrite such ITEM operator before filter push down and directory pruning. For project into scan push down logic will be handled separately in already existing rule {{DrillPushProjectIntoScanRule}}. Basically, we can consider the following queries the same:  
{{select col1 from t}} 
{{select col1 from (select * from t)}} 

*Use cases* 
Since item star columns where not considered during project / filter push down and directory pruning, push down and pruning did not happen. This was causing Drill to read all columns from file (when only several are needed) or ready all files instead. Views with star query is the most common example. Such behavior significantly degrades performance for item star queries comparing to queries without item star. 

*EXAMPLES* 

*Data set* 
will create table with three files each in dedicated sub-folder: 
{noformat} 
use dfs.tmp; 
create table `order_ctas/t1` as select cast(o_orderdate as date) as o_orderdate from cp.`tpch/orders.parquet` where o_orderdate between date '1992-01-01' and date '1992-01-03'; 
create table `order_ctas/t2` as select cast(o_orderdate as date) as o_orderdate from cp.`tpch/orders.parquet` where o_orderdate between date '1992-01-04' and date '1992-01-06'; 
create table `order_ctas/t3` as select cast(o_orderdate as date) as o_orderdate from cp.`tpch/orders.parquet` where o_orderdate between date '1992-01-07' and date '1992-01-09'; 
{noformat} 


*Filter push down* 
{{select * from order_ctas where o_orderdate = date '1992-01-01'}} will read only one file 
{noformat} 
00-00 Screen 
00-01 Project(**=[$0]) 
00-02 Project(T1¦¦**=[$0]) 
00-03 SelectionVectorRemover 
00-04 Filter(condition=[=($1, 1992-01-01)]) 
00-05 Project(T1¦¦**=[$0], o_orderdate=[$1]) 
00-06 Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=/tmp/order_ctas/t1/0_0_0.parquet]], selectionRoot=/tmp/order_ctas, numFiles=1, numRowGroups=1, usedMetadataFile=false, columns=[`**`]]]) 

{noformat} 
{{select * from (select * from order_ctas) where o_orderdate = date '1992-01-01'}} will ready all three files 
{noformat} 
00-00 Screen 
00-01 Project(**=[$0]) 
00-02 SelectionVectorRemover 
00-03 Filter(condition=[=(ITEM($0, 'o_orderdate'), 1992-01-01)]) 
00-04 Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=/tmp/order_ctas/t1/0_0_0.parquet], ReadEntryWithPath [path=/tmp/order_ctas/t2/0_0_0.parquet], ReadEntryWithPath [path=/tmp/order_ctas/t3/0_0_0.parquet]], selectionRoot=/tmp/order_ctas, numFiles=3, numRowGroups=3, usedMetadataFile=false, columns=[`**`]]]) 
{noformat} 

*Directory pruning* 
{{select * from order_ctas where dir0 = 't1'}} will read data only from one folder 
{noformat} 
00-00 Screen 
00-01 Project(**=[$0]) 
00-02 Project(**=[$0]) 
00-03 Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=/tmp/order_ctas/t1/0_0_0.parquet]], selectionRoot=/tmporder_ctas, numFiles=1, numRowGroups=1, usedMetadataFile=false, columns=[`**`]]]) 
{noformat} 

{{select * from (select * from order_ctas) where dir0 = 't1'}} will read content of all three folders 
{noformat} 
00-00 Screen 
00-01 Project(**=[$0]) 
00-02 SelectionVectorRemover 
00-03 Filter(condition=[=(ITEM($0, 'dir0'), 't1')]) 
00-04 Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=/tmp/order_ctas/t1/0_0_0.parquet], ReadEntryWithPath [path=/tmp/order_ctas/t2/0_0_0.parquet], ReadEntryWithPath [path=/tmp/order_ctas/t3/0_0_0.parquet]], selectionRoot=/tmp/order_ctas, numFiles=3, numRowGroups=3, usedMetadataFile=false, columns=[`**`]]]) 
{noformat} 

*Project into Scan push down* 
{{select o_orderdate, count(1) from order_ctas group by o_orderdate}} will ready only one column from the files 
{noformat} 
00-00 Screen 
00-01 Project(o_orderdate=[$0], EXPR$1=[$1]) 
00-02 HashAgg(group=[{0}], EXPR$1=[COUNT()]) 
00-03 Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=/tmp/order_ctas/t1/0_0_0.parquet], ReadEntryWithPath [path=/tmp/order_ctas/t2/0_0_0.parquet], ReadEntryWithPath [path=/tmp/order_ctas/t3/0_0_0.parquet]], selectionRoot=/tmp/order_ctas, numFiles=3, numRowGroups=3, usedMetadataFile=false, columns=[`o_orderdate`]]]) 
{noformat} 

{{select o_orderdate, count(1) from (select * from order_ctas) group by o_orderdate}} will ready all columns from the files 
{noformat} 
00-00 Screen 
00-01 Project(col_vrchr=[$0], EXPR$1=[$1]) 
00-02 StreamAgg(group=[{0}], EXPR$1=[COUNT()]) 
00-03 Sort(sort0=[$0], dir0=[ASC]) 
00-04 Project(col_vrchr=[ITEM($0, 'o_orderdate')]) 
00-05 Scan(groupscan=[ParquetGroupScan [entries=[ReadEntryWithPath [path=/tmp/order_ctas/t1/0_0_0.parquet], ReadEntryWithPath [path=/tmp/order_ctas/t2/0_0_0.parquet], ReadEntryWithPath [path=/tmp/order_ctas/t3/0_0_0.parquet]], selectionRoot=/tmp/order_ctas, numFiles=3, numRowGroups=3, usedMetadataFile=false, columns=[`**`]]]) 
{noformat} 

This Jira aims to fix all three described cases above in order to improve performance for queries with item star columns. 
 ","Duplicated Code, Long Method, , , , "
"   Rename Method,Pull Up Method,Extract Method,Pull Up Attribute,","Limit batch size for Merge Join based on memory Merge join limits output batch size to 32K rows irrespective of row size. This can create very large or very small batches (in terms of memory), depending upon average row width. Change this to figure out output row count based on memory specified with the new outputBatchSize option and average row width of incoming left and right batches. Output row count will be minimum of 1 and max of 64k. ","Duplicated Code, Long Method, , Duplicated Code, Duplicated Code, "
"   Rename Method,Push Down Method,Move Method,Extract Method,Push Down Attribute,Move Attribute,",#N/A,"Duplicated Code, Long Method, , , , , , "
"   Rename Method,Inline Method,","Enhance record batch sizer to retain nesting information for map columns. Enhance the record batch sizer to maintain the columnSizes in nested fashion for maps so given a column, we can get sizing information of all children underneath. ",", , "
"   Rename Class,Move Class,Move Method,","Parquet pushdown planning improvements Currently parquet pushdown planning has certain limitations (https://drill.apache.org/docs/parquet-filter-pushdown/). This Jira aims to fix some of them. List of improvements can be find below: 
1. IS [NOT] NULL / TRUE / FALSE 
2. Timestamp / date / time implicit / explicit casts 
{noformat} 
timestamp -> date 
timestamp -> varchar 
date -> timestamp 
date -> varchar 
time -> timestamp 
time -> date 
time -> varchar 
{noformat}",", , "
"   Extract Method,Inline Method,Move Attribute,","Memory consumption fixes This jira covers work done to improve Drill's handling of memory. Specifically, this includes:

Allow trimming of buffers after the data has been written, so that unused memory can be returned to the pool.

Handle case attempts to write beyond a buffers capacity.

Handle case where a buffer allocator is unable to allocate a new buffer due to memory constraints

Dynamically adjust allocation sizes based on trends","Duplicated Code, Long Method, , , , "
"   Move Class,Rename Method,Move Method,Extract Method,Move Attribute,","Enhance the test schema builder for remaining types The result set loader project enhanced the schema builder used in tests to handle Drill's complex types: maps, unions, lists and repeated lists. The schema builder previously only handled maps. 

This ticket describes the addition of just this one part of the result set loader. 

Also adds a run-time schema improvement: functions to create arrays when testing rather than writing out the {{new String[] \{""foo"", ""bar""\}}} syntax.","Duplicated Code, Long Method, , , , "
"   Move Class,Rename Method,Extract Method,","Support parquet filter push down for complex types Currently parquet filter push down is not working for complex types (including arrays). 

This Jira aims to implement filter push down for complex types which underneath type is among supported simple types for filter push down. For instance, currently Drill does not support filter push down for varchars, decimals etc. Though once Drill will start support, this support will be applied for complex type automatically. 

Complex fields will be pushed down the same way regular fields are, except for one case with arrays. 

Query with predicate {{where users.hobbies_ids[2] is null}} won't be able to push down because we are not able to determine exact number of nulls in arrays fields.  

{{Consider [1, 2, 3]}} vs {{[1, 2]}} if these arrays are in different files. Statistics for the second case won't show any nulls but when querying from two files, in terms of data the third value in array is null. 

 ","Duplicated Code, Long Method, , "
"   Extract Superclass,Rename Method,Extract Method,Inline Method,","Project push down into HBase scan If a query against an HBase table requires only a subset of columns, we should qualify the HBase scan with these columns.

For example
{noformat}
SELECT row_key, f['c1'], f['c2'], g FROM hbase.MyTable
{noformat}

should qualify he HBase scan as {{families => \[g\[""ALL""], f\[""c1"", ""c2""]]}}","Duplicated Code, Long Method, , Duplicated Code, Large Class, , "
"   Move Method,Move Attribute,",Add operator metrics for batch sizing for merge join Add operator metrics for batch sizing stats for merge join.,", , , "
"   Rename Method,",limit batch size for hash aggregate limit batch size for hash aggregate based on memory.,", "
"   Rename Method,","Inconsistent method name ""field"". The following method is names as ""field"", but the method is mainly doing appending. So that, rename the method as ""append"" should be better. 
{code:java} 
private void field(String label, String value) { 
indent(); 
out.append(label) 
.append("" = "") 
.append(value) 
.append(""\n""); 
} 
{code} 
",", "
"   Rename Method,",Unordered Receiver does not report its memory usage The Drill Profile functionality doesn't show any memory usage for the Unordered Receiver operator. This is problematic when analyzing OOM conditions since we cannot account for all of a query memory usage. This Jira is to fix memory reporting for the Unordered Receiver operator.,", "
"   Pull Up Method,Pull Up Attribute,",Store context and name in AbstractStoragePlugin instead of replicating fields in each StoragePlugin 0,", Duplicated Code, Duplicated Code, "
"   Move Method,Extract Method,","Native MapR DB plugin support for Hive MapR-DB json table Hive can create and query MapR-DB tables via maprdb-json-handler: 
[https://maprdocs.mapr.com/home/Hive/ConnectingToMapR-DB.html] 

The aim of this Jira to implement Drill native reader for Hive MapR-DB tables (similar to parquet). 

Design proposal is: 
- to use JsonTableGroupScan instead of HiveScan; 
- to add storage planning rule to convert HiveScan to MapRDBGroupScan; 
- to add system/session option to enable using of this native reader; 
- native reader can be used only for Drill build with mapr profile (there is no reason to leverage it for default profile); 

  

*For documentation:* 
two new options were added: 
store.hive.parquet.optimize_scan_with_native_reader: false, 
store.hive.maprdb_json.optimize_scan_with_native_reader: false, 

store.hive.parquet.optimize_scan_with_native_reader is new option used instead of store.hive.optimize_scan_with_native_readers. The latter is deprecated and will be removed in 1.15. 
(https://issues.apache.org/jira/browse/DRILL-6527). 

 ","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,Move Attribute,","Make storage plugins names case insensitive Storage plugin names to be case insensitive (DFS vs dfs, INFORMATION_SCHEMA vs information_schema). 
Workspace (schemas) names to be case insensitive (ROOT vs root, TMP vs tmp). Even if user has two directories /TMP and /tmp, he can create two workspaces but not both with tmp name. For example, tmp vs tmp_u. 
Table names case sensitivity are treated per plugin. For example, system plugins (information_schema, sys) table names (views, tables) should be case insensitive. Actually, currently for sys plugin table names are case insensitive, information_schema table names are case sensitive. That needs to be synchronized. For file system plugins table names must be case sensitive, since under table name we imply directory / file name and their case sensitivity depends on file system. 

*Documentation* 
https://drill.apache.org/docs/lexical-structure/ should be updated with relevant information.","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Drill Plugins Handler - Storage Plugins Handler service is used on the Drill start-up stage and it updates storage plugins configs from 
storage-plugins-override.conf file. If plugins configs are present in the persistence store - they are updated, 
otherwise bootstrap plugins are updated and the result configs are loaded to persistence store. If the enabled 
status is absent in the storage-plugins-override.conf file, the last plugin config enabled status persists. 
- 'drill.exec.storage.action_on_plugins_override_file' Boot option is added. This is the action, which should be 
performed on the storage-plugins-override.conf file after successful updating storage plugins configs. 
Possible values are: ""none"" (default), ""rename"" and ""remove"". 
- The ""NULL"" issue with updating Hive plugin config by REST is solved. But clients are still being instantiated for disabled 
plugins - DRILL-6412. 
- ""org.honton.chas.hocon:jackson-dataformat-hocon"" library is added for the proper deserializing HOCON conf file 
- additional refactoring: ""com.typesafe:config"" and ""org.apache.commons:commons-lang3"" are placed into DependencyManagement 
block with proper versions; correct properties for metrics in ""drill-override-example.conf"" are specified 

Please find details from design overview document: 
https://docs.google.com/document/d/14JKb2TA8dGnOIE5YT2RImkJ7R0IAYSGjJg8xItL5yMI/edit?usp=sharing","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,",Rename CorrelatePrel to LateralJoinPrel Currently in Drill correlatePrel is a physical relation operator for LateralJoin implementation. Explain plan shows CorrelatePrel which can be confusing. Hence it is good to rename this operator to LateralJoinPrel.,", "
"   Rename Method,Extract Method,","Add store.hive.conf.properties option to allow set Hive properties at session level *Use case* 

Hive external table ddl: 
{noformat} 
create external table my(key int, val string) 
row format delimited 
fields terminated by ',' 
stored as textfile 
location '/data/my_tbl'; 
{noformat} 
Path {{/data/my_tb}} contains sub directory and file in it: {{/data/my_tbl/sub_dir/data.txt}} with the following data: 
{noformat} 
1, value_1 
2, value_2 
{noformat} 
When querying such table from Hive, user gets the following exception: 
{noformat} 
Failed with exception java.io.IOException:java.io.IOException: Not a file: file:///data/my_tbl/sub_dir 
{noformat} 
To be able to query this table user needs to set two properties to true: {{hive.mapred.supports.subdirectories}} and {{mapred.input.dir.recursive}}. 
They can be set at system level in hive-site.xml or at session in Hive console: 
{noformat} 
set hive.mapred.supports.subdirectories=true; 
set mapred.input.dir.recursive=true; 
{noformat} 
Currently to be able to query such table from Drill, user can specify this properties in Hive plugin only: 
{noformat} 
{ 
""type"": ""hive"", 
""configProps"": { 
""hive.metastore.uris"": ""thrift://localhost:9083"", 
""hive.metastore.sasl.enabled"": ""false"", 
""hbase.zookeeper.quorum"": ""localhost"", 
""hbase.zookeeper.property.clientPort"": ""5181"", 
""hive.mapred.supports.subdirectories"": ""true"", 
""mapred.input.dir.recursive"": ""true"" 
} 
""enabled"": true 
} 
{noformat} 
*Jira scope* 
This Jira aims to add new session option to Drill {{store.hive.conf.properties}} which will allow user to specify hive properties at session level. 
User should write properties in string delimited by new line symbol. Properties values should NOT be set in double-quotes or any other quotes otherwise they would be parsed incorrectly. Property name and value should be separated by {{=}}. Each `alter session set` will override previously set properties at session level. If during query Drill couldn't unparse property string, warning will be logged. Properties will be parsed by loading into {{java.util.Properties}}. Default value is empty string (""""). 

Example: 
{noformat} 
alter session set `store.hive.conf.properties` = 'hive.mapred.supports.subdirectories=true\nmapred.input.dir.recursive=true'; 
{noformat}","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","In Some Cases The HashJoin Memory Calculator Over Reserves Memory For The Probe Side During The Build Phase There are two cases where the HashJoin Memory calculator over reserves memory: 

1. It reserves a maximum incoming probe batch size during the build phase. This is not really necessary because we will not fetch probe data until the probe phase. We only have to account for the data received during OK_NEW_SCHEMA. 
2. https://issues.apache.org/jira/browse/DRILL-6646","Duplicated Code, Long Method, , "
"   Rename Method,",#N/A,", "
"   Rename Method,Move Method,Extract Method,Move Attribute,","Improve RemovingRecordBatch to do transfer when all records needs to be copied SelectionVector2 contains list of indexes for the rows that RemovingRecordBatch can copy from underlying RecordBatch. SV2 is created by operator like Filter, Limit, etc to provide the selected rows from underlying buffer. Later then RemovingRecordBatch copies the rows based on indexes in SelectionVector2 to the output container of type NONE.  

For cases when all the rows needs to be copied by RemovingRecordBatch from incoming batch, it can be improved to do full transfer of ValueVectors from input to output container instead of row by row copy. For example if for an incoming batch all rows are selected by the Filter condition in FilterRecordBatch, it will prepare an SV2 with all the record rowIndex. Later RemovingRecordBatch downstream of Filter can potentially do just transfer instead of row by row copy.","Duplicated Code, Long Method, , , , "
"   Rename Method,","Improve RemovingRecordBatch to do transfer when all records needs to be copied SelectionVector2 contains list of indexes for the rows that RemovingRecordBatch can copy from underlying RecordBatch. SV2 is created by operator like Filter, Limit, etc to provide the selected rows from underlying buffer. Later then RemovingRecordBatch copies the rows based on indexes in SelectionVector2 to the output container of type NONE.  

For cases when all the rows needs to be copied by RemovingRecordBatch from incoming batch, it can be improved to do full transfer of ValueVectors from input to output container instead of row by row copy. For example if for an incoming batch all rows are selected by the Filter condition in FilterRecordBatch, it will prepare an SV2 with all the record rowIndex. Later RemovingRecordBatch downstream of Filter can potentially do just transfer instead of row by row copy.",", "
"   Extract Method,Inline Method,","JPPD:Move aggregating the BF from the Foreman to the RuntimeFilter This PR is to move the BloomFilter aggregating work from the foreman to RuntimeFilter. Though this change, the RuntimeFilter can apply the incoming BF as soon as possible.","Duplicated Code, Long Method, , , "
"   Rename Class,Rename Method,Extract Method,","JPPD:Move aggregating the BF from the Foreman to the RuntimeFilter This PR is to move the BloomFilter aggregating work from the foreman to RuntimeFilter. Though this change, the RuntimeFilter can apply the incoming BF as soon as possible.","Duplicated Code, Long Method, , "
"   Rename Method,Move Method,Move Attribute,",Implement Window Functioning We want to support window functioning in Drill.,", , , "
"   Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,",Add JSONRecordReader No repeated fields and late bind schema support yet.,"Duplicated Code, Long Method, , , , , "
"   Rename Class,Rename Method,Move Method,Extract Method,","Drill needs to return complex types (e.g., map and array) as a JSON string Drill needs to help users understand the available columns in a given HBase table's column-family. One way to do this is to implement the DESCRIBE command for a column-family. The purpose of this is to let Drill do the column sampling. The mandatory LIMIT clause specifies the sample size so as to throttle the number of columns returned.

==
Instead of the above narrow proposal, a general mechanism to return complex types in a JSON string so as to allow client tools (such as the ODBC driver) to operate on these complex types.

Returning a map as JSON would provide transparency into HBase column-families and allow the ODBC driver to help surface the names of columns within a column-family.

Returning an array as JSON would provide transparency into CSV files since that conveys the number of columns to the ODBC driver.
","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,",Add Google Compute preemptible support The Google Compute provider should support preemptible instances. Adding this should be fairly straightforward.,"Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Make proxy configuration logic and settings clearer and more in line with Java conventions From the description of https://github.com/jclouds/jclouds/pull/914:

In the absence of any special jclouds.\*proxy\* settings, jclouds should respect the normal JVM conventions for proxy detection, such as the java.net.httpProxyHost property. For it to default to no proxy, unlike everything else in java, is surprising. Previously jclouds only looked for the JVM proxy if jclouds.use-system-proxies was set, and then in a cumbersome way:

* jclouds.use-system-proxies tried to set java.net.useSystemProxy but that is read once at startup and subsequently ignored by the JVM so it had little effect
* so the effect of jclouds.use-system-proxies was effectively to tell jclouds to use the default proxy chosen by the JVM looking at the java.net.* properties, not necessarily the OS proxy referred to by java.net.useSystemProxy
* the default value of jclouds.use-system-proxies was taken from java.net.useSystemProxy and so while that succeeds in causing java.net.useSystemProxy to take effect, it was succeeding in a strange accidental way: the only way for the normal java.net.httpProxyHost and others to take effect was to set jclouds.use-system-proxies=true but make sure java.net.useSystemProxy=false

This adds explicit control over whether the JVM default proxy is usable (true by default), and switches the precedence so that if a user specifies a jclouds.proxy-host then it is used -- irrespective of whether any other system proxies or jvm proxies are specified.

I think this is more what a user would expect if they do need to set special proxy settings for jclouds to use. More importantly in most cases they can now just set JVM proxy settings and the right thing will happen.","Duplicated Code, Long Method, , "
"   Extract Method,Move Attribute,","Managing the header name in the TempAuth (Identity Protocol v1)  State of jclouds
The openstack swift ""official"" client (in python) manage this v1 protocol (http://docs.openstack.org/developer/python-swiftclient/swiftclient.html) so even if we don't have a specification, we will use the code of the official client for the implementation. 

In jclouds, there is currently a sort-of v1 identity protocol in the openstack-swift module: in apis/openstack-swift/src/main/java/org/jclouds/openstack/swift/v1/config/SwiftAuthenticationModule.java, there is a tempAuthCredentials which is almost the identity v1 protocol except that the name of the headers was not the same :
= X-Storage-User vs X-Auth-User
= X-Storage-Path vs X-Auth-Key

Proposal
= Keep the current behaviour as default
= Add 2 parameters to change the header name through variables in the Properties put in the Builder like that :

Properties overrides = new Properties();
overrides.setProperty(""jclouds.keystone.credential-type"", ""tempAuthCredentials"");
overrides.setProperty(""jclouds.swift.tempAuth.headerUser"", ""X-Auth-User"");
overrides.setProperty(""jclouds.swift.tempAuth.headerPass"", ""X-Auth-Pass"");
swiftApi = ContextBuilder.newBuilder(provider)
.endpoint(args[1])
.credentials(identity, credential)
.modules(modules)
.overrides(overrides)
.buildApi(SwiftApi.class);
","Duplicated Code, Long Method, , , "
"   Extract Method,Move Attribute,","Managing the header name in the TempAuth (Identity Protocol v1)  State of jclouds
The openstack swift ""official"" client (in python) manage this v1 protocol (http://docs.openstack.org/developer/python-swiftclient/swiftclient.html) so even if we don't have a specification, we will use the code of the official client for the implementation. 

In jclouds, there is currently a sort-of v1 identity protocol in the openstack-swift module: in apis/openstack-swift/src/main/java/org/jclouds/openstack/swift/v1/config/SwiftAuthenticationModule.java, there is a tempAuthCredentials which is almost the identity v1 protocol except that the name of the headers was not the same :
= X-Storage-User vs X-Auth-User
= X-Storage-Path vs X-Auth-Key

Proposal
= Keep the current behaviour as default
= Add 2 parameters to change the header name through variables in the Properties put in the Builder like that :

Properties overrides = new Properties();
overrides.setProperty(""jclouds.keystone.credential-type"", ""tempAuthCredentials"");
overrides.setProperty(""jclouds.swift.tempAuth.headerUser"", ""X-Auth-User"");
overrides.setProperty(""jclouds.swift.tempAuth.headerPass"", ""X-Auth-Pass"");
swiftApi = ContextBuilder.newBuilder(provider)
.endpoint(args[1])
.credentials(identity, credential)
.modules(modules)
.overrides(overrides)
.buildApi(SwiftApi.class);
","Duplicated Code, Long Method, , , "
"   Rename Class,Extract Superclass,Rename Method,Move Method,Extract Method,Move Attribute,","Add support for full GCE v1beta15 API So in JCLOUDS-192, we've got the 1.6.x line's GCE support working again, but it's not fully implementing all of the v1beta15 API. I'm working on getting that done and added to 1.7.0, but am going to leave the 1.6.x line as is, supporting the APIs existing in v1beta13 running against v1beta15, adding only the new API calls needed for things like zone vs global operations. Here, I'll get it all working. =)","Duplicated Code, Long Method, , , , Duplicated Code, Large Class, "
"   Move Method,Move Attribute,","atmos, aws-s3, azure blob signers doesn't support query parameter authentication Some blob store providers allow using signed requests in two forms: either with an Authorization header or with query parameters. Using the query parameter form of the signed request is necessary for clients that don't support adding a header or for returning a redirect where the Authorization header would be removed.

The jclouds atmos, aws-s3, and azure implementations generate signed requests only with the Authorization header, while cloudfiles-us, hpcloud-objectstorage use query parameters. Ideally the BlobRequestSigner interface would allow the user to decide which form of signed request to create.",", , , "
"   Move Class,Pull Up Method,Pull Up Attribute,","support multi-part upload for generic S3 Generic S3 providers like Cloudian support multi-part upload:

http://www.cloudian.com/cloud-storage-products/cloudian-cloud-storage-platform.html

jclouds should move the AWS-specific provider support to the generic S3 API.",", Duplicated Code, Duplicated Code, "
"   Rename Method,Extract Method,",Add SecurityGroupExtension support for Nova 0,"Duplicated Code, Long Method, , "
"   Rename Method,",Migrate unit tests from ChefApiTest to ChefApiExpectTest Unit tests in ChefApiTest.java are using old way. It is good to migrate them to new way as we have done in ChefApiExpectTest,", "
"   Rename Method,",Migrate unit tests from ChefApiTest to ChefApiExpectTest Unit tests in ChefApiTest.java are using old way. It is good to migrate them to new way as we have done in ChefApiExpectTest,", "
"   Rename Method,","Use the Omnibus installer to install the Chef client Currently, when running Chef Solo or Chef to bootstrap a node, the Chef client is manually installed. The installation process installs Ruby, RubyGems, and the Chef gems.

Since gems in ruby are automatically updated and the latest version is downloaded by default, this procedure may get broken when a backwards incompatible version of the chef-client gem is published. This can be workarouned by specifying the versions of Ruby and the Chef gems to install when creating a ChefContext or building a ChefSolo script, but still isn't strong enough.

Opscode released Omnibus, to easily install the chef-client and all its dependencies. The Omnibus installer will detect the operating system of the node and install an appropriate Ruby and RubyGems distribution compatible with the desired chef-client version in a transparent and isolated way, so it should be the preferred way to install Chef. This will simplify a lot the Chef bootstrap script and reduce considerably the points where failures can happen.

Chef installation using the gems should still be supported, for users that already have an installed Ruby or want more control on what is installed.",", "
"   Rename Method,","Use the Omnibus installer to install the Chef client Currently, when running Chef Solo or Chef to bootstrap a node, the Chef client is manually installed. The installation process installs Ruby, RubyGems, and the Chef gems.

Since gems in ruby are automatically updated and the latest version is downloaded by default, this procedure may get broken when a backwards incompatible version of the chef-client gem is published. This can be workarouned by specifying the versions of Ruby and the Chef gems to install when creating a ChefContext or building a ChefSolo script, but still isn't strong enough.

Opscode released Omnibus, to easily install the chef-client and all its dependencies. The Omnibus installer will detect the operating system of the node and install an appropriate Ruby and RubyGems distribution compatible with the desired chef-client version in a transparent and isolated way, so it should be the preferred way to install Chef. This will simplify a lot the Chef bootstrap script and reduce considerably the points where failures can happen.

Chef installation using the gems should still be supported, for users that already have an installed Ruby or want more control on what is installed.",", "
"   Rename Method,Extract Method,","Jcloud support for custom chef environment. Jcloud support for custom chef environment.
Trying to bootstrap a node in a different environment that _default.
Is there a support for that","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Jcloud support for custom chef environment. Jcloud support for custom chef environment.
Trying to bootstrap a node in a different environment that _default.
Is there a support for that","Duplicated Code, Long Method, , "
"   Rename Method,","Disable S3 virtual host buckets for generic S3 Not all S3-compatible providers support virtual host buckets and thus
we should disable this feature by default. Continue to enable virtual
host buckets for AWS-S3 which supports this although this feature
suffers from DNS settling issues.",", "
"   Rename Method,",Add SecurityGroupExtension support to GCE 0,", "
"   Rename Method,Extract Method,","Allow Image preference logic to be supplied to TemplateBuilder I'd like to tie in to jclouds's TemplateBuilder so that I can use its filtering/matching capabilities but then use custom logic to determine which of the matching images is ""best"" (for my custom value of best, i.e. preferred). 

The driving use case is that I want a portable way to say ""any recent ubuntu or centos"". Normally this is the default of course but it doesn't always do the right thing when other options are specified -- broken ubuntu ""alpha"" images in AWS being the worst offender (asking for 16gb RAM in us-west-1 gives back an awful ubuntu 8.04 alpha, for instance!). It would also handle use cases where someone wants to say ""Ubuntu 11 or 12, or CentOS 6.x, is best. failing that, Ubuntu 10 or CentOS 5.x. (and never any alpha images!)"".

I'm thinking allowing to set an `imageSorter(Ordering)`. This fits with how
the TemplateBuilderImpl currently works (it already uses an Ordering, you just can't change it; and it stays in line with the naming convention of `Sorter` as in `Ordering hardwareSorter()`.)","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Allow Image preference logic to be supplied to TemplateBuilder I'd like to tie in to jclouds's TemplateBuilder so that I can use its filtering/matching capabilities but then use custom logic to determine which of the matching images is ""best"" (for my custom value of best, i.e. preferred). 

The driving use case is that I want a portable way to say ""any recent ubuntu or centos"". Normally this is the default of course but it doesn't always do the right thing when other options are specified -- broken ubuntu ""alpha"" images in AWS being the worst offender (asking for 16gb RAM in us-west-1 gives back an awful ubuntu 8.04 alpha, for instance!). It would also handle use cases where someone wants to say ""Ubuntu 11 or 12, or CentOS 6.x, is best. failing that, Ubuntu 10 or CentOS 5.x. (and never any alpha images!)"".

I'm thinking allowing to set an `imageSorter(Ordering)`. This fits with how
the TemplateBuilderImpl currently works (it already uses an Ordering, you just can't change it; and it stays in line with the naming convention of `Sorter` as in `Ordering hardwareSorter()`.)","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Allow Image preference logic to be supplied to TemplateBuilder I'd like to tie in to jclouds's TemplateBuilder so that I can use its filtering/matching capabilities but then use custom logic to determine which of the matching images is ""best"" (for my custom value of best, i.e. preferred). 

The driving use case is that I want a portable way to say ""any recent ubuntu or centos"". Normally this is the default of course but it doesn't always do the right thing when other options are specified -- broken ubuntu ""alpha"" images in AWS being the worst offender (asking for 16gb RAM in us-west-1 gives back an awful ubuntu 8.04 alpha, for instance!). It would also handle use cases where someone wants to say ""Ubuntu 11 or 12, or CentOS 6.x, is best. failing that, Ubuntu 10 or CentOS 5.x. (and never any alpha images!)"".

I'm thinking allowing to set an `imageSorter(Ordering)`. This fits with how
the TemplateBuilderImpl currently works (it already uses an Ordering, you just can't change it; and it stays in line with the naming convention of `Sorter` as in `Ordering hardwareSorter()`.)","Duplicated Code, Long Method, , "
"   Rename Method,","Allow creating nodes through ComputeService with explicitly specified names Currently, instance naming for nodes created through ComputeService.createNodesInGroup() etc uses a combination of the specified group name and GroupNamingConvention's unique suffix - generally that's a three character random string, but for EC2 it's the id string for the instance. While this is fine for many cases where the instance name doesn't need to be referenced directly by actual humans, say, it's a pain for those cases. Currently, you can work around this by creating instances through the per-api/provider clients/apis, or through some hacks for single instance creation through ComputeService utilizing provider-specific TemplateOptions classes, but there's no generalized way to get real control over the names given to instances through ComputeService. This should be possible.",", "
"   Move Method,Move Attribute,","remove Async interface from all apis and providers In jclouds 1.6.0, we deprecated all async interfaces. Let's use this issue and subtasks to track removing it and all supporting code from 1.7.x (current master)",", , , "
"   Rename Class,Move And Rename Class,","remove Async interface from all apis and providers In jclouds 1.6.0, we deprecated all async interfaces. Let's use this issue and subtasks to track removing it and all supporting code from 1.7.x (current master)",", "
"   Move Method,Move Attribute,","remove Async interface from all apis and providers In jclouds 1.6.0, we deprecated all async interfaces. Let's use this issue and subtasks to track removing it and all supporting code from 1.7.x (current master)",", , , "
"   Rename Class,Move And Rename Class,","remove Async interface from all apis and providers In jclouds 1.6.0, we deprecated all async interfaces. Let's use this issue and subtasks to track removing it and all supporting code from 1.7.x (current master)",", "
"   Rename Method,","remove Async interface from all apis and providers In jclouds 1.6.0, we deprecated all async interfaces. Let's use this issue and subtasks to track removing it and all supporting code from 1.7.x (current master)",", "
"   Rename Method,","remove Async interface from all apis and providers In jclouds 1.6.0, we deprecated all async interfaces. Let's use this issue and subtasks to track removing it and all supporting code from 1.7.x (current master)",", "
"   Move Class,Move Method,Move Attribute,","remove Async interface from all apis and providers In jclouds 1.6.0, we deprecated all async interfaces. Let's use this issue and subtasks to track removing it and all supporting code from 1.7.x (current master)",", , , "
"   Rename Class,Move And Rename Class,","remove Async interface from all apis and providers In jclouds 1.6.0, we deprecated all async interfaces. Let's use this issue and subtasks to track removing it and all supporting code from 1.7.x (current master)",", "
"   Rename Class,Move And Rename Class,","remove Async interface from all apis and providers In jclouds 1.6.0, we deprecated all async interfaces. Let's use this issue and subtasks to track removing it and all supporting code from 1.7.x (current master)",", "
"   Move Class,Move Method,Move Attribute,","remove Async interface from all apis and providers In jclouds 1.6.0, we deprecated all async interfaces. Let's use this issue and subtasks to track removing it and all supporting code from 1.7.x (current master)",", , , "
"   Rename Method,","Avoid InputSupplier<InputStream>; support ByteSource ByteSource has convenience methods and avoids generics notational overhead. Guava is moving towards this:

https://groups.google.com/forum/#!msg/guava-discuss/bChfnnXb9QA/xlmy2UzsmpsJ

Note that ByteSource implements InputSupplier<InputStream> so we should retain source compatibility.",", "
"   Rename Method,","Avoid InputSupplier<InputStream>; support ByteSource ByteSource has convenience methods and avoids generics notational overhead. Guava is moving towards this:

https://groups.google.com/forum/#!msg/guava-discuss/bChfnnXb9QA/xlmy2UzsmpsJ

Note that ByteSource implements InputSupplier<InputStream> so we should retain source compatibility.",", "
"   Rename Method,","Avoid InputSupplier<InputStream>; support ByteSource ByteSource has convenience methods and avoids generics notational overhead. Guava is moving towards this:

https://groups.google.com/forum/#!msg/guava-discuss/bChfnnXb9QA/xlmy2UzsmpsJ

Note that ByteSource implements InputSupplier<InputStream> so we should retain source compatibility.",", "
"   Rename Method,",support Java 8 We should ensure compatibility with Java 8 in advance of its GA release in March. Presently several jclouds tests in core fail due to new collection methods and HashMap ordering differences. I have not yet tested further. We should backport any fixes to 1.7.x.,", "
"   Rename Method,",support Java 8 We should ensure compatibility with Java 8 in advance of its GA release in March. Presently several jclouds tests in core fail due to new collection methods and HashMap ordering differences. I have not yet tested further. We should backport any fixes to 1.7.x.,", "
"   Rename Method,",The OpenStack KeyPairApi is missing get() org.jclouds.openstack.nova.v2_0.extensions.KeyPairApi is missing the get() method.,", "
"   Rename Method,",The OpenStack KeyPairApi is missing get() org.jclouds.openstack.nova.v2_0.extensions.KeyPairApi is missing the get() method.,", "
"   Rename Method,Extract Method,","Google Cloud Storage support Presently users can access Google Cloud Storage via the S3-compatible API, although native support would give access to durable reduced availability, better region support, and resumable uploads.","Duplicated Code, Long Method, , "
"   Move Class,Move Method,","Google Cloud Storage support Presently users can access Google Cloud Storage via the S3-compatible API, although native support would give access to durable reduced availability, better region support, and resumable uploads.",", , "
"   Rename Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,",Refactor SoftLayer support Current SoftLayer design is a bit complicated to be maintained. Looking at SoftLayer Python client I think we should get inspiration to modernize and simplify SoftLayer CCI support.,"Duplicated Code, Long Method, , , , , "
"   Rename Class,Push Down Method,Push Down Attribute,","support AWS signature version 4 On Mar 15, 2012, Amazon announced a more secure way to sign api requests.

https://forums.aws.amazon.com/ann.jspa?annID=1398

New AWS regions, such as Frankfurt, will not support version 2 which jclouds presently uses:

http://docs.aws.amazon.com/AmazonS3/latest/API/sig-v4-authenticating-requests.html

Eventhough some AWS clones like openstack support both versions, not all do. jclouds should support both versions.

This is most important to address for s3 and ec2.",", , , "
"   Rename Class,Push Down Method,Push Down Attribute,Move Attribute,","support AWS signature version 4 On Mar 15, 2012, Amazon announced a more secure way to sign api requests.

https://forums.aws.amazon.com/ann.jspa?annID=1398

New AWS regions, such as Frankfurt, will not support version 2 which jclouds presently uses:

http://docs.aws.amazon.com/AmazonS3/latest/API/sig-v4-authenticating-requests.html

Eventhough some AWS clones like openstack support both versions, not all do. jclouds should support both versions.

This is most important to address for s3 and ec2.",", , , , "
"   Rename Class,Push Down Method,Push Down Attribute,Move Attribute,","support AWS signature version 4 On Mar 15, 2012, Amazon announced a more secure way to sign api requests.

https://forums.aws.amazon.com/ann.jspa?annID=1398

New AWS regions, such as Frankfurt, will not support version 2 which jclouds presently uses:

http://docs.aws.amazon.com/AmazonS3/latest/API/sig-v4-authenticating-requests.html

Eventhough some AWS clones like openstack support both versions, not all do. jclouds should support both versions.

This is most important to address for s3 and ec2.",", , , , "
"   Rename Class,Push Down Method,Push Down Attribute,Move Attribute,","support AWS signature version 4 On Mar 15, 2012, Amazon announced a more secure way to sign api requests.

https://forums.aws.amazon.com/ann.jspa?annID=1398

New AWS regions, such as Frankfurt, will not support version 2 which jclouds presently uses:

http://docs.aws.amazon.com/AmazonS3/latest/API/sig-v4-authenticating-requests.html

Eventhough some AWS clones like openstack support both versions, not all do. jclouds should support both versions.

This is most important to address for s3 and ec2.",", , , , "
"   Rename Method,","Add support for arbitrary CPU and RAM in the ComputeService Some providers such as Abiquo or CloudSigma do not have the concept of Hardware Profiles and allow users to specify arbitrary CPU and RAM values.

The current ComputeService abstraction assumes all providers have Hardware Profiles, and forces implementations of such providers to provide a fixed (hardcoded) list just to conform the interface.

It would be great to modernize the Compute workflow so Hardware Profiles are not mandatory and users can manually set their values when needed.",", "
"   Rename Class,Rename Method,Extract Method,","Enhancements to availability zones api Availability zones API is an openstack extension and is misused in several places, including some tests. I am linking to a PR that makes the extension optional and makes a few live tests work no matter if the provider supports the extension or not.","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,Extract Method,","Enhancements to availability zones api Availability zones API is an openstack extension and is misused in several places, including some tests. I am linking to a PR that makes the extension optional and makes a few live tests work no matter if the provider supports the extension or not.","Duplicated Code, Long Method, , "
"   Rename Method,","Replace hand-written domain classes with Auto-Value ones In doing maintenance and ports, I've noticed that we have drift related to using guava to implement hashCode/equals on domain classes. Having an opportunity for a guava incompatibility on something like this is not high value, in my opinion. Moreover, we have a lot of other inconsistency in our value classes, which have caused bugs, and extra review time on pull requests.

Auto-Value generates concrete implementations and takes out the possibility of inconsistency of field names, Nullability, etc. It is handled at compile time, so doesn't introduce a dependency of note, nor a chance of guava version conflict for our users.

https://github.com/google/auto/tree/master/value

While it may be the case that we need custom gson adapters (ex opposed to the ConstructorAnnotation approach), or a revision to our approach, I believe that this work is worthwhile.

While it is the case that our Builders won't be generated, I still think this is valuable. For example, in many cases, we shouldn't be making Builders anyway (ex. they are read-only objects never instantiated, such as lists). Even if we choose to still write Builders, the problem is isolated there.",", "
"   Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Replace hand-written domain classes with Auto-Value ones In doing maintenance and ports, I've noticed that we have drift related to using guava to implement hashCode/equals on domain classes. Having an opportunity for a guava incompatibility on something like this is not high value, in my opinion. Moreover, we have a lot of other inconsistency in our value classes, which have caused bugs, and extra review time on pull requests.

Auto-Value generates concrete implementations and takes out the possibility of inconsistency of field names, Nullability, etc. It is handled at compile time, so doesn't introduce a dependency of note, nor a chance of guava version conflict for our users.

https://github.com/google/auto/tree/master/value

While it may be the case that we need custom gson adapters (ex opposed to the ConstructorAnnotation approach), or a revision to our approach, I believe that this work is worthwhile.

While it is the case that our Builders won't be generated, I still think this is valuable. For example, in many cases, we shouldn't be making Builders anyway (ex. they are read-only objects never instantiated, such as lists). Even if we choose to still write Builders, the problem is isolated there.","Duplicated Code, Long Method, , , , , "
"   Rename Class,Rename Method,","Replace hand-written domain classes with Auto-Value ones In doing maintenance and ports, I've noticed that we have drift related to using guava to implement hashCode/equals on domain classes. Having an opportunity for a guava incompatibility on something like this is not high value, in my opinion. Moreover, we have a lot of other inconsistency in our value classes, which have caused bugs, and extra review time on pull requests.

Auto-Value generates concrete implementations and takes out the possibility of inconsistency of field names, Nullability, etc. It is handled at compile time, so doesn't introduce a dependency of note, nor a chance of guava version conflict for our users.

https://github.com/google/auto/tree/master/value

While it may be the case that we need custom gson adapters (ex opposed to the ConstructorAnnotation approach), or a revision to our approach, I believe that this work is worthwhile.

While it is the case that our Builders won't be generated, I still think this is valuable. For example, in many cases, we shouldn't be making Builders anyway (ex. they are read-only objects never instantiated, such as lists). Even if we choose to still write Builders, the problem is isolated there.",", "
"   Rename Method,","Replace hand-written domain classes with Auto-Value ones In doing maintenance and ports, I've noticed that we have drift related to using guava to implement hashCode/equals on domain classes. Having an opportunity for a guava incompatibility on something like this is not high value, in my opinion. Moreover, we have a lot of other inconsistency in our value classes, which have caused bugs, and extra review time on pull requests.

Auto-Value generates concrete implementations and takes out the possibility of inconsistency of field names, Nullability, etc. It is handled at compile time, so doesn't introduce a dependency of note, nor a chance of guava version conflict for our users.

https://github.com/google/auto/tree/master/value

While it may be the case that we need custom gson adapters (ex opposed to the ConstructorAnnotation approach), or a revision to our approach, I believe that this work is worthwhile.

While it is the case that our Builders won't be generated, I still think this is valuable. For example, in many cases, we shouldn't be making Builders anyway (ex. they are read-only objects never instantiated, such as lists). Even if we choose to still write Builders, the problem is isolated there.",", "
"   Rename Class,Rename Method,","Replace hand-written domain classes with Auto-Value ones In doing maintenance and ports, I've noticed that we have drift related to using guava to implement hashCode/equals on domain classes. Having an opportunity for a guava incompatibility on something like this is not high value, in my opinion. Moreover, we have a lot of other inconsistency in our value classes, which have caused bugs, and extra review time on pull requests.

Auto-Value generates concrete implementations and takes out the possibility of inconsistency of field names, Nullability, etc. It is handled at compile time, so doesn't introduce a dependency of note, nor a chance of guava version conflict for our users.

https://github.com/google/auto/tree/master/value

While it may be the case that we need custom gson adapters (ex opposed to the ConstructorAnnotation approach), or a revision to our approach, I believe that this work is worthwhile.

While it is the case that our Builders won't be generated, I still think this is valuable. For example, in many cases, we shouldn't be making Builders anyway (ex. they are read-only objects never instantiated, such as lists). Even if we choose to still write Builders, the problem is isolated there.",", "
"   Rename Method,Extract Method,Inline Method,","Replace hand-written domain classes with Auto-Value ones In doing maintenance and ports, I've noticed that we have drift related to using guava to implement hashCode/equals on domain classes. Having an opportunity for a guava incompatibility on something like this is not high value, in my opinion. Moreover, we have a lot of other inconsistency in our value classes, which have caused bugs, and extra review time on pull requests.

Auto-Value generates concrete implementations and takes out the possibility of inconsistency of field names, Nullability, etc. It is handled at compile time, so doesn't introduce a dependency of note, nor a chance of guava version conflict for our users.

https://github.com/google/auto/tree/master/value

While it may be the case that we need custom gson adapters (ex opposed to the ConstructorAnnotation approach), or a revision to our approach, I believe that this work is worthwhile.

While it is the case that our Builders won't be generated, I still think this is valuable. For example, in many cases, we shouldn't be making Builders anyway (ex. they are read-only objects never instantiated, such as lists). Even if we choose to still write Builders, the problem is isolated there.","Duplicated Code, Long Method, , , "
"   Rename Method,","Replace hand-written domain classes with Auto-Value ones In doing maintenance and ports, I've noticed that we have drift related to using guava to implement hashCode/equals on domain classes. Having an opportunity for a guava incompatibility on something like this is not high value, in my opinion. Moreover, we have a lot of other inconsistency in our value classes, which have caused bugs, and extra review time on pull requests.

Auto-Value generates concrete implementations and takes out the possibility of inconsistency of field names, Nullability, etc. It is handled at compile time, so doesn't introduce a dependency of note, nor a chance of guava version conflict for our users.

https://github.com/google/auto/tree/master/value

While it may be the case that we need custom gson adapters (ex opposed to the ConstructorAnnotation approach), or a revision to our approach, I believe that this work is worthwhile.

While it is the case that our Builders won't be generated, I still think this is valuable. For example, in many cases, we shouldn't be making Builders anyway (ex. they are read-only objects never instantiated, such as lists). Even if we choose to still write Builders, the problem is isolated there.",", "
"   Rename Method,","Replace hand-written domain classes with Auto-Value ones In doing maintenance and ports, I've noticed that we have drift related to using guava to implement hashCode/equals on domain classes. Having an opportunity for a guava incompatibility on something like this is not high value, in my opinion. Moreover, we have a lot of other inconsistency in our value classes, which have caused bugs, and extra review time on pull requests.

Auto-Value generates concrete implementations and takes out the possibility of inconsistency of field names, Nullability, etc. It is handled at compile time, so doesn't introduce a dependency of note, nor a chance of guava version conflict for our users.

https://github.com/google/auto/tree/master/value

While it may be the case that we need custom gson adapters (ex opposed to the ConstructorAnnotation approach), or a revision to our approach, I believe that this work is worthwhile.

While it is the case that our Builders won't be generated, I still think this is valuable. For example, in many cases, we shouldn't be making Builders anyway (ex. they are read-only objects never instantiated, such as lists). Even if we choose to still write Builders, the problem is isolated there.",", "
"   Rename Method,","S3 should retry on 500 InternalError jclouds retries on several errors, but not on 500 InternalError with S3. The description suggests that we should retry, ""We encountered an internal error. Please try again.""",", "
"   Move Class,Pull Up Method,Move Method,",support multi-delete for generic S3 Generic S3 providers like DreamObjects support multi-delete for objects. jclouds should move the AWS-specific provider support to the generic S3 API.,", , Duplicated Code, "
"   Move Method,Move Attribute,","Expose component operations of multipart upload Presently jclouds exposes multipart upload via a simple interface:

{code:java}
blobStore.putBlob(containerName, blob, new PutOptions().multipart(true));
{code}

This does not allow more complicated interactions such as parallel uploads, uploads with unknown content-lengths, and other interfaces like writing into an {{OutputStream}}. Further the current {{MultipartUploadStrategy}} implementations duplicate code across the azureblob, gcs, and s3 providers.

I propose to expose the MPU component operations, e.g., initiate, complete, abort, and upload part, via the {{BlobStore}} abstraction. This will allow us to address all the above features.",", , , "
"   Move And Rename Class,","Move ListenerList implementations of interfaces into the interface itself A universal paradigm in Pivot is to have a ""listener"" interface for a class or data structure that is used to notify listeners of changes in the class/data. There is then an ""Adapter"" static class in the interface file that implements the interface with default implementations. Then there is a very separate enclosed static class that implements the ""ListenerList"" interface of that listener interface. And usually (or always) this ""listener list"" class is defined/used only in the class that needs to notify the listeners. However, this class must be very parallel to not only the interface itself, but also the ""Adapter"" class, and yet it is in a different place. 

So, it seems somewhat reasonable to move all these ""listener list"" classes into the interfaces themselves, so all three related things are located in the same file. A preliminary POC of this concept was done with ""Query.java"", and ""QueryListener.java"" and it looks good. 

This doesn't seem to require changes to client code, because the accessor methods only refer to ""ListenerList<....>"" and not to the listener list class itself (in order to be more general, of course), but which helps us to hide the implementing class away inside the interface. 

I will attach the diff of the POC, to hopefully make this more clear. It may seem a somewhat nebulous concept, but the idea is to keep ""like things"" together for clarity.",", "
"   Move And Rename Class,","Move ListenerList implementations of interfaces into the interface itself A universal paradigm in Pivot is to have a ""listener"" interface for a class or data structure that is used to notify listeners of changes in the class/data. There is then an ""Adapter"" static class in the interface file that implements the interface with default implementations. Then there is a very separate enclosed static class that implements the ""ListenerList"" interface of that listener interface. And usually (or always) this ""listener list"" class is defined/used only in the class that needs to notify the listeners. However, this class must be very parallel to not only the interface itself, but also the ""Adapter"" class, and yet it is in a different place. 

So, it seems somewhat reasonable to move all these ""listener list"" classes into the interfaces themselves, so all three related things are located in the same file. A preliminary POC of this concept was done with ""Query.java"", and ""QueryListener.java"" and it looks good. 

This doesn't seem to require changes to client code, because the accessor methods only refer to ""ListenerList<....>"" and not to the listener list class itself (in order to be more general, of course), but which helps us to hide the implementing class away inside the interface. 

I will attach the diff of the POC, to hopefully make this more clear. It may seem a somewhat nebulous concept, but the idea is to keep ""like things"" together for clarity.",", "
"   Move And Rename Class,","Move ListenerList implementations of interfaces into the interface itself A universal paradigm in Pivot is to have a ""listener"" interface for a class or data structure that is used to notify listeners of changes in the class/data. There is then an ""Adapter"" static class in the interface file that implements the interface with default implementations. Then there is a very separate enclosed static class that implements the ""ListenerList"" interface of that listener interface. And usually (or always) this ""listener list"" class is defined/used only in the class that needs to notify the listeners. However, this class must be very parallel to not only the interface itself, but also the ""Adapter"" class, and yet it is in a different place. 

So, it seems somewhat reasonable to move all these ""listener list"" classes into the interfaces themselves, so all three related things are located in the same file. A preliminary POC of this concept was done with ""Query.java"", and ""QueryListener.java"" and it looks good. 

This doesn't seem to require changes to client code, because the accessor methods only refer to ""ListenerList<....>"" and not to the listener list class itself (in order to be more general, of course), but which helps us to hide the implementing class away inside the interface. 

I will attach the diff of the POC, to hopefully make this more clear. It may seem a somewhat nebulous concept, but the idea is to keep ""like things"" together for clarity.",", "
"   Move And Rename Class,Move Method,","Move ListenerList implementations of interfaces into the interface itself A universal paradigm in Pivot is to have a ""listener"" interface for a class or data structure that is used to notify listeners of changes in the class/data. There is then an ""Adapter"" static class in the interface file that implements the interface with default implementations. Then there is a very separate enclosed static class that implements the ""ListenerList"" interface of that listener interface. And usually (or always) this ""listener list"" class is defined/used only in the class that needs to notify the listeners. However, this class must be very parallel to not only the interface itself, but also the ""Adapter"" class, and yet it is in a different place. 

So, it seems somewhat reasonable to move all these ""listener list"" classes into the interfaces themselves, so all three related things are located in the same file. A preliminary POC of this concept was done with ""Query.java"", and ""QueryListener.java"" and it looks good. 

This doesn't seem to require changes to client code, because the accessor methods only refer to ""ListenerList<....>"" and not to the listener list class itself (in order to be more general, of course), but which helps us to hide the implementing class away inside the interface. 

I will attach the diff of the POC, to hopefully make this more clear. It may seem a somewhat nebulous concept, but the idea is to keep ""like things"" together for clarity.",", , "
"   Move And Rename Class,","Move ListenerList implementations of interfaces into the interface itself A universal paradigm in Pivot is to have a ""listener"" interface for a class or data structure that is used to notify listeners of changes in the class/data. There is then an ""Adapter"" static class in the interface file that implements the interface with default implementations. Then there is a very separate enclosed static class that implements the ""ListenerList"" interface of that listener interface. And usually (or always) this ""listener list"" class is defined/used only in the class that needs to notify the listeners. However, this class must be very parallel to not only the interface itself, but also the ""Adapter"" class, and yet it is in a different place. 

So, it seems somewhat reasonable to move all these ""listener list"" classes into the interfaces themselves, so all three related things are located in the same file. A preliminary POC of this concept was done with ""Query.java"", and ""QueryListener.java"" and it looks good. 

This doesn't seem to require changes to client code, because the accessor methods only refer to ""ListenerList<....>"" and not to the listener list class itself (in order to be more general, of course), but which helps us to hide the implementing class away inside the interface. 

I will attach the diff of the POC, to hopefully make this more clear. It may seem a somewhat nebulous concept, but the idea is to keep ""like things"" together for clarity.",", "
"   Move And Rename Class,","Move ListenerList implementations of interfaces into the interface itself A universal paradigm in Pivot is to have a ""listener"" interface for a class or data structure that is used to notify listeners of changes in the class/data. There is then an ""Adapter"" static class in the interface file that implements the interface with default implementations. Then there is a very separate enclosed static class that implements the ""ListenerList"" interface of that listener interface. And usually (or always) this ""listener list"" class is defined/used only in the class that needs to notify the listeners. However, this class must be very parallel to not only the interface itself, but also the ""Adapter"" class, and yet it is in a different place. 

So, it seems somewhat reasonable to move all these ""listener list"" classes into the interfaces themselves, so all three related things are located in the same file. A preliminary POC of this concept was done with ""Query.java"", and ""QueryListener.java"" and it looks good. 

This doesn't seem to require changes to client code, because the accessor methods only refer to ""ListenerList<....>"" and not to the listener list class itself (in order to be more general, of course), but which helps us to hide the implementing class away inside the interface. 

I will attach the diff of the POC, to hopefully make this more clear. It may seem a somewhat nebulous concept, but the idea is to keep ""like things"" together for clarity.",", "
"   Move And Rename Class,","Move ListenerList implementations of interfaces into the interface itself A universal paradigm in Pivot is to have a ""listener"" interface for a class or data structure that is used to notify listeners of changes in the class/data. There is then an ""Adapter"" static class in the interface file that implements the interface with default implementations. Then there is a very separate enclosed static class that implements the ""ListenerList"" interface of that listener interface. And usually (or always) this ""listener list"" class is defined/used only in the class that needs to notify the listeners. However, this class must be very parallel to not only the interface itself, but also the ""Adapter"" class, and yet it is in a different place. 

So, it seems somewhat reasonable to move all these ""listener list"" classes into the interfaces themselves, so all three related things are located in the same file. A preliminary POC of this concept was done with ""Query.java"", and ""QueryListener.java"" and it looks good. 

This doesn't seem to require changes to client code, because the accessor methods only refer to ""ListenerList<....>"" and not to the listener list class itself (in order to be more general, of course), but which helps us to hide the implementing class away inside the interface. 

I will attach the diff of the POC, to hopefully make this more clear. It may seem a somewhat nebulous concept, but the idea is to keep ""like things"" together for clarity.",", "
"   Move And Rename Class,","Move ListenerList implementations of interfaces into the interface itself A universal paradigm in Pivot is to have a ""listener"" interface for a class or data structure that is used to notify listeners of changes in the class/data. There is then an ""Adapter"" static class in the interface file that implements the interface with default implementations. Then there is a very separate enclosed static class that implements the ""ListenerList"" interface of that listener interface. And usually (or always) this ""listener list"" class is defined/used only in the class that needs to notify the listeners. However, this class must be very parallel to not only the interface itself, but also the ""Adapter"" class, and yet it is in a different place. 

So, it seems somewhat reasonable to move all these ""listener list"" classes into the interfaces themselves, so all three related things are located in the same file. A preliminary POC of this concept was done with ""Query.java"", and ""QueryListener.java"" and it looks good. 

This doesn't seem to require changes to client code, because the accessor methods only refer to ""ListenerList<....>"" and not to the listener list class itself (in order to be more general, of course), but which helps us to hide the implementing class away inside the interface. 

I will attach the diff of the POC, to hopefully make this more clear. It may seem a somewhat nebulous concept, but the idea is to keep ""like things"" together for clarity.",", "
"   Move And Rename Class,","Move ListenerList implementations of interfaces into the interface itself A universal paradigm in Pivot is to have a ""listener"" interface for a class or data structure that is used to notify listeners of changes in the class/data. There is then an ""Adapter"" static class in the interface file that implements the interface with default implementations. Then there is a very separate enclosed static class that implements the ""ListenerList"" interface of that listener interface. And usually (or always) this ""listener list"" class is defined/used only in the class that needs to notify the listeners. However, this class must be very parallel to not only the interface itself, but also the ""Adapter"" class, and yet it is in a different place. 

So, it seems somewhat reasonable to move all these ""listener list"" classes into the interfaces themselves, so all three related things are located in the same file. A preliminary POC of this concept was done with ""Query.java"", and ""QueryListener.java"" and it looks good. 

This doesn't seem to require changes to client code, because the accessor methods only refer to ""ListenerList<....>"" and not to the listener list class itself (in order to be more general, of course), but which helps us to hide the implementing class away inside the interface. 

I will attach the diff of the POC, to hopefully make this more clear. It may seem a somewhat nebulous concept, but the idea is to keep ""like things"" together for clarity.",", "
"   Move And Rename Class,","Move ListenerList implementations of interfaces into the interface itself A universal paradigm in Pivot is to have a ""listener"" interface for a class or data structure that is used to notify listeners of changes in the class/data. There is then an ""Adapter"" static class in the interface file that implements the interface with default implementations. Then there is a very separate enclosed static class that implements the ""ListenerList"" interface of that listener interface. And usually (or always) this ""listener list"" class is defined/used only in the class that needs to notify the listeners. However, this class must be very parallel to not only the interface itself, but also the ""Adapter"" class, and yet it is in a different place. 

So, it seems somewhat reasonable to move all these ""listener list"" classes into the interfaces themselves, so all three related things are located in the same file. A preliminary POC of this concept was done with ""Query.java"", and ""QueryListener.java"" and it looks good. 

This doesn't seem to require changes to client code, because the accessor methods only refer to ""ListenerList<....>"" and not to the listener list class itself (in order to be more general, of course), but which helps us to hide the implementing class away inside the interface. 

I will attach the diff of the POC, to hopefully make this more clear. It may seem a somewhat nebulous concept, but the idea is to keep ""like things"" together for clarity.",", "
"   Move And Rename Class,","Move ListenerList implementations of interfaces into the interface itself A universal paradigm in Pivot is to have a ""listener"" interface for a class or data structure that is used to notify listeners of changes in the class/data. There is then an ""Adapter"" static class in the interface file that implements the interface with default implementations. Then there is a very separate enclosed static class that implements the ""ListenerList"" interface of that listener interface. And usually (or always) this ""listener list"" class is defined/used only in the class that needs to notify the listeners. However, this class must be very parallel to not only the interface itself, but also the ""Adapter"" class, and yet it is in a different place. 

So, it seems somewhat reasonable to move all these ""listener list"" classes into the interfaces themselves, so all three related things are located in the same file. A preliminary POC of this concept was done with ""Query.java"", and ""QueryListener.java"" and it looks good. 

This doesn't seem to require changes to client code, because the accessor methods only refer to ""ListenerList<....>"" and not to the listener list class itself (in order to be more general, of course), but which helps us to hide the implementing class away inside the interface. 

I will attach the diff of the POC, to hopefully make this more clear. It may seem a somewhat nebulous concept, but the idea is to keep ""like things"" together for clarity.",", "
"   Move And Rename Class,","Move ListenerList implementations of interfaces into the interface itself A universal paradigm in Pivot is to have a ""listener"" interface for a class or data structure that is used to notify listeners of changes in the class/data. There is then an ""Adapter"" static class in the interface file that implements the interface with default implementations. Then there is a very separate enclosed static class that implements the ""ListenerList"" interface of that listener interface. And usually (or always) this ""listener list"" class is defined/used only in the class that needs to notify the listeners. However, this class must be very parallel to not only the interface itself, but also the ""Adapter"" class, and yet it is in a different place. 

So, it seems somewhat reasonable to move all these ""listener list"" classes into the interfaces themselves, so all three related things are located in the same file. A preliminary POC of this concept was done with ""Query.java"", and ""QueryListener.java"" and it looks good. 

This doesn't seem to require changes to client code, because the accessor methods only refer to ""ListenerList<....>"" and not to the listener list class itself (in order to be more general, of course), but which helps us to hide the implementing class away inside the interface. 

I will attach the diff of the POC, to hopefully make this more clear. It may seem a somewhat nebulous concept, but the idea is to keep ""like things"" together for clarity.",", "
"   Move And Rename Class,","Move ListenerList implementations of interfaces into the interface itself A universal paradigm in Pivot is to have a ""listener"" interface for a class or data structure that is used to notify listeners of changes in the class/data. There is then an ""Adapter"" static class in the interface file that implements the interface with default implementations. Then there is a very separate enclosed static class that implements the ""ListenerList"" interface of that listener interface. And usually (or always) this ""listener list"" class is defined/used only in the class that needs to notify the listeners. However, this class must be very parallel to not only the interface itself, but also the ""Adapter"" class, and yet it is in a different place. 

So, it seems somewhat reasonable to move all these ""listener list"" classes into the interfaces themselves, so all three related things are located in the same file. A preliminary POC of this concept was done with ""Query.java"", and ""QueryListener.java"" and it looks good. 

This doesn't seem to require changes to client code, because the accessor methods only refer to ""ListenerList<....>"" and not to the listener list class itself (in order to be more general, of course), but which helps us to hide the implementing class away inside the interface. 

I will attach the diff of the POC, to hopefully make this more clear. It may seem a somewhat nebulous concept, but the idea is to keep ""like things"" together for clarity.",", "
"   Rename Class,Move And Rename Class,","Move ListenerList implementations of interfaces into the interface itself A universal paradigm in Pivot is to have a ""listener"" interface for a class or data structure that is used to notify listeners of changes in the class/data. There is then an ""Adapter"" static class in the interface file that implements the interface with default implementations. Then there is a very separate enclosed static class that implements the ""ListenerList"" interface of that listener interface. And usually (or always) this ""listener list"" class is defined/used only in the class that needs to notify the listeners. However, this class must be very parallel to not only the interface itself, but also the ""Adapter"" class, and yet it is in a different place. 

So, it seems somewhat reasonable to move all these ""listener list"" classes into the interfaces themselves, so all three related things are located in the same file. A preliminary POC of this concept was done with ""Query.java"", and ""QueryListener.java"" and it looks good. 

This doesn't seem to require changes to client code, because the accessor methods only refer to ""ListenerList<....>"" and not to the listener list class itself (in order to be more general, of course), but which helps us to hide the implementing class away inside the interface. 

I will attach the diff of the POC, to hopefully make this more clear. It may seem a somewhat nebulous concept, but the idea is to keep ""like things"" together for clarity.",", "
"   Move And Rename Class,Move Method,","Move the tutorial ""Ruler"" class into the mainline code for use by others The ""Ruler"" class (and associated Skin and Listener classes) already exists in two places in the tutorials branch, so it should go into the main ""wtk"" code so it can be used by others who might find it useful.",", , "
"   Move Class,Move Method,","Move the tutorial ""Ruler"" class into the mainline code for use by others The ""Ruler"" class (and associated Skin and Listener classes) already exists in two places in the tutorials branch, so it should go into the main ""wtk"" code so it can be used by others who might find it useful.",", , "
"   Rename Method,","Create a new ""gauge"" object which can display a single value in a circular ""speedometer"" fashion For instance, in a monitoring application, things like CPU or disk usage can nicely be displayed as a circular gauge, showing a single (the ""current"") value.  This paradigm is currently being used in such products as Apache Ambari (where it is called a ""Gauge"" widget).",", "
"   Move Method,Inline Method,","Don't set selected index in Rollup and Expander until expand transition is complete Because the expand transition doesn't start until the selection change event has already fired, outside listeners have no way of knowing when the transition will end. This prevents a listener from calling scrollAreaToVisible() when the container has fully expanded, for example.

",", , , "
"   Rename Method,Push Down Method,","Change Locale on Validators Unable to change the Locale used in Validators.

For example, in TextInputValidatorTest, in the textinputFloatRange I've seen that giving inputs in the current Locale all works good, but I haven't find a way to change the Locale, or to set a different format to validators ... how can i do this ? 
Maybe the simplest thing could be to set a different Locale and let validators using it, in this case all the application would use only a Locale, but i think this is a common case (only a few times i had to manage more locales in the same application, showing more at the same time to the same user ... ). The best could be to have new settings alive without having to restart the application.

I have to run my applications in a multi-locale environment (but any user with its default or chosen locale) ... thanks.

As a sample, a portion of code that uses this could go for example in TextInputValidatorTest.


Hints for the implementation:

(1) make DecimalValidator not abstract - the reason it is currently abstract is a historical hold-over
(2) make the DecimalValidator constructors public
(3) modify the various *Validator classes to have extra constructors that take a Locale parameter.
",", , "
"   Rename Class,Rename Method,Move Method,",Complete TextArea 0,", , "
"   Rename Method,Extract Method,","Context menu handler Create framework-level support for context menus by creating a Component.ContextMenuHandler interface. When the display host detects a right click, it will:

1) Get a reference to the lowest-level component that the mouse is over, and if that's non-null...
2) Construct the path from the display to that component, instantiate a MenuPopup
3) Walk the path, passing the menu popup's menu to each ContextMenuHandler that is found along the path
4) Open the menu popup if the menu is non-empty after the path has been walked","Duplicated Code, Long Method, , "
"   Rename Method,Move Method,Inline Method,","Clean up multiple selection implementation Convert Span to a ""struct""; replace SpanSequence with ListSelection and ListSelectionSequence. Return an instance of ListSelectionSequence from ListView#getSelectedRanges() and TableView#getSelectedRanges(). This will avoid the need to copy the selection contents. Fire selected range add and remove events only for ranges that are actually added and removed (don't re-fire for existing selections).

",", , , "
"   Extract Superclass,Extract Method,","Add a collapsible flag to Expander When set to false, the skin would not present the user with any way to collapse the expander. the component would throw if a caller tried to set it to be an invalid state (not collapsible and not expanded).","Duplicated Code, Long Method, , Duplicated Code, Large Class, "
"   Rename Method,",Change selected to highlighted in TablePane Row & Column Selected is the wrong term here. This will also cause the selectionBackgroundColor property to change to highlightBackgroundColor.,", "
"   Rename Method,","Eliminate Component ""displayable"" property; use ""visible"" for this purpose Currently, some skins treat the displayable flag a ""preferred visibility"". This may not be the correct interpretation. Arguably, a non-displayable component should simply be taken out of the flow, not made invisible.

Also, some skins currently respect the displayable flag that may not need to (e.g. WindowSkin).

We should eliminate this property and use a component's visibility instead. This is the approach taken by AWT. We should also review which containers are currently trying to respect ""displayable"" and remove this code where it doesn't make sense.

",", "
"   Extract Method,Inline Method,","Allow Containers to obtain the keyboard focus This will allow us to resolve issue PIVOT-213 as well as address some other potential use cases that we can't currently handle (for example, making TextArea a container; this will allow TextArea to act as a highly customizable form, similar to how forms are supported in HTML). 

Additionally, focusable containers are supported by other windowing toolkits including AWT, so this will also eliminate a negative comparison point.

","Duplicated Code, Long Method, , , "
"   Rename Method,","Allow Containers to obtain the keyboard focus This will allow us to resolve issue PIVOT-213 as well as address some other potential use cases that we can't currently handle (for example, making TextArea a container; this will allow TextArea to act as a highly customizable form, similar to how forms are supported in HTML). 

Additionally, focusable containers are supported by other windowing toolkits including AWT, so this will also eliminate a negative comparison point.

",", "
"   Rename Method,Move Method,Move Attribute,","Create new ButtonGroup class that implements org.apache.pivot.collections.Group and Iterable<Button> Move Button.Group out into its own top-level class that implements org.apache.pivot.collections.Group. Currently, there is no way to enumerate a group's contents, and it seems like there should be (Swing's ButtonGroup class does support this).

We'd end up with something along these lines:

ButtonGroup : Group, Iterable<Button> {
add(Button):void
remove(Button):boolean

getSelection():Button
setSelection(Button):void
}

ButtonGroupListener {
selectionChanged(previousSelection:Button):void
}

Button {
...

getGroup():ButtonGroup
setGroup(ButtonGroup):void
}

ButtonGroup#add() and remove() would call Button#setGroup(), and vice versa. Similarly, ButtonGroup#setSelection() would call Button#setSelected(), and vice versa. We'd also move the named group dictionary to ButtonGroup.

",", , , "
"   Rename Method,","Make component editors fire events There are several use cases that call for event notifications from the editors (TableViewRowEditor, TreeViewNodeEditor, and ListViewItemEditor). These include preview events as well. Here's the associated API change:

public interface Editor {
public boolean isEditing();
- public void save();
+ public void saveChanges();
- public void cancel();
+ public void cancelEdit();
}

TableView {
public interface RowEditor extends Editor {
- public void edit(TableView tableView, int rowIndex, int columnIndex);
+ public void editRow(TableView tableView, int rowIndex, int columnIndex);
+ public ListenerList<RowEditorListener> getRowEditorListeners();
}

public interface RowEditorListener {
public Vote previewEditRow(RowEditor rowEditor, TableView tableView, int rowIndex, int columnIndex);
public void editRowVetoed(RowEditor rowEditor, Vote reason);
public void rowEditing(RowEditor rowEditor, TableView tableView, int rowIndex, int columnIndex);
public Vote previewSaveChanges(RowEditor rowEditor, TableView tableView, int rowIndex, int columnIndex, Dictionary<String, Object> changes);
public void saveChangesVetoed(RowEditor rowEditor, Vote reason);
public void changesSaved(RowEditor rowEditor, TableView tableView, int rowIndex, int columnIndex);
public void editCancelled(RowEditor rowEditor, TableView tableView, int rowIndex, int columnIndex);
}
}

TreeView {
public interface NodeEditor extends Editor {
- public void edit(TreeView treeView, Path path);
+ public void editNode(TreeView treeView, Path path);
+ public ListenerList<NodeEditorListener> getNodeEditorListeners();
}

public interface NodeEditorListener {
public Vote previewEditNode(NodeEditor nodeEditor, TreeView treeView, Path path);
public void editNodeVetoed(NodeEditor nodeEditor, Vote reason);
public void nodeEditing(NodeEditor nodeEditor, TreeView treeView, Path path);
public Vote previewSaveChanges(NodeEditor nodeEditor, TreeView treeView, Path path, Object changes);
public void saveChangesVetoed(NodeEditor nodeEditor, Vote reason);
public void changesSaved(NodeEditor nodeEditor, TreeView treeView, Path path);
public void editCancelled(NodeEditor nodeEditor, TreeView treeView, Path path);
}
}

ListView {
public interface ItemEditor extends Editor {
- public void edit(ListView listView, int index);
+ public void editItem(ListView listView, int index);
+ public ListenerList<ItemEditorListener> getItemEditorListeners();
}

public interface ItemEditorListener {
public Vote previewEditItem(ItemEditor itemEditor, ListView listView, int index);
public void editItemVetoed(ItemEditor itemEditor, Vote reason);
public void itemEditing(ItemEditor itemEditor, ListView listView, int index);
public Vote previewSaveChanges(ItemEditor itemEditor, ListView listView, int index, Object changes);
public void saveChangesVetoed(ItemEditor itemEditor, Vote reason);
public void changesSaved(ItemEditor itemEditor, ListView listView, int index);
public void editCancelled(ItemEditor itemEditor, ListView listView, int index);
}
}
",", "
"   Rename Method,","Add a ""variableItemHeight"" style to TerraListViewSkin Setting this value to ""true"" would ease the restriction that all items in a list view are the same height. When true, the height of each item would be determined by asking the renderer for its preferred height.
",", "
"   Rename Class,Rename Method,","Add DisabledCheckmarkFilter to TreeView and ListView This will allow targetted checkboxes to be enabled/disabled.

/**
* Returns the disabled checkmark filter.
*
* @return
* The disabled checkmark filter, or <tt>null</tt> if no disabled checkmark filter is
* set.
*/
public Filter<?> getDisabledCheckmarkFilter();

/**
* Sets the disabled checkmark filter.
*
* @param disabledItemFilter
* The disabled checkmark filter, or <tt>null</tt> for no disabled checkmark filter.
*/
public void setDisabledCheckmarkFilter(Filter<?> disabledCheckmarkFilter);",", "
"   Rename Method,Extract Method,","Renderers should be passed index/path ListView and TableView renderers would be passed the item/row index, and TreeView's renderer would be passed the row index as well as the node's path.
","Duplicated Code, Long Method, , "
"   Rename Method,Inline Method,","Make CalendarDate a struct-like class We're not using the mutator methods on CalendarDate, so we might as well make a struct-like class like we did with Bounds, Point, Dimensions, etc.",", , "
"   Rename Method,Extract Method,","Add a ""variableRowHeight"" style to TerraTableViewSkin 0","Duplicated Code, Long Method, , "
"   Move Method,Extract Method,Inline Method,Push Down Attribute,Move Attribute,","Provide a means for programmatically detecting current application context For example, define static ""isActive()"" methods on DesktopApplicationContext and BrowserApplicationContext.
","Duplicated Code, Long Method, , , , , , "
"   Rename Method,","Rename TextInput and TextArea insertText() to insert() Also remove index argument. This will be more consistent with delete() and will helps to clarify the intent of the method (which is to replace the current selection with the inserted text).
",", "
"   Pull Up Method,Extract Method,Pull Up Attribute,","Add a ""repeatable"" property to ListButton I found this to be a requirement in my application: if the user clicks the label part of a LinkButton, the button should fire immediately without showing the popup, thus invoking the action with the currently selected entry. If the user clicks the triangle part, the popup should be shown.

I patched LinkButton to add a new boolean property called ""showPopupOnTriggerClickOnly"" to TerraListButtonSkin (I couldn't think of a better name, sorry). If set to true, the ListButton popup will only show up if the user clicks the triangle, but not if the user clicks the rest of the button. However, ButtonPressListeners fire as usual, if the user clicks any part of the button. If the property is false, the behavior is as it was before. The default value of the property is false.

I tested the patch in my application and ComponentExplorer and it works good.

It would be nice if you integrate the patch. Otherwise I'd still have the option to write a custom skin, but I think this patch could be interesting to other developers as well.","Duplicated Code, Long Method, , Duplicated Code, Duplicated Code, "
"   Rename Method,","Rename alternateRowColor style to alternateRowBackgroundColor Also, add a new alternateRowColor style that will define the foreground color for alternate rows.

Apply this change to both TerrraTableViewSkin and TerraListViewSkin.
",", "
"   Rename Method,","Window and DesktopApplicationContext should support java.awt.Window.setIconImages() to display multi-resolution icons DesktopApplicationContext currently sets the Window icon into the host frame using java.awt.Window.setIconImage(Image). However, since Java6 there's a new method called setIconImages(List<Image> icons) to set multi-resolution icons, e.g. you can set a 16x16 icon for the window title bar, a 48x48 icon to be shown in Microsoft Windows' task switcher, or a 64x64 icon to be shown in Windows Vista/7 Aero task list. (It is up to the runtime implementation to choose an appropriate icon from the list provided by setIconImages().)

Without the possibility to set multi-resolution icons the OS scales the image either up or down to fit the current environment. This might lead to poor results.

Suggestion: add a new method setIconImages(List<Image>) method to pivot.wtk.Window and, if set, use it in DesktopApplicationContext. Or maybe add setIconPictures(List<Picture>) to get around the issue of downcasting to Picture in updateFrameTitleBar().

Thanks,
Dirk.",", "
"   Rename Method,","Rename Alert/Prompt getSelectedOption() to getSelectedOptionIndex() The current method name implies that the option value will be returned, rather than the option index. Renaming the method will eliminate this ambiguity. A new getSelectedOption() method should be added that returns the actual option value.
",", "
"   Rename Class,Move And Rename Class,Move Method,Move Attribute,","Move message processing functionality to pivot-core Move the subscribe(), unsubscribe(), and sendMessage() methods from ApplicationContext to a new org.apache.pivot.util.MessageBus class; rename ApplicationContextMessageListener to org.apache.pivot.util.MessageBusListener.
",", , , "
"   Rename Method,","Bindable improvements Add arguments to Bindable#initialize() to provide the caller access to the serializer's namespace, resources, and location. This will allow untrusted applications to take advantage of Bindable without needing to use the BXML annotation, among other things.
",", "
"   Rename Method,","Fire selection change events when selection changes indirectly Currently, selection change events are fired only when an explicit call has been made that affects the selection. For example, in ListView, calling either setSelectedRanges() or clearSelection() will fire this event. However, an operation that indirectly changes the selection state (such as adding or removing an item from the ListView's model data) does not trigger an event. This was originally done by design - selectedRangesChanged() includes the previous selection as an argument, and we didn't want to have to manually re-construct that every time the selection changed as a side effect of a model change:

public void selectedRangesChanged(ListView listView, Sequence<Span> previousSelectedRanges);

However, in practice, working within this model can be challenging. More than once I have registered a selection change listener expecting to receive notification of all selection changes, forgetting that it is not designed that way. I'm guessing that other developers may be confused by this as well.

So, I am proposing that components that maintain a selection state also fire selection change events when the selection changes indirectly. In this case, a null value would be passed for the previous selection. This will save the effort of re-constructing the previous selection info and will give the listener additional information about the nature of the change (i.e. null == indirect state change).

This change should also be propagated to TextInput, which has a similar issue with character change events. Currently, TextInput fires character change events via TextInputCharacterListener and text change events via TextInputTextListener. The textChanged() event does not pass the previous text value, which is inconsistent with other change events. textChanged() should be incorporated into TextInputCharacterListener and should pass the previous text value when it is changed via an explicit call to setText(); otherwise, it should pass null.

The updated version of TextArea should probably follow the same approach.

",", "
"   Push Down Method,Extract Method,","Allow <bxml:include> tag to include arbitrary content This will support a number of additional use cases including externalizing styles.
","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Make tab pane button and accordion header content and renderer configurable Currently, TabPane button content is specified via the TabPane.label and TabPane.icon attached properties. Accordion header content is set via Accordion.label and Accordion.icon. This limits the content of these buttons to an icon and/or a text string.

It might be better to allow callers to specify data for these buttons directly. The attached label and icon properties could be replaced with ""tabButtonData"" and ""panelHeaderData"" (or simply ""buttonData"" and headerData""), and each container could allow a caller to specify a renderer (via get/setButtonDataRenderer() and get/setHeaderDataRenderer(), respectively).

","Duplicated Code, Long Method, , "
"   Rename Method,","Make tab pane button and accordion header content and renderer configurable Currently, TabPane button content is specified via the TabPane.label and TabPane.icon attached properties. Accordion header content is set via Accordion.label and Accordion.icon. This limits the content of these buttons to an icon and/or a text string.

It might be better to allow callers to specify data for these buttons directly. The attached label and icon properties could be replaced with ""tabButtonData"" and ""panelHeaderData"" (or simply ""buttonData"" and headerData""), and each container could allow a caller to specify a renderer (via get/setButtonDataRenderer() and get/setHeaderDataRenderer(), respectively).

",", "
"   Move And Rename Class,Rename Method,",Allow caller to specific table view header renderer on a per-column basis 0,", "
"   Move Class,Rename Method,Move Attribute,","Allow serializers to fire events as data is read This will allow callers to hook into the serialization process and update the UI incrementally, rather than waiting for readObject() to return.
",", , "
"   Rename Class,Extract Interface,Rename Method,Move Method,Extract Method,","Rollup should differentiate between user-added and skin-added components Rollup should differentiate between user-added and skin-added components, like TabPane, etc. Ideally, this might be accomplished by defining ""heading"" and ""content"" components, similar to how ScrollPane defines ""view"", ""rowHeader"", and ""columnHeader"". Strictly speaking, we don't need to support an arbitrary number of child components in a Rollup, since this could be handled by setting the content component to a container.

","Duplicated Code, Long Method, , , Large Class, "
"   Rename Method,Inline Method,","Provide automated support for setting enum values By adding support for converting strings to enum values in BeanAdapter.coerce(), we can potentially eliminate all of the conversion overloads we are currently using.
",", , "
"   Rename Method,"," Component#userData should allow multiple clients to co-exist It is very handy to be able to stash extra info on a component, but when multiple libraries all want to do this, it quickly gets out of hand.

When multiple libraries all want to do this, they can conflict e.g. if I add LibraryA and LibraryB to my pivot application, and both libraries want to enhance standard Pivot behaviour by stashing stuff in the Component, they
will overwrite each others UserData.

A better solution would be to use a map of values with methods like
void putUserData(Class key, Object userdata)
Object getUserData(Class key)",", "
"   Push Down Method,Push Down Attribute,","extending Pivot collections with Monads and functional methods Add some usuful methods to Pivot Collections like those needed to process all elements, going more ""function style"", like addAll, contains, etc ...
To simplify usage from people coming from other languages, try to align to other languages names/conventions (for what is possible) , like Scala, C# .
Verify even if add a interface (defining a single method) that could be passed to some methods to contain logic for processing all elements (like the apply() in Scala), and chose a right name for it (maybe function or other similar).

Some info here:
http://apache-pivot-developers.417237.n3.nabble.com/Some-idea-on-extending-Pivot-collections-td3321472.html

But for more changes in Collections (still to be discussed), wait the 3.0 ...
",", , , "
"   Rename Method,","extending Pivot collections with Monads and functional methods Add some usuful methods to Pivot Collections like those needed to process all elements, going more ""function style"", like addAll, contains, etc ...
To simplify usage from people coming from other languages, try to align to other languages names/conventions (for what is possible) , like Scala, C# .
Verify even if add a interface (defining a single method) that could be passed to some methods to contain logic for processing all elements (like the apply() in Scala), and chose a right name for it (maybe function or other similar).

Some info here:
http://apache-pivot-developers.417237.n3.nabble.com/Some-idea-on-extending-Pivot-collections-td3321472.html

But for more changes in Collections (still to be discussed), wait the 3.0 ...
",", "
"   Rename Method,",Ability for the file manager to ingest a file in place.  There are cases where we have some products in a directory and we want to ingest and catalog them without actually moving them to a new location. We call this kind of ingestion an in place ingestion. It is very useful for the file manager to be able to ingest in place.,", "
"   Rename Method,","Make Resource Manager FAILURE and SUCCESS aware instead of just COMPLETE aware This patch adds a FAILURE and a SUCCESS status to the resource manager, alerting users to Jobs that have arrived in either state. Currently, the resource manager only understands whether a job is COMPLETE or not.",", "
"   Rename Method,","XMLPS should be able to stream large results Currently, XMLPS stores *ALL* the rows of a ResultSet in a CDEResult object. In addition, this CDEResult is converted to a String for the HTTPResponse, nearly doubling the memory usage. With large results, heap space can easily run out, despite increasing max heap space for the servlet container (e.g. -Xmx1024m). XMLPS should be able to stream/chunk its results, taking into consideration the following:
# ResultSets _represent_ an iterable collection of rows without actually _storing_ all the rows in memory
# HTTPResponse#getOutputStream() offers a streaming response, where chunked transfer-encoding and content-length headers are managed automatically by the servlet container (such as Tomcat)
",", "
"   Rename Method,Inline Method,","Integrate CAS protocol into PushPull Now that Brian's got CAS Protocol done, let's integrate it into PushPull.",", , "
"   Rename Method,","Changes to Curator web app to better support high frequency updates. From: https://reviews.apache.org/r/8684/

The patch contain two changes to the Curator update metadata methods:

o The new method, used by VFASTR, was converted to use the XML-RPC client instead of an embedded catalog, so that only one CAS catalog access the back-end store at a time

o The old method, used by other existing systems, was converted to use a shared Catalog instance, as opposed to create a new Catalog instance for each request. This is necessary to minimize use of resources, such as database connections.

Note that the second update method should also be converted to use the XML-RPC interface, but only when a proper testing platform is available.",", "
"   Rename Method,Move Method,Extract Method,Move Attribute,",Make Resource Manager work without Ganglia Disabling the resource monitor in Assignment Monitor feature required.,"Duplicated Code, Long Method, , , , "
"   Extract Method,Inline Method,","Add support for local sessions This improvement is in the bucket of making ZooKeeper work at a large scale. We are planning on having about a 1 million clients connect to a ZooKeeper ensemble through a set of 50-100 observers. Majority of these clients are read only - ie they do not do any updates or create ephemeral nodes.

In ZooKeeper today, the client creates a session and the session creation is handled like any other update. In the above use case, the session create/drop workload can easily overwhelm an ensemble. The following is a proposal for a ""local session"", to support a larger number of connections.

1. The idea is to introduce a new type of session - ""local"" session. A ""local"" session doesn't have a full functionality of a normal session.
2. Local sessions cannot create ephemeral nodes.
3. Once a local session is lost, you cannot re-establish it using the session-id/password. The session and its watches are gone for good.
4. When a local session connects, the session info is only maintained on the zookeeper server (in this case, an observer) that it is connected to. The leader is not aware of the creation of such a session and there is no state written to disk.
5. The pings and expiration is handled by the server that the session is connected to.

With the above changes, we can make ZooKeeper scale to a much larger number of clients without making the core ensemble a bottleneck.

In terms of API, there are two options that are being considered
1. Let the client specify at the connect time which kind of session do they want.
2. All sessions connect as local sessions and automatically get promoted to global sessions when they do an operation that requires a global session (e.g. creating an ephemeral node)

Chubby took the approach of lazily promoting all sessions to global, but I don't think that would work in our case, where we want to keep sessions which never create ephemeral nodes as always local. Option 2 would make it more broadly usable but option 1 would be easier to implement.

We are thinking of implementing option 1 as the first cut. There would be a client flag, IsLocalSession (much like the current readOnly flag) that would be used to determine whether to create a local session or a global session.


","Duplicated Code, Long Method, , , "
"   Move Class,Extract Interface,","Enabling a large number of watches for a large number of clients In my ZooKeeper, I see watch manager consuming several GB of memory and I dug a bit deeper.

In the scenario I am testing, I have 10K clients connected to an observer. There are about 20K znodes in ZooKeeper, each is about 1K - so about 20M data in total.
Each client fetches and puts watches on all the znodes. That is 200 million watches.

It seems a single watch takes about 100 bytes. I am currently at 14528037 watches and according to the yourkit profiler, WatchManager has 1.2 G already. This is not going to work as it might end up needing 20G of RAM just for the watches.

So we need a more compact way of storing watches. Here are the possible solutions.
1. Use a bitmap instead of the current hashmap. In this approach, each znode would get a unique id when its gets created. For every session, we can keep track of a bitmap that indicates the set of znodes this session is watching. A bitmap, assuming a 100K znodes, would be 12K. For 10K sessions, we can keep track of watches using 120M instead of 20G.
2. This second idea is based on the observation that clients watch znodes in sets (for example all znodes under a folder). Multiple clients watch the same set and the total number of sets is a couple of orders of magnitude smaller than the total number of znodes. In my scenario, there are about 100 sets. So instead of keeping track of watches at the znode level, keep track of it at the set level. It may mean that get may also need to be implemented at the set level. With this, we can save the watches in 100M.


Are there any other suggestions of solutions?

Thanks


",", Large Class, "
"   Rename Method,","improve ZxidRolloverTest (test seems flakey) In our jenkins job to run the ZooKeeper unit tests, org.apache.zookeeper.server.ZxidRolloverTest sometimes fails.

E.g.,

{noformat}
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /foo0
at org.apache.zookeeper.KeeperException.create(KeeperException.java:90)
at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)
at org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:815)
at org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:843)
at org.apache.zookeeper.server.ZxidRolloverTest.checkNodes(ZxidRolloverTest.java:154)
at org.apache.zookeeper.server.ZxidRolloverTest.testRolloverThenRestart(ZxidRolloverTest.java:211)
{noformat}
",", "
"   Extract Method,Inline Method,","Support multiple ZooKeeper client, with different configurations, in a single JVM I have two ZK client in one JVM, one is secure client and second is normal client (For non secure cluster).

""zookeeper.sasl.client"" system property is ""true"" by default, because of this my second client connection is failing.

We should pass all client configurations in client constructor like HDFS client.

For example :
{code}
public ZooKeeper(String connectString, int sessionTimeout, Watcher watcher, Configuration conf) throws IOException
{
......
......
}
{code}","Duplicated Code, Long Method, , , "
"   Move Class,Extract Interface,Move Method,Move Attribute,","Eliminate using statics to initialize the sever. Should allow server to be more embeddable in OSGi enviorments. Patrick request I open up this in issue in this [email thread|http://n2.nabble.com/ActiveMQ-is-now-using-ZooKeeper-td1573272.html]

The main culprit I've noticed is:
{code}
ServerStats.registerAsConcrete();
{code}

But there may be others.",", , , Large Class, "
"   Rename Method,Inline Method,",Cleanup and fixes to BookKeeper We observed one race condition when multiple threads try to write concurrently. This patch should fix it. I will also remove some commented code.,", , "
"   Rename Method,Extract Method,","Add check to validate dataDir and dataLogDir parameters at startup According to  -ZOOKEEPER-2960- we should at a startup check to validate that dataDir and dataLogDir parameters are set correctly. 

Perhaps we should introduce a check of some kind? If datalogdir is different that datadir and snapshots exist in datalogdir we throw an exception and quit.","Duplicated Code, Long Method, , "
"   Rename Method,","Zookeeper client supports IPv6 address and document the ""IPV6 feature"" This issue is the follow-up work of [ZOOKEEPER-3057|https://issues.apache.org/jira/browse/ZOOKEEPER-3057] 
1.ZK server side supports ipv6 style like this: server.1=[2001:db8:1::242:ac11:2]:2888:3888,but zk client side supports ipv6 like this:2001:db8:1::242:ac11:2:2181.we need unify them. 
Look at the kafka example [KAFKA-1123|https://issues.apache.org/jira/browse/KAFKA-1123]. its producer client also supports ipv6 like this: [2001:db8:1::242:ac11:2] 
2.document the ""IPV6 feature"" to let user know.",", "
"   Move Class,Pull Up Method,Pull Up Attribute,","Refactor QuorumPeerMainTest.java: move commonly used functions to base class Move the following methods to QuorumPeerTestBase.java: 
- tearDown() 
- LaunchServers() 
- waitForOne(), waitForAll() 
- logStates() 
",", Duplicated Code, Duplicated Code, "
"   Pull Up Method,Extract Method,Pull Up Attribute,","Use session map to improve the performance when closing session in Netty Previously, it needs to go through all the cnxns to find out the session to close, which is O(N), N is the total connections we have. 

This will affect the performance of close session or renew session if there are lots of connections on this server, this JIRA is going to reuse the session map code in NIO implementation to improve the performance.","Duplicated Code, Long Method, , Duplicated Code, Duplicated Code, "
"   Rename Method,Extract Method,Inline Method,","Masking bookie failure during writes to a ledger The idea of this jira is to work out the changes necessary to make a client mask the failure of a bookie while writing to a ledger. I'm submitting a preliminary patch, but before I submit a final one, I need to have 288 committed.","Duplicated Code, Long Method, , , "
"   Move Class,Move Method,Extract Method,Move Attribute,","headers (version+) in log/snap files Moved from SourceForge to Apache.
http://sourceforge.net/tracker/index.php?func=detail&aid=1961767&group_id=209147&atid=1008547","Duplicated Code, Long Method, , , , "
"   Move Class,Move Method,Extract Method,","bookkeeper should have a streaming api so that its easier to store checpoints/snapshots in bookkeeper. currently, bookkeeper api allows just a bytes interface which is 
ld.write(bytes).

We should have an interface like 

Stream s = ledger.createStream() (I am not very sure of the interface right now but will post a more concrete one after giving it a little more thought)

now this stream can be used to wirte checkpoints as 
s.write(bytes)
and then closed to s.close() to close the snapshot. 
This api could use the current api to implement snapshots as chunks of bytes (buffered by stream s) that can be written via ld.write(bytes).
","Duplicated Code, Long Method, , , "
"   Rename Method,","Asynchronous version of createLedger() While there are async versions for read and write, there is no async version for creating a ledger. This can cause applications to have to change their whole thread design. 

It should be easier and more consistent to add an async version of createLedger().",", "
"   Rename Class,Rename Method,Pull Up Method,Extract Method,Pull Up Attribute,","Refactor Followers and related classes into a Peer->Follower hierarchy in preparation for Observers For the Observers patch (ZOOKEEPER-368), a lot of functionality is shared between Followers and Observers. To avoid copying code, it makes sense to push the common code into a parent Peer class and specialise it for Followers and Observers. At the same time, some of the lengthier methods in Follower can be broken up to make the code more readable.","Duplicated Code, Long Method, , Duplicated Code, Duplicated Code, "
"   Rename Method,","Server supports listening on a specified network address The issue in maililist is located:
http://mail-archives.apache.org/mod_mbox/hadoop-zookeeper-user/200912.mbox/%3c4ac0d28c0912210242g58230a9ds1c55361561c70d61@mail.gmail.com%3e

I have checked the server size code, seems no this option provided. This feature is useful when we have more than two network interfaces, one for Internet and others for intranet. We want to run ZooKeeper in our intranet and not be exposed to outside world.",", "
"   Move Class,Move Method,Move Attribute,","Make the ZooKeeperServer more DI friendly Proposed changes were discussed in [this mailing list thread|http://mail-archives.apache.org/mod_mbox/hadoop-zookeeper-dev/200807.mbox/%3Caf2843cd0807180907v44b310bg232be99ac0b47a27@mail.gmail.com%3E]:

Basic goals are: 
* Decouple the current configuration system from the public API. I
see stuff like ZooKeeperServer being coupled to ServerConfig a bit.
* Allow the use of setter injection in addition to constructor
injection. This is the most important thing needed to let spring more
easily configure the objects.
* Move the main() methods out of the ZooKeeperServer class.


",", , , "
"   Rename Method,Extract Method,",Optimize memory usage Look at the way component keeps it's state and optimize the memory usage,"Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,",Backport WicketFilter from 2.0 to 1.3 Wicket 2.0 uses a filter for serving Wicket pages and resources. This should be backported into the 1.x stream.,"Duplicated Code, Long Method, , "
"   Rename Method,Move Method,Inline Method,Move Attribute,","Make Application Class More Bean-ish The Application class has getters for properties like applicationSettings and securitySettings. Couldn't we make those properties writable also? I realize that the internal implementation might have to change a bit. Currently, the Settings class implements all of those interfaces and it uses a single instance of Settings by default. The reason that I want this is so that I can set up my Application object in Spring and access it via wicket-spring. The current implementation of Application doesn't facilitate the ""set up"" part very well.",", , , , "
"   Rename Method,","set session locale when constructing session object Currently, we create the session objects like this:

WebApplication#getSession:

if (session == null)
{
// Create session using session factory
session = getSessionFactory().newSession(request);
// Set the client Locale for this session
session.setLocale(request.getLocale());

What I propose is to change the constructor from Session/ WebSession to take in a Locale parameter as well. That would make it possible for custom session classes to fix the locale by setting it in the constructor. Now that is only possible by overriding Session#getLocale

",", "
"   Rename Method,","set session locale when constructing session object Currently, we create the session objects like this:

WebApplication#getSession:

if (session == null)
{
// Create session using session factory
session = getSessionFactory().newSession(request);
// Set the client Locale for this session
session.setLocale(request.getLocale());

What I propose is to change the constructor from Session/ WebSession to take in a Locale parameter as well. That would make it possible for custom session classes to fix the locale by setting it in the constructor. Now that is only possible by overriding Session#getLocale

",", "
"   Rename Method,","StringRequestTarget is bloated and needs some care when looking at StringRequestTarget I found the following things unnecessary

(1) create a Charset object where a String is sufficient for the encoding
(2) Write into a stream first, then read back from it and write to the response stream using an internal buffer
(3) flush the output stream
(4) having to specify the charset in the 'contentType' and again in the 'charset' parameter.

I made up an own version of StringRequestTarget and attached a patch for it.",", "
"   Pull Up Method,Extract Method,","RadioGroup/CheckGroup should support embedded RadioGroups/CheckGroups it should be possible to have markup like this

<span wicket:id=""checkgroup1"">
<span wicket:id=""checkgroup2"">
<table>
<tr wicket:id=""repeater"">
<td><input wicket:id=""check-for-group1""/></td>
<td><input wicket:id=""check-for-group2""/></td>
</tr>
</table>

the constructor of check should take an additional reference to the check group it belongs to","Duplicated Code, Long Method, , Duplicated Code, "
"   Rename Method,","Remove IComponentBorder in favor of IBehavior Yeah i think we can depricate IComponentBorder in 1.4 and point to IBehavior
before/after
and then remove it in 1.5 and maybe remote also **

public *final* Component setComponentBorder(*final* IComponentBorder border)
in component or make it

*

public
* *final* Component setComponentBorder(*final* IBehavior border)

to make it more clear to people that a border can be set by using a behavior
(else who knows..)
- Hide quoted text -

On Sat, May 16, 2009 at 12:21, Juergen Donnerstag <
juergen.donnerstag@gmail.com> wrote:

> Hi,
>
> question: looking at that code
>
> // Call implementation to render component
> final IComponentBorder border =
> getComponentBorder();
> if (border != null)
> {
> border.renderBefore(this);
> }
> notifyBehaviorsComponentBeforeRender();
> onRender(markupStream);
> notifyBehaviorsComponentRendered();
> if (border != null)
> {
> border.renderAfter(this);
> }
>
> IComponentBorder could be implemented via a behavior as well. Is there
> any reason why IComponentBorder needs this special treatment and why
> it is not implemented as a behavior?
>
> -Juergen
>
-----------------
> Yeah i think we can depricate IComponentBorder in 1.4 and point to IBehavior
> before/after

i like the ComponentBorder stuff, because you can set it without
influencing any beahavior or component stuff.. just render some content
before and after a component..

i use it in debugging environment

mm:)
------ 
Wait for some and give everybody a chance to comment",", "
"   Rename Method,","IChainingModel implementation This is a direct implementation of IChainingModel, largely taken from AbstractPropertyModel. It needs to be generic typing for 1.4.",", "
"   Pull Up Method,Pull Up Attribute,","IChainingModel implementation This is a direct implementation of IChainingModel, largely taken from AbstractPropertyModel. It needs to be generic typing for 1.4.",", Duplicated Code, Duplicated Code, "
"   Rename Method,",Improve diagnostics on serialization exceptions The JDK's default serialization exception doesn't give a whole lot of information other than the object that didn't implement serializable. We can try to improve on this by giving more info of the object tree that was being serialized,", "
"   Move And Rename Class,Rename Method,Extract Method,",Improve diagnostics on serialization exceptions The JDK's default serialization exception doesn't give a whole lot of information other than the object that didn't implement serializable. We can try to improve on this by giving more info of the object tree that was being serialized,"Duplicated Code, Long Method, , "
"   Move And Rename Class,Inline Method,Move Attribute,",Improve diagnostics on serialization exceptions The JDK's default serialization exception doesn't give a whole lot of information other than the object that didn't implement serializable. We can try to improve on this by giving more info of the object tree that was being serialized,", , , "
"   Move And Rename Class,Rename Method,Extract Method,",Improve diagnostics on serialization exceptions The JDK's default serialization exception doesn't give a whole lot of information other than the object that didn't implement serializable. We can try to improve on this by giving more info of the object tree that was being serialized,"Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Locate .properties files using the same convention as markup files original inquiry on mailing list
http://www.mail-archive.com/users@wicket.apache.org/msg47803.html

--
I am curious. Why are .properties files not located in the same way as .html? I've overridden:

[ResourceStreamLocator]
public IResourceStream locate( Class<?> clazz, String aPath, String aStyle, Locale aLocale, String anExtension )

I notice that property file locating doesn't invoke this method, but only invokes the lesser arg version with the style/variation/locale already embedded in the path. This is an inconvenience for me because I'm trying to inspect the style during location. Perhaps I shouldn't be doing what I'm trying to do, but after reading the docs, I expected locating to work the way it does for .html, but .properties threw me.
","Duplicated Code, Long Method, , "
"   Move Class,Move Method,",InjectorHolder.getInjector().inject(this) doesn't work with wicket-guice Using Wicket-Guice integration for dependency injection does not provide the helper mechanism 'InjectorHolder.getInjector().inject(this)' for classes which do not extend o.a.w.Component.,", , "
"   Rename Class,Extract Method,","Make visitor api cleaner clean up the visitor api so we are not returning magic objects like IVisitor.CONTINUE and IVisitor.CONTINUE_BUT_DO_NOT_GO_DEEPER. it should also be possible to return null from the visitors, currently reserved for IVisitor.CONTINUE","Duplicated Code, Long Method, , "
"   Rename Method,","[Patch] Allow to access html resources if they are not associated with a java class Html files can be used as static resources but in Wicket1.X they are not accessible because PackageResourceGuard blocks html extension files. I understand for ""application"" html files (associated with a java class) but not for ohers. The attachement allow ""non-application html file"" to be used as resources

I also saw In trunk html files can be used as ""static"" resources.
why all html files? Why in trunk?",", "
"   Rename Method,","Refactor / rework pageability I need to add a NavigatorLabel to a GridView, now NavigatorLabel has constructors accepting DataTable, DataView and PageableListView but not a GridView.
Instead of creating a new constructor accepting a GridView, I think it would be far better to merge IPageable and the private NavigatorLabel.PageableComponent into a single public interface and making all pageable components implement this interface.
I understand this approach breaks APIs and will need one major version, maybe two allowing for deprecations in between.
A less breaking alternative (but not as good in my opinion) is to change the constructor accepting a DataView to make it accept an AbstractPageableView, which is a common ancestor of both DataView and GridView and defines the methods needed to satisfy NavigatorLabel.PageableComponent.
",", "
"   Rename Class,Rename Method,","Refactor / rework pageability I need to add a NavigatorLabel to a GridView, now NavigatorLabel has constructors accepting DataTable, DataView and PageableListView but not a GridView.
Instead of creating a new constructor accepting a GridView, I think it would be far better to merge IPageable and the private NavigatorLabel.PageableComponent into a single public interface and making all pageable components implement this interface.
I understand this approach breaks APIs and will need one major version, maybe two allowing for deprecations in between.
A less breaking alternative (but not as good in my opinion) is to change the constructor accepting a DataView to make it accept an AbstractPageableView, which is a common ancestor of both DataView and GridView and defines the methods needed to satisfy NavigatorLabel.PageableComponent.
",", "
"   Rename Class,Move And Rename Class,Pull Up Method,Push Down Method,Move Method,Pull Up Attribute,",refactor/ damage control IRequestCycleProcessor See discussion here: http://www.nabble.com/refactor--damage-control-IRequestCycleProcessor-tf3178965.html#a8821208,", , Duplicated Code, , Duplicated Code, "
"   Rename Class,Move And Rename Class,Pull Up Method,Move Method,",refactor/ damage control IRequestCycleProcessor See discussion here: http://www.nabble.com/refactor--damage-control-IRequestCycleProcessor-tf3178965.html#a8821208,", , Duplicated Code, "
"   Rename Method,Move Method,Extract Method,",Remove HeaderContributor and friends in favor of IHeaderContributor 0,"Duplicated Code, Long Method, , , "
"   Rename Method,","AbstractResource should give access to the error message for http errors AbstractResource lets you set the errorCode if something goes wrong but not a custom error message. This is really annoying because the user/developer does not get any hint why something goes wrong.

current:

response.sendError(data.getErrorCode(), null); // <------ always null, no custom error message



I attached a simple patch to fix this.

",", "
"   Move Method,Move Attribute,","add (even more) support for front-end proxies (schema, client ip, ...) Would be nice to have something like this

http://code.google.com/p/xebia-france/wiki/XForwardedFilter

in core. People's life would be a lot easier and it's really quite easy to add (actually just parsing a few headers and wrapping the original request)",", , , "
"   Rename Class,Move And Rename Class,Rename Method,","add (even more) support for front-end proxies (schema, client ip, ...) Would be nice to have something like this

http://code.google.com/p/xebia-france/wiki/XForwardedFilter

in core. People's life would be a lot easier and it's really quite easy to add (actually just parsing a few headers and wrapping the original request)",", "
"   Rename Method,Extract Method,","Add timestamp part to resource filenames for better caching Even though we have getResourceSettings().setAddLastModifiedTimeToResourceReferenceUrl() this still is far from perfect. It will add a query parameter to resource filenames so caches should invalidate when the parameter changes. However if caching is very aggressive altering the query param might not be enough. Then there will be stale resources left in the cache of the browser or some intermediate proxy. Users will complain and you have to tell them to press F5, clear the cache, or whatever :-(

So I decided to implement support for adding the timestamp of the resource as part of the filename.

When you have a resource link like

<link rel=""stylesheet"" type=""text/css"" href=""wicket/resource/my.great.app.HomePage/css/style.css""/>

a timestamp (the last modified timestamp of the file) will be injected into the base name of the file

<link rel=""stylesheet"" type=""text/css"" href=""wicket/resource/my.great.app.HomePage/css/style-ts1282376261000.css""/>

the format is

[path-component]* / [base-filename] ""-ts"" [timestamp-in-milliseconds] (.extension)

The prefix ""-ts"" (TS = timestamp) is to avoid naming conflicts with filenames that already got with a numeric part before the extension.

Locales, style and variations are taken into consideration (e.g. style.css, style_de.css, style_en.css)

When running your test cases the MockApplication which WicketTester provides in the default case has timestamps disabled so you can check you rendered markup against some predictable url.

You can control and check timestamp behavior with

getResourceSettings().setUseTimestampOnResources()

and 

getResourceSettings().getUseTimestampOnResources()

Default behavior is 'enabled'

You are now able to configure your resource caching for a very large lifetime (say 'infinite' :-) and get the best possible network performance and utilization of proxies.","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,","PageParameters api too verbose, needs to be optimized method names are too long and they do not chain",", "
"   Rename Method,Move Method,Extract Method,Inline Method,","Add IRequestListener interface to allow easier framework extension points See http://apache-wicket.1842946.n4.nabble.com/Session-attach-tp3004389p3004681.html

Basically, add an interface for onBeginRequest and onEndRequest that can be plugged into the WebRequestCycle. This allows framework extensions to plugin their listeners rather than extending WRC, and requiring that you then extend them.
","Duplicated Code, Long Method, , , , "
"   Rename Method,","Use standard exception handling in AjaxRequestTarget Now the respond method of the AjaxRequestTarget is catching all the RuntimeExceptions that are throwed. I would prefer to handle this exceptions by myself. I have a requestTarget that is a wrapper of the wicket AjaxRequestTarget where we could made something in case of errors in the respond method of the AjaxRequestTarget.

I'm using the AjaxRequestTarget to replace a wizard panel, in case of error, I would display an errors panel instance of the original panel.
",", "
"   Push Down Method,Extract Method,Push Down Attribute,","Strange IResourceStream type hierarchy Current type hierarchy looks like

IResourceStream
IStringResourceStream
AbstractResourceStream
AbstractStringResourceStream

It propobly should rather be

IResourceStream
AbstractResourceStream
IStringResourceStream
AbstractStringResourceStream
","Duplicated Code, Long Method, , , , "
"   Rename Method,Move Attribute,","URLResourceStream loads target content twice. In PageA.class, there is an 'Include' component that tries to load http://localhost:8080/page-b.
In PageB.class, which is mounted as 'page-b', will print some dummy letters when class is initialized.

When web server is requested to load PageA, server console will print the dummy letters twice. That is, one request will print 2 lines of dummy letters.

I traced into Include.class, it uses URLResourceStream.class to load the URL. 
After reading the source, I found this URLResourceStream.class requested the URL twice: 1st in the constructor, 2nd in getInputStream().

Can this be amended to request once to save some bandwith/server time?",", , "
"   Rename Method,Extract Method,","New features in WicketTester New methods:
* executeBehavior(final AbstractAjaxBehavior behavior) to execute an ajax behavior
* executeListener(Component component) for invoking a listener
* assertResultPage(final Class pageClass, final String filename) to assert last rendered Page against an expected HTML document as file
* assertResultPage(final String expectedDocument) to assert last rendered Page against an expected HTML document as a String","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,Inline Method,","New features in WicketTester New methods:
* executeBehavior(final AbstractAjaxBehavior behavior) to execute an ajax behavior
* executeListener(Component component) for invoking a listener
* assertResultPage(final Class pageClass, final String filename) to assert last rendered Page against an expected HTML document as file
* assertResultPage(final String expectedDocument) to assert last rendered Page against an expected HTML document as a String","Duplicated Code, Long Method, , , "
"   Rename Class,Rename Method,",Refactor IBehavior from interface into a concrete class discussion and vote: http://markmail.org/thread/4sqwjdvribsqdy3e,", "
"   Extract Superclass,Pull Up Method,Extract Method,Pull Up Attribute,",Backport link hierarchy from trunk Link hierarchy in trunk is cleaner and allows having disabled ajax links.,"Duplicated Code, Long Method, , Duplicated Code, Large Class, Duplicated Code, Duplicated Code, "
"   Rename Method,","Allow IResourceStream.length() to return -1 When IResourceStream.length() returns -1, Wicket's ResourceStreamRequestTargetResponse will not send a Content-Length header. When an IResourceStream doesn't know in advance the number of bytes, that allows to send a response without buffering the whole resouse.",", "
"   Rename Method,","IHeaderResponse.renderOnUnLoadJavascript(String javascript); Where there is a renderOnLoadJavascript there ought to be a renderOnUnLoadJavascript too.

This is all just copy past, so it's a little smelly.
Beyond that I wouldn't mind to be able to specify which element an event should be added.

Index: wicket/src/main/java/org/apache/wicket/markup/html/internal/HeaderResponse.java
===================================================================
--- wicket/src/main/java/org/apache/wicket/markup/html/internal/HeaderResponse.java (Revision 529942)
+++ wicket/src/main/java/org/apache/wicket/markup/html/internal/HeaderResponse.java (Arbeitskopie)
@@ -197,4 +197,19 @@
}
}

+ /**
+ * @see org.apache.wicket.markup.html.IHeaderResponse#renderOnUnLoadJavascript(java.lang.String)
+ */
+ public void renderOnUnLoadJavascript(String javascript)
+ {
+ List token = Arrays.asList(new Object[] { ""javascript-event"", ""unload"", javascript });
+ if (wasRendered(token) == false)
+ {
+ renderJavascriptReference(WicketEventReference.INSTANCE);
+ JavascriptUtils.writeJavascript(getResponse(),
+ ""Wicket.Event.add(window, \""unload\"", function() { "" + javascript + "";});"");
+ markRendered(token);
+ }
+ }
+
}
Index: wicket/src/main/java/org/apache/wicket/markup/html/IHeaderResponse.java
===================================================================
--- wicket/src/main/java/org/apache/wicket/markup/html/IHeaderResponse.java (Revision 529942)
+++ wicket/src/main/java/org/apache/wicket/markup/html/IHeaderResponse.java (Arbeitskopie)
@@ -174,4 +174,11 @@
* @param javascript
*/
public void renderOnLoadJavascript(String javascript);
+
+ /**
+ * Renders javascript that is executed after the page is unloaded.
+ * 
+ * @param javascript
+ */
+ public void renderOnUnLoadJavascript(String javascript);
}",", "
"   Extract Method,Inline Method,","Investigate whether we can use component meta data for the storage of feedback messages Investigate this. See also http://www.nabble.com/Re%3A-svn-commit%3A-r530991---in--incubator-wicket-trunk-jdk-1.4-wicket-src-main-java-org-apache-wicket%3A-Session.java-feedback-FeedbackMessages.java-p10119808.html

Advantages of doing this:
* it is a one-one mapping of the concept that you set a message on a component;
* you'll never have to worry about cleanup up; just clean up rendered messages, and leave unrendered for whenever they are request or until the component is garbage collected.

Disadvantages:
* no central storage place, making it harder to track.
* probably less efficient in both storage and processing.","Duplicated Code, Long Method, , , "
"   Rename Method,","TextField should determine the object type from the model if the model supports it Currently it is mandatory to specify the type of model object in textfield's constructor for the convesion to work properly. With certain models (PropertyModel, CompoundPropertyModel) wicket should determine the target property type automatically.",", "
"   Rename Method,","make getConvertedInput final again and remove final from convert, which should be renamed to convertInput See http://www.nabble.com/Re%3A-Use-getConverterInput-rather-than-updateModel-in-FormComponentPanel-p11399356.html
",", "
"   Rename Method,Extract Method,","Improve PageStore * create AbstractFileStore which contains the (de)serialization logic 
* create SimpleSynchronousFilePageStore to demonstrate developing custom pageStores
* create DiskPageStore that uses one file per pagemap to improve performance under hight load
* move page versions infromation from sessionstore to pagestore (used when getting page with -1 specified as (ajax) version number
* make it possible for certain pagestores to reuse serialized page data when serializing the pagemap (improve clustering efficiency)","Duplicated Code, Long Method, , "
"   Move Class,Extract Method,","Improve PageStore * create AbstractFileStore which contains the (de)serialization logic 
* create SimpleSynchronousFilePageStore to demonstrate developing custom pageStores
* create DiskPageStore that uses one file per pagemap to improve performance under hight load
* move page versions infromation from sessionstore to pagestore (used when getting page with -1 specified as (ajax) version number
* make it possible for certain pagestores to reuse serialized page data when serializing the pagemap (improve clustering efficiency)","Duplicated Code, Long Method, , "
"   Rename Method,","Backport of Header contribution filtering to 1.x The attached patch backports to branch 1.x the modifications made with rev.461786 to the wicket trunk (IHeaderResponse and related classes)

Hope this help!

",", "
"   Rename Method,","Update ImageButton to handle ResourceReferenc To be consistent with Image and its support for ResourceReferences, ImageButton was modified to exhibit the same behaviors.

New constructors and methods have been added.

Existing methods have been modified to behave similiar as Image but maintaining the Button constructs.",", "
"   Rename Method,Extract Method,","Merge the portlet support branch into the trunk I will provide easy to review patches for all the core wicket changes required for merging the wicket-1.3.0-beta3-portlet-support branch back into trunk.

Note: for efficiency reasons, I'll provide patches against the -beta3 release for now. When the merge plan is accepted I'll have to synchronize again with the latest trunk changes since the -beta3 release of course,
but doing so already for just the review patches is going to delay more than I think is needed right now.

As also indicated by Martijn Dashorst, the target for the merge is before the -beta4 cutoff as we hope to go to RC mode after that.","Duplicated Code, Long Method, , "
"   Rename Method,Move Method,Extract Method,","Improve multiple DISTINCT aggregation. Currently, tajo provides three stage for optimizing distinct query aggregation. But it just supports one column for distinct aggregation as follows:
{code:title=Query1|borderStyle=solid}
select a.flag, count(distinct a.id) as cnt, sum(distinct a.id) as total
from table1
group by a.flag
{code}

If you write two more columns for distinct aggregation, you can't apply optimized distinct aggregation as follows:
{code:title=Query2|borderStyle=solid}
select a.flag, count(distinct a.id) as cnt, sum(distinct a.id) as total
, count(distinct a.name) as cnt2, count(distinct a.code) as cnt3
from table1
group by a.flag
{code}

In this case, you may see low performance for your query. Thus, we need to improve multiple DISTINCT aggregation. Correctly, we should support three stage for multiple DISTINCT aggregation.","Duplicated Code, Long Method, , , "
"   Move Class,Move Method,Extract Method,Move Attribute,","Improve the function system to allow other function implementation types In the current function system, each function implementation is a single Java class subclassed from org.apache.tajo.catalog.function.Function. 

In this approach, there are many rooms for improvement. This approach always uses Datum as input and output values of functions, creating unnecessary objects. It does not likely to exploit given information included query statements; for example, some parameters are constants or variables.

In this issue, I propose the improvement to allow the function system to support other function implementation types. In addition, I propose three function implementation types:
- legacy Java class function provided by the current Tajo
- static method in Java class
- code generation by ASM

Later, we could expand this feature to allow Pig or Hive functions in Tajo.","Duplicated Code, Long Method, , , , "
"   Move Class,Move Method,Move Attribute,","Separate logical plan and optimizer into a maven module We already have a bunch of codes for logical planner, its optimizer, expressions, and expression optimizer. They have played a key role in Tajo project. 

As Tajo is being evolved, many parts started to require planner and optimization code. It's because we are trying to make good use of planning information in more parts.

But, since the planner and optimization parts are included in tajo-core, other maven modules should depend on {{tajo-core}} which is the biggest maven module in Tajo.

So, I propose to separate logical planner, logical optimizer, expression and expression optimizer from tajo-core into a separate maven module.",", , , "
"   Rename Method,","Concurrent execution of independent execution blocks Currently, Tajo can execute ExecutionBlocks one by one even though there remain enough resources to execute two or more ExecutionBlocks.
We can improve the query processing performance by executing two or more independent ExecutionBlocks if possible.",", "
"   Move Class,Rename Method,Move Method,Extract Method,Inline Method,","Remove Hadoop dependency from tajo-client module tajo-client is a client module to allow user applications to access Tajo cluster. tajo-client is also used for tajo-jdbc. Since tajo-client depends hadoop-client module, JDBC or user applications should include lots of third-party libraries and hadoop. Especially, it is very hard for JDBC to be single JDBC jar.

The main purpose of this issue is to remove Hadoop dependency from tajo-client module. In addition, I'll remove further dependencies from tajo-client if possible.","Duplicated Code, Long Method, , , , "
"   Push Down Method,Push Down Attribute,","Implements queryable virtual tables for catalog information I would like to propose queryable interfaces for catalog information. And these information may contain tables, columns, any other information on tajo catalog.
Currently, TajoCli offers some meta commands for retrieving table list, however queryable virtual tables can provide a handy way to tajo users.",", , , "
"   Move Class,Rename Method,Extract Method,Inline Method,","HA TajoClient should not connect TajoMaster at the first. *Problem*

We TajoClient is opened, TajoClient initially tries to connect TajoMaster. This manner does not guarantee high availability. A known TajoMaster host may be not work anymore. So, this manner still has some failure point.

Also, this manner prohibits a Tajo cluster to run on some dynamic cluster environments like Yarn.

*Solution*

Tajo HA client should get directly TajoMaster addresses and others from HA component without contacting TajoMaster.","Duplicated Code, Long Method, , , "
"   Move Class,Rename Method,","Clean up the logical plan's json format In the current implementation, the logical plan uses some ugly json document format. We need to cleanup and improve this format.",", "
"   Move Class,Move And Rename Class,Move Method,","Separate query and ddl execution codes from GlobalEngine This is a code cleanup issue. The patch separates query execution, ddl execution, and hooks from GlobalEngine.",", , "
"   Rename Method,Move Method,Extract Method,","Improve memory usage of Hash-shuffle Currently, Hash-shuffle keeps intermediate file appender and tuple list in memory and the required memory will be in proportion to the input size
If input size is 10GB, the hash-join key partition count will be 78125 (10TB / 128MB) and the required memory is 10GB (78125 * 128KB).

We should improve the hash-shuffle file writer as following :
* Separate the buffer from the file writer
* Keep the tuples in off-heap buffer and reuse the buffer
* Flush the buffers, if total buffer capacity is required more than maxBufferSize
* Write the partition files asynchronously","Duplicated Code, Long Method, , , "
"   Rename Method,",Cleanup the relationship of QueryInProgress and QueryJobManager Each QueryInProgress instance maintains an individual event handler involving each thread. It complicates the relationship between QueryJobManager and QueryInprogress. The main objective of this issue is to remove each event handler from QueryInProgress and distinguishes their roles of both classes.,", "
"   Move Class,Rename Class,Move And Rename Class,Rename Method,","Rename TajoMasterProtocol to QueryCoordinatorProtocol TajoMaster mainly has three roles:
* query coordination (including query scheduler)
* cluster resource management
* client endpoint

TajoMasterProtocol has played a role of query coordinator. But, its name is not expressive in terms of its purpose. This patch proposes the rename from TajoMasterProtocol to QueryCoordinatorProtocol.",", "
"   Move Class,Rename Class,Move Method,Extract Method,Inline Method,","HAServiceUtil should not directly use HDFS. HAServiceUtil is tightly coupled with HDFS Filesystem. We have a plan to implement multiple HA implementations like HDFS and Zookeeper. We need to abstract it to support multiple implementations.

Also, it forces TajoClient to have Hadoop HDFS dependency. If we decouple using HDFS from HAServiceUtil, we can eliminate HDFS dependency from TajoClient.","Duplicated Code, Long Method, , , , "
"   Move Method,Extract Method,Move Attribute,","Support for compression/decompression of CSVFile Currently, TextFile does not support compression/decompression.
* Utilizing Hadoop CompressionCodecFactory","Duplicated Code, Long Method, , , , "
"   Rename Method,","Change the default output file format. Currently, the default output file is CSV. Due to its nature, CSV has mainly three problems:
* Its line or field delimiter can be duplicated to some character included in the result data.
* Plan text file is likely to be larger than other file formats.
* Its read and write performance is slow.

We need to change the default output file format into other file formats. We also need to investigate which file format is the best for it.",", "
"   Rename Method,Extract Method,Inline Method,Push Down Attribute,Move Attribute,","Improve the memory usage of physical executors *Introduction*
Basically, the tuple instance is maintained as a singleton in physical operators. However, there are some memory-based operator types which need to keep multiple tuples in the memory. In these operators, multiple instances must be created for each tuple.

*Problem*
Currently, there are some temporal routines to avoid unexpected problems due to the singleton instance of tuple. However, the methodology is inconsistent and complex, which causes unexpected bugs.

*Solution*
A consistent methodology is needed to handle this problem. Only the operators that keep multiple tuples in memory must maintain those tuples with separate instances.","Duplicated Code, Long Method, , , , , "
"   Rename Method,Extract Method,","Python UDF support Python has abundant users and third-party libraries. This language is widely used in data analytic area. So, it would be great if Tajo supports Python UDF.","Duplicated Code, Long Method, , "
"   Rename Method,","Bump up hadoop to 2.2.0 Hadoop 2.2.0 has been released. We need to bump up hadoop to 2.2.0. This hadoop version uses protobuf-2.5.0. So, Tajo also needs to bump up protobuf-2.5.0.",", "
"   Rename Method,Move Method,Extract Method,","Refactor FilterPushDownRule::visitJoin() into well-defined, small methods FilterPushDownRule::visitJoin() is too long and complicated. It handles various cases in a single method. We need to refactor this method into several small and well-defined methods.","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Improve the join order algorithm to consider missed cases of associative join operators TAJO-1277 fixes a bug related to the associativity of join operators, but there are still some missed cases that join operators are associative. This work should include the cases described in the following links:
* http://stackoverflow.com/questions/20022196/are-left-outer-joins-associative
* https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Joins","Duplicated Code, Long Method, , "
"   Rename Method,",Support multi-bytes delimiter for CSV file Supports multi-character / non-ascii delimiter for CSV file.,", "
"   Rename Class,Move Method,Extract Method,","Improve broadcast table cache Currently, broadcast implementation keep a tuples on scan operator and It create a duplicated table cache in memory.
We should improve it","Duplicated Code, Long Method, , , "
"   Rename Method,",In predicate support See the title. In predicate is a basic part of SQL standards. We need to support this.,", "
"   Rename Method,Extract Method,Pull Up Attribute,","RpcConnectionPool should check reference counter of connection before close Connections in the pool is shared one and should be closed only when it's not referenced by other threads. Furthermore, current pool implementation locks whole connections for connecting/closing a connection, making bad interferences on other operations (on sane connection).","Duplicated Code, Long Method, , Duplicated Code, "
"   Rename Method,",Implement hash anti-join operator Hash anti-join operator is used in 'NOT IN' clause. We need it.,", "
"   Extract Superclass,Rename Method,Pull Up Method,Pull Up Attribute,","Add Bind method to EvalNode EvalNode just has eval() method to evaluate some tuples. It does not have any method to prepare actual execution.

For example, FieldEval should know an actual field index of an input tuple. This process is also performed in eval() method.

It has some problems:
* eval() method should involve two works, and it does different behavior with some flag or null check.
* 'if condition' in eval() method for different behaviors involves unnecessary branches, causing performance degradation.

So, we should add {{bind(Schema)}} method to EvalNode and then move some preparation code from eval() to bind(Schame) method.

Also, we should refactor the signature {{eval(Schema, Tuple)}} to {{eval(Tuple)}}.",", Duplicated Code, Large Class, Duplicated Code, Duplicated Code, "
"   Rename Method,Move Method,Extract Method,","Implement INSERT OVERWRITE clause 'INSERT INTO' appends data into existing table. In contrast, 'INSERT OVERWRITE' will overwrite existing table data. It is used as an idiom in analytical fields based HDFS. We need it.","Duplicated Code, Long Method, , , "
"   Move And Rename Class,","Improve Hive Compatibility Currently, Hive has been released to 1.1.0. Also from Hive 0.14.0, it changed hcatalog package name. But Tajo just provides hive 0.12.0 and hive 0.13.1. Thus, we need to improve hive compatibility. For this issue, I’ll remove hcatalog dependency. It’s not problem because hcatalog just has been used to find hive data types and Tajo can find right data types with Hive serdeConstants.",", "
"   Rename Method,Extract Method,","count(distinct column) should be supported. The distinct included in count function should be supported.
{code}
select count(distinct col1) from table
{code}","Duplicated Code, Long Method, , "
"   Extract Superclass,Move Method,Extract Method,Pull Up Attribute,Move Attribute,",Apply TAJO-1407 to ExternalSortExec Tipped by @Hyoungjun Kim. Seemed possible to apply easily.,"Duplicated Code, Long Method, , , , Duplicated Code, Large Class, Duplicated Code, "
"   Rename Class,Rename Method,Move Method,Extract Method,","Eliminate QueryConf and its file write QueryConf is a subclass of org.apache.hadoop.conf.Configuration. This has a similar mechanism of JobConf of MapReduce. It has overheads and too big to be distributed across a number of nodes. I've added QueryMeta to the patch of TAJO-144. QueryMeta contains a set of key-value pairs and is based on protobuf. Since QueryMeta is a wrapper class of a protobuf message and will contain only specific information for each query, it will be very lightweight and is easy to be disseminated across a number of nodes.

In this issue, we will replace QueryConf by QueryMeta, and we will eliminate the part where QueryConf is written as a xml file on HDFS.","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Use dedicated thread to release resource allocated to container Currently, it uses thread pool, which is used for launching tasks. But it can make dead-lock situation.","Duplicated Code, Long Method, , "
"   Extract Superclass,Pull Up Method,Extract Method,Pull Up Attribute,",Refactoring of HashJoinExecs It's not in condition to make some improvements. No offense.,"Duplicated Code, Long Method, , Duplicated Code, Large Class, Duplicated Code, Duplicated Code, "
"   Rename Method,Move Method,Extract Method,","Improve broadcast join planning The global engine generates a logical plan, and then marks some parts of the plan as broadcast plan which means that they and their input will be broadcasted to all workers. 

Currently, broadcast parts are identified according to some rigid and hard-coded rules. This will limit the broadcast opportunities in many cases.
So, in this issue, I propose refactoring the broadcast planner to be more general.

Here are brief rules for broadcast join plan.
* A relation node is broadcastable if its input size does not exceed the pre-defined threshold.
* Output of an execution block (EB) is broadcastable if its every input is broadcastable.
* Given an EB containing a join and its child EBs, those EBs can be merged into a single EB if at least one child EB's output is broadcastable.
* The total size of broadcast relations of an EB cannot exceed the pre-defined threshold.
** After merging EBs according to the first rule, the result EB may not satisfy the second rule. In this case, enforce repartition join for large relations to satisfy the second rule.
* For outer joins, preserved-row relations are not broadcastable to avoid input data duplication. That is, full outer join cannot be executed with broadcast join.
** Here is brief backgrounds for this rule. Data of preserved-row relations will be appeared in the join result regardless of join conditions. If multiple tasks execute outer join with broadcasted preserved-row relations, they emit duplicates results.
** Even though a single task can execute outer join when every input is broadcastable, broadcast join is not allowed if one of input relation consists of multiple files.","Duplicated Code, Long Method, , , "
"   Rename Class,Rename Method,Extract Method,Move Attribute,",Python UDAF support We need to support Python UDAF as well as UDF (TAJO-1344).,"Duplicated Code, Long Method, , , "
"   Move Class,Rename Method,Push Down Method,Move Method,Extract Method,","Add test cases to verify join plans We are lacking test cases to verify query plans even though they directly affect to query processing performance. This is important especially for join queries because their plans can be changed while optimizing join order that affects to performance significantly. So, we need to verify the optimal join plan first.

There can be some approaches to test query plans. Here are some candidates what I consider.
* Adding a special class that verifies query plans while traversing it. 
* Verifying the result of *EXPLAIN* query. 

I think that the second approach looks good. Here are some reasons.
* Easy to implement. It's just string match.
* Easy to verify both logical plan and global plan without adding any special classes to traverse query plan.
* This is the most important reason. With the second approach, we can guarantee that our query planner is deterministic.","Duplicated Code, Long Method, , , , "
"   Rename Method,Extract Method,",Remove QueryMaster client sharing in TajoMaster and TajoWorker QueryMaster client should remove after use. It can possible that connection refused in very large cluster because too many open connections,"Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,Inline Method,",JDBC Tablespace support The main objective of this issue is to implement JDBC-based storage and its Tablespace implementation.,"Duplicated Code, Long Method, , , "
"   Move Class,Move And Rename Class,Rename Method,Pull Up Method,Move Method,Extract Method,Move Attribute,","Improve off-heap RowBlock OffHeapRowBlock was added by TAJO-907. but it does not support memory pooling.
We already use the Netty PooledByteBufAllocator to DelimitedTextFile. and we can easy to add the implementation by PooledByteBufAllocator","Duplicated Code, Long Method, , , , Duplicated Code, "
"   Rename Method,",Add examples for TajoClient v2 This patch adds the example code for TajoClient v2.,", "
"   Move And Rename Class,Move Class,Move Method,Move Attribute,","Implement Tajo JDBC Driver Tajo is required to be integrated with legacy BI or OLAP tools. For this, Tajo should provide a JDBC driver.",", , , "
"   Rename Method,Move Method,Extract Method,","Improve the performance of cross join Cross join is one of the very heavy operations. Furthermore, this operator is performed by a single worker in the current implementation. (Please see the implementation of HashPartitioner. If partitionKeyIds is empty, getPartition() always returns a single value.)

One possible alternative is executing cross join with broadcast join. That is, outer table (smaller one) is always broadcasted, and join is performed by the machine who stores a part of inner table.

To do so, a new session variable is required to set the broadcast threshold for cross join.","Duplicated Code, Long Method, , , "
"   Rename Method,","Move Tajo into Java 8 As we discussed in the mailing list (http://search-hadoop.com/m/ZGIO9XqiTPKruL7/java+8&subj=+DISCUSSION+Migration+to+Java+8), this patch moves Tajo into Java 8. It changes the language level in maven and fix some unit test failure.",", "
"   Move Class,Extract Superclass,Extract Method,","Implements StorageManager for scanning asynchronously The current StorageManager does not provide scan scheduling function. All scan operations run concurrently. This is the cause of random disk access and disk read performance is not good.
The proposed StorageManager is based on double buffering. Each disk has a scheduler to schedule by order of scanned adjust. Each Scanner has a InputStream and a Tuple pool. The next() operation of ScanNode is blocked until Tuple pool is filled. Assigned Scanner by the scheduler read data(xMB) and fills Tuple Pool and notifies to next() operation. After scanning Scanner re-enter DiskScanQueue.
In this way Scanner can pass column vector to Vectorized Query Engine.
See the attached file.
","Duplicated Code, Long Method, , Duplicated Code, Large Class, "
"   Pull Up Method,Extract Method,","Remove QueryMasterTask cache immediately, if it stored to persistent storage Query information is stored asyncronously to the persistent storage and It does exist in LRU cache for a while.
The cache entry should be removed immediately, if stored to persistent storage","Duplicated Code, Long Method, , Duplicated Code, "
"   Move Method,Extract Method,Move Attribute,","Add a shutdown hook manager in order to set priorities Sometimes, event-group thread of netty’s is not stopped. because jvm will start a shutdown-hook after all thread is stopped without kill signal. it is dead-lock state. 
We should explicitly call to shutdown the event-group thread of netty’s and should set a shutdown-hook order

For example, before you start up TajoMaster, runs tsql, you can find deadlock","Duplicated Code, Long Method, , , , "
"   Move Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Refactor GlobalPlanner and global plan data structure Above all, I'm sorry for submitting a big patch. This patch modifies and refactors broadly global planning, logical planning, and physical planning parts. It was hard to separate this issue into smaller issues.

Especially, this patch primarily rewrites GlobalPlanner and MasterPlan (global plan data structure) as follows:
* Removed GlobalPlanOptimizer
* Added DirectedGraph interface, SimpleDirectedGraph concret class, and a visitor class to visit a graph in post-order traverse way.
* Improved MasterPlan by using new graph API
** query block graphs and an execution block graph are represented by SimpleDirectedGraph.
** Now, we can traverse above graphs easily by using graph APIs.
** Added DataChannel class to represent a data flow between execution blocks.
* MasterPlan.toString() prints a text graph to represent relationships among execution blocks and a distributed plan.
* Add more sophisticated explain feature for a distributed plan and logical plan. It is very useful for plan debugging.
* Now, the limit operator is pushed down to child execution block.
** So, the intermediate data volume of a sort query with limit is reduced significantly.
* TableSubQuery (inline view) is supported. It follows SQL standards. So, you can do a query as follows:
{code}
SELECT *
FROM
(
SELECT
l_orderkey,
l_partkey,
url
FROM
(
SELECT
l_orderkey,
l_partkey,
CASE
WHEN
l_partkey IS NOT NULL THEN ''
WHEN l_orderkey = 1 THEN '1'
ELSE
'2'
END AS url
FROM
lineitem
) res1
JOIN
(
SELECT
*
FROM
part
) res2
ON l_partkey = p_partkey
) result
{code}

In addition, I've refactored as follows:
* Column has a qualifier name.
* Improved Schema to deal with qualified column names
* When a TableDesc instance is retrieved, it is forced to have qualifier columns.
* Fixed TAJO-162 bug.
* Lots of trivial improvement and refactors.","Duplicated Code, Long Method, , , , , "
"   Move Class,Extract Method,","Refactor Rpc clients to take Connection Parameters Currently, Rpc client implementations only takes few parameters. This refactoring allows rpc clients to take flexible parameters. I also add connection timeout and cleaned up some routines.","Duplicated Code, Long Method, , "
"   Move Method,Move Attribute,","Add string pattern matching operators This patch adds some pattern matching operators, such as ILIKE, SIMILAR TO, REGEX operators, and improves LIKE operator. In addition, it improves null handling on those string pattern matching operators.

Also, I've added the manual to the query language wiki page (https://wiki.apache.org/tajo/QueryLanguage#PatternMatching).
",", , , "
"   Rename Method,","LogicalNode should have an identifier to distinguish each logical node instance. In some parts of LogicalPlan, an instance object id is used as a key in some map data. If we have identifiers to distinguish each logical node instance, it would be better than the current implementation.",", "
"   Rename Method,Extract Method,Inline Method,","Query master uses too much memory during range shuffle I ran a simple sort query on a 8TB table as follows.
{noformat}
tpch10tb> select * from lineitem order by l_orderkey;
{noformat}

After the first stage is completed, query master divides the range of the sort key (l_orderkey) into multiple partitions for range shuffle. Here, the partitioning time took about 9 minutes.

Here is the log.
{noformat}
...
2015-10-26 14:23:10,782 INFO org.apache.tajo.engine.planner.global.ParallelExecutionQueue: Next executable block eb_1445835438802_0004_000002
2015-10-26 14:23:10,782 INFO org.apache.tajo.querymaster.Query: Scheduling Stage:eb_1445835438802_0004_000002
2015-10-26 14:23:10,796 INFO org.apache.tajo.querymaster.Stage: org.apache.tajo.querymaster.DefaultTaskScheduler is chosen for the task scheduling for eb_1445835438802_0004_000002
2015-10-26 14:23:10,796 INFO org.apache.tajo.querymaster.Stage: eb_1445835438802_0004_000002, Table's volume is approximately 663647 MB
2015-10-26 14:23:10,796 INFO org.apache.tajo.querymaster.Stage: eb_1445835438802_0004_000002, The determined number of non-leaf tasks is 10370
2015-10-26 14:23:10,816 INFO org.apache.tajo.querymaster.Repartitioner: eb_1445835438802_0004_000002, Try to divide [(6000000000), (1)) into 10370 sub ranges (total units: 10370)
2015-10-26 14:24:58,996 INFO org.apache.tajo.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2440ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=2214ms
GC pool 'PS Scavenge' had collection(s): count=1 time=622ms
2015-10-26 14:27:24,040 WARN org.apache.tajo.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 13237ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=12635ms
GC pool 'PS Scavenge' had collection(s): count=1 time=674ms
2015-10-26 14:28:51,914 WARN org.apache.tajo.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 20873ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=20486ms
GC pool 'PS Scavenge' had collection(s): count=1 time=644ms
2015-10-26 14:30:52,392 WARN org.apache.tajo.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 30986ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=30546ms
GC pool 'PS Scavenge' had collection(s): count=1 time=696ms
2015-10-26 14:32:07,550 WARN org.apache.tajo.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 15449ms
GC pool 'PS MarkSweep' had collection(s): count=1 time=14593ms
GC pool 'PS Scavenge' had collection(s): count=1 time=1148ms
2015-10-26 14:32:15,807 INFO org.apache.tajo.querymaster.Stage: 10370 objects are scheduled
...
{noformat}","Duplicated Code, Long Method, , , "
"   Rename Method,",Rename the name 'option' to 'property' in TableMeta. This is a trivial patch to rename 'option' to 'property' in TableMeta. This is necessary to make naming more consistent.,", "
"   Rename Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","RCFile compatible to apache hive * Support both the text and the binary serialization/deserialization.
** dafault : org.apache.tajo.storage.BinarySerializeDeserialize
* use SequenceFile.metadata.
** key: rcfile.serde
** value: org.apache.tajo.storage.BinarySerializeDeserialize, org.apache.tajo.storage.TextSerializeDeserialize
* improve memory efficiency
* support tajo pushdown projection
* support compression","Duplicated Code, Long Method, , , , , "
"   Rename Method,Move Method,Inline Method,","Add TableStatUpdateRewriter Currently, we set table volumes into TableDesc in LogicalPlanner. In this case, we cannot employ the push-downed filters for getting table volumes. For it, we should postpone getting table volumes as late as possible before join ordering optimization.

This patch move the table stat update code into the last rewrite rune in pre rewriters.",", , , "
"   Rename Method,","By default, Optimizer should use the table volume in TableStat. Currently, the optimizer by default gets table volumes through storage manager and employ them for join optimization. But, in some cases, it causes performance degradation because aggregating all file volumes is not cheap in large partitioned tables on S3 or HDFS.

So, this patch improves TableStatUpdateRewriter to use table volumes of TableStat by default, and it also adds a session variable 'USE_TABLE_VOLUME' to allow the optimizer to use the table volume through storage handler.",", "
"   Rename Method,","Upgrading ORC reader version Currently Tajo uses presto-orc-0.86, but it was old version even when it was integrated because Presto was using JDK 1.8 at that time.

Now that Tajo is based on JDK 1.8, it can be upgraded to a recent version 0.132.
It becomes more robust and some minor features are added.
Additionally hive compatibility is improved, so upgrading is necessary to support Hive-catalog.",", "
"   Move Class,Push Down Method,Push Down Attribute,","Implement an Adapter for the legacy Schema See TAJO-2042. In this issue, I'll implement an adapter for both legacy and new schema implementations.",", , , "
"   Rename Class,Rename Method,",Apply new identifier system to new schema This patch applies TAJO-2104 to new schema system.,", "
"   Extract Method,Inline Method,","Refactor Schema to be immutable In order to make Schema to be simplified, we need to refactor it to be immutable object.","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Implement Radix sort Radix sort is known for very fast sort algorithm when the length of the sort key is not long. We can benefit from Radix sort if it is used when it is faster than Tim sort.

In this issue, I will implement Radix sort for Tajo, and conduct some benchmark tests to compare its performance with Tim sort.","Duplicated Code, Long Method, , "
"   Move Class,Rename Class,Move Method,","Implement regexp_replace function regexp_replace function replaces substrings matched to a given regular expressions. Like other functions, we will follow PostgreSQL for non-standard functions as following:
{noformat}
regexp_replace(string text, pattern text, replacement text)
{noformat}",", , "
"   Rename Method,Move Method,","Improve disk load, when queries run simultaneously Currently, tajo uses {{tajo.worker.resource.disks}} for resource scheduler and it effects the read performance of leaf task. but it does not consider to run query simultaneously.
The disk load control should move to worker node instead of scheduler.",", , "
"   Rename Method,Extract Method,","Simplify rpc address in default configuration The cluster mode have to set binding address as following configuration. 
{{tajo.catalog.client-rpc.address}} and {{tajo.resource-tracker.rpc.address}} is same server with {{tajo.master.umbilical-rpc.address}}.
So if these configuration is not set, we can use {{tajo.master.umbilical-rpc.address}} to default connecting hostname

{code:xml}
<property>
<name>tajo.master.umbilical-rpc.address</name>
<value>hostname:26001</value>
</property>

<property>
<name>tajo.master.client-rpc.address</name>
<value>hostname:26002</value>
</property>

<property>
<name>tajo.resource-tracker.rpc.address</name>
<value>hostname:26003</value>
</property>

<property>
<name>tajo.catalog.client-rpc.address</name>
<value>hostname:26005</value>
</property>
{code}","Duplicated Code, Long Method, , "
"   Move Class,Rename Method,Pull Up Method,Extract Method,","Implement type cast expression We need to implement type cast expression defined as:

{noformat}
CAST ( expression AS type )
expression::type
{noformat}","Duplicated Code, Long Method, , Duplicated Code, "
"   Rename Class,Move And Rename Class,Extract Superclass,Rename Method,Move Method,Extract Method,Move Attribute,","PullServer as an Auxiliary service of Yarn We are going to support Yarn as Tajo's one of resource schedulers. To do so, the PullServer should be capable of executing as an Auxiliary service.","Duplicated Code, Long Method, , , , Duplicated Code, Large Class, "
"   Rename Method,Pull Up Method,Move Method,Extract Method,Inline Method,Move Attribute,","Apply new type implementation to Schema and Catalog. See TAJO-2042. The main objective of this issue is to apply new types in Schema, Column, and Catalog. For test, I've added array and map type to DDL statements.","Duplicated Code, Long Method, , , , , Duplicated Code, "
"   Move Method,Extract Method,","NULL characters in meta of csv table should be supported '\N' represents default NULL value in hive. but do not get it.
* default null value is empty string. (without text column)
* add table option for null value.
","Duplicated Code, Long Method, , , "
"   Rename Method,Pull Up Method,Extract Method,",Fragment interface cleanup We need to improve Fragment interface consistent for various types of fragments.,"Duplicated Code, Long Method, , Duplicated Code, "
"   Rename Method,","Implement an example HTTP tablespace See discussion at http://mail-archives.apache.org/mod_mbox/tajo-dev/201605.mbox/browser.
This tikcet is to implement a simple example http tablespace.",", "
"   Rename Method,","Add 'ALTER TABLE UNSET PROPERTY' statement to Tajo DDL For version 0.11.x now, there is only set statement for table property. When the user makes a typo with DDL statement, this mistake cannot be removed. We also need the way to remove already set properties.",", "
"   Rename Method,Extract Method,","Maximize disk read bandwidth utilization of StorageManagerV2 by moving Tuple creation role to next() Currently, Tuple creation mechanism of StorageManagerV2 is as follows:
1) At file scan, scheduled scanner reads data from disk, makes a Tuple, and insert it to the Tuple pool
2) next() of the scanner just pulls an already created Tuple from the Tuple pool asynchronously

Because of Tuple creation time, scanner cannot fully use its time to read disk, which results in less disk read bandwidth utilization

So, if Tuple creation role is moved to next() and scanners spend their whole time to read file at file scan,
we can fully utilize disk read bandwidth","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Rearrange DataType enumeration and Refactor type systems This patch rebuilds DataType enumeration and refactors type systems for more efficiency and extendibility of type. In detail, it does as follows:
* Remove the array return value from EvalNode, all functions, and Catalog.
** Now, all operators, functions, and expressions return only one data type.
** Instead, a data type can be array.
* Add Type.PROTOBUF type that enables Tajo to use a protocol buffer class as a type.
* Remove ArrayDatum and Type.Array which are used in some functions, requiring return two or more values.
** Instead, they use generated protobuf type as return types.
* Add more rich data types to DataType.Type.
* Add ProtobufDatum and ProtobufDatumFactory that help create Builder with only a data type.
* Cleanup Catalog and many others.","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,","Rename join operators and add other join operators to PhysicalPlanner The current physical operators for join have different naming rules as follows:
* LeftOuterHashJoin, FullOuterHashJoin, ...
* HashSemiJoin, HashAntiJoin

This patch renames join operators with consistency as follows: 
* {Algorithm}{Join Type} Join

For example, outer joins will have the following names:
* HashLeftOuterJoin
* MergeRightOuterJoin
* HashFullOuterJoin

This patch adds other join algorithms (left semi/anti hash join) to physical operators and adds In-subquery clause and exists predicate to Tajo algebra.",", "
"   Rename Method,","Implement LogicalPlanVerifier to check if a logical plan is valid The current Tajo does not have any verification system to check whether a logical plan is valid, or not. LogicalPlanVerifier will verify the followings of a logical plan:
* operand types checking
** some operators have type restrictions. For example, plus(\+)'s operands must be numerical values.
* Table and column existence check. 
** Examples are as follows:
*** a create table statement must check if the table already exists
*** columns included in select list must exist in corresponding tables.",", "
"   Move And Rename Class,",Improving web UI Tajo Web UI needs to be improved for more convenient management,", "
"   Rename Method,Extract Method,","Rearrange default port numbers and config names. Tajo uses many port numbers and config properties. But, they are inconsistent. The main objective of this issue is to rearrange the default port numbers and config properties. Also, we should make an wiki page to describe them.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Rearrange default port numbers and config names. Tajo uses many port numbers and config properties. But, they are inconsistent. The main objective of this issue is to rearrange the default port numbers and config properties. Also, we should make an wiki page to describe them.","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,Move Method,Extract Method,","Protocol buffer De/Serialization for LogicalNode In the current implementation, the logical plan is serialized into a JSON object and sent to each worker.
However, the transmission of JSON object incurs the high overhead due to its large size.
ProtocolBuffer is a good alternative because its overhead is quite small and already used in other modules of Tajo.","Duplicated Code, Long Method, , , "
"   Rename Method,","Maintaining connectivity to Tajo master regardless of the restart of the Tajo master Currently, when you restart the Tajo master, you should restart all the workers and clients also.

When client or worker has problem with connection to Tajo master due to the master restart, it needs to close the previous connection and try to reconnect to the master",", "
"   Push Down Method,Extract Method,Inline Method,","Maintaining connectivity to Tajo master regardless of the restart of the Tajo master Currently, when you restart the Tajo master, you should restart all the workers and clients also.

When client or worker has problem with connection to Tajo master due to the master restart, it needs to close the previous connection and try to reconnect to the master","Duplicated Code, Long Method, , , , "
"   Move Method,Extract Method,Move Attribute,","Separating QueryMaster and TaskRunner roles in worker In C++ implementation of Tajo worker,
I think it is better to maintain Java code of QueryMaster and implement only TaskRunner as C++ code, rather than implementing both QueryMaster and TaskRunner as C++ code.

Accordingly, standby mode worker will have following 3 modes:
1) TaskRunner + QueryMaster (current implementation)
2) TaskRunner only (C++)
3) QueryMaster only (java)
And, most workers work as C++ TaskRunner, and 1 or 2 workers work as separate QueryMasters (java process).
For backward compatibility, default mode should be mode 1 (TaskRunner + QueryMaster) 

To achieve the goal, it needs to separate java TaskRunner and java QueryMaster Worker first.
(And then, implement C++ TaskRunner)","Duplicated Code, Long Method, , , , "
"   Move Class,Rename Class,","Separating QueryMaster and TaskRunner roles in worker In C++ implementation of Tajo worker,
I think it is better to maintain Java code of QueryMaster and implement only TaskRunner as C++ code, rather than implementing both QueryMaster and TaskRunner as C++ code.

Accordingly, standby mode worker will have following 3 modes:
1) TaskRunner + QueryMaster (current implementation)
2) TaskRunner only (C++)
3) QueryMaster only (java)
And, most workers work as C++ TaskRunner, and 1 or 2 workers work as separate QueryMasters (java process).
For backward compatibility, default mode should be mode 1 (TaskRunner + QueryMaster) 

To achieve the goal, it needs to separate java TaskRunner and java QueryMaster Worker first.
(And then, implement C++ TaskRunner)",", "
"   Rename Method,Move Method,Move Attribute,","Add Table Partitioning Table partitioning gives many facilities to maintain large tables. First of all, it enables the data management system to prune many input data which are actually not necessary. In addition, it gives the system more optimization opportunities that exploit the physical layouts.

Basically, Tajo should follow the RDBMS-style partitioning system, including range, list, hash, and so on. In order to keep Hive compatibility, we need to add Hive partition type that does not exists in existing DBMS systems.",", , , "
"   Rename Class,Move Attribute,","Refactor TableDesc, TableMeta, and Fragment In the current implementation, TableDesc and TableMeta are implemented through interface and its implementation. This is unnecessary abstraction. For simplicity, this patch removes interfaces and merge them into the concrete classes.

In addition, TableDesc and TableMeta's role is ambiguous. This patch clarifies their roles as follows:
* TableMeta contains usual physical information which is used in workers. 
* TableDesc contains logical information about a table and others which are not used in workers.

As a result, I've moved TableStats and Schema from TableMeta to TableDesc. 

Besides, in the current implementation, Fragment also is subclassed of TableDesc. But, this relationship is wrong. Fragment is independent one of TableDesc. This patch changes this relationship. They are independent ones.
",", , "
"   Rename Class,Move And Rename Class,Rename Method,Extract Method,","Improve Fragment to be more generic The current Fragment is only for a file. This patch improves Fragment to be more generic. 

First of all, I've changed Fragment to an interface and the original Fragment to FileFragment respectively. FragmentProto is changed to contain a table name and a bytestring which contains an storage-dependent contents. Then, the added FragmentConvertor transforms FragmentProto to a specified Fragment instance. It would be very useful to represent various fragment types like a row range of Hbase and database tables.","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,Move Method,Inline Method,Move Attribute,","Improve Fragment to be more generic The current Fragment is only for a file. This patch improves Fragment to be more generic. 

First of all, I've changed Fragment to an interface and the original Fragment to FileFragment respectively. FragmentProto is changed to contain a table name and a bytestring which contains an storage-dependent contents. Then, the added FragmentConvertor transforms FragmentProto to a specified Fragment instance. It would be very useful to represent various fragment types like a row range of Hbase and database tables.",", , , , "
"   Rename Method,","drop table command should not remove data files in default h3. Problem
In the current implementation, Tajo removes a data directory when a user issues 'drop table' command. But, it is very dangerous in many cases. For example, some users may lost large data sets.

h3. Solution
In default, 'DROP TABLE' should not remove a data directory. -, and we need to add a config property 'tajo.command.drop_table.data_removal' for those who want to change the behavior of 'drop table'.- In addition, Tajo should provide 'DROP TABLE table_name PURGE' for removing all data.",", "
"   Rename Method,","drop table command should not remove data files in default h3. Problem
In the current implementation, Tajo removes a data directory when a user issues 'drop table' command. But, it is very dangerous in many cases. For example, some users may lost large data sets.

h3. Solution
In default, 'DROP TABLE' should not remove a data directory. -, and we need to add a config property 'tajo.command.drop_table.data_removal' for those who want to change the behavior of 'drop table'.- In addition, Tajo should provide 'DROP TABLE table_name PURGE' for removing all data.",", "
"   Rename Class,Move Class,Rename Method,Move Method,Extract Method,","Implement killQuery feature In the current version, killQuery feature is partially implemented. We need to complete this feature and add a command to tsql.","Duplicated Code, Long Method, , , "
"   Rename Class,Extract Method,","Make TaskScheduler be pluggable The task scheduler can be changed according to the task scheduling algorithm, the locality policy of the storage, and so on.
Thus, we need to improve the task scheduler interface to be pluggable.","Duplicated Code, Long Method, , "
"   Rename Method,","Improve GreedyHeuristicJoinOrderAlgorithm to deal with non-commutative joins GreedyHeuristicJoinOrderAlgorithm is a default cost-based join order algorithm. It is designed only for inner joins, and it cannot deal with non-commutative joins, such as left/right/full outer join and semi/anti join. The main goal of this issue is to improve this algorithm to deal with them.",", "
"   Rename Method,Extract Method,","Improve TajoResourceManager to support more elaborate resource management h3. Status of the current Tajo Resource Manager (RM)
* Tajo RM manages CPU, DISK resource incompletely, and it only provides resource management through memory allocations. 
* In addition, Tajo RM considers the memory resource as the fixed number of slots.

h3. Problem
In many cases, workloads can be categorized into I/O intensive job and CPU and memory consuming job. For example, scan and hash partition or INSERT OVERWRITE may be belong to I/O intensive job. In general, Aggregation can be belong to CPU-memory consuming job. The current RM is not fit to support selectively I/O intensive job or CPU-memory consuming job because it provides only memory slots. We need more elaborate resource management mechanism.

In addition, in most resource management systems, the remain resource less than required resource is not allocated in response to a resource request. It is not good to fully utilize the cluster resources. In order to mitigate this problem, we need to add resilience to allocation mechanism. For example, min-max request would be useful for it.

h3. Proposal
* Tajo RM should provides resource management for disk and cpu-memory.
** Tajo RM should provide allocation request call with min, max memory request, and min, max disk request.
*** min-max request will be useful to fully utilize remain cluster resources.
* Each resource request should have a priority. The priority can be disk or memory.
** If the priority is disk
*** disk allocation will be limited depending on the remain disk resource
*** memory allocation will be not limited regardless of the remain memory resource, and just reduce the remain memory resource.
** If the priority is memory
*** memory allocation will be limited depending on the remain memory resource
*** disk allocation will be not limited regardless of the remain disk resource, and just reduce the remain disk resource.
* disk resource in each worker is represented as a float value.
** The initial disk resource will be the number of disks which participate in HDFS data directory.
","Duplicated Code, Long Method, , "
"   Move Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Implement substr function h3. Function Definition
{code}
text substr(string text, from int4 [, count int4])
{code}

h3. Description
* It extracts substring (same as substring(string from from for count)). 
* If string is null, the result also should be null.
* Note that *from* is one based index.
* *from* can be negative integer.
* *count* can be omitted. If then, all string after *from* will be returned.
* *count* cannot be negative integer.
* If *count* is 0, the result should be '' instead of null.

h3. Example and Sementic
{code}
hyunsik=> select substr('abcdef', 3, 2);
substr 
--------
cd

(1 row)

hyunsik=> select substr('abcdef',3) ;
substr 
--------
cdef
(1 row)

hyunsik=> select substr('abcdef',1,1) ;
substr 
--------
a

(1 row)
hyunsik=> select substr('abcdef',0,1) ;
substr 
--------

(1 row)

hyunsik=> select substr('abcdef',0,2) ;
substr 
--------
a
(1 row)

{code}","Duplicated Code, Long Method, , , , , "
"   Move Class,Move And Rename Class,Move Method,Move Attribute,","Implement strpos(string, substring) function h3. Function Definition
{code}
int strpos(string text, substring text)
{code}

h3. Description
This function finds the location of specified substring.
* If *string* or *substring* is null, the result should be null.
* If *substring* is '', the result should be 1.
* If there is no matched *substring*, the result should be 0.
* The result is one-based index.

h3. Example.
{code}
hyunsik=> select strpos('tajo','jo');
strpos 
--------
3
(1 row)
{code}",", , , "
"   Rename Method,Move Method,Extract Method,Inline Method,","Add Database support to Tajo Currently, all tables reside in single namespace (default). Tajo should support multiple namespaces (i.e., databases) so that users can create tables in independent namespace.","Duplicated Code, Long Method, , , , "
"   Rename Method,","Add Database support to Tajo Currently, all tables reside in single namespace (default). Tajo should support multiple namespaces (i.e., databases) so that users can create tables in independent namespace.",", "
"   Rename Method,Extract Method,","Improve TajoClient to directly get query results in the first request Currently, TajoClient cannot deal with simple queries (e.g., select * from table limit 1 or select 1) which are executed in TajoMaster without distributed execution. The final results are always stored in HDFS, and TajoClient gets the result via scanner with TableDesc obtained from GetQueryResultResponse. 

For simple queries directly executed at TajoMaster, TajoClient needs to directly get some binary serialized rows results from GetQueryStatusResponse or GetQueryResultResponse instead of reading materialized tables.

This feature would be also useful for low latency queries, EXPLAIN clauses and expr-only statements without FROM clause.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Improve ExternalSortExec with N-merge sort and final pass omission Background:

The current ExternalSortExec just uses the binary external merge sort algorithm (http://en.wikipedia.org/wiki/External_sorting#External_merge_sort). In other words, for each pass, ExternalSortExec just merges two files into one sorted file.

Proposal:
The goal of this proposal is to improve ExternalSortExec with the following improvements:
* N-merge sort - we can merge N files though more memory at each pass. It will reduce the number of passes. Consequently, it will reduces considerable I/O overheads.
* the final pass omission - a physical operator is pipelined by the parent operator. The final pass of the merge sort must also be invoked by the parent physical operator. So, we can omit the final pass of the merge sort.","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,","Implement quote_ident function I'd like to develop this function.
",", "
"   Move Class,Rename Method,","Increase the default value of worker memory Since the default value of the worker memory and the memory required to launch the query master are same, the worker that launches a query master doesn't have any free memory without an extra configuration.

In a cluster consisting of one machine, this causes ""no available worker resource for an execution block"", and any queries cannot be executed.
Though it is not an error, but it can make users confused.",", "
"   Rename Method,Move Method,Extract Method,Move Attribute,","Refactoring TaskScheduler to assign multiple fragments In the current implementation, each task processes only one fragment.
However, processing multiple fragments in a task will increase the query processing performance according to the storage layout and the user queries.
In this issue, TaskScheduler is refactored to enable assigning multiple fragments to each task.
Followings should be contained.
* Schedule Fragments instead of QueryUnits in TaskScheduler
** The QueryUnit creation is postponed until TaskScheduler receives task requests from workers.
** When TaskScheduler receives task requests from workers, it dynamically creates an QueryUnit and assigns one or more fragments.
** The fragment scheduling should take into account the disk load balancing.","Duplicated Code, Long Method, , , , "
"   Rename Class,Rename Method,","Improve function system Currently, function system was designed few years ago. So, there are lack of key features necessary for functions and user-defined functions.

I have discussed this issue with [~epsilon]. We could summary the following issues.


* There is no way to describe explanation of functions.
** For user convenience, Tajo needs to show users function information, including signature, parameters, results, descriptions, and examples.
* TajoMaster registers lots of functions at startup time. This is a burden to maintain registration codes. We need to improve this to automatically register built-in functions in a specific package.
* Currently, the way to find matched function is too strict.
** Due to this function match system, we have to register functions for each parameter type combination.
** Function match mechanism should consider type compatibility. For example, count(val:integer) can be compatibly to count(val:long). In this case, we need to only register count(val:bigint) function. Tajo has to find count(val:bigint) even though count(val:integer) function is called.
* We need more elaborate UDF regestration system.
** Currently, registering a user-defined function requires system restart. There is no way to register UDFs in runtime.
** Tajo should provide a run-time UDF registration system. 
** It should involve user jars distribution.
** It should provide 'CREATE FUNCTION' and 'DROP FUNCTION' statements.
** The registration functions are stored in catalog system, and they should be loaded even though a Tajo cluster is restarted.

This is an umbrella issue. we'll create one or more sub tasks for the above issues.",", "
"   Rename Class,Rename Method,","Add missing visitor methods of AlgebraVisitor and BaseAlgebraVisitor This patch primarily adds all missing operator types' visitor methods of AlgebraVisitor and implements concrete methods in BaseAlgebraVisitor. Currently, BaseAlgebraVisitor may cause incorrect PlanningException because BaseAlgebraVisitor does not handle all operator types. This patch eliminates this potential bug.

In addition, this patch contains two refactors in order to eliminate duplicate names:
* Rename tajo.algebra.DataType to DataTypeExpr
* Rename tajo.algebra.Target to TargetExpr",", "
"   Rename Method,","Add missing visitor methods of AlgebraVisitor and BaseAlgebraVisitor This patch primarily adds all missing operator types' visitor methods of AlgebraVisitor and implements concrete methods in BaseAlgebraVisitor. Currently, BaseAlgebraVisitor may cause incorrect PlanningException because BaseAlgebraVisitor does not handle all operator types. This patch eliminates this potential bug.

In addition, this patch contains two refactors in order to eliminate duplicate names:
* Rename tajo.algebra.DataType to DataTypeExpr
* Rename tajo.algebra.Target to TargetExpr",", "
"   Move And Rename Class,Move Method,Extract Method,Inline Method,Move Attribute,","Divide SubQuery into FSM and execution block parts SubQuery plays two roles, which are a finite state machine and a unit to represent a part of a distributed direct acyclic graph. So, its design and code are somewhat ugly.

The issue is for dividing SubQuery into two parts, a FSM for a subquery and an execution block representation.

This issue would be the first step to improve Tajo's DAG framework.","Duplicated Code, Long Method, , , , , "
"   Rename Class,Extract Method,",Make serializer/deserializer configurable in CSVFile The CSVFile serializer/deserializer is fixed to TextSerializeDeserialize in the LazyTuple. This should be configurable.,"Duplicated Code, Long Method, , "
"   Rename Method,","Improve intermediate file Currently, some intermediate file is text format, Tajo should change to binary format.
* Support configurable storage type(RAW, CSV)
",", "
"   Move Method,Move Attribute,","Adopt AMRMClient to RMContainerAllocator, RMCommunicator Hadoop Yarn 2.0.3 introduced AMRMClient that contains common utility methods for AMRMProtocol. We can make RMContainerAllocator and RMCommunicator simpler by adopting AMRMClient to them.",", , , "
"   Move Method,Extract Method,","Implement extract() function EXTRACT (*field* FROM *source*)

it returns *field* part of *source*.
*source* must be a value expression of type *timestamp*, *time*, or *interval*.
For *date* type, it should be cast to *timestamp* and used.

*field* can be:
- century
- day: for *timestamp*, the day of the month (1 - 31); for *interval*, the number of days
- decade: the year divided by 10
- dow: the day of the week as Sunday(0) to Saturday(6)
- doy: the day of the year (1 - 365)
- epoch: for *timestamp*, the number of seconds since 1970-01-01 00:00:00 UTC (can be negative); for *interval* values, the total number of seconds in the interval
- hour
- isodow: the day of the week as Monday(1) to Sunday(7)
- isoyear: the ISO 8601 year, which begins with the Monday of the week containing the 4th of January, so in early January or late December the ISO year may be different from the Gregorian year.
- microseconds
- millennium
- milliseconds
- minute
- month
- quarter: the quarter of the year (1 - 4)
- second:the seconds field, including fractional parts
- timezone: the time zone offset from UTC, measured in seconds
- timezone_hour: the hour component of the time zone offset
- timezone_minute: the minute component of the time zone offset
- week
- year

Detailed explanation can be found in http://www.postgresql.org/docs/9.1/static/functions-datetime.html
","Duplicated Code, Long Method, , , "
"   Move Class,Move And Rename Class,Move Method,Move Attribute,","Separate tajo-jdbc and tajo-client from tajo-core-backend Currently, tajo-client and tajo-jdbc are included in tajo-core-backend, which depends on lots of third-party libraries. So, even just client programs should include unnecessary third-party libraries. This patch separates tajo-jdbc and tajo-client from tajo-core-backend to individual maven modules. As a result, the client and jdbc's dependencies are more simplified than before.

After this patch, 'mvn package \-Pdist' commands generates tajo jdbc drivers in $\{TAJO_HOME}/tajo-dist/target/tajo-$\{tajo.version}/share/jdbc-dist. There will be the following files in the directory:

{noformat}
joda-time-2.3.jar
tajo-catalog-common-0.8.0-SNAPSHOT.jar
tajo-client-0.8.0-SNAPSHOT.jar
tajo-common-0.8.0-SNAPSHOT.jar
tajo-jdbc-0.8.0-SNAPSHOT.jar
tajo-rpc-0.8.0-SNAPSHOT.jar
tajo-storage-0.8.0-SNAPSHOT.jar
{noformat}

In order to load the Tajo JDBC driver, client programs must be able to locate the all above *JAR* files* and *hadoop's JAR files*.

For that, users should set classpath for them. If the jar files are located in the directory '/usr/local/share/tajo-jdbc' and hadoop binary is located in '/opt/hadoop', you should set classpath as follows:

{code}
export CLASSPATH=`/opt/hadoop/bin/hadoop classpath`:/usr/local/share/tajo-jdbc/*:$CLASSPATH
{code}

Note that the command '$\{HADOOP_HOME}/bin/hadoop classpath' prints out hadoop's classpaths via stdout.",", , , "
"   Rename Method,","Visit methods of LogicalPlanVisitor should take a query block as parameter A logical plan is composed of multiple query blocks. Each logical node must belong to one query block. A query block instance provides lots of information. So, it is essential information in many rewrite rules and optimizer implementations. 

However, so far, individual rewrite rule or optimizer implementation have dealt with query block directly. It may be error-prone and cause duplicated codes. This patch refactors each visitor method of LogicalPlanVisitor to take a query block as a parameter. 

I'm expecting that this change will provide more convenience for rewrite rules and optimization development.",", "
"   Rename Method,","Visit methods of LogicalPlanVisitor should take a query block as parameter A logical plan is composed of multiple query blocks. Each logical node must belong to one query block. A query block instance provides lots of information. So, it is essential information in many rewrite rules and optimizer implementations. 

However, so far, individual rewrite rule or optimizer implementation have dealt with query block directly. It may be error-prone and cause duplicated codes. This patch refactors each visitor method of LogicalPlanVisitor to take a query block as a parameter. 

I'm expecting that this change will provide more convenience for rewrite rules and optimization development.",", "
"   Rename Class,Extract Superclass,Rename Method,Extract Method,","Rename the name 'partition', actually meaning shuffle to 'shuffle'. So far, we have used the word 'partition' to indicate shuffle. This name was originated from repartition in the database studies. However, this name makes us hard to distinguish the shuffle and table partition. We ned to change such names and prefix into 'shuffle'.","Duplicated Code, Long Method, , Duplicated Code, Large Class, "
"   Rename Class,Rename Method,",Supporting TIME types in DatumFactory.createFromInt8 DatumFactory.createFromInt8 should support TIME type,", "
"   Rename Method,","Rename killQuery of QMClientProtocol to closeQuery As we discussed in TAJO-305, the name 'killQuery' is wrong and does not represent its purpose. This patch renames killQuery to closeQuery. This is a trivial change.",", "
"   Rename Method,","Add getParentCount(), getParents(), getParent() functions to DirectedGraph See the title.
In the current implementation, DirectedGraph provides only getParent() because it assumes that there is only one parent. However, multiple parents should be supported in DirectedGraph.",", "
"   Rename Method,Move Method,Extract Method,","Cleanup SubQuery This patch cleans up tajo.master.SubQuery. 

This patch does as follows:
* Add setStartTime() and setFinishTime() methods
* Clean up SubQuery::cleanUp() and rename it to buildAndSetTableMeta()
* Clean up InitAndRequestContainer::transition method
** Divide codes into multiple methods named for their objectives
** Make the control flow very simple
* Add some methods to access member variables of SubQuery
* Remove tajo.master.Priority","Duplicated Code, Long Method, , , "
"   Rename Class,Rename Method,Move Method,Extract Method,Inline Method,","Rewrite the projection part of logical planning The projection part of LogicalPlanner was designed long time ago. It has evolved to support many SQL expressions. However, due to its rough design, it is hard to be improved for further SQL expressions and it causes many bugs.

The current logical planner has the following problems:
* other expressions except for column can be used in group-by clause.
** TAJO-422
* other expressions except for column can not be used in order-by clause.
** TAJO-444
* An expression including some aggregation function must be evaluated in group-by executor.
** As a result, some aggregation operator like HashAggregateExec has to keep all intermediate results of a complex expression in a hash table.
** It also causes frequent GC and large memory consumption.

The too high code complexity also causes many bugs like
* TAJO-434 - java.lang.NullPointerException for invalid column name
* TAJO-428 - CASE WHEN IS NULL condition is a problem using LEFT OUTER JOIN
* TAJO-463 - ProjectionPushDownRule incorrectly rewrite the output schema of StoreTableNode
* TAJO-443 - Order by query gives NullPointerException at at org.apache.tajo.catalog.Schema.getColumnId(Schema.java:142)

The major reason of this problem is as follows:
* TargetListManager keeps only the final target list.
** SELECT col1, sum(col2) as col2, ... <- the final target list
* TargetListManager deals with each expression described in a target list or other clauses like group-by clause as a singleton expression.

The main objective of this issue is to rewrite the projection part of logical planning in order to those problems.

For 2 weeks, I've rewritten this part. I'll submit the patch soon.","Duplicated Code, Long Method, , , , "
"   Rename Method,","Rewrite the projection part of logical planning The projection part of LogicalPlanner was designed long time ago. It has evolved to support many SQL expressions. However, due to its rough design, it is hard to be improved for further SQL expressions and it causes many bugs.

The current logical planner has the following problems:
* other expressions except for column can be used in group-by clause.
** TAJO-422
* other expressions except for column can not be used in order-by clause.
** TAJO-444
* An expression including some aggregation function must be evaluated in group-by executor.
** As a result, some aggregation operator like HashAggregateExec has to keep all intermediate results of a complex expression in a hash table.
** It also causes frequent GC and large memory consumption.

The too high code complexity also causes many bugs like
* TAJO-434 - java.lang.NullPointerException for invalid column name
* TAJO-428 - CASE WHEN IS NULL condition is a problem using LEFT OUTER JOIN
* TAJO-463 - ProjectionPushDownRule incorrectly rewrite the output schema of StoreTableNode
* TAJO-443 - Order by query gives NullPointerException at at org.apache.tajo.catalog.Schema.getColumnId(Schema.java:142)

The major reason of this problem is as follows:
* TargetListManager keeps only the final target list.
** SELECT col1, sum(col2) as col2, ... <- the final target list
* TargetListManager deals with each expression described in a target list or other clauses like group-by clause as a singleton expression.

The main objective of this issue is to rewrite the projection part of logical planning in order to those problems.

For 2 weeks, I've rewritten this part. I'll submit the patch soon.",", "
"   Move Class,Rename Class,Move Method,Inline Method,","Parallel Container Launch of TaskRunnerLauncherImpl TaskRunnerLauncherImpl plays a role to launch remote containers via the proxy of ContainerManager. This implementation sequentially launches containers, so it may take a long time in large clusters. For example, it consumes about 5 seconds for starting 128 containers in our cluster, consisting of 32 nodes. 

To relieve this time-consuming work, we can make this part parallel by using ExecutorService.",", , , "
"   Rename Method,Extract Method,","Upgrade to Netty 4 Currently, rpc package uses Netty 3. Netty 4 is more stable and will get significant performance benefits. We need to upgrade Netty version to 4.","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,Extract Method,","Upgrade to Netty 4 Currently, rpc package uses Netty 3. Netty 4 is more stable and will get significant performance benefits. We need to upgrade Netty version to 4.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,Inline Method,","Change some EvalNode::eval to directly return a Datum value After TAJO-501, we can ensure that all expressions except for AggregationFunctionCallEval can be evaluated by calling once eval() instead of calling eval followed by terminate. In addition, current EvalNode implementation involves unnecessary memory consumption to keep EvalContext for all expressions which are not even aggregation eval. 

If we change EvalNode::eval() to directly return a Datum value, it would reduce the memory consumption and CPU costs.","Duplicated Code, Long Method, , , "
"   Rename Method,","InsertNode and CreateTableNode should play their roles Currently, CreateTableNode and InsertNode are just intermediate representations. They are rewritten to StoreTableNode. But, StoreTableNode does not contain some necessary fields, such as output table, target table, target columns, overwrite flag and create table flag. So far, these fields are kept in QueryContext.

This implementation causes unnecessary and complex rewrite of DistributedQueryHookManager. As a result, it is hard to maintain and manage CREATE/INSERT plans.

The main objective of this issue is to improve LogicalPlanner to use CreateTableNode and InsertNode throughout the planning phase and eliminate complex rewrite in DistributedQueryHookManager.",", "
"   Rename Method,Pull Up Method,Extract Method,","InsertNode and CreateTableNode should play their roles Currently, CreateTableNode and InsertNode are just intermediate representations. They are rewritten to StoreTableNode. But, StoreTableNode does not contain some necessary fields, such as output table, target table, target columns, overwrite flag and create table flag. So far, these fields are kept in QueryContext.

This implementation causes unnecessary and complex rewrite of DistributedQueryHookManager. As a result, it is hard to maintain and manage CREATE/INSERT plans.

The main objective of this issue is to improve LogicalPlanner to use CreateTableNode and InsertNode throughout the planning phase and eliminate complex rewrite in DistributedQueryHookManager.","Duplicated Code, Long Method, , Duplicated Code, "
"   Rename Method,","Add a method to the TajoClient to get finished query lists The current TajoClient only provides a method for retrieving lists of running queries. However, users may want to see the queries which are already finished, failed or even not started. (some queries are not started immediately after TAJO-540.)",", "
"   Rename Class,Pull Up Method,Pull Up Attribute,","Add a sort-based physical executor for column partition store ColumnPartitionStoreExec keeps numerous open files while it is storing all data. In addition, it's random write gives burden to HDFS namenode.

To solve this problem, I would like to propose a sort-based physical executor for column partition store. It assumes that input tuples are sorted in an ascending or descending order of partition keys. It means that it needs extra sort operation. But, it opens only one file simultaneously. It writes all data sequentially. In many cases, it would be the best choice for column partition store.",", Duplicated Code, Duplicated Code, "
"   Rename Method,Move Method,Extract Method,","Add a sort-based physical executor for column partition store ColumnPartitionStoreExec keeps numerous open files while it is storing all data. In addition, it's random write gives burden to HDFS namenode.

To solve this problem, I would like to propose a sort-based physical executor for column partition store. It assumes that input tuples are sorted in an ascending or descending order of partition keys. It means that it needs extra sort operation. But, it opens only one file simultaneously. It writes all data sequentially. In many cases, it would be the best choice for column partition store.","Duplicated Code, Long Method, , , "
"   Rename Method,","Improve distributed merge sort In Tajo, sort operator is similar to merge sort, and it works in a distributed manner. The first sort phase sorts each fragment in local machine, the intermediate data are shuffled in range partition, and then the second sort phase in each node sorts the range-partitioned data.

However, the second sort phase reads all shuffled data via one scanner. It misses the opportunity to exploit already-sorted data. This patch improves the second sort phase to merge directly multiple already-sorted intermediate data sets. It significantly reduces the response time of sort queries.

I carried out some simple benchmark with the following query on TPC-H 100GB data sets:
{code:sql}
select l_orderkey from lineitem order by l_orderkey;
{code}

The lineitem table occupies 75GB. The query response time are dramatically reduced from 480 to 260 secs. This patch exploits the design of TAJO-36. So, this patch requires TAJO-36.",", "
"   Move Class,Rename Method,Move Method,Extract Method,","Refactoring Tajo RPC In the current implementation, all client rpc use same channel pool.
It can cause a channel closed exception. we need both shared pool and new pool.
In details, 
* Fix the TajoAsyncDispatcher hang
* Fix the Fetcher timeout
* Fix the TaskRunner thread leak
* Fix the client rpc reconnecting
* Fix the unittest failure(No available resources)
* Improve RPC thread sharing","Duplicated Code, Long Method, , , "
"   Rename Class,Rename Method,","Improve distinct aggregation query processing Currently, distinct aggregation queries are executed as follows:
* the first stage: it just shuffles tuples by hashing grouping keys.
* the second stage: it sorts them and executes sort aggregation.

This way executes queries including distinct aggregation functions with only two stages. But, it leads to large intermediate data during shuffle phase.

This kind of query can be rewritten as two queries:

{code:title=original query}
SELECT grp1, grp2, count(*) as total, count(distinct grp3) as distinct_col from rel1 group by grp1, grp2;
{code}

{code:title=rewritten query}
SELECT grp1, grp2, sum(cnt) as total, count(grp3) as distinct_col from (
SELECT grp1, grp2, grp3, count(*) as cnt from rel1 group by grp1, grp2, grp3) tmp1 group by grp1, grp2
) table1;
{code}

I'm expecting that this rewrite will significantly reduce the intermediate data volume and query response time in most cases.",", "
"   Rename Method,Move Method,Move Attribute,","Explaning a logical node should use ExplainLogicalPlanVisitor. Currently, many parts use LogicalNode::toString() for explaning plans. But, we already have ExplainLogicalPlanVisitor class to generate pretty print strings.

This patch improves logical planning related parts to use ExplainLogicalPlanVisitor instead of toString().

For this, I added PlannerUtil::buildExplainString for generating pretty print explain strings and simplified obsolete toString() methods of all LogicalNodes.

It much improves the readability of explain string. I expect that it would be helpful for debugging and users' understanding of query plans.",", , , "
"   Rename Method,Extract Method,","Implement query unit cases for HCatalogStore. We already have HCatalogStore for catalog store. And we already have TestHCatalogStore. But, it just verifies CatalogStore methods. 

Therefore, we can't verify PhysicalOperator operation for HCatalogStore, and it is lack of HCatalogStore. 

For reference, we already found _NoSuchColumnException_ at [TAJO-641|https://issues.apache.org/jira/browse/TAJO-641].","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,","Support quoted identifiers In SQL standards, non-ascii identifiers are supported by using double quotation as follows:
{code}
SELECT a, b, ""씨"", d from table1
{code}

We should support quoted identifiers.",", "
"   Extract Superclass,Move Method,Extract Method,Move Attribute,","Improve the IN operator to support sub queries Currently, the IN operator can be used with only sets of values.
We need to improve it to support sub queries as the following example query.
{noformat}
tajo> select * from nation where n_regionkey in (select r_regionkey from region); 
{noformat}","Duplicated Code, Long Method, , , , Duplicated Code, Large Class, "
"   Rename Method,Extract Method,",Broadcast JOIN should supports multiple tables Currently implementation Tajo uses single table for broadcast join even if there are several small. Tajo should be supported this feature.,"Duplicated Code, Long Method, , "
"   Rename Method,","Supports expressions in 'IN predicate' Tajo does not support expression IN statement.

{noformat}
tpch100> select * from nation where n_nationkey in (1, 1+1, 1+2);
ERROR: extraneous input '+' expecting {',', ')'}
LINE 1:47 select * from nation where n_nationkey in (1, 1+1, 1+2)
{noformat}",", "
"   Move Method,Extract Method,",Arrange TajoCli output message. See title. TajoCli prints information and error message with various format. These message must be arranged because ETL program uses TajoCli's information or error message.,"Duplicated Code, Long Method, , , "
"   Move Method,Extract Method,Move Attribute,","APIs in TajoClient and JDBC should be case sensitive. Identifiers that APIs in TajoClient and Tajo JDBC driver take are normalized in client API side. So, identifiers composed of upper and lower mixed characters should be used with double quote (""). This convention is different from existing JDBC driver's convention. It makes ugly code when users use TajoClient.

This patch changes their APIs to be case sensitive. In addition, this patch adds some missing JDBC APIs and fixes wrong behaviors of {{getTables()}} and {{getColumns()}} in JDBC APIs.","Duplicated Code, Long Method, , , , "
"   Rename Method,","APIs in TajoClient and JDBC should be case sensitive. Identifiers that APIs in TajoClient and Tajo JDBC driver take are normalized in client API side. So, identifiers composed of upper and lower mixed characters should be used with double quote (""). This convention is different from existing JDBC driver's convention. It makes ugly code when users use TajoClient.

This patch changes their APIs to be case sensitive. In addition, this patch adds some missing JDBC APIs and fixes wrong behaviors of {{getTables()}} and {{getColumns()}} in JDBC APIs.",", "
"   Rename Method,","Implements function COALESCE See title.
The next description is from the postgresql document(http://www.postgresql.org/docs/9.1/static/functions-conditional.html). 

{noformat}
COALESCE(value [, ...])
{noformat}
The COALESCE function returns the first of its arguments that is not null. Null is returned only if all arguments are null. It is often used to substitute a default value for null values when data is retrieved for display, for example:
{code:sql}
SELECT COALESCE(description, short_description, '(none)') ...
{code}
Like a CASE expression, COALESCE only evaluates the arguments that are needed to determine the result; that is, arguments to the right of the first non-null argument are not evaluated. This SQL-standard function provides capabilities similar to NVL and IFNULL, which are used in some other database systems.

",", "
"   Pull Up Method,Extract Method,",JDBC driver should support cancel() method. Some OLAP or ETL tool call JDBC's cancel() function when a query is too late. So Tajo's JDBC should support cancel() function in Statement or PreparedStatement class.,"Duplicated Code, Long Method, , Duplicated Code, "
"   Rename Method,Move Method,Extract Method,","Improve shuffle URI Currently, shuffle uri use the string field. but most params is a number in uri
We need change to Varint of protocol buffer.
https://developers.google.com/protocol-buffers/docs/encoding#varints","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Refactor and Improve TajoCli This patch improves TajoCli. This improvement is not backward compatible, but this improvement makes TajoCli more intuitive and similar to PostgreSQL cli. So, I believe this gives users better experience. In addition, I've added some useful options.

In detail, the patch does as follows:
* Refactor TajoCli to be based on command pattern
* multiple lines for one SQL statement are recorded as one history
* command prefix '/' is replaced by '\'.
* the command '/t' is replaced by '\d'.
** '\d' lists tables or describes a table.
* jline version is updated to 2.11.
* add a help command '\?' to list all commands that can be used in TajoCli
* add a cli option '-c' for running a single SQL statement or command
* add a cli option '-f' for executing statements included in a text file
* add a script 'bin/tsql'","Duplicated Code, Long Method, , "
"   Move Class,Rename Method,Pull Up Method,Push Down Method,Extract Method,Pull Up Attribute,Push Down Attribute,","Implicit type conversion support Arithmetic and logical operations between different operands are usual in SQL queries. Function invocations with parameters which are different to the parameters of function definition are also usual. Implicit type conversion should occur in such cases.

Currently, however, Tajo does not support implicit type conversion. We should implement this feature.","Duplicated Code, Long Method, , , Duplicated Code, , Duplicated Code, "
"   Rename Method,","Multiple distinct should be supported. Currently the following query is not supported.
{code}
default> select id, count(distinct age), count(distinct name) from table2 group by id;
ERROR: different DISTINCT columns are not supported yet: age, name
{code}",", "
"   Rename Method,","Multiple distinct should be supported. Currently the following query is not supported.
{code}
default> select id, count(distinct age), count(distinct name) from table2 group by id;
ERROR: different DISTINCT columns are not supported yet: age, name
{code}",", "
"   Rename Class,Rename Method,Move Method,Extract Method,Move Attribute,","Datetime type refactoring. Currently Tajo uses Joda time library for Datetime related features.
I tested Joda library with a next test code. It is difficult to express various time range with Joda library. 
So I propose that Tajo uses PostgreSQL style date/time features. I already migrated PostgreSQL's date/time code to Tajo. I will attach that patch soon.
{code}
Calendar cal = Calendar.getInstance();
cal.set(Calendar.YEAR, 1582);
cal.set(Calendar.MONTH, 9);
cal.set(Calendar.DAY_OF_MONTH, 14);

SimpleDateFormat df = new SimpleDateFormat(""yyyy-MM-dd HH:mm:ss"");

DateTime defaultCalDate = new DateTime(1582, 10, 14, 10, 0, 0, 0);
Chronology julianChrono = JulianChronology.getInstance();
DateTime julianCalDate = new DateTime(1582, 10, 14, 10, 0, 0, 0, julianChrono);

System.out.println(""Java Calendar :"" + df.format(cal.getTime()));
System.out.println(""ISO Calendar : "" + defaultCalDate);
System.out.println(""Julian Calendar: "" + julianCalDate);

System.out.println(""ISO Calendar's dayOfWeek: "" + defaultCalDate.getDayOfWeek());
System.out.println(""Julian Calendar's dayOfWeek: "" + julianCalDate.getDayOfWeek());

System.out.println(""ISO Calendar's getCenturyOfEra: "" + defaultCalDate.getCenturyOfEra());
System.out.println(""Julian Calendar's getCenturyOfEra: "" + julianCalDate.getCenturyOfEra());
{code}

{noformat}
Java Calendar :1582-10-24 16:49:35
ISO Calendar : 1582-10-14T10:00:00.000+08:27:52
Julian Calendar: 1582-10-14T10:00:00.000+08:27:52
ISO Calendar's dayOfWeek: 4
Julian Calendar's dayOfWeek: 7
ISO Calendar's getYearOfCentury: 15
Julian Calendar's getYearOfCentury: 16
{noformat}","Duplicated Code, Long Method, , , , "
"   Move Method,Extract Method,","Task scheduling with considering disk load balance After HDFS-3672, HDFS namenode provides not only block locations but also disk locations. With this information, Tajo can make scheduling decision with considering both locality and disk load balancing. It will improve query performance.","Duplicated Code, Long Method, , , "
"   Rename Method,Move Method,Extract Method,Move Attribute,","Clean up the task history in woker The task history currently store the thread in memory. it can’t store to other storage(hdfs, local file ..). It would be nice if task history separate from taskrunner","Duplicated Code, Long Method, , , , "
"   Rename Method,","Implements TRUNCATE table. Tajo should support TRUNCATE table feature.
{noformat}
TRUNCATE [TABLE] <table name1> [, <table name2>, ...]
{noformat}",", "
"   Rename Class,Move Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Integration of tajo algebra module and SQL parser. In the current implementation, QueryAnalyzer transforms a sql statement into a data structure that represents a parser tree and a query block. They has the following limitations: 
* They can't support multiple block queries including table and scalar subqueries. 
* They are tightly coupled to a certain grammar.

Before ASF incubation, I have developed tajo-algebra and tajo-frontend-sql. Tajo-algebra is a kind of intermediate layer. It represents a just relational algebraic expression. With this, we can do more interesting things. For example, we can enable users to describe a logical plan for a certain query. It also very helpful to support an another DSL.

For this work, tajo-algebra should be improved to support full specifications, and we should rewrite a SQL parser to use tajo-algebra.","Duplicated Code, Long Method, , , , , "
"   Rename Method,","ConstEval should not be included in target list of projectable nodes In some applications, aliased constant values can be used in WHERE, GROUP BY, HAVING, ORDER BY clauses. For those cases, current planner evaluates constant value in target list of projectable nodes.

{code}
SELECT '1994' end as year, ... FROM lineitem WHERE group by year;
{code}

This approach works well so far, but there are rooms for significant improvement.

The main problem is that constant target requires many workaround code in NamedExprManager, and TargetListManager. As a result, it makes code complexity higher. The second problem is that many constant values evaluated in each row consume unnecessary I/O and network bandwidth for storing and transmitting.

The solution seems to be simple. In logical planning phase, we should rewrite column references which actually indicates constant values.",", "
"   Extract Interface,Extract Method,Push Down Attribute,","Runtime code generation for evaluating expression trees We have used EvalNode for two purposes:
* logical planning of expressions
* evaluation of expressions

EvalNode still is very nice for the purpose of logical planning. But, each EvalNode tree takes Datum included in a tuple and results in a Datum as a evaluation result. 

So, the current approach requires many object creations, and causes interpret overheads, meaning that each one evaluation involves tree traverses and many function calls. interpretation involves also many branches, and it is harmful to CPU pipelining.

I propose Java byte code generation for each expression, and I'll use ASM (http://asm.ow2.org/) for it. This approach will write native java byte code, eliminating many condition branches and function calls. In addition, it is easier to deal with java primitive data types for expressions.","Duplicated Code, Long Method, , , Large Class, "
"   Rename Class,Extract Method,","Refactoring Mysql/Maria Catalog Store MysqlStore and MariaDBStore are almost same.
so, it is better to make one parent class.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Session variables should override query configs in TajoConf. Currently, we should use tajo-site in order to change the configurations related queries, such as optimization option or some parameters. It is never practical because we need to restart a Tajo cluster in order to change the config.

The main purpose of this issue is to refactor the system of session variable and some part to recognize query configs to accept the session variables. Also, when there are duplicated configs in session and TajoConf, session variables should override the existing config of TajoConf.","Duplicated Code, Long Method, , "
"   Rename Method,Pull Up Method,Extract Method,","Session variables should override query configs in TajoConf. Currently, we should use tajo-site in order to change the configurations related queries, such as optimization option or some parameters. It is never practical because we need to restart a Tajo cluster in order to change the config.

The main purpose of this issue is to refactor the system of session variable and some part to recognize query configs to accept the session variables. Also, when there are duplicated configs in session and TajoConf, session variables should override the existing config of TajoConf.","Duplicated Code, Long Method, , Duplicated Code, "
"   Rename Method,Extract Method,","Output file can be punctuated depending on the file size. There are some file formats (e.g., Parquet) which are not splittable. They can usually span multiple HDFS blocks if one file is very large. It causes remote HDFS access and limits the parallel degree, resulting in significant performance degradation.

We can solve this problem if StoreTableExec or {Col|SortBased}PartitionStoreExec can punctuate the final output file according to the written size.

In addition, we need to support a session variable to determine the per file size of final output files. So, TAJO-928 blocks this issue.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,Inline Method,Pull Up Attribute,","Output file can be punctuated depending on the file size. There are some file formats (e.g., Parquet) which are not splittable. They can usually span multiple HDFS blocks if one file is very large. It causes remote HDFS access and limits the parallel degree, resulting in significant performance degradation.

We can solve this problem if StoreTableExec or {Col|SortBased}PartitionStoreExec can punctuate the final output file according to the written size.

In addition, we need to support a session variable to determine the per file size of final output files. So, TAJO-928 blocks this issue.","Duplicated Code, Long Method, , , Duplicated Code, "
"   Rename Class,Move Class,Rename Method,Move Method,Inline Method,Move Attribute,","Upgrade Parquet to 1.5.0. Parquet 1.5.0 was released in May 2014. We need to upgrade the parquet version to 1.5.0. For it, it just needs modify parquet version and parquet format version in stroage/pom.xml file.",", , , , "
"   Rename Method,Extract Method,",RawFile should release a DirectBuffer immediately RawFile allocated memory is a native DirectBuffer. This memory will be freed when the finalize method is called by gc.,"Duplicated Code, Long Method, , "
"   Move Method,Extract Method,",Add database selection submit button in catalogview.jsp for text based browse. See the title. User can not select the database in the text based browser like elinks. This should be improved.,"Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","provide access to component parameters from within mixins A mixin can't access the parameters of a component because the Bindings property of the InternalComponentResourcesImpl class is private and the respective interface does not provide a access method. 

I was trying to create a mixin that would render only the value of a form element (without the tags) when it was in a certain state. There also might be use cases where mixins are used to collect data from the components they are attached and therefore also needs access to the components parameters.

see threads:
http://www.nabble.com/Antwort%3A--T5--how-to-read-the-value-of-a-component-parameter-within-a-mixin-tf4487995.html
http://www.nabble.com/-T5--how-to-read-the-value-of-a-component-parameter-within-a-mixin-tf4487597.html","Duplicated Code, Long Method, , "
"   Move Class,Move Method,Move Attribute,","In places where an invalid key is used to accessed a named value, Tapestry should report the possible names better (using HTML lists, rather than a long comma-separated string) When you use an invalid name, such as a misspelling of a component type, Tapestry is great about listing out the possible names you could have used. However, when there's a lot of those (such as a service id, or a component type, or page name in a large application), the format is hard to parse: long, long, long comma-separated list.

A better approach would be to use two or three columns of <ul> and <li> elements to present the options in a useful order.",", , , "
"   Rename Method,Extract Method,","In places where an invalid key is used to accessed a named value, Tapestry should report the possible names better (using HTML lists, rather than a long comma-separated string) When you use an invalid name, such as a misspelling of a component type, Tapestry is great about listing out the possible names you could have used. However, when there's a lot of those (such as a service id, or a component type, or page name in a large application), the format is hard to parse: long, long, long comma-separated list.

A better approach would be to use two or three columns of <ul> and <li> elements to present the options in a useful order.","Duplicated Code, Long Method, , "
"   Rename Method,",Rewrite live reload integration tests to use new SeleniumTestCase instead of deprecated AbstractIntegrationTestSuite 0,", "
"   Rename Method,","Add support for startup() methods in modules, as an easy way to add startup logic There's a mechanism for adding startup logic, by contributing to the RegistryStartup service configuration.

It would be nice if there was an optimized way to accomplish the same thing, i.e., starutp() methods (with injected parameters) as an easier way to accomplish the same thing.",", "
"   Move Method,Move Attribute,",Provide hook to post-process properties files before rolling them into component Messages Client has a use case where the properties file contain keywords that are substituted after the raw file is read. Hacking this into T5.1 took a lot of effort (read: cut-n-paste). It would be nice if there was a cleaner way.,", , , "
"   Rename Method,",The application global message catalog should be injectable into services It would be nice if services could see the global message catalog.,", "
"   Move Class,Extract Method,Pull Up Attribute,","Live class reloading should extend to proxied objects (such as from ObjectLocator.proxy() In other words, if it is proxiable (because there's an interface and an implementation class for that interface) then make with the live class reloading.","Duplicated Code, Long Method, , Duplicated Code, "
"   Rename Class,Push Down Attribute,","Live class reloading should extend to proxied objects (such as from ObjectLocator.proxy() In other words, if it is proxiable (because there's an interface and an implementation class for that interface) then make with the live class reloading.",", , "
"   Move Method,Extract Method,","Live class reloading should extend to proxied objects (such as from ObjectLocator.proxy() In other words, if it is proxiable (because there's an interface and an implementation class for that interface) then make with the live class reloading.","Duplicated Code, Long Method, , , "
"   Move Method,Move Attribute,",Introduce public service responsible for handling page activation Needed for a client with a specific need to extend the semantics of page activation (adding a second event for security checks).,", , , "
"   Rename Method,","Define new namespace, p:, for block parameters Towards greater conciseness:

How about making these equivalent:

<t:beaneditform t:id=""new"" object=""newPosting"" submitlabel=""message:post"" include=""title,content"">

<t:parameter name=""content"">
<t:label for=""content""/>
<br/>
<t:richtextarea t:id=""content"" rows=""10"" cols=""80"" value=""newPosting.content""/>
</t:parameter>

</t:beaneditform>

-and-

<t:beaneditform t:id=""new"" object=""newPosting"" submitlabel=""message:post"" include=""title,content"">

<p:content>
<t:label for=""content""/>
<br/>
<t:richtextarea t:id=""content"" rows=""10"" cols=""80"" value=""newPosting.content""/>
</p:content>

Obviuosly, p: would need to be mapped to a Tapestry namespace.",", "
"   Rename Method,","When in development mode, Tapestry should pretty-print JSON content Reading giant blobs of JSON can be a challenge to read given how deeply Tapestry tends to nest everything.",", "
"   Extract Superclass,Rename Method,Pull Up Method,Extract Method,Inline Method,","JavaScript initialization inside the partial page render Ajax response should be unquoted Currently, all the JS in an Ajax partial page render update comes inside the ""script"" key, as a long string. This is inefficient, since so many characters and quotes need to be escaped, and it is also harder to read and debug (especially since pretty printing is turned off, since pretty printing inside a string is no more readable that not).

Instead, the calls to Tapestry.init() can be handled differently in an Ajax response, and encoded in a new key, ""inits"", an array of Tapestry.init() parameters (multiple calls to init because of the scheduling: early, normal, late).","Duplicated Code, Long Method, , Duplicated Code, Large Class, , Duplicated Code, "
"   Move And Rename Class,Rename Method,Inline Method,","Easy way to customize search locations for page and component templates It would be nice if there was a standard, public way to override Tapestry's default search for page and component templates.",", , "
"   Rename Method,","Eliminate page pooling using shared page instances that separate their structure from the mutable state This has been suggested before, but the recent changes to class transformation API makes it much more reasonable to accomplish.

The goal here is to identify all transient or mutable state in the page and store it via the PerThreadManager. This will be invisible to user code; the pages will appear to be individual instances with internal state ... but in fact, it will be a single instance (per locale) with all internal, mutable state stored elsewhere.

Because this changes the semantics of some aspects of the component class transformation pipeline, there will be a compatibility mode that will allow pages to be pooled as with 5.1, while any third party libraries that contribute workers update.

Why do all this? For large applications with very complex pages, this will be a big win, as Tapestry has been shown to strain the limits of available JVM heap (surprising to me, but apparently true) once you have a few dozen (or hundred) page instances (of each page type) floating around.",", "
"   Rename Method,","Eliminate page pooling using shared page instances that separate their structure from the mutable state This has been suggested before, but the recent changes to class transformation API makes it much more reasonable to accomplish.

The goal here is to identify all transient or mutable state in the page and store it via the PerThreadManager. This will be invisible to user code; the pages will appear to be individual instances with internal state ... but in fact, it will be a single instance (per locale) with all internal, mutable state stored elsewhere.

Because this changes the semantics of some aspects of the component class transformation pipeline, there will be a compatibility mode that will allow pages to be pooled as with 5.1, while any third party libraries that contribute workers update.

Why do all this? For large applications with very complex pages, this will be a big win, as Tapestry has been shown to strain the limits of available JVM heap (surprising to me, but apparently true) once you have a few dozen (or hundred) page instances (of each page type) floating around.",", "
"   Rename Method,","Eliminate page pooling using shared page instances that separate their structure from the mutable state This has been suggested before, but the recent changes to class transformation API makes it much more reasonable to accomplish.

The goal here is to identify all transient or mutable state in the page and store it via the PerThreadManager. This will be invisible to user code; the pages will appear to be individual instances with internal state ... but in fact, it will be a single instance (per locale) with all internal, mutable state stored elsewhere.

Because this changes the semantics of some aspects of the component class transformation pipeline, there will be a compatibility mode that will allow pages to be pooled as with 5.1, while any third party libraries that contribute workers update.

Why do all this? For large applications with very complex pages, this will be a big win, as Tapestry has been shown to strain the limits of available JVM heap (surprising to me, but apparently true) once you have a few dozen (or hundred) page instances (of each page type) floating around.",", "
"   Extract Interface,Rename Method,Extract Method,","New annotations @Decorate and @Advise to identify methods that decorate or annotate services This would be similar to (and reuse logic from, one would hope) the @Contribute annotation.","Duplicated Code, Long Method, , Large Class, "
"   Rename Method,Extract Method,","Extend Link with new methods for producing absolute URLs (that include scheme, hostname, etc.) 0","Duplicated Code, Long Method, , "
"   Rename Method,",BeanBlockContribution should be split into two sub-classes: EditBlockContribution and DisplayBlockContribution 0,", "
"   Extract Superclass,Rename Method,","Option to disable live service reloading via a JVM system property Some people are uncomfortable with live class reloading, or feel that it may be too resource intensive, even for development. There are also some ""leaky abstractions"" around the use of non-public methods from live-reloaded classes. In any case, an option to turn it off it desired.",", Duplicated Code, Large Class, "
"   Rename Method,","Define a special CSS class to prevent a client-side form from submitting normally (for Ajax use cases) Just occured to me during class; right now there's some logic about getting the FormManager for the form, and I'm hoping to minimize that kind of thing now, before locking down the Javascript APIs in 5.3.",", "
"   Rename Method,","When contributing to a service configuration, values should be coerced to the correct type rather than rejected if not the correct type This will make it much easier to migrate the behavior of services as long as contributions of the old type can be coerced to contributions of the new type.

It may also make it easier to contribute symbol constants, as it will be possible to pass true, not ""true"", or literal numbers.",", "
"   Rename Method,","@Autobuild annotation for parameters, implicitly invokes ObjectLocator.autobuild() Rather than injecting the ObjectLocator to invoke autobuild() on it, it would be nice if we could inject the RESULT of ObjectLocator.autobuild().

",", "
"   Rename Method,",Functional programming improvements Add support for Tuples to the tapestry-func library,", "
"   Move Class,Rename Method,",The services used to handle live reloading should be made public User services should be able to register for check-for-updates and/or invalidation event notifications without having to use any Tapestry internals. Firing such events should still be private/internal.,", "
"   Extract Interface,Extract Method,",Tapestry tests should be able to be run from more than just Jetty Tapestry and its client applications may intentionally or inadvertently have servlet container specific functionality. Tapestry should provide tests for Tomcat as well as Jetty.,"Duplicated Code, Long Method, , Large Class, "
"   Rename Method,","Allow for multiple application root packages by contributing additional LibraryMappings with empty virtual folder So, the first package is defined in web.xml, but addition ones can be defined as ComponentClassResolver contributions.",", "
"   Move Method,Move Attribute,","Provide basic integration with JPA 2 Provide basic integration with JPA 2. The goals are:

- Support for multiple EntityManagers
- Injection of EntityManager into both: components and services
- Provide a way to configure EntityManagers programmatically, not only via persistence.xml 
- Create ValueEncoder for entities on-the-fly
- Provide ""entity"" PersistentFieldStrategy
- Provide ""entity"" ApplicationStatePersistenceStrategy
- Provide @CommitAfter annotations

We also should consider if a tapestry-orm or similar project is needed in order to reuse common source code.",", , , "
"   Rename Class,Extract Interface,","Provide basic integration with JPA 2 Provide basic integration with JPA 2. The goals are:

- Support for multiple EntityManagers
- Injection of EntityManager into both: components and services
- Provide a way to configure EntityManagers programmatically, not only via persistence.xml 
- Create ValueEncoder for entities on-the-fly
- Provide ""entity"" PersistentFieldStrategy
- Provide ""entity"" ApplicationStatePersistenceStrategy
- Provide @CommitAfter annotations

We also should consider if a tapestry-orm or similar project is needed in order to reuse common source code.",", Large Class, "
"   Move Class,Rename Class,","Provide basic integration with JPA 2 Provide basic integration with JPA 2. The goals are:

- Support for multiple EntityManagers
- Injection of EntityManager into both: components and services
- Provide a way to configure EntityManagers programmatically, not only via persistence.xml 
- Create ValueEncoder for entities on-the-fly
- Provide ""entity"" PersistentFieldStrategy
- Provide ""entity"" ApplicationStatePersistenceStrategy
- Provide @CommitAfter annotations

We also should consider if a tapestry-orm or similar project is needed in order to reuse common source code.",", "
"   Move Method,Extract Method,","Provide basic integration with JPA 2 Provide basic integration with JPA 2. The goals are:

- Support for multiple EntityManagers
- Injection of EntityManager into both: components and services
- Provide a way to configure EntityManagers programmatically, not only via persistence.xml 
- Create ValueEncoder for entities on-the-fly
- Provide ""entity"" PersistentFieldStrategy
- Provide ""entity"" ApplicationStatePersistenceStrategy
- Provide @CommitAfter annotations

We also should consider if a tapestry-orm or similar project is needed in order to reuse common source code.","Duplicated Code, Long Method, , , "
"   Rename Method,","Deprecate MultiZoneUpdate, replace with an injectable service to collect zone updates MultiZoneUpdate presumes that there's a single place where all the zone to be updated are known. This is not necessarilly the case.

I'd like to see something like:

@Inject
private ZoneUpdater zoneUpdater();

Object onSuccess()
{
zoneUpdater.update(""foo"", fooBlock);
zoneUpdater.update(""bar"", barBlock);

return myZone.getBody();
}

The main point here is that different event handlers would all be able to invoke ZoneUpdater.update() .

This would also allow a single response to render main content (for the requesting Zone on the client) plus zone updates to named zones.",", "
"   Pull Up Method,Pull Up Attribute,","New features for SeleniumTestCase Refactor up a few standard methods often added in subclasses, such as opening a page by page name (i.e., so we can avoid the ugliness of getBaseURL() ), expose the underlying selenium object (for when SeleniumTestCase doesn't implement *all* the method of Selenium), and a few other common operations.",", Duplicated Code, Duplicated Code, "
"   Rename Method,Move Method,Inline Method,","Reduce memory utilization of Tapestry page instances This is an umbrella bug for a number of changes aimed at reducing the overall memory footprint of a Tapestry page. Tapestry pages have been seen to be quite large in some production sites, and the shared-instance strategy introduced in 5.2 is still insufficient. A significant amount of space is consumed using Maps and Lists (for bindings, of sub-components, etc.) in a way that is optimized for a smaller code base and efficient read access, even though most of the information is only read when a page is first loaded, and is rarely needed or accessed after that.",", , , "
"   Rename Method,","Reduce memory utilization of Tapestry page instances This is an umbrella bug for a number of changes aimed at reducing the overall memory footprint of a Tapestry page. Tapestry pages have been seen to be quite large in some production sites, and the shared-instance strategy introduced in 5.2 is still insufficient. A significant amount of space is consumed using Maps and Lists (for bindings, of sub-components, etc.) in a way that is optimized for a smaller code base and efficient read access, even though most of the information is only read when a page is first loaded, and is rarely needed or accessed after that.",", "
"   Rename Class,Rename Method,","Reduce memory utilization of Tapestry page instances This is an umbrella bug for a number of changes aimed at reducing the overall memory footprint of a Tapestry page. Tapestry pages have been seen to be quite large in some production sites, and the shared-instance strategy introduced in 5.2 is still insufficient. A significant amount of space is consumed using Maps and Lists (for bindings, of sub-components, etc.) in a way that is optimized for a smaller code base and efficient read access, even though most of the information is only read when a page is first loaded, and is rarely needed or accessed after that.",", "
"   Rename Method,","Reduce memory utilization of Tapestry page instances This is an umbrella bug for a number of changes aimed at reducing the overall memory footprint of a Tapestry page. Tapestry pages have been seen to be quite large in some production sites, and the shared-instance strategy introduced in 5.2 is still insufficient. A significant amount of space is consumed using Maps and Lists (for bindings, of sub-components, etc.) in a way that is optimized for a smaller code base and efficient read access, even though most of the information is only read when a page is first loaded, and is rarely needed or accessed after that.",", "
"   Rename Method,","Reduce memory utilization of Tapestry page instances This is an umbrella bug for a number of changes aimed at reducing the overall memory footprint of a Tapestry page. Tapestry pages have been seen to be quite large in some production sites, and the shared-instance strategy introduced in 5.2 is still insufficient. A significant amount of space is consumed using Maps and Lists (for bindings, of sub-components, etc.) in a way that is optimized for a smaller code base and efficient read access, even though most of the information is only read when a page is first loaded, and is rarely needed or accessed after that.",", "
"   Rename Method,","Reduce memory utilization of Tapestry page instances This is an umbrella bug for a number of changes aimed at reducing the overall memory footprint of a Tapestry page. Tapestry pages have been seen to be quite large in some production sites, and the shared-instance strategy introduced in 5.2 is still insufficient. A significant amount of space is consumed using Maps and Lists (for bindings, of sub-components, etc.) in a way that is optimized for a smaller code base and efficient read access, even though most of the information is only read when a page is first loaded, and is rarely needed or accessed after that.",", "
"   Rename Method,","Reduce memory utilization of Tapestry page instances This is an umbrella bug for a number of changes aimed at reducing the overall memory footprint of a Tapestry page. Tapestry pages have been seen to be quite large in some production sites, and the shared-instance strategy introduced in 5.2 is still insufficient. A significant amount of space is consumed using Maps and Lists (for bindings, of sub-components, etc.) in a way that is optimized for a smaller code base and efficient read access, even though most of the information is only read when a page is first loaded, and is rarely needed or accessed after that.",", "
"   Move Class,Rename Method,","Reduce memory utilization of Tapestry page instances This is an umbrella bug for a number of changes aimed at reducing the overall memory footprint of a Tapestry page. Tapestry pages have been seen to be quite large in some production sites, and the shared-instance strategy introduced in 5.2 is still insufficient. A significant amount of space is consumed using Maps and Lists (for bindings, of sub-components, etc.) in a way that is optimized for a smaller code base and efficient read access, even though most of the information is only read when a page is first loaded, and is rarely needed or accessed after that.",", "
"   Rename Method,","Reduce memory utilization of Tapestry page instances This is an umbrella bug for a number of changes aimed at reducing the overall memory footprint of a Tapestry page. Tapestry pages have been seen to be quite large in some production sites, and the shared-instance strategy introduced in 5.2 is still insufficient. A significant amount of space is consumed using Maps and Lists (for bindings, of sub-components, etc.) in a way that is optimized for a smaller code base and efficient read access, even though most of the information is only read when a page is first loaded, and is rarely needed or accessed after that.",", "
"   Rename Method,","Reduce memory utilization of Tapestry page instances This is an umbrella bug for a number of changes aimed at reducing the overall memory footprint of a Tapestry page. Tapestry pages have been seen to be quite large in some production sites, and the shared-instance strategy introduced in 5.2 is still insufficient. A significant amount of space is consumed using Maps and Lists (for bindings, of sub-components, etc.) in a way that is optimized for a smaller code base and efficient read access, even though most of the information is only read when a page is first loaded, and is rarely needed or accessed after that.",", "
"   Rename Method,","Reduce memory utilization of Tapestry page instances This is an umbrella bug for a number of changes aimed at reducing the overall memory footprint of a Tapestry page. Tapestry pages have been seen to be quite large in some production sites, and the shared-instance strategy introduced in 5.2 is still insufficient. A significant amount of space is consumed using Maps and Lists (for bindings, of sub-components, etc.) in a way that is optimized for a smaller code base and efficient read access, even though most of the information is only read when a page is first loaded, and is rarely needed or accessed after that.",", "
"   Rename Method,","Reduce memory utilization of Tapestry page instances This is an umbrella bug for a number of changes aimed at reducing the overall memory footprint of a Tapestry page. Tapestry pages have been seen to be quite large in some production sites, and the shared-instance strategy introduced in 5.2 is still insufficient. A significant amount of space is consumed using Maps and Lists (for bindings, of sub-components, etc.) in a way that is optimized for a smaller code base and efficient read access, even though most of the information is only read when a page is first loaded, and is rarely needed or accessed after that.",", "
"   Rename Method,","Reduce memory utilization of Tapestry page instances This is an umbrella bug for a number of changes aimed at reducing the overall memory footprint of a Tapestry page. Tapestry pages have been seen to be quite large in some production sites, and the shared-instance strategy introduced in 5.2 is still insufficient. A significant amount of space is consumed using Maps and Lists (for bindings, of sub-components, etc.) in a way that is optimized for a smaller code base and efficient read access, even though most of the information is only read when a page is first loaded, and is rarely needed or accessed after that.",", "
"   Rename Method,Inline Method,","Reduce memory utilization of Tapestry page instances This is an umbrella bug for a number of changes aimed at reducing the overall memory footprint of a Tapestry page. Tapestry pages have been seen to be quite large in some production sites, and the shared-instance strategy introduced in 5.2 is still insufficient. A significant amount of space is consumed using Maps and Lists (for bindings, of sub-components, etc.) in a way that is optimized for a smaller code base and efficient read access, even though most of the information is only read when a page is first loaded, and is rarely needed or accessed after that.",", , "
"   Rename Method,Extract Method,","Tapestry-specific JavaDoc plugin that generates parameter documentation, etc. Having a split between the JavaDoc and the Component Reference has always been a challenge. It would be nicer to just have JavaDoc that is extended with Tapestry-specific content on component classes, such as examples (from the optional .xdoc file) and parameter/etc. documentation. This is further driven by the switch to a Gradle build, as the existing module for generating the Component Reference is Maven specific.","Duplicated Code, Long Method, , "
"   Rename Method,","Tapestry-specific JavaDoc plugin that generates parameter documentation, etc. Having a split between the JavaDoc and the Component Reference has always been a challenge. It would be nicer to just have JavaDoc that is extended with Tapestry-specific content on component classes, such as examples (from the optional .xdoc file) and parameter/etc. documentation. This is further driven by the switch to a Gradle build, as the existing module for generating the Component Reference is Maven specific.",", "
"   Rename Method,","Optimize document scans used by Tapestry.FieldEventManager to not locate the label or icon until actually needed While creating the Field Event Manager we are initializing not only the basic features but also extra information for being use later, like label and icon. 

For getting the icon we need to search for the element in the DOM (using a $) and for the label it is searching the DOM for a specific label, which is a very expensive operation in ie7.

If we move the initialization of these elements until they are really needed, we are saving some client side timing.

### Eclipse Workspace Patch 1.0
#P tapestry-core
Index: src/main/resources/org/apache/tapestry5/tapestry.js
===================================================================
--- src/main/resources/org/apache/tapestry5/tapestry.js (revision 1131061)
+++ src/main/resources/org/apache/tapestry5/tapestry.js (working copy)
@@ -1667,13 +1667,6 @@
initialize : function(field) {
this.field = $(field);

- var id = this.field.id;
-
- var selector = ""label[for='"" + id + ""']"";
-
- this.label = this.field.up(""form"").down(selector);
- this.icon = $(id + '_icon');
-
this.translator = Prototype.K;

var fem = $(this.field.form).getFormEventManager();
@@ -1698,7 +1691,24 @@
this.validateInput.bindAsEventListener(this));
}
},
+ 
+ getLabel : function() {
+ if (!this.label) {
+ var id = this.field.id;
+ var selector = ""label[for='"" + id + ""']"";
+ this.label = this.field.form.down(selector);
+ }
+ return this.label;
+ },

+ getIcon : function() {
+ if (!this.icon) {
+ var id = this.field.id;
+ this.icon = $(id + '_icon');
+ }
+ return this.icon;
+ },
+
/**
* Removes validation decorations if present. Hides the ErrorPopup, if it
* exists.
@@ -1706,11 +1716,11 @@
removeDecorations : function() {
this.field.removeClassName(""t-error"");

- if (this.label)
- this.label.removeClassName(""t-error"");
+ if (this.getLabel())
+ this.getLabel().removeClassName(""t-error"");

- if (this.icon)
- this.icon.hide();
+ if (this.getIcon())
+ this.getIcon().hide();

if (this.errorPopup)
this.errorPopup.hide();
@@ -1730,12 +1740,12 @@

this.field.addClassName(""t-error"");

- if (this.label)
- this.label.addClassName(""t-error"");
+ if (this.getLabel())
+ this.getLabel().addClassName(""t-error"");

- if (this.icon) {
- if (!this.icon.visible())
- new Effect.Appear(this.icon);
+ if (this.getIcon()) {
+ if (!this.getIcon().visible())
+ new Effect.Appear(this.getIcon());
}

if (this.errorPopup == undefined)

",", "
"   Rename Method,",Excessive warnings Tracking issue for cleaning up compiler warnings.,", "
"   Rename Method,","Add a @QueryParameter annotation for parameters to event handler method It would be nice, is some cases, to have Tapestry map query parameters to event handler method parameters, rather than path info. This is typically about the Ajax case, where it is more reliable (and easier) to take a URL and add query parameters to it than it is to add extra path info.

public void onActionFromAjaxWidget(@QueryParameter(""action"") String widgetAction, @QueryParameter(""count"") int count) { .... }
",", "
"   Rename Method,Move Method,Move Attribute,",Reduce thread contention inside ComponentClassResolverImpl There is looking to be a fair amount of thread lock contention inside ComponentClassResolverImpl; use of ConcurrentBarrier appears to not be sufficient (or poorly tuned).,", , , "
"   Rename Method,","Components which use PrimaryKeyEncoder should be changed to use ValueEncoder, and PrimaryKeyEncoder should be deprecated While working on an application, I noticed that my objects were being serialized ""weird"" into a form by the loop component. I realized that I hadn't provided the primary key encoder, and once I did things worked as expected. That got me to thinking that it would be nice if the Loop component, and other components that rely on PrimaryKeyEncoders, could check to see if there is an encoder available for the value-type, if none is explicitly bound by the user. That way, module-authors could provide PrimaryKeyEncoders that makes things work ""like magic"". 
For example, tapestry-hibernate could contribute PrimaryKeyEncoders for each entity type so that the objects are automatically, and properly, encoded into forms.",", "
"   Rename Method,Extract Method,","Component fields should not need to be private, merely non-public Currently, Plastic assets, early, that all instance fields are private. Instead, it should check fields as transformations are applied to them, and ensure that they are merely non-public. Access to the fields from other classes (including inner classes) must be routed through access methods.

Inner classes will now need a limited set of transformations, to handle the cases where a protected or package private field is directly accessed, in which case, the appropriate accessor method will be used instead.

It seems possible that two transformed classes that each access the other's non-public fields might cause an endless loop; if so, this should be identified and reported.","Duplicated Code, Long Method, , "
"   Rename Method,Move Method,Extract Method,","Component fields should not need to be private, merely non-public Currently, Plastic assets, early, that all instance fields are private. Instead, it should check fields as transformations are applied to them, and ensure that they are merely non-public. Access to the fields from other classes (including inner classes) must be routed through access methods.

Inner classes will now need a limited set of transformations, to handle the cases where a protected or package private field is directly accessed, in which case, the appropriate accessor method will be used instead.

It seems possible that two transformed classes that each access the other's non-public fields might cause an endless loop; if so, this should be identified and reported.","Duplicated Code, Long Method, , , "
"   Rename Method,","Component fields should not need to be private, merely non-public Currently, Plastic assets, early, that all instance fields are private. Instead, it should check fields as transformations are applied to them, and ensure that they are merely non-public. Access to the fields from other classes (including inner classes) must be routed through access methods.

Inner classes will now need a limited set of transformations, to handle the cases where a protected or package private field is directly accessed, in which case, the appropriate accessor method will be used instead.

It seems possible that two transformed classes that each access the other's non-public fields might cause an endless loop; if so, this should be identified and reported.",", "
"   Rename Method,",KaptchaField should have a parameter to allow it to operate as a visible text field rather than a password field Most captcha implementations are based on input type=text instead of input type=password. It would be nice to allow for both.,", "
"   Rename Method,","Improve error reporting when a javascript asset is intended to be included on page which has no <html> element 
I know - silly of me to have a page with no <body></body> element. I spent an entire day trying to figure out why the javascript wasn't getting included.

It was my error, but I can see other people making the same mistake...a little error-reporting would be nice.",", "
"   Rename Method,Extract Method,","An extension component template (with a root t:extend element) should allow t:block elements to be nested Currently, this isn't possible, and the <t:block> elements must be placed inside some available <t:replace> block. It makes sense that defining a block specific to a sub-component should be allowed, even if other aspects of the containing template are not to be overridden.","Duplicated Code, Long Method, , "
"   Rename Method,","Add a SubmitMode (for Submit and LinkSubmit components) for unconditionally submitting the form As described in TAP5-1856, there's a functionality gap that prevents a button from submitting a form but still allowing server-side processing to occur (such as having the Submit component fire its selected event).",", "
"   Rename Method,",Typo in interface LocalizationSetter.setNonPeristentLocaleFromLocaleName() The method is missing an 's' in Peristent.,", "
"   Rename Method,","Support multiple @PageActivationContext The @PageActivationContext annotation (and PageActivationContextWorker) could be improved to accept an ""index"" parameter. This way, I could have multiple @PageActivationContext properties.

eg
{code}
public class MyPage {
@PageActivationContext(index=0)
private Category category;

@PageActivationContext(index=1)
private Item item;

...
}
{code}

I'd expect tapestry to generate the following URL's:
- /mypage (category and item is null)
- /mypage/category1 (item is null)
- /mypage/$N/item1 (category is null)
- /mypage/category1/item1 


",", "
"   Rename Class,Extract Interface,Rename Method,Move Method,Move Attribute,","Configuration interface should support contributing a class (which is autobuilt) in addition to an instance Frequently, we inject the ObjectLocator to invoke autobuild() and contribute the result.

Adding an addInstance() method (that takes a class) on the three configuration interfaces would simplify contribution code and remove the need to inject the ObjectLocator.",", , , Large Class, "
"   Rename Class,Rename Method,","Add support for distributed documentation Please add support for a distributed documentation system. The basic requirements are:

1. Access to a list of Pages/Compoents/Mixins. (ComponentClassResolver supports pages)
2. Access to a Map of all Configurations. The map would have the configuration class as the Key and contain an object such a list or map that contains the configuration.
3. Access to a list of configured services.

From this it should be possible to build documentation of a running system. 

Thanks
Barry",", "
"   Rename Method,","When you have multiple forms on the same page that share (some of) the same properties, it is not possible to differentiate validation constraints and messages in the message catalog For example, if you have both a login form and a registration form that both collect a userId property from the user, then:

userid-required=You must provide the same user id you supplied when registerring.

Will be applied to both the login form (correct) and the registration form (inaccurate and very confusing).

Extra differentiation is needed:

login-userid-required=You must provide ...

",", "
"   Rename Method,","The Hibernate ENTITY session persistent strategy should store transient entities as-is The ENTITY session persistence strategy does the right thing for persistent entities; however, when first creating an entity, it is often useful to store the transient entity in the session. In this case, the entity itself should be stored, not the entity type and entity PK.",", "
"   Rename Method,","Unify injection; allow @Inject annotation on fields of service implementations This has come up again and again, especially in recent training. Although I'm very strongly in favor of constructor injection (and the use of final fields to store dependencies), being allowed to inject into a field, even if it requires the use of reflection, would be a real boon.",", "
"   Rename Method,","Prevent interaction with page until fully loaded With the emphasis on JavaScript, there is an issue where a user on a slow connection interacts with the page before the page has fully loaded its JavaScript and run initializations. This can lead to client-side JavaScript exceptions, or server-side failures (when ordinary requests are sent to URLs that expect an Ajax/XHR request).

The right solution is for Tapestry to provide a ""pageloading mask"", a div that masks the entire page from user input (and provides a loading image) until page initializations complete.

The implementaton of this uses a <script> tag, with document.write, to introduce the mask element at the top of the page (so that non-JavaScript-enabled clients will be able to interact with the page to some degree).

CSS animations are used to fade in the mask after a delay. The presentation of the mask can be modified via CSS overrides. By default, it is black with 50% opacity.",", "
"   Rename Method,","Prevent interaction with page until fully loaded With the emphasis on JavaScript, there is an issue where a user on a slow connection interacts with the page before the page has fully loaded its JavaScript and run initializations. This can lead to client-side JavaScript exceptions, or server-side failures (when ordinary requests are sent to URLs that expect an Ajax/XHR request).

The right solution is for Tapestry to provide a ""pageloading mask"", a div that masks the entire page from user input (and provides a loading image) until page initializations complete.

The implementaton of this uses a <script> tag, with document.write, to introduce the mask element at the top of the page (so that non-JavaScript-enabled clients will be able to interact with the page to some degree).

CSS animations are used to fade in the mask after a delay. The presentation of the mask can be modified via CSS overrides. By default, it is black with 50% opacity.",", "
"   Extract Method,Inline Method,","Tapestry should identify where in the template undefined components (with id, but no type or matching embedded component) are located Was diagnosing TAP5-317 and found a lot of ways to improve T5's exception reporting in this area.","Duplicated Code, Long Method, , , "
"   Move And Rename Class,Rename Method,","Expose a LinkCreationHub service to allow for listeners that wish to observe (and modify) new Link instances Currently, LinkFactory (an internal service) has methods for adding and removing listeners; these should be refactored out into their own public service.",", "
"   Move Class,Rename Method,Extract Method,Inline Method,","Tapestry Performance Improvements Make it work. Make it right. Now we're making it fast.

T5 is as fast or faster than JSP for trivial pages; for pages that render a lot (i.e., lots of loops) T5 is not keeping up. It may not (it does so much more) but we need to narrow the gap.","Duplicated Code, Long Method, , , "
"   Move Class,Rename Class,Move And Rename Class,Rename Method,Extract Method,Move Attribute,","Control over creation of page render and component event requests should be encapsulated into an overridable service I would like to propose an extension of the Link interface with a setAbsoluteURI(String absoluteURI) method, or something alike. This will give more flexibility when handling the link in the LinkCreationListener.

In my usecase, where I need the locale of the browser displayed as the first part of the URI (eg http://domain.com/en_US/myPage), I use a dispatcher to detect the locale (or change to it), and have to completely copy the LinkFactoryImpl into my own LocaleAwareLinkFactory (which gets contributed as an alias) to be able to set the URI to what I want on Link instantiation. If I can change the URI at a later stage I only need to add my own LinkCreationListener.","Duplicated Code, Long Method, , , "
"   Rename Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Tapestry should encode the user's locale into the URL rather than as an HTTP cookie It would be nice if the user's locale showed up in the URL, perhaps just after the context path. Example:

/context/en/article or /context/de/admin/edit.

This would make the link encoding and decoding rules more complex, as it should still support the current URLs. Basically, it will have to see if the first ""folder"" in the request path matches one of the configured locales.","Duplicated Code, Long Method, , , , , "
"   Move Class,Move Method,","Allow component libraries to contribute extra resources to the global application catalog Components in libraries have the same desire to share common message strings that components in an application do.

My earlier thoughts were to define a way to have a lib.properties for a component library.

However, my current thinking is to extend ComponentMessagesSource (the source for the application message catalog as well) so that component libraries can contribute Resources for message catalogs into the global message catalog (typically, before the app.properties file, so that an application has the ability to override specific component library messages).",", , "
"   Rename Method,","Allow component libraries to contribute extra resources to the global application catalog Components in libraries have the same desire to share common message strings that components in an application do.

My earlier thoughts were to define a way to have a lib.properties for a component library.

However, my current thinking is to extend ComponentMessagesSource (the source for the application message catalog as well) so that component libraries can contribute Resources for message catalogs into the global message catalog (typically, before the app.properties file, so that an application has the ability to override specific component library messages).",", "
"   Extract Superclass,Move Method,",Allow injection of Tapestry services into Spring beans 0,", , Duplicated Code, Large Class, "
"   Rename Method,","Tapestry should verify that all public methods of a module class are meaningful to Tapestry (build, decorate, contribute or bind), other methods should cause an exception to be thrown as a likely typo in the name 0",", "
"   Rename Class,Move Method,Move Attribute,","Add a context: binding prefix to make it super-easy to reference context assets from templates Yes, you can do asset:context:foo.gif but that's longer than I'd like. You really want to use Assets to reference context assets to a) ensure that URLs are correct (short and relative ... even if the page URL shifts due to activation context), and b) get the benefits of gzip compression, versioning in the URL and far-future expires headers.",", , , "
"   Move And Rename Class,Rename Method,Move Method,","Create a service that fits into the ComponentClassTransformWorker chain and can be configured to extract component meta-data from class annotations In other words, if we have Moe, Larry and Curly and they all look like this:

public class MoeWorker implements ComponentClassTransformWorker
{
public void transform(ClassTransformation transformation, MutableComponentModel model)
{
Moe annotation = transformation.getAnnotation(Moe.class);

if (annotation != null)
model.setMeta(""meta-data.moe"", annotation.value());
}
}


It would be nice if we could just make a contribution:

configuration.add(""meta-data.moe"", Moe.class);

For each of Moe, Larry and Curly instead. This would apply only when the annotation has a single attribute whose type is String.
",", , "
"   Rename Class,Rename Method,","Add a LazyAdvisor service that can allow method invocations on services to be lazily evaluated A lazy decorator would be very cool, as it would allow method invocations to be deferred until actually needed.

This would only work on method invocations whose return value was an interface type and throws no checked exceptions.

The decorator should ignore any methods that do not qualify.

This will actually be a snap to implement, using the AspectDecorator.",", "
"   Extract Method,Move Attribute,","Have a common handler/filter pipeline for both component event and page render requests, to make it easier to add filters that apply to both types of requests Currently, if you want to put a filter in place that afects both types of request, you have to a contribute a ComponentEventRequestFilter to the ComponentEventRequestHandler service, and a nearly identical PageRenderRequestFilter to the PageRenderRequestHandler service.

It would be nice if there was a service that acted as a facade around the two existing pipelines. The terminator of that pipeline could forward the request into one of the two existing pipelines. 

The common example of this is a ""is logged in"" filter that sends a redirect if the user is not logged in; you want to do this for both types of requests.","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,",Maven repository location http://archiva.openqa.org/repository/releases has a unwanted trailing slash in the master pom.xml 0,"Duplicated Code, Long Method, , "
"   Rename Method,","Tracking issue for changes required by com.formos.tapestry:tapestry-template The off-Apache project tapestry-template (which allows Tapestry pages to be used as templates for generating static files or mail content) requires a few tweaks to tapestry-core.

The following site will be live soon:

http://tapestry.formos.com/nightly/tapestry-template",", "
"   Extract Interface,Rename Method,Inline Method,","Easier way to expose parameters of an embedded component in a containing component It would be nice if there was a simple annotation that allowed a parameter of a contained component to be exposed as a parameter of the containing component. Currently, you have to create a field and apply the @Parameter annotation, and use an ""inherit:"" binding on the child component.

Note that the new parameter should appear as a parameter of the containing component (in terms of Component Reference documentation).",", , Large Class, "
"   Rename Method,","Change If and Unless to render thier template element if provided (i.e., when using t:type) as well as informal parameters I use a lot of dummy texts in my template to see what the page will actually look like when rendered. To filter out these fragments I created a ""Dummy"" component whose setupRender method always returns ""false"".

I don't know if this is efficient but it would be nice to have something like this already in T5's component library.

More efficient would be a special marker XML attribute that the template parser recognizes. Maybe something like a t:test attribute which is works like <t:if test=""..."">...</t:if>. If the contained expression evaluations to false or null the marked element is filtered.

<ul>
<li t:test=""show.item"">conditional item</li>
<li t:test=""literal:false"">dummy item</li>
</ul>
",", "
"   Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Cleanup and simplfy PageTester to remove ComponentInvocation, InvocationTarget, etc. There's a huge amount of unnecessary clutter to support PageTester in the form of the ComponentInvocationMap, CompnentInvication, InvocationTarget, etc. It makes the code related to generating Links very confusing. It needs to be cleaned up before we can even consider allow applications to take more control over link creation and dispatch.

","Duplicated Code, Long Method, , , , , "
"   Rename Method,Extract Method,Inline Method,Move Attribute,","When rendering a PageLink, it should not be necessary to load the target page instance just to see if there's a page activation context Currently, generating a page render link (inside LinkSource) will load an instance of the target page to fire a ""passivate"" event on the instance.

This is wasteful. Tapestry should know what events the page instance handles (this could be determined during class transformation, in the same way that we determine what render phases each component implements). It should then be possible to avoid loading a page unless it has a passivate event handler.

This could make a big different to application start-up time, as each page referenced from the initial page must be fully loaded.

The downside is that often, loading the pages early causes an early failure: i.e. if page Login has an error and page Index has a page link to it, we'll see that error on first request (because the PageLink forces a load of the Login page). With this improvement, the Login page error wouldn't be seen until the user navigated to the page. I see that as acceptible.","Duplicated Code, Long Method, , , , "
"   Rename Class,Move Method,Move Attribute,","Add ParallelExecutor service to allow operations to be performed asynchronously in a thread pool I'm looking for areas in T5 that can be better done in parallel (i.e., because they are I/O bound). I'm still finding out what works, but this code was too simple and useful not to keep.",", , , "
"   Rename Method,Extract Method,","ObjectLocator.getService(Class) should be expanded to pass a varargs of marker annotation types I have @SomeMarker annotation:

@Marker({@SomeMarker})
void SomeService buildSomeService() {...}

How get SomeService by his marker (in ObjectLocator I not found like method).

method like this: <T> T getService(<? extends Annotation> marker, Class<T> interface)","Duplicated Code, Long Method, , "
"   Rename Method,","ObjectLocator.getService(Class) should be expanded to pass a varargs of marker annotation types I have @SomeMarker annotation:

@Marker({@SomeMarker})
void SomeService buildSomeService() {...}

How get SomeService by his marker (in ObjectLocator I not found like method).

method like this: <T> T getService(<? extends Annotation> marker, Class<T> interface)",", "
"   Rename Method,","In the exception report page, JVM system property org.apache.catalina.jsp_classpath should be displayed as a list, like other .path value 0",", "
"   Move Method,Move Attribute,","JavaScript libraries should be combined into a single request It should be possible to create a virtual resource under /assets that represents all the JavaScript libraries for a particular page. I envision this as a Base64 stream that is the actually a composite list of paths to the real JavaScript files; encoded in such a way that when multiple pages use the same set of JavaScript libraries, they can share a single stream.

The combined stream should be gzipped (if client supports it) and should be packed/minimalized. Some extra bookkeeping is needed to track what JS files were combined (the client tracks loaded JS libraries for when an Ajax partial update requires new libraries that were not required for the main page render, for example, a partial update that includes a form with client-side validation).",", , , "
"   Push Down Method,Extract Method,Inline Method,","Reduce eden space memory footprint by avoiding Lists and Maps within Elements Through 5.1.0.0, each Element had an optional List of its child-nodes, and a CaseInsensitiveMap of its attributes.

These could be removed, replaced with some linked lists, and thus, optimized for creation of the DOM and streaming, with less emphasis on manipulating the DOM after the fact. Should be a significant decrease in number of short-lived objects (Maps, Map.Entry, List, etc.).","Duplicated Code, Long Method, , , , "
"   Rename Method,","Provide support for URL rewriting Tapestry should provide some way, configured via Tapestry-IoC, to provide URL rewriting.",", "
"   Move Class,Move Method,Move Attribute,","Provide support for URL rewriting Tapestry should provide some way, configured via Tapestry-IoC, to provide URL rewriting.",", , , "
"   Move Method,Move Attribute,","Add a method to DOM Element class to allow the collection of Attributes to be obtained I've written a Jaxen (http://jaxen.codehaus.org/) extension so I can use XPath with the Tapestry DOM. All going well so far, except that I can't get sensible access to attributes because there is no way to iterate over all attributes in org.apache.tapestry5.dom.Element. 

Please add a ""Collection<Attribute> getAttributes()"" method to Element and make the Attribute class public.",", , , "
"   Pull Up Method,Extract Method,Pull Up Attribute,Move Attribute,","Provide an alternate approach to decorating services based on aspects It would be nice if decorators could be passed an AspectInterceptorBuilder and could choose to advise methods. Such a decorator method could return void, and would not need to have the delegate passed in.

Difficulty: what is the order when a single service has advised methods and interceptors (via traditional 5.0-style decorators)? All the advised method changes would be grouped together and the outcome may not be as desired. It may be required that any single service have 5.0 style decorators or 5.1 style aspects, but not both.
","Duplicated Code, Long Method, , , Duplicated Code, Duplicated Code, "
"   Rename Class,Rename Method,Extract Method,",There should be a simple way to override automatic JavaScript libraries and Stylesheets 0,"Duplicated Code, Long Method, , "
"   Rename Method,Move Method,","URLRewriting should distinguish between incoming and outgoing requests Currently, the new URLRewriting support uses a single method on the set of rewrite rules called ""process"".
This method is called both for transforming incoming request urls and for rewriting ""outbound"" links. Generally, however, urls from incoming requests are going to be translated into tapestry-aware urls and urls for links will be mapped from tapestry-aware urls to some external form. To facilitate the ""in"" vs. ""out"" mapping, URL Rewriting should provide a mechanism to distinguish between rewriting incoming urls vs. rewriting links. 

Three possible ways of doing that are:

1) have a separate service for incoming vs. outgoing rewriting
2) Alter the URLRewriterRule API to change from the single ""process"" method to two methods: ""processIncoming"" and ""processOutgoing"" (or something along those lines)
3) Alter URLRewriterRule API to pass an additional ""RewriteContext"" method parameter. The context would include (for now) a single method ""boolean isIncoming"" or perhaps ""boolean isOutgoing"". (Alternatively, we could change the method signature to just provide a boolean ""isOutgoing"" parameter; but providing the RewriteContext would allow the API to evolve better in the future should we find that additional context information is useful).

Currently leaning towards #2. The positive point in #3 could be handled in the future by the introduction of a per-thread ""helper"" service, RewriteContext or some such, that could be injected directly into the rewrite rules. 

",", , "
"   Rename Method,","Allow page classes to have a ""Page"" suffix that is not included in the URL I have an application with a lot of read-only pages. For example, I have a page that shows a company and I would like a URI such as: /company/1234

However, if I name the page class ""Company"" then I get a naming clash with the domain object ""Company"". What I would like to do is call the Tapestry 5 class ""CompanyPage"" - after all, that is what the class represents and it's certainly how the team refers to that thing internally and with our business (i.e. ""Have you seen the new company page?"").

So, please could the ComponentClassResolverImpl remove the suffix ""Page"" (if it exists) from the class name when it constructs the logical page name?



",", "
"   Rename Method,Pull Up Method,Extract Method,","Allow blackbird to be disabled in production mode blackbird's use of F2 to show the console is interfering with our application which uses F-keys as hotkeys to access various parts of the application.

It should be possible to either completely disable blackbird in production mode (avoiding unnecessary .css and .js downloads) or at least disable the console hotkey.","Duplicated Code, Long Method, , Duplicated Code, "
"   Extract Method,Move Attribute,","Tapestry should automatically compress content sent to the client, if the client supports it Most browsers will accept gzip compression of the text stream. Tapestry should identify which content types may be compressed, and (perhaps) minimum byte counts to trigger compression. Thus text/html and text/javascript streams might be compressed, but jpeg and png (which are already compressed) pass through unchanged.","Duplicated Code, Long Method, , , "
"   Rename Class,Move Method,Extract Method,Move Attribute,","Change template parser to not use StAX, as it is not (yet) compatible with Google App Engine The StAX APIs are not on the GAE ""white list"".

Should be reasonable ot change the code, by using a SAX parser that parses the template into a list of tokens, and then iterate down the token list as we do today using StAX. End result will be fewer dependencies to boot.","Duplicated Code, Long Method, , , , "
"   Rename Method,","JavaScript libraries should be automatically packed/minimalized Tapestry should catch downloads of JavaScript libraries, and should ""pack"" the JavaScript ... remove comments and unecessary whitespace. I believe Dojo has a library to do this, it may even shorten variable names (!).

A smart implementation of this would manage to cache the compressed JS, and notice when the uncompressed version changed.",", "
"   Rename Method,Extract Method,Inline Method,","JavaScript libraries should be automatically packed/minimalized Tapestry should catch downloads of JavaScript libraries, and should ""pack"" the JavaScript ... remove comments and unecessary whitespace. I believe Dojo has a library to do this, it may even shorten variable names (!).

A smart implementation of this would manage to cache the compressed JS, and notice when the uncompressed version changed.","Duplicated Code, Long Method, , , "
"   Pull Up Method,Pull Up Attribute,","JavaScript libraries should be automatically packed/minimalized Tapestry should catch downloads of JavaScript libraries, and should ""pack"" the JavaScript ... remove comments and unecessary whitespace. I believe Dojo has a library to do this, it may even shorten variable names (!).

A smart implementation of this would manage to cache the compressed JS, and notice when the uncompressed version changed.",", Duplicated Code, Duplicated Code, "
"   Rename Class,Rename Method,","JavaScript libraries should be automatically packed/minimalized Tapestry should catch downloads of JavaScript libraries, and should ""pack"" the JavaScript ... remove comments and unecessary whitespace. I believe Dojo has a library to do this, it may even shorten variable names (!).

A smart implementation of this would manage to cache the compressed JS, and notice when the uncompressed version changed.",", "
"   Move Method,Move Attribute,","JavaScript libraries should be automatically packed/minimalized Tapestry should catch downloads of JavaScript libraries, and should ""pack"" the JavaScript ... remove comments and unecessary whitespace. I believe Dojo has a library to do this, it may even shorten variable names (!).

A smart implementation of this would manage to cache the compressed JS, and notice when the uncompressed version changed.",", , , "
"   Extract Method,Inline Method,","Provide a way that a component sub-class can merge its template with that of its container This is an idea I've picked up from Wicket; which has a special element, similar to Tapestry t:body, but used to indicate where the child component's template should go. In Wicket, pages are often given a common L&F not by the use of a common component, but by extending a common base class, and mixing the base class' template with the sub-class.","Duplicated Code, Long Method, , , "
"   Rename Method,","It is too much work to hide all T5 pages inside a virtual folder, for use in mixed-implementation deployments In a mixed-implementation deployment (mixing Tapestry 5 with Tapestry 4 or some other framework), it would be nice if the T5 apps could be ""hidden"" in a virtual /t5 folder.

This is doable, but awkward and ugly, today.

Ideally, this would be a matter of changing the web.xml mapping to:

<url-filter>/t5/*</url-filter>

and making some form of configuration change, i.e.,

configuration.add(ConfigurationConstants.TAPESTRY_APP_FOLDER, ""t5"");

This would affect link generation, prefixing urls with ""t5/"" (including the virtual /assets folder, which would be /t5/assets). Since the /t5 portion is part of the URL mapping, it would not be part of the request pathInfo, so existing dispatch code would not need to change. 

Of course, Websphere has some bugs in this area that might cause some grief.",", "
"   Rename Method,Move Method,Move Attribute,","Zone should include an option to periodically update itself By default, it should re-render its own body, unless some other renderable result is return by event handlers for the update event it will trigger.",", , , "
"   Rename Method,Extract Method,","Improve Tapestry's property expression language to include OGNL-like features While I really the use of prop and it's typesafeness, I still find myself frequently in need of more complicated expressions enough that I would be willing to pay the speed penalty of reflection in order to not have to define a public getter for it. In some situations, you can't actually make an equivilent getter. For instance, in integrations tests I've had more than one situation where I wanted to call a setter with a specific value from the template. With OGNL this is easy but with prop it's impossible. I would very much like to see prop remain the default binding but have OGNL available when it's needed. I think not having it would be a severe limitation of T5.","Duplicated Code, Long Method, , "
"   Move Method,Extract Method,","Improve Tapestry's property expression language to include OGNL-like features While I really the use of prop and it's typesafeness, I still find myself frequently in need of more complicated expressions enough that I would be willing to pay the speed penalty of reflection in order to not have to define a public getter for it. In some situations, you can't actually make an equivilent getter. For instance, in integrations tests I've had more than one situation where I wanted to call a setter with a specific value from the template. With OGNL this is easy but with prop it's impossible. I would very much like to see prop remain the default binding but have OGNL available when it's needed. I think not having it would be a severe limitation of T5.","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Improve Tapestry's property expression language to include OGNL-like features While I really the use of prop and it's typesafeness, I still find myself frequently in need of more complicated expressions enough that I would be willing to pay the speed penalty of reflection in order to not have to define a public getter for it. In some situations, you can't actually make an equivilent getter. For instance, in integrations tests I've had more than one situation where I wanted to call a setter with a specific value from the template. With OGNL this is easy but with prop it's impossible. I would very much like to see prop remain the default binding but have OGNL available when it's needed. I think not having it would be a severe limitation of T5.","Duplicated Code, Long Method, , "
"   Move Method,Move Attribute,","Move away from Javassist A long-term, multi-release strategy to replace ClassFactory/ClassGen and ClassTransformation methods with equivalents that are not tied to Javassist.

Over a couple of releases, the methods could be introduced (still implemented on top of Javassist), and the Javassist-centric methods deprecated, then eventually disabled (NotImplementedException) or even removed.

Rationale: Javassist is unprofessionally and fitfully maintained; many users have problems under Java6 due to Javassist.",", , , "
"   Rename Method,Extract Method,Move Attribute,","Move away from Javassist A long-term, multi-release strategy to replace ClassFactory/ClassGen and ClassTransformation methods with equivalents that are not tied to Javassist.

Over a couple of releases, the methods could be introduced (still implemented on top of Javassist), and the Javassist-centric methods deprecated, then eventually disabled (NotImplementedException) or even removed.

Rationale: Javassist is unprofessionally and fitfully maintained; many users have problems under Java6 due to Javassist.","Duplicated Code, Long Method, , , "
"   Rename Method,","Move away from Javassist A long-term, multi-release strategy to replace ClassFactory/ClassGen and ClassTransformation methods with equivalents that are not tied to Javassist.

Over a couple of releases, the methods could be introduced (still implemented on top of Javassist), and the Javassist-centric methods deprecated, then eventually disabled (NotImplementedException) or even removed.

Rationale: Javassist is unprofessionally and fitfully maintained; many users have problems under Java6 due to Javassist.",", "
"   Rename Method,","Move away from Javassist A long-term, multi-release strategy to replace ClassFactory/ClassGen and ClassTransformation methods with equivalents that are not tied to Javassist.

Over a couple of releases, the methods could be introduced (still implemented on top of Javassist), and the Javassist-centric methods deprecated, then eventually disabled (NotImplementedException) or even removed.

Rationale: Javassist is unprofessionally and fitfully maintained; many users have problems under Java6 due to Javassist.",", "
"   Rename Class,Extract Interface,Extract Method,","Move away from Javassist A long-term, multi-release strategy to replace ClassFactory/ClassGen and ClassTransformation methods with equivalents that are not tied to Javassist.

Over a couple of releases, the methods could be introduced (still implemented on top of Javassist), and the Javassist-centric methods deprecated, then eventually disabled (NotImplementedException) or even removed.

Rationale: Javassist is unprofessionally and fitfully maintained; many users have problems under Java6 due to Javassist.","Duplicated Code, Long Method, , Large Class, "
"   Rename Method,","Move away from Javassist A long-term, multi-release strategy to replace ClassFactory/ClassGen and ClassTransformation methods with equivalents that are not tied to Javassist.

Over a couple of releases, the methods could be introduced (still implemented on top of Javassist), and the Javassist-centric methods deprecated, then eventually disabled (NotImplementedException) or even removed.

Rationale: Javassist is unprofessionally and fitfully maintained; many users have problems under Java6 due to Javassist.",", "
"   Rename Method,","Move away from Javassist A long-term, multi-release strategy to replace ClassFactory/ClassGen and ClassTransformation methods with equivalents that are not tied to Javassist.

Over a couple of releases, the methods could be introduced (still implemented on top of Javassist), and the Javassist-centric methods deprecated, then eventually disabled (NotImplementedException) or even removed.

Rationale: Javassist is unprofessionally and fitfully maintained; many users have problems under Java6 due to Javassist.",", "
"   Rename Method,","Move away from Javassist A long-term, multi-release strategy to replace ClassFactory/ClassGen and ClassTransformation methods with equivalents that are not tied to Javassist.

Over a couple of releases, the methods could be introduced (still implemented on top of Javassist), and the Javassist-centric methods deprecated, then eventually disabled (NotImplementedException) or even removed.

Rationale: Javassist is unprofessionally and fitfully maintained; many users have problems under Java6 due to Javassist.",", "
"   Rename Method,","Move away from Javassist A long-term, multi-release strategy to replace ClassFactory/ClassGen and ClassTransformation methods with equivalents that are not tied to Javassist.

Over a couple of releases, the methods could be introduced (still implemented on top of Javassist), and the Javassist-centric methods deprecated, then eventually disabled (NotImplementedException) or even removed.

Rationale: Javassist is unprofessionally and fitfully maintained; many users have problems under Java6 due to Javassist.",", "
"   Rename Method,","Move away from Javassist A long-term, multi-release strategy to replace ClassFactory/ClassGen and ClassTransformation methods with equivalents that are not tied to Javassist.

Over a couple of releases, the methods could be introduced (still implemented on top of Javassist), and the Javassist-centric methods deprecated, then eventually disabled (NotImplementedException) or even removed.

Rationale: Javassist is unprofessionally and fitfully maintained; many users have problems under Java6 due to Javassist.",", "
"   Rename Method,Push Down Method,Push Down Attribute,","Move away from Javassist A long-term, multi-release strategy to replace ClassFactory/ClassGen and ClassTransformation methods with equivalents that are not tied to Javassist.

Over a couple of releases, the methods could be introduced (still implemented on top of Javassist), and the Javassist-centric methods deprecated, then eventually disabled (NotImplementedException) or even removed.

Rationale: Javassist is unprofessionally and fitfully maintained; many users have problems under Java6 due to Javassist.",", , , "
"   Rename Method,","Move away from Javassist A long-term, multi-release strategy to replace ClassFactory/ClassGen and ClassTransformation methods with equivalents that are not tied to Javassist.

Over a couple of releases, the methods could be introduced (still implemented on top of Javassist), and the Javassist-centric methods deprecated, then eventually disabled (NotImplementedException) or even removed.

Rationale: Javassist is unprofessionally and fitfully maintained; many users have problems under Java6 due to Javassist.",", "
"   Rename Class,Rename Method,","Move away from Javassist A long-term, multi-release strategy to replace ClassFactory/ClassGen and ClassTransformation methods with equivalents that are not tied to Javassist.

Over a couple of releases, the methods could be introduced (still implemented on top of Javassist), and the Javassist-centric methods deprecated, then eventually disabled (NotImplementedException) or even removed.

Rationale: Javassist is unprofessionally and fitfully maintained; many users have problems under Java6 due to Javassist.",", "
"   Move Method,Extract Method,","Move away from Javassist A long-term, multi-release strategy to replace ClassFactory/ClassGen and ClassTransformation methods with equivalents that are not tied to Javassist.

Over a couple of releases, the methods could be introduced (still implemented on top of Javassist), and the Javassist-centric methods deprecated, then eventually disabled (NotImplementedException) or even removed.

Rationale: Javassist is unprofessionally and fitfully maintained; many users have problems under Java6 due to Javassist.","Duplicated Code, Long Method, , , "
"   Move Class,Rename Method,Move Method,Extract Method,Move Attribute,","Optimize page construction for repeated construction of the same page Construction of pages is probably the largest expense for any request, as it involves considerable work to identify what components to instantiate, what bindings to create, and what template tokens to be converted into what ComponentPageElements.

It should be possible to devise a ""page template"" that is a list of commands for constructing a page. The current PageLoaderProcessor would generate that list of commands. Creating a page instance would be a matter of executing the commands. This would decrease the amount of time needed to generate the 2nd (and later) instances of a page, and would increase the likelyhood that common page elements for literal text could be re-used across page instances.

Making page instance creation less expensive would allow Tapestry to more aggressively cull unused page instances (i.e., shorten the active window) while not sacrificing the ability to handle a request surge.

","Duplicated Code, Long Method, , , , "
"   Move Class,Move Method,","The TypeCoercer should be able to coerce String to Enum types, even without a specific contribution In the current implementation to allow coersion form string literal to Enum we need to add contribution to the TypeCoercer. This is working. But this coersion can be performed by default using just Enum.valueOf (Class, String), so there are not required to register new coersion for every used enum type. In this case whe are lost the case insensibility of enum coersions, but if this is meaningfull for everyone we can still use contributed coersions, and if no one is found fallback do the default. Enum.valueOf",", , "
"   Rename Method,","Reorganize ComponentClassTransformWorkers to start moving away from Javassist Begin moving code into forms that can be implemented without Javassist, by creating new methods on ClassTransformation.",", "
"   Move Method,Extract Method,Move Attribute,","Implement an agnostic tapestry.js layer + adapters to allow developers to switch from prototype to jquery As per the discussion on the mailing about Tapestry 5 and jQuery, i create this JIRA to compile the toughts of everyone for this feature.

As Howard said on the mailing list, goals are :

Goal #1: Backwards compatibility
Goal #2: Documented
Goal #3: Plugability / Extensibility / Overridablilty

First design thoughts suggested by howard are (extracted from Howard's answer) :

1. tapestry.js defines a Tapestry namespace with key function properties for the standard stuff
2. split current tapestry.js into more smaller files
3. In addition to tapestry.js, ... include either tapestry-prototype-adapter.js (plus prototype.js and scriptaculous.js) OR tapestry-jquery-adapter.js (plus jquery.js).
4. tapestry.js [should] be smaller handlers that often just fire additional events; a cascade of events that eventually results in server-side requests

Objectives :

1. make certain parts more pluggable i.e. Popup Bubbles
2. write javascript with functional closures
3. ... element could have at most one active animation; the animation would have to complete before the next one (cf. jQuery animation and queuing mechanism)

Challenges :

1. Remove prototype code from tapestry.js
2. Keep backward compatibility with existing Tapestry object

","Duplicated Code, Long Method, , , , "
"   Rename Method,Pull Up Method,Move Method,Extract Method,Pull Up Attribute,","Enhance Velocity's LogSystem and internal use thereof After several very, very long debates with Geir over commons-logging, i have become largely convinced that it is something we should do with hesitation (i.e. let's talk about it for 2.0, but not before). i've also begun to have some frustrations with commons-logging in both work projects and in VelocityTools.

regardless of these issues, it is clear to me that Velocity's LogSystem and use of it is in great need of improvement. we need to lower the priority of many messages, eliminate some, and above all, upgrade the system itself to be more useful.

some specific, unevaluated, off-the-cuff ideas are:
-make logging a null-op if no logger is found, rather than panic and break
-detect jdk 1.4+ logging
-add a trace level
-add is<Level>Enabled
-make it possible to grab a LogSystem instance of sorts, so Velocity extensions can use it more sensibly (i'm tired of the hacks we must do in Tools)

those are just a few ideas, and they might not all even be feasible. still, i'm hoping to take a whack at some of this and hoping that others can help me out. my volunteer time is still rather limited.","Duplicated Code, Long Method, , , Duplicated Code, Duplicated Code, "
"   Rename Method,Extract Method,Inline Method,","Improved Syntax for Maps and Collections I would like to see some syntatic sugar for Maps and Collections and perhaps other Objects too. 
(I have read that there will be a syntax for map literals in 1.5, that's a first step.)

I want to have something like in groovy: http://groovy.codehaus.org/Collections
Scroll down to ""Slicing with the subscript operator"".

PS: Please do never implement this terrible confusing groovy map bean syntax, where ""map.foo"" ist equivalent to ""map.get(""foo"")"".
","Duplicated Code, Long Method, , , "
"   Pull Up Method,Pull Up Attribute,","Extend the MethodInvocation exception to be able to give the velocity macro writer a usefull error page We use velocity macros that invoke methods in a java written web engine.
When an invoked method fails because of an exception, it is not
possible to use the
MethodInvocation exception to give the velocity macro writer a usefull
error page since the MethodInvocation Exception has not cause set.
So to be short the reason why the method invocation failed can not be
routed back to the veloticy macro writer on a running system.

I extended the MethodInvocationException.java and the method execute in
ASTMethod.java 

proposed changes in MethodInvocationException.java : 
===============================================
package org.apache.velocity.exception; 

import org.apache.velocity.exception.VelocityException;
import org.apache.velocity.runtime.parser.Token;

/*
* Copyright 2001,2004 The Apache Software Foundation.
* 
* Licensed under the Apache License, Version 2.0 (the ""License"");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
* 
* http://www.apache.org/licenses/LICENSE-2.0
* 
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an ""AS IS"" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*/

/**
* Application-level exception thrown when a reference method is 
* invoked and an exception is thrown.
* <br>
* When this exception is thrown, a best effort will be made to have
* useful information in the exception's message. For complete 
* information, consult the runtime log.
*
* @author <a href=""mailto:geirm@optonline.net"">Geir Magnusson Jr.</a>
* @version $Id: MethodInvocationException.java,v 1.2.14.1 2004/03/03 23:22:54 geirm Exp $
*/
public class MethodInvocationException extends VelocityException
{
private String methodName = """";
private String referenceName = """";
private Throwable wrapped = null;
private int line; // Added by CX
private int column; // Added by CX

/**
* CTOR - wraps the passed in exception for
* examination later
*
* @param message 
* @param e Throwable that we are wrapping
* @param methodName name of method that threw the exception
*/
public MethodInvocationException( String message, Throwable e, String methodName )
{
super(message);
this.wrapped = e;
this.methodName = methodName;
} 

/**
* Returns the name of the method that threw the
* exception
*
* @return String name of method
*/
public String getMethodName()
{
return methodName;
}

/**
* returns the wrapped Throwable that caused this
* MethodInvocationException to be thrown
* 
* @return Throwable thrown by method invocation
*/
public Throwable getWrappedThrowable()
{
return wrapped;
}

/**
* Sets the reference name that threw this exception
*
* @param reference name of reference
*/
public void setReferenceName( String ref )
{
referenceName = ref;
}

/**
* Retrieves the name of the reference that caused the 
* exception
*
* @return name of reference
*/
public String getReferenceName()
{
return referenceName;
}

/**
* Retrieves the line number where the error occured
* 
* @return line number
*/
public int getLine() 
{
return line;
}

/**
* Sets the line number where the error occured
*
* @param line
*/
public void setLine(int line) 
{
this.line = line;
}

/**
* Retrieves the line number where the error occured
* 
* @return column number
*/
public int getColumn() 
{
return column;
}

/**
* Sets the column number where the error occured
* 
* @param column
*/
public void setColumn(int column) 
{
this.column = column;
}

}

===============================================
Proposed changes in ASTMethod.java
===============================================
package org.apache.velocity.runtime.parser.node;

/*
* Copyright 2000-2001,2004 The Apache Software Foundation.
* 
* Licensed under the Apache License, Version 2.0 (the ""License"");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
* 
* http://www.apache.org/licenses/LICENSE-2.0
* 
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an ""AS IS"" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*/

import org.apache.velocity.context.InternalContextAdapter;
import org.apache.velocity.runtime.parser.*;
import org.apache.velocity.util.introspection.IntrospectionCacheData;
import org.apache.velocity.util.introspection.VelMethod;
import org.apache.velocity.util.introspection.Info;

import org.apache.velocity.exception.MethodInvocationException;
import java.lang.reflect.InvocationTargetException;

import org.apache.velocity.app.event.EventCartridge;

/**
* ASTMethod.java
*
* Method support for references : $foo.method()
*
* NOTE :
*
* introspection is now done at render time.
*
* Please look at the Parser.jjt file which is
* what controls the generation of this class.
*
* @author <a href=""mailto:jvanzyl@apache.org"">Jason van Zyl</a>
* @author <a href=""mailto:geirm@optonline.net"">Geir Magnusson Jr.</a>
* @version $Id: ASTMethod.java,v 1.24.4.1 2004/03/03 23:22:59 geirm Exp $ 
*/
public class ASTMethod extends SimpleNode
{
private String methodName = """";
private int paramCount = 0;

public ASTMethod(int id)
{
super(id);
}

public ASTMethod(Parser p, int id)
{
super(p, id);
}

/** Accept the visitor. **/
public Object jjtAccept(ParserVisitor visitor, Object data)
{
return visitor.visit(this, data);
}

/**
* simple init - init our subtree and get what we can from 
* the AST
*/
public Object init( InternalContextAdapter context, Object data)
throws Exception
{
super.init( context, data );

/*
* this is about all we can do
*/

methodName = getFirstToken().image;
paramCount = jjtGetNumChildren() - 1;

return data;
}

/**
* invokes the method. Returns null if a problem, the
* actual return if the method returns something, or 
* an empty string """" if the method returns void
*/
public Object execute(Object o, InternalContextAdapter context)
throws MethodInvocationException
{
/*
* new strategy (strategery!) for introspection. Since we want 
* to be thread- as well as context-safe, we *must* do it now,
* at execution time. There can be no in-node caching,
* but if we are careful, we can do it in the context.
*/

VelMethod method = null;

Object [] params = new Object[paramCount];

try
{
/*
* check the cache 
*/

IntrospectionCacheData icd = context.icacheGet( this );
Class c = o.getClass();

/*
* like ASTIdentifier, if we have cache information, and the
* Class of Object o is the same as that in the cache, we are
* safe.
*/

if ( icd != null && icd.contextData == c )
{
/*
* sadly, we do need recalc the values of the args, as this can 
* change from visit to visit
*/

for (int j = 0; j < paramCount; j++)
params[j] = jjtGetChild(j + 1).value(context);

/*
* and get the method from the cache
*/

method = (VelMethod) icd.thingy;
}
else
{
/*
* otherwise, do the introspection, and then
* cache it
*/

for (int j = 0; j < paramCount; j++)
params[j] = jjtGetChild(j + 1).value(context);

method = rsvc.getUberspect().getMethod(o, methodName, params, new Info("""",1,1));

if (method != null)
{ 
icd = new IntrospectionCacheData();
icd.contextData = c;
icd.thingy = method;
context.icachePut( this, icd );
}
}

/*
* if we still haven't gotten the method, either we are calling 
* a method that doesn't exist (which is fine...) or I screwed
* it up.
*/

if (method == null)
return null;
}
catch( MethodInvocationException mie )
{
/*
* this can come from the doIntrospection(), as the arg values
* are evaluated to find the right method signature. We just
* want to propogate it here, not do anything fancy
*/

throw mie;
}
catch( Exception e )
{
/*
* can come from the doIntropection() also, from Introspector
*/

rsvc.error(""ASTMethod.execute() : exception from introspection : "" + e);
return null;
}

try
{
/*
* get the returned object. It may be null, and that is
* valid for something declared with a void return type.
* Since the caller is expecting something to be returned,
* as long as things are peachy, we can return an empty 
* String so ASTReference() correctly figures out that
* all is well.
*/

Object obj = method.invoke(o, params);

if (obj == null)
{
if( method.getReturnType() == Void.TYPE)
return new String("""");
}

return obj;
}
catch( InvocationTargetException ite )
{
/*
* In the event that the invocation of the method
* itself throws an exception, we want to catch that
* wrap it, and throw. We don't log here as we want to figure
* out which reference threw the exception, so do that 
* above
*/

EventCartridge ec = context.getEventCartridge();

/*
* if we have an event cartridge, see if it wants to veto
* also, let non-Exception Throwables go...
*/

if ( ec != null && ite.getTargetException() instanceof java.lang.Exception)
{
try
{
return ec.methodException( o.getClass(), methodName, (Exception)ite.getTargetException() );
}
catch( Exception e )
{
MethodInvocationException miex = new MethodInvocationException( 
""Invocation of method '"" 
+ methodName + ""' in "" + o.getClass() 
+ "" threw exception "" 
+ e.getClass() + "" : "" + e.getMessage(), 
e, methodName );
miex.initCause(ite.getTargetException());
miex.setLine(first.beginLine);
miex.setColumn(first.beginColumn);
throw miex;
}
}
else
{
/*
* no event cartridge to override. Just throw
*/

MethodInvocationException miex = new MethodInvocationException( 
""Invocation of method '"" 
+ methodName + ""' in "" + o.getClass() 
+ "" threw exception "" 
+ ite.getTargetException().getClass() + "" : ""
+ ite.getTargetException().getMessage(), 
ite.getTargetException(), methodName );
miex.initCause(ite.getTargetException());
miex.setLine(first.beginLine);
miex.setColumn(first.beginColumn);
throw miex;
}
}
catch( Exception e )
{
rsvc.error(""ASTMethod.execute() : exception invoking method '"" 
+ methodName + ""' in "" + o.getClass() + "" : "" + e );

return null;
} 
}
}


",", Duplicated Code, Duplicated Code, "
"   Move And Rename Class,","Remove ""Exception"" type throwing. I have to use Checkstyle coding standards at my job. Some methos of Velocity throw exceptions using the raw ""Exception"" type. So Checkstyle points an error everywhere I use Velocity and, unfortunately, that's a fact I cannot override in my source code. So it would be nice if those ""throws Exception"" are replaced by some Velocity proper exception.",", "
"   Rename Method,","Velocity 1.4 does not support Iterable collections. In the new for loop in Java 5 e.g.

for(Object obj: iterable)

the collection iterated only needs to be of type Iterable

However, In Velocity a foreach loop must be either a Collection or a Map but cannot be just an Iterable.

Suggestion, support Iterable containers.",", "
"   Rename Method,","add documentation to explain precedence for resolving property The Velocity user guide is not clear about the precedence for resolving the property of a variable. As in the UberspectImpl.java code, the precedence to resolve a property should be something like in this order:

getbar()
getBar()
get(""bar"")
isBar()

This information will be very useful to the user. I suggest that this is added to the user guide under the ""Properties"" section.",", "
"   Pull Up Method,Extract Method,","Throw more exceptions and log less errors Now that Velocity application exceptions are based on RuntimeException, we have more opportunity to use exceptions to signal application level problems. I'm particularly concerned about initialization problems that are logged and may be missed. We need to review all logged error messages and see if it would be more appropriate to throw an exception instead. Some of these places we may need to leave as is for backwards compatibility reasons. (e.g. macro in the global macro library doesn't parse properly).

Llewellyn Falco made a good case for this on the dev list recently:

http://www.mail-archive.com/velocity-dev@jakarta.apache.org/msg15067.html

#####
I still would like to put in my vote that sending error's to the log is an incredibly BAD idea.

If something is not working, it should be LOUDLY shown as an exception.
If it is working I don't really need a log.

The (velocity) log should be there for velocity developers (those programming the actual velocity code) not users.

I don't ever care to see tomcat's log, I care to see the things I log while in tomcat.

Most of all, many many many people do not check the log at all, let alone frequently.
####","Duplicated Code, Long Method, , Duplicated Code, "
"   Move Method,Extract Method,","Support varargs in method calls If possible and as much as possible without breaking JDK 1.3/JDK 1.4 runtime support, i would like to see us support varargs for method calls. So,

public class MyTool {
public List combine(List... lists) {
...
}
}

#set( $twoLists = $myTool.combine($list1, $list2) )
#set( $threeLists = $myTool.combine($list1, $list2, $list3) )

will work out of the box, without need for the user to rebuild Velocity themselves.","Duplicated Code, Long Method, , , "
"   Rename Method,","Parsing errors on content inside #literal() #end block I have some velocity templates that include quit some javascript. Inside the javascript a javascrip template engine is used which also uses ${varname}
Escaping each occurance would make the code rather unreable, so to prevent velocity from parsing the javascript code, I put a #literal() around it.

However, velocity still PARSES the contents of this block, which of course results in parsing exceptions.

My feeling with ""literal"" is that it is completely UNINTERPRETED content?

This SHOULD work:

#literal()
var myId = 'someID';
$('#test).append($.template('<div id=""${myId}""></div>').apply({myId: myId}));
#end",", "
"   Rename Method,",Minor performance tweaks based on Findbugs findings Mainly change two inner classes to static inner classes and a few other slight modifications. See the patch.,", "
"   Rename Method,","! preceding a reference in strict mode ignores a null exception Change strict mode (runtime.references.strict = true) behavior such that when Velocity attempts to render a reference that evaluates to null then throw an exception. If the reference is preceded by '$!' as in $!foo, then simply render nothing and ignore the null value as it does for non strict mode.
",", "
"   Rename Method,Pull Up Method,Extract Method,","have #if handle empty strings/arrays/collections/maps more conveniently An idea from the dev list:
-------------------------------------------------------------------------------------------------
On Sat, Feb 7, 2009 at 3:41 PM, <serg472@gmail.com> wrote:
> Hello,
> I wanted to share with you a few ideas I have about new simple
> improvements for DisplayTools. I should be able to make patches for
> them if you are interested.
>
> 1. Add new method
>
> isEmpty(object)
>
> that will return true if the object is null or empty (for strings it's
> zero length; for collections, maps and arrays it's zero size). This
> should help with annoying null checks. (Probably a better place for
> this method would be Engine, not Tools)

yeah, not something for tools. would be interesting to have the
Uberspect pretend that every non-null reference has an isEmpty()
method, or perhaps just add 0-length strings, empty collections, empty
maps and 0-length arrays to the list of things that #if( $foo )
considers false.
-------------------------------------------------------------------------------------------------","Duplicated Code, Long Method, , Duplicated Code, "
"   Rename Method,","Add macro default parameters Add the ability to specify default parameters to macros, for example:

#macro(foo $x $y=""legit"")$x$y#end

calling this with #foo(2) would give:

2legit

Any number of default parameters can be specified, but no non-default parameters can not follow default parameters. Assignment of calling values begins from left to right and all left over default arguments are assigned their default values.


",", "
"   Rename Method,","Implement default values in formal references We should take profit of the formal reference syntax to implement default values: 

{{  ${foo|""bar""}}} 

It could even be repeated, or nested... 

{{  ${foo|$bar|""baz""}}} 

{{  ${foo|${bar|""baz}}}} 

 ",", "
"   Move And Rename Class,Rename Method,Move Method,","Change the inheritance model of Abstract FileSystem classes to composition along with a Factory class, which allows us to use different FS implementations. 0",", , "
"   Rename Method,Extract Method,Inline Method,",Remove download-hadoop profile requirement and cache downloads 0,"Duplicated Code, Long Method, , , "
"   Rename Method,",Remove download-hadoop profile requirement and cache downloads 0,", "
"   Rename Method,",Normalize the user:group mapping for end to end tests 0,", "
"   Move Class,Extract Superclass,",Fix Sentry Precommit tests 0,", Duplicated Code, Large Class, "
"   Rename Method,Extract Method,","Support filter pushdown in DB Store client to reduce data transfer from DB Store service The authorization provider retrieves all the privileges for the given set of groups. This could be a huge data set if there are a large number of privileges in the system. A downstream consumer like HiveServer2 will be reading this for each query. This could impact the DB store performance if there are multiple active queries and numerous privilege rules.
We could consider pushing the filters like DB object name to the policy provider to prune the privilege result set at the source.
","Duplicated Code, Long Method, , "
"   Move Method,Move Attribute,","Support filter pushdown in DB Store client to reduce data transfer from DB Store service The authorization provider retrieves all the privileges for the given set of groups. This could be a huge data set if there are a large number of privileges in the system. A downstream consumer like HiveServer2 will be reading this for each query. This could impact the DB store performance if there are multiple active queries and numerous privilege rules.
We could consider pushing the filters like DB object name to the policy provider to prune the privilege result set at the source.
",", , , "
"   Move Method,Extract Method,","Hive bindings should enable MR level ACLs for session user Hive with Sentry requires to impersonation to be turned off. The file system access and MR jobs is directly handled by user hive.
The Hive bindings should enable set MR job ACLs so that the session user can access the jobs from other interfaces like Hue. This can be done by adding the userid to config properties mapreduce.job.acl-modify-job and mapreduce.job.acl-view-job.","Duplicated Code, Long Method, , , "
"   Rename Method,Move Method,Move Attribute,","Hive bindings should enable MR level ACLs for session user Hive with Sentry requires to impersonation to be turned off. The file system access and MR jobs is directly handled by user hive.
The Hive bindings should enable set MR job ACLs so that the session user can access the jobs from other interfaces like Hue. This can be done by adding the userid to config properties mapreduce.job.acl-modify-job and mapreduce.job.acl-view-job.",", , , "
"   Rename Method,Extract Method,","Hive bindings should enable MR level ACLs for session user Hive with Sentry requires to impersonation to be turned off. The file system access and MR jobs is directly handled by user hive.
The Hive bindings should enable set MR job ACLs so that the session user can access the jobs from other interfaces like Hue. This can be done by adding the userid to config properties mapreduce.job.acl-modify-job and mapreduce.job.acl-view-job.","Duplicated Code, Long Method, , "
"   Rename Method,Move Method,Extract Method,","Sentry-168: Trivial fixes to README and pom.xml Minor fixes to README and pom.xml:

* We cannot use mvn compile to compile Sentry because of this issue: http://jira.codehaus.org/browse/MEXEC-91
* We need wget to run hive end to end tests
* Add descriptions for the rat excludes","Duplicated Code, Long Method, , , "
"   Rename Class,Extract Method,Move Attribute,","Create tool to dump and load of entire Sentry service We are storing entire content of the Sentry service in a database. It would be useful to have a sentry tool that would be able to dump entire content in sentry specific format (that will be independent on the used database backend) and it's counterpart that would be able to read this format and load the data into another instance of the Sentry Service.

Such tool can be very helpful for:

* Backups
* Migration from one backend store to another
* Debugging content of the underlying database","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","SHOW GRANT ROLE xxx ON [SERVER, DATABASE, TABLE, URI] xxx Handling the filtering of privileges based on DB objects at Service level will also reduce the amount of privilege data transferred on the wire","Duplicated Code, Long Method, , "
"   Rename Method,",Support SHOW CURRENT ROLES 0,", "
"   Rename Method,Move Method,",Create a new mvn cluster test profile for provider db tests 0,", , "
"   Rename Method,","Add more granular privileges to the DBModel Specifically it would be good to split ""All"" privilege into ""Create"", ""Drop"" and ""Alter""",", "
"   Move Class,Push Down Method,Push Down Attribute,","High availability for the SENTRY service(Zookeeper part) According the feedback in Reviewboard, collated Zookeeper related jira to one patch.",", , , "
"   Move Class,Push Down Method,Push Down Attribute,","Client factory for generic authorization model Base on SENTRY-600: To use clientFactory, {{SentryGenericServiceClientFactory}} will use for create generic model client.
{{SentryGenericServiceClient}} need to change to interface, it's a limitation of dynamic proxy.",", , , "
"   Rename Method,Extract Method,","Improve Metastoreplugin Cache Initialization time Currently the cache initialization logic is sequential viz. It sequentially retrieves the DB objects, then the Table and partition objects within a bunch of nested for loops. For schemas with many tables/partitions, this might take a while. This should be optimized","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,","Solr Update authorization tests for Sentry Need to implement some end-to-end tests for validating Solr ""Update"" authorization.",", "
"   Rename Method,","Implement grant user to role Currently, sentry only support grant group to role, there should be a reasonable feature to grant user to role, for Hive, the following command should be supported in sentry:
GRANT role_name TO USER user
",", "
"   Move Class,Move Attribute,",Move the class PolicyFileConstants and KeyValue to provider-common Move the class PolicyFileConstants and KeyValue to provider-common,", , "
"   Rename Method,",Sentry client should support cache based kerberos ticket for secure zookeeper connection The Sentry service client create a Jaas context for keytab based login for connecting to secure zookeeper. It should also support ticket cache based login for clients that don't have keytab or not performing keytab based login.,", "
"   Rename Class,Rename Method,Push Down Method,Extract Method,Push Down Attribute,","Generate audit trail for Sentry generic model when authorization metadata change Currently, Sentry can generate audit logs for authorization metadata change requests for Hive. For the components Sqoop, Solr which are used Sentry generic model also need this function.","Duplicated Code, Long Method, , , , "
"   Rename Method,Extract Method,","Exceptions in MetastoreCacheInitializer should probably not prevent HMS from starting up Right now if there is an error condition in evaluating paths in MetastoreCacheInitializer in one of the tasks, we throw a Runtime Exception (in Sentry-888). Changed the behavior to be:

* Retry the failed tasks x times. Each retry has a y millis wait duration. x and y are based upon the user config (sentry.hdfs.sync.metastore.cache.retry.max.num and sentry.hdfs.sync.metastore.cache.retry.wait.duration.millis).
* After retry failure, throw exception or sync incomplete paths update based on user config (sentry.hdfs.sync.metastore.cache.fail.on.partial.update). The default values is fail on partial update, which will throw runtime exception.

{noformat}
for (Future<CallResult> result : results) {
CallResult callResult = result.get();

// Fail the HMS startup if tasks are not all successful and
// fail on partial updates flag is set in the config.
if (callResult.getSuccessStatus() == false && failOnRetry) {
throw new RuntimeException(callResult.getFailure());
}
}
{noformat}","Duplicated Code, Long Method, , "
"   Move Class,Rename Class,Move And Rename Class,Move Method,Move Attribute,","Refactor the sentry to integrate with external components quickly *Problem*
Currently, many components are integrated with Sentry, eg, Solr, Sqoop. But when do the integration, some duplicated work need to be done, for example, sentry-core-model-xxx and sentry-policy-xxx should be created for new component and the code is kind of duplicated. These makes the integration complex and the source maintenance will be hard. 
Considering others components will be integrated with Sentry, eg, Kafka. The Sentry should be refactored to be easy integrated.

*Refactor point*
*1*. PolicyEngine: Currently, Sentry has many PolicyEngine, eg, SimpleSearchPolicyEngine for Solr, SimpleSqoopPolicyEngine for Sqoop. One CommonPolicyEngine should be ok for these external components. 
*2*. Privilege: Currently, SearchWildcardPrivilege, IndexerWildcardPrivilege, SqoopWildcardPrivilege have the same implementation. One CommonPrivilege is enough.
*3*. Action: Currently, SearchActionFactory, SqoopActionFactory are never used in Privilege.imply(). The idea for these ActionFactory is good, but it is component related and should be located in sentry-xxx-binding project.
",", , , "
"   Rename Method,Pull Up Method,Move Method,Extract Method,Inline Method,Move Attribute,",Create Enterprise OMRS Connector The Enterprise OMRS Connector provides a connector that implements the OMRS Connector API defined in JIRA ATLAS-1773 that is able to aggregate the metadata from multiple metadata repositories in response to metadata requests.,"Duplicated Code, Long Method, , , , , Duplicated Code, "
"   Rename Method,Extract Method,","Export: Support type-based Export *Background* 

Atlas administrators my want to export all data of a type. This may be needed in scenario where it may be necessary to move data to a different cluster. 

*Suggested Approach* 

Within _AtlasExportRequest_, support additional parameter that is a list of types. Additional option type say _forType_ will help with identifying what is specified. 

Processes this list as starting entity within _ExportService_. 

Example: 
{code:java} 
{ 
""itemsToExport"": [ 
{ ""typeName"": ""hive_db,hdfs_path,hbase_namespace,hbase_table"" } 
], 
""options"": {  
""fetchType"": ""FULL"", 
""matchType"": ""forType"" 
} 
} 
{code} 
 ","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,Pull Up Method,Extract Method,Pull Up Attribute,","Data Migration: Moving Data from Earlier Versions of Atlas to Most Recent *Background* 

The most recent version of Atlas uses _JanusGraph_ as database. Earlier versions used to use _Titan v0.5.4_. The formats used for storing data have changed. 

The current version of Atlas also implement features within Atlas entities. This makes the storages structures differ. 

A data migration approach thus becomes necessary to address the format incompatibilities. 

*Approach* 

Earlier version of Atlas could use _Export_ process to extract data out of the _Titan_ database. This ZIP file would then be moved to a cluster with newer version of Atlas installed. An _Import_ process on the new cluster would update the new cluster with data migrated to the new format. 

It should be possible for _Data Migration_ to be initiated from Ambari, so that it becomes part of the Ambari's upgrade process. 

It should also be possible to see the status of the migration as it progresses. During the process, no hook messages should be processed or any notifications sent out.","Duplicated Code, Long Method, , Duplicated Code, Duplicated Code, "
"   Rename Method,Move Method,Extract Method,Inline Method,","Rename entity-classification edge label to 'classifiedAs' and add edge properties We currently store classification and propagated classification names in the entity vertex as properties (traitNames and propagatedTraitNames). This needs to be updated to fetch these values from outgoing edges from entity vertex. 
# Classification edge and propagated classification edge label needs to be updated to a static name - 'classifiedAs' and additional properties needs to be added to the edge: 
## _name_ - name of the classification  
## _isPropagated_ - whether this tag is associated or propagated one 
# Modify advanced (DSL) search to handle searching on edges instead of vertex property for tag based searches. 

 ","Duplicated Code, Long Method, , , , "
"   Move Class,Rename Class,Move And Rename Class,",Consistent class naming with removal of old typesystem code 0,", "
"   Rename Method,Move Method,Extract Method,Move Attribute,",Atlas Glossary support 0,"Duplicated Code, Long Method, , , , "
"   Move Class,Rename Method,Extract Method,",Atlas Glossary support 0,"Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,",Update Metric queries for faster execution 0,"Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,Move Method,Extract Method,","Data Migration: Import: Infer Types that Store Edge Ids *Background* 

Existing implementation of _Data Migration_ needs end user to specify properties of types that need post processing. 

Post processing is essentially replacing the stored edge ids with the ones in the newly migrated database. 

This is error prone, where the creator of types may not be aware of this, and hence may forget to specify the type and property during migration. This will result in entities of the type being unusable. 

*Solution* 

Infer the types and the properties in following way: 
* Navigating through all the types in type registry. 
* Find attributes that have arrays with array elements as object ids. 

Pass these type-properties map to the migration process. 

This will eliminate the need for letting user specify these properties.","Duplicated Code, Long Method, , , "
"   Move Class,Move Method,Move Attribute,","Update entity notifications to replace AtlasEntity with AtlasEntityHeader Replacing AtlasEntity used in entity notifications with AtlasEntityHeader (which includes fewer attributes) can improve performance, especially for entities having large number of attribute or attributes having large number of array/map elements (like a hive_table containing 1000s of columns). 

In addition, it should be possible to specify which entity attributes to include in the notification via type-def - for example by updating AtlasAttributeDef with addition of a flag named 'includeInNotification'.",", , , "
"   Extract Superclass,Extract Method,","Remove complex array and map attribute's edge information from entity vertex Currently entities with complex types like: array<AtlasEntity>, array<AtlasStruct>, map<String, AtlasStruct>, map<String, AtlasEntity> store list of edge ids in the entity vertex. This JIRA removes all edge id information from vertex and uses edge label and properties to retrieve such attributes. 

 ","Duplicated Code, Long Method, , Duplicated Code, Large Class, "
"   Rename Method,","Change primitive map type storage in vertex Currently primitive map type in atlas is stored in vertex as: 
* typeName.mapAttr         = [key1, key2, key3] 
* typeName.mapAttr.key1 = value1 
* typeName.mapAttr.key2 = value2 
* typeName.mapAttr.key3 = value3 

Since JanusGraph supports Map datatype, we can store map value within a single vertex property. 

Also we need to create edge label and property key for array and map primitive types.",", "
"   Move Method,Extract Method,","[Glossary] Derive category's qualifiedName using hierarchy Currently the qualifiedName is derived using displayName and Glossary's qualifiedName. 

  

In this patch, the qualifiedName is derived from it's Hierarchy and any changes in the hierarchy itself will trigger a cascaded update of all children. 

  

E.g. Cat1 is parent category, Cat2 is child category 

  

Derived qualfiedName of Cat2 = Cat2.Cat1@<glossaryQualifiedName> 

  

The following actions trigger a cascaded update of the children categories 

  
# Change of anchor 
# Change of Parent 
# Removal of children from a parent (similar to #2)","Duplicated Code, Long Method, , , "
"   Extract Method,Pull Up Attribute,","Export Process: Add Support for Incremental Export *Background* 

Use case: Export & Import process is used to synchronize data between clusters. 

Data is exported from a cluster and imported into another cluster. Subsequent exports should only contain entities that have been created or updated since the last export. This helps makes the export process quicker and the payload will be smaller. 

*Implementation Approach* 

Add new _fetchType_: _incremental,_ with additional parameter _fromTime_ which is a timestamp. During Export process, entities' that have ___modificationTimestamp_ greater than _fromTime_ would be included in the export. ","Duplicated Code, Long Method, , Duplicated Code, "
"   Rename Class,Move Method,Move Attribute,","Export & Import Process: Add Support for Replication Attributes *Background* 

In a scenario where data is continually replicated between multiple Atlas clusters, it will be worth having detailed captured within the system. These details should be such that they are apparent in the entities' properties. 

*Approach Guidance* 
* Add 2 additional attributes to _Referencable_. Say _replicatedFromCluster_ and _replicatedToCluster._ 
* Update _ExportService_ and _ImportService_ such that entities that are part of the process are updated with the cluster for which they are to be replicated to.  
* Update audits to denote the operation.",", , , "
"   Rename Method,Extract Method,","Re-evaluate classification propagation during entity delete The current behavior - when we delete an entity *f1*, all tags associate to *f1* which got propagated 

to downstream entities are removed – *PII* tag propagated to *process1* and *t1* is removed 

*Proposed Change* 

_*Soft Delete:*_ 

When entity *f1* is deleted, retain the propagated classification edges, so the downstream entities 

– *process1* and *t1* continue to have *PII* classification associated to them 

_*Hard Delete:*_ 

In cases of hard delete, the source entity (*f1*) is deleted from atlas, but its classification vertex 

(*PII*) continue to exist and will continue propagating to *process1* and *t1*. 

 ","Duplicated Code, Long Method, , "
"   Move Class,Move Method,","Export Process: Support to Optionally Skip Lineage Entities from Being Exported *Background* 

In scenarios where importing lineage information can cause ambiguity, it will be worthwhile having an option within Export to skip exporting lineage. 

*Approach Guidance* 
* Create a mechanism within _ExportService_ to filter out entities based on a criteria. 
* Within the filter, specify a criteria to skip entities of type _Process_. 
* _AtlasExportRequest_ should have an option to specify filters.  
* _ImportService_ should not be impacted.",", , "
"   Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Export & Import Process: Add Data to AtlasCluster About Last Successful Import *Background* 

Scenario: Couple of Atlas clusters are setup such that incremental export and import is setup via some custom program. Example, Cluster _cl1_ exports does incremental export and the output is imported into cluster _cl2_. 

It may be worth while to maintain some information about the last successful import. This way the next incremental export will have correct parameters for its export. 

*Approach Guidance* 
* Create a new model, say _AtlasSyncInfo_, that stores information about top level entity used for export and the timestamp that should be used for subsequent incremental export. 
* Utilize _AtlasCluster_'s _additionalInfo_ fields to store serialized version of _AtlasSyncInfo_. 
* Provide REST APIs for retrieval of this information.","Duplicated Code, Long Method, , , , , "
"   Move Method,Extract Method,Inline Method,","Import Process: Tag Entities After Import when 'replicatedFrom' option is Present *Background* 

It will help user identify entities that are part of Atlas cluster replication if they are tagged as so. 

*Approach Guidance* 
* Within _AuditWriter_, add logic to tag entities. 
* Update _EntityGraphMapper_ to tag entities without changing their _modificationTimestamp_.","Duplicated Code, Long Method, , , , "
"   Rename Method,","Provide option whether to delete propagated classification on entity delete during add classification By default, when an entity is deleted - its associated classifications which have been propagated to downstream entities will be retained. 

This JIRA provides a boolean option when adding a new classification to an entity - *""Remove Propagations on Entity Delete""* 

When this flag is set to : 

*TRUE* - Propagated classifications are removed during entity delete 

*FALSE* - Propagated classifications are retained during entity delete",", "
"   Rename Method,Extract Method,","Remove redundant encoding of vertex property keys JanusGraph has RESERVED characters which are restricted as property keys. 

RESERVED_CHARS => *{ } ""* 

Atlas encodes them before writing to vertex/edge. Lot of duplicate encoding is currently present. Encoding is costly operation and should be done only once. This JIRA will refactor and remove duplicate encoding.","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,","Rename AtlasCluster to AtlasServer *Background* 

_AtlasCluster_ to represent cluster to which entities are imported to or exported from can be confusing for user. 

*Solution* 

Rename _AtlasCluster to AtlasServer._",", "
"   Move Method,Extract Method,","Export Process: Incremental: Improve Approach for Fetching Changed Entities *Background* 

Existing approach for getting modified entities tends to iterate through the entire set of entities and then filter. This results in no benefit in terms of time and effort for incremental export. It does benefit Import, where smaller set is imported. 

*Approach* 
* User Gremlin queries to fetch entities that match criteria. 
* Modify existing logic that does deep traversal to support this new logic. 

 ","Duplicated Code, Long Method, , , "
"   Rename Method,Move Method,","Import Process: New Transform Framework: AddClassification Action *Background* 

The new transformation framework introduced earlier does not have provision for applying the transform that adds classification to an entity. 

*Approach Guidance* 
* New condition that is entity-level. 
* New action that applies to entity. 
* Mechanism to detect and create new classification if necessary. 
* Apply the new action to entity that results in addition of new classification for that entity.",", , "
"   Rename Class,Rename Method,","Decouple MiniAccumuloCluster from integration test base class (apologies if I already had a ticket for this somewhere, I couldn't find it)

Our integration tests are very nice and automated at the moment because we can use MiniAccumuloCluster to ""provision"" an Accumulo instance (or used a shared instance), and run a test against it. For the most part, this works well and provides an accurate test harness.

Thus, to run integration tests, you need a sufficiently beefy machine since the same host will be running all of Accumulo as well as performing any client work. When resources are available to use, it would be nice to leverage them -- whether these are yarn, mesos, a vanila installation, etc.

In addition to the additional computational power from using extra hardware, it also encourages us to use the public API as much as possible instead of relying on ""hidden"" impl methods in MiniAccumuloCluster.

I propose making changes to the IT test base (AbstractMacIT, SimpleMacIT, ConfigurableMacIT) to add an extra step between them an test classes to allow ""injection"" of a more generic Accumulo ""cluster"" that is not associated with MAC.",", "
"   Rename Method,","Decouple MiniAccumuloCluster from integration test base class (apologies if I already had a ticket for this somewhere, I couldn't find it)

Our integration tests are very nice and automated at the moment because we can use MiniAccumuloCluster to ""provision"" an Accumulo instance (or used a shared instance), and run a test against it. For the most part, this works well and provides an accurate test harness.

Thus, to run integration tests, you need a sufficiently beefy machine since the same host will be running all of Accumulo as well as performing any client work. When resources are available to use, it would be nice to leverage them -- whether these are yarn, mesos, a vanila installation, etc.

In addition to the additional computational power from using extra hardware, it also encourages us to use the public API as much as possible instead of relying on ""hidden"" impl methods in MiniAccumuloCluster.

I propose making changes to the IT test base (AbstractMacIT, SimpleMacIT, ConfigurableMacIT) to add an extra step between them an test classes to allow ""injection"" of a more generic Accumulo ""cluster"" that is not associated with MAC.",", "
"   Rename Method,","Decouple MiniAccumuloCluster from integration test base class (apologies if I already had a ticket for this somewhere, I couldn't find it)

Our integration tests are very nice and automated at the moment because we can use MiniAccumuloCluster to ""provision"" an Accumulo instance (or used a shared instance), and run a test against it. For the most part, this works well and provides an accurate test harness.

Thus, to run integration tests, you need a sufficiently beefy machine since the same host will be running all of Accumulo as well as performing any client work. When resources are available to use, it would be nice to leverage them -- whether these are yarn, mesos, a vanila installation, etc.

In addition to the additional computational power from using extra hardware, it also encourages us to use the public API as much as possible instead of relying on ""hidden"" impl methods in MiniAccumuloCluster.

I propose making changes to the IT test base (AbstractMacIT, SimpleMacIT, ConfigurableMacIT) to add an extra step between them an test classes to allow ""injection"" of a more generic Accumulo ""cluster"" that is not associated with MAC.",", "
"   Rename Method,","More IT stabilizations Bunch of tests still have the timeout parameter on the Test annotation which makes the scaling factor useless. Also, we can reuse the scaling factor when tests sit to wait for ""something"" to happen.",", "
"   Move Method,Extract Method,Move Attribute,","Warning about synconclose is way too spammy The location of the warning about {{dfs.datanode.synconclose}} not being set to true is way too spammy. We only know to know about it once, not every time a volume is chosen.","Duplicated Code, Long Method, , , , "
"   Move Method,Move Attribute,","Replace SortedMap<Key,Value> from Tablet constructor The TabletServer reads the metadata table and places the Key Value pairs for the given tablet into a SortedMap which it passes to the Tablet constructor.

Tablet then has a bunch of custom code which extracts out the log entries, data files, time, last location, scan files, etc.

All of that could be encapsulated into its own classes instead of extra methods on Tablet.",", , , "
"   Move Class,Move Method,Move Attribute,","Consolidate thrift/rpc utility classes in the same package Lots of inner classes floating around in TServerUtils which could really be consolidated into a thrift package inside of server-base.

Would reduce the overall size of TServerUtils and make things a bit more consumable.",", , , "
"   Rename Method,","Add ability to retrieve property value by String key. Normally, to retrieve the value for a property we have to get a Property enum instance. Since table arbitrary properties don't have Property enum instances, we then can't use the simple get. Instead, to retrieve a table arbitrary property we have to create a temporary map and a filtering function.

Add the ability to fetch a single value based on the String key.",", "
"   Rename Method,","speed up WAL roll-overs After reading the proposal on HBASE-10278, I realized there are many ways to make the Accumulo WAL roll-over faster.

# Open two WALogs, but use only one until it reaches the WALog roll-over size
# Rollover consists only of swapping the writers
# WALog roll consists of the final close, which can happen in parallel
# Don't mark the tablets with log entries: they are already marked with the tserver
# The tserver can make notes about the logs-in-use in the metadata table(s) as part of opening the log.
# The master can copy the log entries to tablets while unassigning them, piggybacking on the unassigment mutation.
# Tablet servers can remove their current log entries from the metadata tables when they have no tablets using them.

There are two issues: 
# tablets will have an empty file in recovery, nearly all the time, but the recovery code already handles that case. 
# presently, a tablet doesn't have a marker for a log it did not use. Many more tablets will attempt to recover when it is unnecessary.

This would also address ACCUMULO-2889.",", "
"   Rename Method,","speed up WAL roll-overs After reading the proposal on HBASE-10278, I realized there are many ways to make the Accumulo WAL roll-over faster.

# Open two WALogs, but use only one until it reaches the WALog roll-over size
# Rollover consists only of swapping the writers
# WALog roll consists of the final close, which can happen in parallel
# Don't mark the tablets with log entries: they are already marked with the tserver
# The tserver can make notes about the logs-in-use in the metadata table(s) as part of opening the log.
# The master can copy the log entries to tablets while unassigning them, piggybacking on the unassigment mutation.
# Tablet servers can remove their current log entries from the metadata tables when they have no tablets using them.

There are two issues: 
# tablets will have an empty file in recovery, nearly all the time, but the recovery code already handles that case. 
# presently, a tablet doesn't have a marker for a log it did not use. Many more tablets will attempt to recover when it is unnecessary.

This would also address ACCUMULO-2889.",", "
"   Rename Method,Move Method,Inline Method,","speed up WAL roll-overs After reading the proposal on HBASE-10278, I realized there are many ways to make the Accumulo WAL roll-over faster.

# Open two WALogs, but use only one until it reaches the WALog roll-over size
# Rollover consists only of swapping the writers
# WALog roll consists of the final close, which can happen in parallel
# Don't mark the tablets with log entries: they are already marked with the tserver
# The tserver can make notes about the logs-in-use in the metadata table(s) as part of opening the log.
# The master can copy the log entries to tablets while unassigning them, piggybacking on the unassigment mutation.
# Tablet servers can remove their current log entries from the metadata tables when they have no tablets using them.

There are two issues: 
# tablets will have an empty file in recovery, nearly all the time, but the recovery code already handles that case. 
# presently, a tablet doesn't have a marker for a log it did not use. Many more tablets will attempt to recover when it is unnecessary.

This would also address ACCUMULO-2889.",", , , "
"   Rename Method,Move Method,Extract Method,","speed up WAL roll-overs After reading the proposal on HBASE-10278, I realized there are many ways to make the Accumulo WAL roll-over faster.

# Open two WALogs, but use only one until it reaches the WALog roll-over size
# Rollover consists only of swapping the writers
# WALog roll consists of the final close, which can happen in parallel
# Don't mark the tablets with log entries: they are already marked with the tserver
# The tserver can make notes about the logs-in-use in the metadata table(s) as part of opening the log.
# The master can copy the log entries to tablets while unassigning them, piggybacking on the unassigment mutation.
# Tablet servers can remove their current log entries from the metadata tables when they have no tablets using them.

There are two issues: 
# tablets will have an empty file in recovery, nearly all the time, but the recovery code already handles that case. 
# presently, a tablet doesn't have a marker for a log it did not use. Many more tablets will attempt to recover when it is unnecessary.

This would also address ACCUMULO-2889.","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","More consolidation in TServerUtils I did a bunch of cleanup to ThriftUtil and TServerUtils as a part of ACCUMULO-2815, but upon review, there are a few places where more cleanup could be done.

* TThreadPoolServer factory method can be consolidated across SSL and SASL
* Dynamically resizing thread pools for SSL and SASL (like the HsHa server uses)
* Remove unused arguments from SASL tserver method","Duplicated Code, Long Method, , "
"   Move And Rename Class,Move Attribute,","Create combiners for wikisearch example The wikisearch example uses aggregators, which are deprecated in 1.4. Since this is a new example in 1.4, they should be combiners.",", , "
"   Extract Superclass,Pull Up Method,Extract Method,","BatchScanner optimization for AccumuloInputFormat Currently {{AccumuloInputFormat}} produces a split for reach {{Range}} specified in the configuration. Some table indexing schemes, for instance z-order geospacial index, produce large number of small ranges resulting in large number of splits. This is specifically a concern when using {{AccumuloInputFormat}} as a source for Spark RDD where each Split is mapped to an RDD partition.

Large number of small RDD partitions leads to poor parallism on read and high overhead on processing. A desirable alternative is to group ranges by tablet into a single split and use {{BatchScanner}} to produce the records. Grouping by tablets is useful because it represents Accumulos attempt to distributed stored records and can be influance by the user through table splits.

The grouping functionality already exists in the internal {{TabletLocator}} class. 

Current proposal is to modify {{AbstractInputFormat}} such that it generates either {{RangeInputSplit}} or {{MultiRangeInputSplit}} based on a new setting in {{InputConfigurator}}. {{AccumuloInputFormat}} would then be able to inspect the type of the split and instantiate an appropriate reader.

The functinality of {{TabletLocator}} should be exposed as a public API in 1.7 as it is useful for optimizations.","Duplicated Code, Long Method, , Duplicated Code, Large Class, Duplicated Code, "
"   Rename Method,","Multi data center replication The use case here is where people have multiple data centers and need to replicate the data in between them. Accumulo can model this replication after the way that HBase currently handles the replication as detailed here (http://hbase.apache.org/replication.html). 

There will be one master Cluster and multiple slave clusters. Accumulo will use the Master-Push model to replicate the statements from the master clusters WAL to the various slaves WALs.",", "
"   Rename Method,","Multi data center replication The use case here is where people have multiple data centers and need to replicate the data in between them. Accumulo can model this replication after the way that HBase currently handles the replication as detailed here (http://hbase.apache.org/replication.html). 

There will be one master Cluster and multiple slave clusters. Accumulo will use the Master-Push model to replicate the statements from the master clusters WAL to the various slaves WALs.",", "
"   Rename Class,Move Method,Inline Method,Move Attribute,","Multi data center replication The use case here is where people have multiple data centers and need to replicate the data in between them. Accumulo can model this replication after the way that HBase currently handles the replication as detailed here (http://hbase.apache.org/replication.html). 

There will be one master Cluster and multiple slave clusters. Accumulo will use the Master-Push model to replicate the statements from the master clusters WAL to the various slaves WALs.",", , , , "
"   Rename Class,Move Method,Move Attribute,","Multi data center replication The use case here is where people have multiple data centers and need to replicate the data in between them. Accumulo can model this replication after the way that HBase currently handles the replication as detailed here (http://hbase.apache.org/replication.html). 

There will be one master Cluster and multiple slave clusters. Accumulo will use the Master-Push model to replicate the statements from the master clusters WAL to the various slaves WALs.",", , , "
"   Rename Method,","Multi data center replication The use case here is where people have multiple data centers and need to replicate the data in between them. Accumulo can model this replication after the way that HBase currently handles the replication as detailed here (http://hbase.apache.org/replication.html). 

There will be one master Cluster and multiple slave clusters. Accumulo will use the Master-Push model to replicate the statements from the master clusters WAL to the various slaves WALs.",", "
"   Rename Method,","Multi data center replication The use case here is where people have multiple data centers and need to replicate the data in between them. Accumulo can model this replication after the way that HBase currently handles the replication as detailed here (http://hbase.apache.org/replication.html). 

There will be one master Cluster and multiple slave clusters. Accumulo will use the Master-Push model to replicate the statements from the master clusters WAL to the various slaves WALs.",", "
"   Rename Method,","Multi data center replication The use case here is where people have multiple data centers and need to replicate the data in between them. Accumulo can model this replication after the way that HBase currently handles the replication as detailed here (http://hbase.apache.org/replication.html). 

There will be one master Cluster and multiple slave clusters. Accumulo will use the Master-Push model to replicate the statements from the master clusters WAL to the various slaves WALs.",", "
"   Move Class,Extract Method,","run integration tests as a map/reduce job When the functional tests were moved to java, we lost the ability to run the tests via map/reduce. It would be nice to run the ITs in under 2 hours. and take advantage of an entire cluster, especially after making large sweeping changes.","Duplicated Code, Long Method, , "
"   Rename Method,","run integration tests as a map/reduce job When the functional tests were moved to java, we lost the ability to run the tests via map/reduce. It would be nice to run the ITs in under 2 hours. and take advantage of an entire cluster, especially after making large sweeping changes.",", "
"   Move And Rename Class,","Multi-table Accumulo input format Just realized we had no MR input method which supports multiple Tables for an input format. I would see it making the table the mapper's key and making the Key/Value a tuple, or alternatively have the Table/Key be the key tuple and stick with Values being the value.",", "
"   Rename Method,Inline Method,","Multi-table Accumulo input format Just realized we had no MR input method which supports multiple Tables for an input format. I would see it making the table the mapper's key and making the Key/Value a tuple, or alternatively have the Table/Key be the key tuple and stick with Values being the value.",", , "
"   Move Class,Extract Superclass,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Multi-table Accumulo input format Just realized we had no MR input method which supports multiple Tables for an input format. I would see it making the table the mapper's key and making the Key/Value a tuple, or alternatively have the Table/Key be the key tuple and stick with Values being the value.","Duplicated Code, Long Method, , , , Duplicated Code, Large Class, , "
"   Move And Rename Class,","Multi-table Accumulo input format Just realized we had no MR input method which supports multiple Tables for an input format. I would see it making the table the mapper's key and making the Key/Value a tuple, or alternatively have the Table/Key be the key tuple and stick with Values being the value.",", "
"   Move Class,Extract Superclass,Move Method,Extract Method,Move Attribute,","Multi-table Accumulo input format Just realized we had no MR input method which supports multiple Tables for an input format. I would see it making the table the mapper's key and making the Key/Value a tuple, or alternatively have the Table/Key be the key tuple and stick with Values being the value.","Duplicated Code, Long Method, , , , Duplicated Code, Large Class, "
"   Rename Class,Rename Method,","Multi-table Accumulo input format Just realized we had no MR input method which supports multiple Tables for an input format. I would see it making the table the mapper's key and making the Key/Value a tuple, or alternatively have the Table/Key be the key tuple and stick with Values being the value.",", "
"   Rename Method,","Multi-table Accumulo input format Just realized we had no MR input method which supports multiple Tables for an input format. I would see it making the table the mapper's key and making the Key/Value a tuple, or alternatively have the Table/Key be the key tuple and stick with Values being the value.",", "
"   Rename Method,Pull Up Method,Move Method,Extract Method,Move Attribute,","Add per table sampling I am working on prototyping adding hash based sampling to Accumulo. I am trying to accomplish the following goals in the prototype.

# Have each RFile store a sample per locality group. Also store the configuration used to generate the sample.
# Use sampling functions that ensure the same row columns exist across the samples in all RFiles. Hash mod is a good candidate that gives a random sample that's consistent across files.
# Have scanners support scanning RFile's samples sets. Scan should fail if RFiles have different sample configuration. Different sampling config implies the RFile's sample sets contain a possibly disjoint set of row columns.
# Support generating sample data for RFiles generated for bulk import
# Support sample data in the memory map
# Support enabling and disabling sampling per table AND configuring a sample function.

I am currently using the following function in my prototype to determine what data an RFile stores in its sample set. This code will always select same subset of rows for each RFile's sample set. I have not yet made the function configurable.

{code:java}
public class RowSampler implements Sampler {

private HashFunction hasher = Hashing.murmur3_32();

@Override
public boolean accept(Key k) {
ByteSequence row = k.getRowData();
HashCode hc = hasher.hashBytes(row.getBackingArray(), row.offset(), row.length());
return hc.asInt() % 1009 == 0;
}
}
{code}

Although not yet implemented, the divisor in this RowSample could be configurable. RFiles with sample data would store the fact that a RowSample with a divisor of 1009 was used to generate sample data.

","Duplicated Code, Long Method, , , , Duplicated Code, "
"   Move Class,Move Method,Move Attribute,","Add per table sampling I am working on prototyping adding hash based sampling to Accumulo. I am trying to accomplish the following goals in the prototype.

# Have each RFile store a sample per locality group. Also store the configuration used to generate the sample.
# Use sampling functions that ensure the same row columns exist across the samples in all RFiles. Hash mod is a good candidate that gives a random sample that's consistent across files.
# Have scanners support scanning RFile's samples sets. Scan should fail if RFiles have different sample configuration. Different sampling config implies the RFile's sample sets contain a possibly disjoint set of row columns.
# Support generating sample data for RFiles generated for bulk import
# Support sample data in the memory map
# Support enabling and disabling sampling per table AND configuring a sample function.

I am currently using the following function in my prototype to determine what data an RFile stores in its sample set. This code will always select same subset of rows for each RFile's sample set. I have not yet made the function configurable.

{code:java}
public class RowSampler implements Sampler {

private HashFunction hasher = Hashing.murmur3_32();

@Override
public boolean accept(Key k) {
ByteSequence row = k.getRowData();
HashCode hc = hasher.hashBytes(row.getBackingArray(), row.offset(), row.length());
return hc.asInt() % 1009 == 0;
}
}
{code}

Although not yet implemented, the divisor in this RowSample could be configurable. RFiles with sample data would store the fact that a RowSample with a divisor of 1009 was used to generate sample data.

",", , , "
"   Rename Method,","Improve validation of configuration and arguments with guava Predicates There's a few places in our code which could benefit from additional validation of arguments and configuration by using the built-in Guava Predicates. This makes some checks that we're doing a bit more readable, and enables us to do more expressive validation checks.

Specifically, I see room for improvement in our {{PropertyType}} validators (which currently do very limited regex validation) and, to a lesser degree, {{o.a.a.core.util.Validator}}.

I intend to replace these with proper Predicates to determine their validity.",", "
"   Rename Method,","Create a user level API for RFile Users can bulk import RFiles. Currently the only way users can create RFiles using Accumulo's public API is via AccumuloFileOutputFormat. There is no way to read RFiles in the public API. Also, the internal APIs for reading and writing RFiles are cumbersome to use.

I am experimenting with a simple RFile API like the following. Below is an example of writing data.

{code:java}
LocalFileSystem localFs = FileSystem.getLocal(new Configuration());
RFileWriter writer = RFileFactory.newWriter()
.withFileName(""/tmp/test100M.rf"")
.withFileSystem(localFs).build();

writer.startDefaultLocalityGroup();
for (int r = 0; r < 10000000; r++) {
for (int cq = 0; cq < 10; cq++) {
writer.append(genKey(r, cq), genVal(r, cq));
}
}

writer.close();
{code}

Below is an example of reading data.

{code:java}
LocalFileSystem localFs = FileSystem.getLocal(new Configuration());
Scanner scanner = RFileFactory.newScanner()
.withFileName(""/tmp/test100M.rf"")
.withFileSystem(localFs)
.withDataCache(250000000)
.withIndexCache(1000000).build();
{code}

",", "
"   Move Method,Extract Method,","Create a user level API for RFile Users can bulk import RFiles. Currently the only way users can create RFiles using Accumulo's public API is via AccumuloFileOutputFormat. There is no way to read RFiles in the public API. Also, the internal APIs for reading and writing RFiles are cumbersome to use.

I am experimenting with a simple RFile API like the following. Below is an example of writing data.

{code:java}
LocalFileSystem localFs = FileSystem.getLocal(new Configuration());
RFileWriter writer = RFileFactory.newWriter()
.withFileName(""/tmp/test100M.rf"")
.withFileSystem(localFs).build();

writer.startDefaultLocalityGroup();
for (int r = 0; r < 10000000; r++) {
for (int cq = 0; cq < 10; cq++) {
writer.append(genKey(r, cq), genVal(r, cq));
}
}

writer.close();
{code}

Below is an example of reading data.

{code:java}
LocalFileSystem localFs = FileSystem.getLocal(new Configuration());
Scanner scanner = RFileFactory.newScanner()
.withFileName(""/tmp/test100M.rf"")
.withFileSystem(localFs)
.withDataCache(250000000)
.withIndexCache(1000000).build();
{code}

","Duplicated Code, Long Method, , , "
"   Rename Method,","TinyLFU-based BlockCache [LruBlockCache|https://github.com/apache/accumulo/blob/master/core/src/main/java/org/apache/accumulo/core/file/blockfile/cache/LruBlockCache.java] appears to be based on HBase's. I currently have a patch being reviewed in [HBASE-15560|https://issues.apache.org/jira/browse/HBASE-15560] that replaces the pseudo Segmented LRU with the TinyLFU eviction policy. That should allow the cache to make [better predictions|https://github.com/ben-manes/caffeine/wiki/Efficiency] based on frequency and recency, such as improved scan resistance. The implementation uses [Caffeine|https://github.com/ben-manes/caffeine], the successor to Guava's cache, to provide concurrency and keep the patch small.

Full details are in the JIRA ticket. I think it should be easy to port if there is interest.",", "
"   Rename Method,Extract Method,","Generalized configuration object for Accumulo rfile interaction Taken from https://github.com/apache/accumulo/pull/90/files#r59489073

On [~ShawnWalker]'s PR for ACCUMULO-4187 which adds rate-limiting on major compactions, we noted that many of the changes were related to passing an extra argument (RateLimiter) around through all of the code which is related to file interaction.

It would be nice to move to a centralized configuration object instead of having to add a new argument every time some new feature is added to the file-path.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Stabilize tablet assignment during transient failure When a tablet server dies, Accumulo attempts to reassign the tablets it was hosting as quickly as possible to maintain availability. If multiple tablet servers die in quick succession, such as from a rolling restart of the Accumulo cluster or a network partition, this behavior can cause a storm of reassignment and rebalancing, placing significant load on the master.

To avert such load, Accumulo should be capable of maintaining a steady tablet assignment state in the face of transient tablet server loss. Instead of reassigning tablets as quickly as possible, Accumulo should be await the return of a temporarily downed tablet server (for some configurable duration) before assigning its tablets to other tablet servers.","Duplicated Code, Long Method, , "
"   Move Class,Move Method,Extract Method,Move Attribute,","Make caching implementation configurable It would be nice to make the caching implementation configurable. ACCUMULO-4177 introduced a new cache type. Instead of Accumulo having a list of built in cache implementations, it could have a configuration property for specifying a block cache factory class. Accumulo could ship with multiple implementations of this as it does after 4177 and allow users to easily experiment with implementations that Accumulo did not ship with.

It would be nice to have ACCUMULO-3384, so that these custom cache impls could use that for custom config.","Duplicated Code, Long Method, , , , "
"   Move Class,Move And Rename Class,","Make caching implementation configurable It would be nice to make the caching implementation configurable. ACCUMULO-4177 introduced a new cache type. Instead of Accumulo having a list of built in cache implementations, it could have a configuration property for specifying a block cache factory class. Accumulo could ship with multiple implementations of this as it does after 4177 and allow users to easily experiment with implementations that Accumulo did not ship with.

It would be nice to have ACCUMULO-3384, so that these custom cache impls could use that for custom config.",", "
"   Rename Method,","Replace meaningless method names There are a few abbreviated method names reused throughout the code that are meaningless to new users. Method names such as ""nk"", ""nf"" or ""ane"" are not helpful when trying to determine how a class works. This seems to only occur in Tests but I think it would improve readability a lot, especially for newbies. I for one think Tests are a good place to learn how a class works and choosing brevity over clarity greater hinders the readability of a Test.",", "
"   Rename Method,","Replace meaningless method names There are a few abbreviated method names reused throughout the code that are meaningless to new users. Method names such as ""nk"", ""nf"" or ""ane"" are not helpful when trying to determine how a class works. This seems to only occur in Tests but I think it would improve readability a lot, especially for newbies. I for one think Tests are a good place to learn how a class works and choosing brevity over clarity greater hinders the readability of a Test.",", "
"   Rename Method,","Replace meaningless method names There are a few abbreviated method names reused throughout the code that are meaningless to new users. Method names such as ""nk"", ""nf"" or ""ane"" are not helpful when trying to determine how a class works. This seems to only occur in Tests but I think it would improve readability a lot, especially for newbies. I for one think Tests are a good place to learn how a class works and choosing brevity over clarity greater hinders the readability of a Test.",", "
"   Move Class,Move Method,Move Attribute,","Simplify Accumulo logging configuration The current implementation of Accumulo logging configuration is confusing. There are several configuration files in the conf/ dir and it is hard to know which ones are used in which situation. With the refactoring of Accumulo's scripts in 2.0, it should be easier now to refactor Accumulo's logging. Below are some suggest improvements brought up during code reviews:

Regarding generic_logger.properties...
- Rather than produce these system properties in Accumulo code, we should use the log4j feature to do ${sys:HOSTNAME} (might be log4j2 only, but that's okay... we should switch to that)
- Maybe we shouldn't enable the rolling file appender in the default configs. Users can add that if they want to. A better option would probably be to use the syslog appender by default.",", , , "
"   Rename Method,","Simplify Accumulo memory configuration If Accumulo memory configuration can be simplified using percentages, then users will no longer need to use {{accumulo create-config}} to create accumulo-site.xml and accumulo-env.sh configuration files. 

Accumulo should instead ship with simple configuration files, that have sane defaults, and need limited changes from users.
",", "
"   Rename Method,","AccumuloClassLoader should load accumulo-site.xml from classpath Currently, it is expects ACCUMULO_CONF_DIR to be set and loads it from that directory.",", "
"   Rename Class,Rename Method,","Move user manual to Accumulo website Current documentation - The Accumulo user manual lives in the Accumulo repo in an asciidoc format. After every release or doc change, a single page html file must be generated and copied to website.

Proposal - Convert all documentation (starting with 2.0) from asciidoc to markdown, move to Accumulo website and serve using Jekyll. Unreleased documentation will be published (with a warning) but not linked to. After a release, remove warning and add links. Wait a week or two for additional changes before copying all documentation to new directory for next release.

Pros
* Easier to link between pages and external Javadocs
* Documentation can be broken up into distinct pages which is easier to read and better for SEO.
* Easier to update documentation after releases. Only one commit necessary.
* Jekyll+Markdown is more customizable and becoming more of a standard than asciidoc.
* Documentation changes that affect multiple releases can be made with one PR.

Cons
* Documentation will no longer ship with tarball
* Developers cannot update code and docs in one PR",", "
"   Rename Method,Move Method,Inline Method,","Simplify how we use AccumuloConfiguration Working on ACCUMULO-4050, I've realized that there's a bunch of simplifications we can do to the existing AccumuloConfiguration and related objects. These are generally lots of small refactorings, but there's sufficient cleanup that I need to do it separately from ACCUMULO-4050, otherwise it's going to overhwelm me and I'm never going to finish that issue.

Specific things include:

* Remove redundant ways to get the default config
* Move static type converter method clutter from the AccumuloConfiguration API (to separate class)
* Streamline default config construction with stream syntax
* Simplify internal predicates for filtering properties with lambdas
* Rename ServerConfigurationFactory.getConfiguration() so it reflects that it is the system config
",", , , "
"   Move Method,Extract Method,Inline Method,","Remove ServerConfiguration.getInstance() In working with ACCUMULO-4050, I've realized that we need to do some internal refactoring to get better control over HdfsZooInstance. This would enable better testing (because we could inject a mock Instance more easily, and in more places). It would also allow us to reuse objects without storing them statically in the JVM.

Fully realizing this would involve a lot of work, moving the static state to a single ""context"" object constructed on server startup, and shared (in part or in whole) as needed throughout the runtime server code.

However, I think we can get there incrementally, by starting with eliminating the ServerConfiguration.getInstance() method. This causes SystemConfigurationFactory to also have a getInstance() method, and that means that SystemConfigurationFactory is being used like the context object I describe, but redundantly instead of AccumuloServerContext, or a server-specific subclass.

Eliminating ServerConfiguration.getInstance() might involve an intermediate step of adding Instance parameters to many methods which currently take only a SystemConfigurationFactory, because components will not be able to get the Instance from that configuration factory any longer. However, even this intermediate step will be progress towards moving to a single shared context object, which provides access to both the Instance and the configuration factory.

If we can move directly to the context object, that would probably be better, but it would involve a lot more changes, in particular to the way the server code is initialized. Then again, those changes might be good to prioritize anyway, because all our server components seem to initialize differently, and it would be nice to rewrite their bootstrap code to follow the same pattern.","Duplicated Code, Long Method, , , , "
"   Rename Method,",HostRegex balancer should allow migration even when we have pending migrations The HostRegexTableBalancer current halts all migrations when there are pending migrations. I propose fixing this to allow adding additional migrations even when there are pending migrations up to a specified amount.,", "
"   Rename Method,","LocalityGroupIterator very inefficient with large locality groups On one of our systems we tracked some scans that were taking an extremely long time to complete (many hours). As it turns out the scan was relatively simple in that it was scanning a tablet for all keys that had a specific column family. Note that there was very little data that actually matched this column familiy. Upon tracing the code we found that it was spending a large amount of time in the LocalityGroupIterator. Stack traces continually found the code to be at line 128 or 129 of the LocalityGroupIterator. Those line numbers are consistent from the 1.6 series all the way to 2.0.0 (master). In this case the column family being searched for was included in one of a dozen or so locality groups on that table, and the locality group itself had 40 or so column families. We see several things that can be done here:

1) The code that checks the group column families against those being searched for can quickly exit once if finds a match
2) The code that checks the group column families against those being searched for can look at the relative size of those two groups an invert the logic appropriately for a more efficient loop.
3) We could create a cached map of column families to locality groups allowing us to avoid examining each locality group every time we seek.",", "
"   Rename Method,","LocalityGroupIterator very inefficient with large locality groups On one of our systems we tracked some scans that were taking an extremely long time to complete (many hours). As it turns out the scan was relatively simple in that it was scanning a tablet for all keys that had a specific column family. Note that there was very little data that actually matched this column familiy. Upon tracing the code we found that it was spending a large amount of time in the LocalityGroupIterator. Stack traces continually found the code to be at line 128 or 129 of the LocalityGroupIterator. Those line numbers are consistent from the 1.6 series all the way to 2.0.0 (master). In this case the column family being searched for was included in one of a dozen or so locality groups on that table, and the locality group itself had 40 or so column families. We see several things that can be done here:

1) The code that checks the group column families against those being searched for can quickly exit once if finds a match
2) The code that checks the group column families against those being searched for can look at the relative size of those two groups an invert the logic appropriately for a more efficient loop.
3) We could create a cached map of column families to locality groups allowing us to avoid examining each locality group every time we seek.",", "
"   Extract Method,Inline Method,","Allow the properties for AccumuloFileOutputFormat to be set in a mapreduce job Currently there is no way to set any of the configuration options for AccumuloFileOutputFormat in a mapreduce job. Specifically the compression code, file block compression size, and index block compression size to name a few. Since AccumuloFileOutputFormat calls FileOperations.getInstance.openWriter() which takes both a Configuration and AccumuloConfiguration one could inside the openWriter function check to see if the parameters are specified in the Configuration object and if not take the default parameters from the AccumuloConfiguration object.","Duplicated Code, Long Method, , , "
"   Rename Method,","Create WeakReference Map to replace Table.ID constructor Taken from feedback on the PR #279:
Could maybe avoid duplicates by making constructor (of Table.ID) private and doing Table.ID.of(tableId), which draws from an internal WeakReference map.

If the object deduplication in KeyExtent is still valid, this can be pushed down to the Table.ID and Namespace.ID classes, replacing the optimization in KeyExtent.",", "
"   Rename Class,Move Method,","No APIs to configure iterators and locality groups for new table In Accumulo 1.7 the ability to set table properties at table creation time was added. For existing tables there are APIs in table operations that allow setting locality groups and iterators for existing tables. When setting table properties at table creation time there is not good API for iterators and locality groups. There should be some way in the API to do this. There may be other things besides iterators and locality groups that should also be supported at table creation time. 

",", , "
"   Rename Method,Extract Method,Inline Method,","Improve Thrift Transport pool Accumulo has a pool of recently opened connections to tablet servers.  When connecting to tablet servers, this pool is checked first. The pool is built around a map of list.  There are two problems with this pool : 
* It has global lock around the map of list 
* When trying to find a connection it does a linear search for a non reserved connection (this is per tablet server) 

Could possibly move to a model of having a list of unreserved connections and a set of reserved connections per tablet server. Then to get a connection, could remove from the unreserved list and add to the reserved set.  This would be a constant time operation. 

For the locking, could move to a model of using a concurrent map and locking per tserver instead of locking the entire map. 

 ","Duplicated Code, Long Method, , , "
"   Rename Method,","In tablet server start scan authenticates twice The code that handles a start scan RPC call checks authentication twice.  Each call to authenticate takes a bit of time.  It would be nice if it only did it once. 

At [TabletServer line 479|https://github.com/apache/accumulo/blob/rel/1.8.1/server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java#L479] a call to canScan is made which calls authenticate.  Then at [TabletServer line 482|https://github.com/apache/accumulo/blob/rel/1.8.1/server/tserver/src/main/java/org/apache/accumulo/tserver/TabletServer.java#L482] a call to check authorizations is made which also authenticates.  ",", "
"   Rename Method,","Shell should have way to specify instance name while using ZK hosts from configuration Currently the shell uses HDFSZooInstance unless otherwise specified. It is possible for a user to force a ZooInstance, in which case they specify a list of ZKHosts and the instance name. However, there is no way to specify an instance name and utilize the list of ZK hosts in the configuration file. We should look into expanding the shell to include this creation option, possibly by segregating the zookeeper hosts and instance name into two seperate arguments.",", "
"   Extract Method,Pull Up Attribute,",Make shell commands use the table option consistently Shell commands usually create a tableOpt Option and manually parse it. This could be made easier with a utility.,"Duplicated Code, Long Method, , Duplicated Code, "
"   Push Down Method,Push Down Attribute,","Make size of batch scanner client size buffer configurable The batch scanner has a buffer on the client side where results read from tservers are stored. This buffer holds 1000 entries and it not configurable by the client. When it fills up, all threads reading from tservers block.",", , , "
"   Pull Up Method,Pull Up Attribute,","Make size of batch scanner client size buffer configurable The batch scanner has a buffer on the client side where results read from tservers are stored. This buffer holds 1000 entries and it not configurable by the client. When it fills up, all threads reading from tservers block.",", Duplicated Code, Duplicated Code, "
"   Rename Method,","BatchWriters do not track Throwables beyond Constraint Violations Working on ACCUMULO-259 and adapting the Security Random Walk test to account for cached credentials, to prevent it from crapping out on ambiguous credentials (credentials which are currently propagating). And I noticed that our client side Writers provide no means of feedback to the client if there is a non-ConstraintViolationException.

That is, looking at the TabletServerBatchWriter specifically, we keep track of which servers had errors, which key extents had errors, and what ConstrainViolations we saw, but we don't actually track the actual Throwables we get, aside from ConstraintViolation. We should keep a Collection or Set of the Throwables we see so client code can do proper case checking as well.",", "
"   Extract Method,Inline Method,","Improve iterator configuration for MapReduces The InputFormatBase currently uses AccumuloIterator and AccumuloIteratorOption configuration objects to serialize iterator information for MapReduce. These objects predate IteratorSetting, which is now used to configure iterators. We should just make IteratorSetting Writable so it can be used directly for serializing iterator information.","Duplicated Code, Long Method, , , "
"   Rename Method,","Batch Scanner needs timeout The batch scanner needs a user configurable time out. When the batch scanner is used to query lots of tablet in parallel, if one tablet or tablet server is unavailable for some reason it will cause the scan to hang indefinitely. Users need more control over this behavior.

It seems like the batch scanner could behave in one of the following ways : 
* Read as much data as possible, then throw an exception when a tablet or tablet server has timed out
* Throw an exception as soon as a tablet or tablet server times out, even if data could still be read from other tablets successfully.

The timeout can default to max long to preserve the current behavior.


",", "
"   Pull Up Method,Extract Method,","Batch Scanner needs timeout The batch scanner needs a user configurable time out. When the batch scanner is used to query lots of tablet in parallel, if one tablet or tablet server is unavailable for some reason it will cause the scan to hang indefinitely. Users need more control over this behavior.

It seems like the batch scanner could behave in one of the following ways : 
* Read as much data as possible, then throw an exception when a tablet or tablet server has timed out
* Throw an exception as soon as a tablet or tablet server times out, even if data could still be read from other tablets successfully.

The timeout can default to max long to preserve the current behavior.


","Duplicated Code, Long Method, , Duplicated Code, "
"   Move Class,Extract Method,Inline Method,Move Attribute,",Modify ClassLoader to support different applications / multi-tenancy I'd like to expand the current classloader to support loading classes from HDFS and different application contexts. I'll be modifying the ticket as the idea matures.,"Duplicated Code, Long Method, , , , "
"   Rename Method,Pull Up Method,Extract Method,","Add hooks to shell for transforming scan range Hooks in the shell that can manipulate the range and columns passed to a scan range would be nice. 

My use case for this wanting to undo what a formatter did. For example, suppose I use the new HexFormatter to view a table that has binary data like the following.

Below, I scan my table w/o the hex formatter. And its hard to read.

{noformat}
root@test15 graph> scan
\x06\x07)d\x7F\x08\xA1\x00\x08\x01\x02\xEFd@\xC3\xD2S\xC8 edge:count [] 1
\x06\x07)d\x7F\x08\xA1\x00\x08\x08\x1C\xCE>\x8Ed\xE7\xDA edge:count [] 1
\x06\x07)d\x7F\x08\xA1\x00\x08\x0A\xC7\xCED\xA54\xC0, edge:count [] 1
\x06\x07)d\x7F\x08\xA1\x00\x08\x0C\x04\\Nz\x12}\x92 edge:count [] 1
\x06\x07)d\x7F\x08\xA1\x00\x08\x0E\x95\x80\x1A\xA6\xEE\xEF, edge:count [] 1
\x06\x07)d\x7F\x08\xA1\x00\x08\x0E\xCDBHYeP\xE0 edge:count [] 1

{noformat}


So I add the HexFormatter and scan again, now its easier to read.

{noformat}
root@test15 graph> formatter -t graph -f org.apache.accumulo.core.util.format.HexFormatter 
root@test15 graph> scan
0607-2964-7f08-a100-0801-02ef-6440-c3d2-53c8 6564-6765 636f-756e-74 [] 31
0607-2964-7f08-a100-0808-1cce-3e8e-64e7-da 6564-6765 636f-756e-74 [] 31
0607-2964-7f08-a100-080a-c7ce-44a5-34c0-2c 6564-6765 636f-756e-74 [] 31
0607-2964-7f08-a100-080c-045c-4e7a-127d-92 6564-6765 636f-756e-74 [] 31
0607-2964-7f08-a100-080e-9580-1aa6-eeef-2c 6564-6765 636f-756e-74 [] 31
0607-2964-7f08-a100-080e-cd42-4859-6550-e0 6564-6765 636f-756e-74 [] 31
0607-2964-7f08-a100-0816-ccf8-3526-daf4-27 6564-6765 636f-756e-74 [] 31
0607-2964-7f08-a100-0817-b645-dcf2-b73b-65 6564-6765 636f-756e-74 [] 31
{noformat}

However, if I want to scan a range of the table I have to use the \xXX convention which is cumbersome. I would like to do the following : 

{noformat}
scan -b 0607-2964-7f08-a100 -e 0607-2964-7f08-a101
{noformat} 

To do this I propose adding the following hook to the shell. A user could configure a scan interpreter on the shell like they configure a formatter. The scan interpreter would take the command line arguments and translate or interpret them. For example, a HexScanInterpeter could take in 0607-2964-7f08-a100 and output the binary representation for that hex string.

{code:java}
public interface ScanInterpreter {

public Text interpretRow(Text row);

public Text interpretBeginRow(Text row);

public Text interpretEndRow(Text row);

public Text interpretColumnFamily(Text cf);

public Text interpretColumnQualifier(Text cq);
}
{code}

Originally I was thinking of adding the methods above to the Formatter interface. However Christopher Tubbs convinced me to create a separate interface. His argument was that you may want to use something like the HexScanInterpreter with different Formatters. Inorder to make configuration easier we discussed adding a '-i' option to the shell formatter command. This option would configure a Formatter that also implements the ScanInterpreter interface, saving the user from entering two commands.","Duplicated Code, Long Method, , Duplicated Code, "
"   Move Method,Extract Method,Inline Method,Move Attribute,","Use a CLI library consistently to parse parameters in all utilities. In a few utilities, the command-line parsing is pretty lazy. There's no usage, just an NPE if you don't provide all the magic options on the command-line. In particular, Initialize doesn't use an off-the-shelf library for command-line parsing, and it really should.

See ACCUMULO-744.

In addition, many command-line utilities can and should be able to read accumulo-site.xml, and do not need to provide username/password/instance/zookeeper information by default.
","Duplicated Code, Long Method, , , , , "
"   Rename Method,","RFile should compress using common prefixes of key elements Relative keys have proven themselves as a great way to compress within dimensions of the key. However, we could probably do better, since we know that our data is sorted lexicographically, we can make a reasonable assumption that we will get better compression if we only store the fact that a key (or portion of a key) has a common prefix with the previous key, even if it is not an exact match.

Currently, in RFile, unused bits from the delete flag byte are being used to store flags that show whether an element of the key is exactly the same as the previous, or if it is different. We can change the semantics of these flags to store three states per element of the key: exact match as previous key, has a common prefix as previous key, no relative key compression. If we don't want to add a byte to store 2 bits for 3 states per element, we can just take the ordinal value of the unused 7 bits of the delete flag field and map it to an enumeration of relative key flags.

In the case of a common prefix flag enabled for a given element of the current key when reading the RFile, we can interpret the first bytes of that element as a VInt expressing the length of the common prefix relative to the previous key's same element. Because this will add at least one byte to the the length of that element, we will not want to use the common prefix compression if the common prefix is less than 2 bytes. For less than 2 bytes in common (1 or 0 bytes in common), we'd select the no compression flag for that element.",", "
"   Rename Method,Extract Method,","table namespaces A large cluster is a valuable shared resource. The current permission system and simple table naming structure does not allow for delegation of authority and safe partitioning within this shared resource.

Use cases:
# create a namespace (like ""test"") and delegate the {{grant}} permission to tables created in that namespace to a user that would manage those tables. Presently, {{grant}} is never delegated.
# create simple ""test"" and ""production"" namespaces that are trivial for users to switch between. For example, instead of having tables ""test_index"" and ""test_documents"" the client would support ""index"" and ""documents"" with an API to support switching trivially between the the different environments.
# create a set of tables in a namespace called ""latest"" This namespace is re-created periodically with a map-reduce job. If code changes inadvertently create a corrupt ""latest,"" users can switch to the set of tables known as ""safest"" In this way, users can experiment, and provide feedback on incremental improvements, while have a safe fallback.
# two applications hosted on the same cluster that can share a table, which has been ""aliased"" into their namespace. Namespace-local permissions are ignored, but a (most likely read-only) view of the table is available. This would be helpful for reference tables.
# quotas/priorities. Implement namespace-specific priorities and resource allocations. It is reasonable to run namespace-specific queries and ingest on production equipment. Large cluster resources are always limited, and often the *only* place where near-production quality software can be run at full scale.

","Duplicated Code, Long Method, , "
"   Rename Method,","table namespaces A large cluster is a valuable shared resource. The current permission system and simple table naming structure does not allow for delegation of authority and safe partitioning within this shared resource.

Use cases:
# create a namespace (like ""test"") and delegate the {{grant}} permission to tables created in that namespace to a user that would manage those tables. Presently, {{grant}} is never delegated.
# create simple ""test"" and ""production"" namespaces that are trivial for users to switch between. For example, instead of having tables ""test_index"" and ""test_documents"" the client would support ""index"" and ""documents"" with an API to support switching trivially between the the different environments.
# create a set of tables in a namespace called ""latest"" This namespace is re-created periodically with a map-reduce job. If code changes inadvertently create a corrupt ""latest,"" users can switch to the set of tables known as ""safest"" In this way, users can experiment, and provide feedback on incremental improvements, while have a safe fallback.
# two applications hosted on the same cluster that can share a table, which has been ""aliased"" into their namespace. Namespace-local permissions are ignored, but a (most likely read-only) view of the table is available. This would be helpful for reference tables.
# quotas/priorities. Implement namespace-specific priorities and resource allocations. It is reasonable to run namespace-specific queries and ingest on production equipment. Large cluster resources are always limited, and often the *only* place where near-production quality software can be run at full scale.

",", "
"   Rename Class,Move And Rename Class,Rename Method,","table namespaces A large cluster is a valuable shared resource. The current permission system and simple table naming structure does not allow for delegation of authority and safe partitioning within this shared resource.

Use cases:
# create a namespace (like ""test"") and delegate the {{grant}} permission to tables created in that namespace to a user that would manage those tables. Presently, {{grant}} is never delegated.
# create simple ""test"" and ""production"" namespaces that are trivial for users to switch between. For example, instead of having tables ""test_index"" and ""test_documents"" the client would support ""index"" and ""documents"" with an API to support switching trivially between the the different environments.
# create a set of tables in a namespace called ""latest"" This namespace is re-created periodically with a map-reduce job. If code changes inadvertently create a corrupt ""latest,"" users can switch to the set of tables known as ""safest"" In this way, users can experiment, and provide feedback on incremental improvements, while have a safe fallback.
# two applications hosted on the same cluster that can share a table, which has been ""aliased"" into their namespace. Namespace-local permissions are ignored, but a (most likely read-only) view of the table is available. This would be helpful for reference tables.
# quotas/priorities. Implement namespace-specific priorities and resource allocations. It is reasonable to run namespace-specific queries and ingest on production equipment. Large cluster resources are always limited, and often the *only* place where near-production quality software can be run at full scale.

",", "
"   Rename Method,","ColumnVisibilities should do more to create normalized representations of expressions ColumnVisibilities offer a `flatten()` functionality, which attempts to normalize a given expression. So, if I had an expression, `b&a&c`, and I `flatten`'d it, I'd get back `a&b&c`. 

Through some testing, I found that this is applied and not applied depending on how the expression is written. For instance, if I had something like `(b&a)&c`, I would get back something like `c&a&b`.

It's not much more code to provide a correct normalized form of expressions as well do some more work to detect and eliminate expressions that boil down to `expr&expr` or `expr|expr`.

I've attached a sample program that shows some output of what the current capability is and what I think it should output.",", "
"   Extract Interface,Rename Method,Move Method,Extract Method,Move Attribute,","look into replacing cloudtrace HBase has created their own distributed tracing library, and today I bumped into zipkin. zipkin has a reasonable visualization, and seems to work with thrift. We should look into replacing our tracing with one of these.","Duplicated Code, Long Method, , , , Large Class, "
"   Move Class,Extract Superclass,Rename Method,Extract Method,","look into replacing cloudtrace HBase has created their own distributed tracing library, and today I bumped into zipkin. zipkin has a reasonable visualization, and seems to work with thrift. We should look into replacing our tracing with one of these.","Duplicated Code, Long Method, , Duplicated Code, Large Class, "
"   Rename Method,","Improve C++ support for thrift RPC code generation A user emailed me requesting better support for cpp code generation for our thrift RPC. The improved features requested were:

# Include cpp namespace
# Add --gen cpp line to thrift.sh
# Rename major/minor to majors/minors to deconflict with sysmacro.h
",", "
"   Rename Class,Rename Method,Extract Method,","Iterator to transform key parts Iterators that transform parts of the key can be tricky if any transformation affects sort ordering. Implement an iterator that takes care of the tricky details that come with modifying sort order (e.g., handling scan-time iterator reconstruction and the associated seek).","Duplicated Code, Long Method, , "
"   Rename Method,","Support pluggable encryption in walogs There are some cases where users want encryption at rest for the walogs. It should be fairly trivial to implement it in such a way to insert a CipherOutputStream into the data path (defaulting to using a NullCipher) and then making the Cipher pluggable to users can insert the appropriate mechanisms for their use case.

This also means swapping in CipherInputStream and putting in a check to make sure the Cipher type's match at read and write time. Possibly a versioning mechanism so people can migrate Ciphers.",", "
"   Rename Method,Pull Up Attribute,",Support for new State API in FlinkRunner 0,", Duplicated Code, "
"   Rename Class,Move And Rename Class,Rename Method,",Support for new State API in FlinkRunner 0,", "
"   Rename Method,","Implement the API for Static Display Metadata As described in the following doc, we would like the SDK to allow associating display metadata with PTransforms.

https://docs.google.com/document/d/11enEB9JwVp6vO0uOYYTMYTGkr3TdNfELwWqoiUg5ZxM/edit?usp=sharing",", "
"   Push Down Method,Push Down Attribute,","Migrate the remaining tests to use TestPipeline as a JUnit rule. Following up on [BEAM-1176|https://issues.apache.org/jira/browse/BEAM-1176], the following tests still have direct calls to {{TestPipeline.create()}}:
* {{AvroIOGeneratedClassTest#runTestRead}}
* {{ApproximateUniqueTest#runApproximateUniqueWithDuplicates}}
* {{ApproximateUniqueTest#runApproximateUniqueWithSkewedDistributions}}
* {{SampleTest#runPickAnyTest}}
* {{BigtableIOTest#runReadTest}}

Consider using [parametrised tests|https://github.com/Pragmatists/junitparams] as suggested by [~lcwik].",", , , "
"   Rename Class,Push Down Method,","Migrate the remaining tests to use TestPipeline as a JUnit rule. Following up on [BEAM-1176|https://issues.apache.org/jira/browse/BEAM-1176], the following tests still have direct calls to {{TestPipeline.create()}}:
* {{AvroIOGeneratedClassTest#runTestRead}}
* {{ApproximateUniqueTest#runApproximateUniqueWithDuplicates}}
* {{ApproximateUniqueTest#runApproximateUniqueWithSkewedDistributions}}
* {{SampleTest#runPickAnyTest}}
* {{BigtableIOTest#runReadTest}}

Consider using [parametrised tests|https://github.com/Pragmatists/junitparams] as suggested by [~lcwik].",", , "
"   Move Class,Move Method,Move Attribute,","Migrate the remaining tests to use TestPipeline as a JUnit rule. Following up on [BEAM-1176|https://issues.apache.org/jira/browse/BEAM-1176], the following tests still have direct calls to {{TestPipeline.create()}}:
* {{AvroIOGeneratedClassTest#runTestRead}}
* {{ApproximateUniqueTest#runApproximateUniqueWithDuplicates}}
* {{ApproximateUniqueTest#runApproximateUniqueWithSkewedDistributions}}
* {{SampleTest#runPickAnyTest}}
* {{BigtableIOTest#runReadTest}}

Consider using [parametrised tests|https://github.com/Pragmatists/junitparams] as suggested by [~lcwik].",", , , "
"   Move Class,Move Method,Extract Method,Move Attribute,","Auto set ""enableAbandonedNodeEnforcement"" in TestPipeline At the moment one has to manually set {{enableAbandonedNodeEnforcement(false)}} in tests that do not run the TestPipeline, otherwise one gets an {{AbandonedNodeException}} on account of having nodes that were not run.

This could probably be auto detected using the {{RunnableOnService}} and {{NeedsRunner}} annotations, the presence of which indicates a given test does indeed use a runner. 

Essentially we need to check if {{RunnableOnService}} / {{NeedsRunner}} are present on a given test and if so set {{enableAbandonedNodeEnforcement(true)}}, otherwise set {{enableAbandonedNodeEnforcement(false)}}.

[~tgroh], [~kenn]","Duplicated Code, Long Method, , , , "
"   Move Method,Extract Method,","Replace public constructors with static factory methods for Sum.[*]Fn classes {{Sum.SumDoubleFn}}, {{SumIntegerFn}} and {{SumLongFn}} are not using the {{X.of()}} or {{X.from()}} or other instance creation via static method patterns that are ubiquitous in Beam.

Following a discussion on the dev list, it would be great to preserve a consistent look and feel and change the creation patterns for these classes to something like {{SumFn.ofLong()}} etc.

See also the corresponding [dev list thread|https://lists.apache.org/thread.html/5d8e905ee49b116d13811c2a96da65eeb44ab7c002870f50936ee1ad@%3Cdev.beam.apache.org%3E].","Duplicated Code, Long Method, , , "
"   Rename Method,","Align the naming of ""generateInitialSplits"" and ""splitIntoBundles"" to better reflect their intention See [dev list thread|https://lists.apache.org/thread.html/ac5717566707153e85da880cc75c8d047e1c6606861777670bb9107c@%3Cdev.beam.apache.org%3E].",", "
"   Rename Method,","Basic Java harness capable of understanding process bundle tasks and sending data over the Fn Api Create a basic Java harness capable of understanding process bundle requests and able to stream data over the Fn Api.

Overview: https://s.apache.org/beam-fn-api",", "
"   Move Method,Move Attribute,","Basic Java harness capable of understanding process bundle tasks and sending data over the Fn Api Create a basic Java harness capable of understanding process bundle requests and able to stream data over the Fn Api.

Overview: https://s.apache.org/beam-fn-api",", , , "
"   Move Class,Move Method,","Basic Java harness capable of understanding process bundle tasks and sending data over the Fn Api Create a basic Java harness capable of understanding process bundle requests and able to stream data over the Fn Api.

Overview: https://s.apache.org/beam-fn-api",", , "
"   Move Class,Rename Method,Move Method,Pull Up Attribute,",Update Flink Runner to Flink 1.2.0 When we update to 1.2.0 we can use the new internal Timer API that is available to Flink operators: {{InternalTimerService}} and also use broadcast state to store side-input data.,", , Duplicated Code, "
"   Rename Method,","Use Flink-native side outputs Once Flink has support for side outputs we should use them instead of manually dealing with the {{RawUnionValues}}.

Side outputs for Flink is being tracked in https://issues.apache.org/jira/browse/FLINK-4460.",", "
"   Rename Method,Move Method,",Create Dataflow Runner Package Move Dataflow runner out of SDK core and into new Dataflow runner maven module.,", , "
"   Rename Method,",Refactor HBaseIO to hide visibility of Coders/Serializable classes 0,", "
"   Rename Class,Extract Method,",Need Source/Sink for Spanner Is there a source/sink for Spanner in the works? If not I would gladly give this a shot.,"Duplicated Code, Long Method, , "
"   Move And Rename Class,Rename Method,Move Method,","KafkaIO does not allow using Kafka serializers and deserializers KafkaIO does not allow to override the serializer and deserializer settings of the Kafka consumer and producers it uses internally. Instead, it allows to set a `Coder`, and has a simple Kafka serializer/deserializer wrapper class that calls the coder.

I appreciate that allowing to use Beam coders is good and consistent with the rest of the system. However, is there a reason to completely disallow to use custom Kafka serializers instead?

This is a limitation when working with an Avro schema registry for instance, which requires custom serializers. One can write a `Coder` that wraps a custom Kafka serializer, but that means two levels of un-necessary wrapping.

In addition, the `Coder` abstraction is not equivalent to Kafka's `Serializer` which gets the topic name as input. Using a `Coder` wrapper would require duplicating the output topic setting in the argument to `KafkaIO` and when building the wrapper, which is not elegant and error prone.


",", , "
"   Rename Method,",Add support for bounded sources in streaming mode 0,", "
"   Rename Class,Extract Method,","Support real Bundle in Flink runner The Bundle is very important in the beam model. Users can use the bundle to flush buffer, can reuse many heavyweight resources in a bundle. Most IO plugins use the bundle to flush. 

Moreover, FlinkRunner can also use Bundle to reduce access to the FlinkState, such as first placed in JavaHeap, flush into RocksDbState when invoke finishBundle , this can reduce the number of serialization.

But now FlinkRunner calls the finishBundle every processElement. We need support real Bundle.

I think we can have the following implementations:

1.Invoke finishBundle and next startBundle in {{snapshot}} of Flink. But sometimes this ""Bundle"" maybe too big. This depends on the user's checkpoint configuration.

2.Manually control the size of the bundle. The half-bundle will be flushed to a full-bundle by count or eventTime or processTime or {{snapshot}}. We do not need to wait, just call the startBundle and finishBundle at the right time.

[Proposal document|https://docs.google.com/document/d/1UzELM4nFu8SIeu-QJkbs0sv7Uzd1Ux4aXXM3cw4s7po/edit?usp=sharing]
","Duplicated Code, Long Method, , "
"   Move And Rename Class,Rename Method,Move Method,Extract Method,Move Attribute,","Create Elasticsearch IO compatible with ES 5.x The current Elasticsearch IO (see https://issues.apache.org/jira/browse/BEAM-425) is only compatible with Elasticsearch v 2.x. The aim is to have an IO compatible with ES v 5.x. Beyond being able to address v5.x elasticsearch instances, we could also leverage the use of the Elasticsearch pipeline API and also better split the dataset (be as close as possible of desiredBundleSize) thanks to the new ES split API that allows ES shards splitting.","Duplicated Code, Long Method, , , , "
"   Rename Method,","withCoder() error in JdbcIO JavaDoc example Follow the JavaDoc of JdbcIO, job fails with exception:
{code}
Exception in thread ""main"" java.lang.IllegalStateException: JdbcIO.read() requires a coder to be set via withCoder(coder)
at com.google.common.base.Preconditions.checkState(Preconditions.java:176)
at org.apache.beam.sdk.io.jdbc.JdbcIO$Read.validate(JdbcIO.java:329)
at org.apache.beam.sdk.io.jdbc.JdbcIO$Read.validate(JdbcIO.java:249)
at org.apache.beam.sdk.Pipeline.applyInternal(Pipeline.java:419)
at org.apache.beam.sdk.Pipeline.applyTransform(Pipeline.java:350)
at org.apache.beam.sdk.values.PBegin.apply(PBegin.java:58)
at org.apache.beam.sdk.Pipeline.apply(Pipeline.java:172)
{code}

It requires a Coder provided by withCoder. Need to add it in the example.

Other point is, does it really need to specify a Coder? Users should register it with CoderRegistry I think.",", "
"   Rename Method,Extract Method,",Accumulable MetricsContainers. Make {{MetricsContainer}} accumulable. This can reduce duplication between runners and make implementing metrics easier for runner authors.,"Duplicated Code, Long Method, , "
"   Move Class,Extract Method,","AutoService registration of coders, like we do with PipelineRunners Today, registering coders for auxiliary data types for a library transform is not very convenient. It the appears in an output/covariant position then it might be possible to use {{getDefaultOutputCoder}} to solve things. But for writes/contravariant positions this is not applicable and the library transform must contort itself to avoid requiring the user to come up with a coder for a type they don't own.

Probably the best case today is an explicit call to {{LibraryTransform.registerCoders(Pipeline)}} which is far too manual.

This could likely be solved quite easily with {{@AutoService}} and a static global coder registry, as we do with pipeline runners.","Duplicated Code, Long Method, , "
"   Rename Method,",Bigtable: improve user agent The bigtable-client-core has changed the way it generates user agent strings to automatically provide the information we were providing manually before. Update the BigtableIO client code to fit the new improved scheme.,", "
"   Rename Method,",Fix use of deprecated Spark APIs in the runner. 0,", "
"   Move Class,Move Method,Extract Method,",Support Splittable DoFn in Flink Streaming runner 0,"Duplicated Code, Long Method, , , "
"   Move Class,Move Method,","Thin Java SDK Core Before first stable release we need to thin out {{sdk-java-core}} module. Some candidates for removal, but not a non-exhaustive list:

{{sdk/io}}

* anything BigQuery related
* anything PubSub related
* everything Protobuf related
* TFRecordIO
* XMLSink

{{sdk/util}}

* Everything GCS related
* Everything Backoff related
* Everything Google API related: ResponseInterceptors, RetryHttpBackoff, etc.
* Everything CloudObject-related
* Pubsub stuff

{{sdk/coders}}

* JAXBCoder
* TableRowJsoNCoder

",", , "
"   Move Class,Inline Method,","Thin Java SDK Core Before first stable release we need to thin out {{sdk-java-core}} module. Some candidates for removal, but not a non-exhaustive list:

{{sdk/io}}

* anything BigQuery related
* anything PubSub related
* everything Protobuf related
* TFRecordIO
* XMLSink

{{sdk/util}}

* Everything GCS related
* Everything Backoff related
* Everything Google API related: ResponseInterceptors, RetryHttpBackoff, etc.
* Everything CloudObject-related
* Pubsub stuff

{{sdk/coders}}

* JAXBCoder
* TableRowJsoNCoder

",", , "
"   Rename Method,","Thin Java SDK Core Before first stable release we need to thin out {{sdk-java-core}} module. Some candidates for removal, but not a non-exhaustive list:

{{sdk/io}}

* anything BigQuery related
* anything PubSub related
* everything Protobuf related
* TFRecordIO
* XMLSink

{{sdk/util}}

* Everything GCS related
* Everything Backoff related
* Everything Google API related: ResponseInterceptors, RetryHttpBackoff, etc.
* Everything CloudObject-related
* Pubsub stuff

{{sdk/coders}}

* JAXBCoder
* TableRowJsoNCoder

",", "
"   Move Method,Move Attribute,","Thin Java SDK Core Before first stable release we need to thin out {{sdk-java-core}} module. Some candidates for removal, but not a non-exhaustive list:

{{sdk/io}}

* anything BigQuery related
* anything PubSub related
* everything Protobuf related
* TFRecordIO
* XMLSink

{{sdk/util}}

* Everything GCS related
* Everything Backoff related
* Everything Google API related: ResponseInterceptors, RetryHttpBackoff, etc.
* Everything CloudObject-related
* Pubsub stuff

{{sdk/coders}}

* JAXBCoder
* TableRowJsoNCoder

",", , , "
"   Move Method,Move Attribute,","Thin Java SDK Core Before first stable release we need to thin out {{sdk-java-core}} module. Some candidates for removal, but not a non-exhaustive list:

{{sdk/io}}

* anything BigQuery related
* anything PubSub related
* everything Protobuf related
* TFRecordIO
* XMLSink

{{sdk/util}}

* Everything GCS related
* Everything Backoff related
* Everything Google API related: ResponseInterceptors, RetryHttpBackoff, etc.
* Everything CloudObject-related
* Pubsub stuff

{{sdk/coders}}

* JAXBCoder
* TableRowJsoNCoder

",", , , "
"   Move Class,Move Method,","Thin Java SDK Core Before first stable release we need to thin out {{sdk-java-core}} module. Some candidates for removal, but not a non-exhaustive list:

{{sdk/io}}

* anything BigQuery related
* anything PubSub related
* everything Protobuf related
* TFRecordIO
* XMLSink

{{sdk/util}}

* Everything GCS related
* Everything Backoff related
* Everything Google API related: ResponseInterceptors, RetryHttpBackoff, etc.
* Everything CloudObject-related
* Pubsub stuff

{{sdk/coders}}

* JAXBCoder
* TableRowJsoNCoder

",", , "
"   Move Method,Move Attribute,","Thin Java SDK Core Before first stable release we need to thin out {{sdk-java-core}} module. Some candidates for removal, but not a non-exhaustive list:

{{sdk/io}}

* anything BigQuery related
* anything PubSub related
* everything Protobuf related
* TFRecordIO
* XMLSink

{{sdk/util}}

* Everything GCS related
* Everything Backoff related
* Everything Google API related: ResponseInterceptors, RetryHttpBackoff, etc.
* Everything CloudObject-related
* Pubsub stuff

{{sdk/coders}}

* JAXBCoder
* TableRowJsoNCoder

",", , , "
"   Rename Method,","window support There're several methods, TUMBLE/HOP/SESSION, introduced in Calcite 1.12, which represent a window-aggregation operation. 

In BeamSQL, it's expected to leverage these methods to determine window function, set trigger strategy, also handle event_time/watermark properly. 
",", "
"   Rename Method,","Move CloudObject to Dataflow runner This entails primarily eliminating Coder.asCloudObject() by adding the needed accessors, and possibly a serialization registrar discipline, for coders in the Runner API proto.",", "
"   Rename Method,",Create Parquet IO Would be nice to support Parquet files with projection and predicates.,", "
"   Rename Method,Move Attribute,",Create Parquet IO Would be nice to support Parquet files with projection and predicates.,", , "
"   Rename Method,Move Method,",Create Parquet IO Would be nice to support Parquet files with projection and predicates.,", , "
"   Move Method,Extract Method,","BeamKafkaCSVTable: support column type other than string Currently, BeamKafkaCSVTable only supports String column, need to support other types.

Also, use a more robust library to parse CSV.","Duplicated Code, Long Method, , , "
"   Rename Method,","Support for recursive wildcards in GcsPath When working with heavily nested folder structures in Google Cloud Storage, it's great to make use of recursive wildcards, which the current API explicitly does not support.

This code hasn't been touched in 2 years so it's likely that simply no one's gotten around to it yet.",", "
"   Rename Class,Extract Method,","Support custom user Jackson modules for PipelineOptions HadoopFileSystem was added with support for passing Hadoop Configuration through PipelineOptions in #BEAM-2031.

This was done by making the ObjectMapper within PipelineOptionsFactory find and load Jackson modules using Jackson supported ServiceLoader pattern.

Serializing PipelineOptions requires runners to also use an ObjectMapper which has been similarly configured.","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Allow coder factories to create Coders for a wider range of types By allowing the CoderFactory to see they type it can look at annotations on the type allowing:
* @DefaultCoder to become a CoderFactory
* Creating a ProtoCoderFactory which delegates to the ProtoCoder for all Message types
* Creating a SerializableCoderFactory which delegates to the SerializableCoder for all Serializable types
* Creating a WritableCoderFactory which delegates to the Hadoop WritableCoder for all Hadoop Writable types

This requires plumbing through TypeDescriptor as the primary method of looking up coders within the CoderRegistry

This also removes the concept of fallback coder providers since every coder factory is treated as a fallback.","Duplicated Code, Long Method, , , , , "
"   Move And Rename Class,Rename Method,","Refine DSL interface As part of BEAM-2010, it update the interface to explain SQL query, to link with Pipeline, so below part is supported:
{code}
//prepare environment of BeamSQL
BeamSQLEnvironment sqlEnv = BeamSQLEnvironment.create();
//register table metadata
sqlEnv.addTableMetadata(String tableName, BeamSqlTable tableMetadata);
//register UDF
sqlEnv.registerUDF(String functionName, Method udfMethod);


//explain a SQL statement, SELECT only, and return as a PCollection;
PCollection<BeamSQLRow> phase1Stream = sqlEnv.explainSQL(pipeline, String sqlStatement);
{code}",", "
"   Rename Class,Rename Method,","Rename RemoveDuplicates to Distinct I had a really tough time finding this transform in the docs. I suggest changing this class' name to Distinct instead of RemoveDuplicates. At the very least, the JavaDoc for RemoveDuplicates should have the word distinct in it to make this more findable/searchable.",", "
"   Rename Method,","Make the write transform of HBaseIO simpler HBaseIO imitated the interface of Cloud Bigtable to have an easy migration path for users from/to Bigtable. 

Bigtable Mutation object does not include the row key, so the BigtableIO needed to have a KV with the row key and the Mutation in order to change the data. Hbase does not have this restriction because the row key is part of the Mutation object, this issue is to simplify the Write transform even if it will make a small difference",", "
"   Move Class,Rename Method,Pull Up Method,Move Method,Pull Up Attribute,Move Attribute,","Abstract StateInternalsTest for the different state internals/Runners For the test of InMemoryStateInternals, ApexStateInternals, FlinkStateInternals, SparkStateInternals, etc..
Have a common base class for the state internals test that has an abstract method createStateInternals() and all the test methods. Then an actual implementation would just derive from that and only implement the method for creating the state internals.",", , , Duplicated Code, Duplicated Code, "
"   Rename Method,Move Method,Move Attribute,","Abstract StateInternalsTest for the different state internals/Runners For the test of InMemoryStateInternals, ApexStateInternals, FlinkStateInternals, SparkStateInternals, etc..
Have a common base class for the state internals test that has an abstract method createStateInternals() and all the test methods. Then an actual implementation would just derive from that and only implement the method for creating the state internals.",", , , "
"   Move Method,Extract Method,","DSL SQL: Public classes/methods should not expose/use calcite types Calcite is an internal implementation detail of how Beam SQL is operating. To prevent a hard dependence on Calcite, public methods and classes should not rely on consuming/producing Calcite types.

For example, BeamSqlRecordType uses org.apache.calcite.sql.type.SqlTypeName instead of using the Java SQL types (https://docs.oracle.com/javase/8/docs/api/java/sql/Types.html).

This task is to create an ApiSurfaceTest to help find, fix, and prevent org.apache.calcite.* from being exposed. Example ApiSurfaceTest: https://github.com/apache/beam/blob/367fcb28d544934797d25cb34d54136b2d7d6e99/sdks/java/core/src/test/java/org/apache/beam/SdkCoreApiSurfaceTest.java","Duplicated Code, Long Method, , , "
"   Rename Class,Rename Method,",Backlog size retrieval for Kinesis source Implement backlog size retrieval for Kinesis source with the use of Amazon CloudWatch. This will allow the runners to scale the amount of resources allocated to the pipeline.,", "
"   Rename Method,Move Method,Inline Method,Move Attribute,","Unify Flink Operator Wrappers Right now, we have {{FlinkAbstractParDoWrapper}} with subclasses {{FlinkParDoBoundWrapper}} and {{FlinkParDoBoundMultiWrapper}} as well as {{FlinkGroupAlsoByWindowWrapper}}. They do essentially the same thing, but slightly differently. The first three are implemented as a {{FlatMapFunction}} while the latter is implemented as a {{StreamOperator}} (which is more low-level and gives access to state and timers and such).

We should unify this into one wrapper. (With possibly a more concise name...)

In the process of this we should also make sure that we always use a {{DoFnRunner}} via {{DoFnRunners.createDefault}}. This will help reduce bugs such as [BEAM-241].",", , , , "
"   Rename Method,Move Method,Inline Method,Move Attribute,","Unify Flink Operator Wrappers Right now, we have {{FlinkAbstractParDoWrapper}} with subclasses {{FlinkParDoBoundWrapper}} and {{FlinkParDoBoundMultiWrapper}} as well as {{FlinkGroupAlsoByWindowWrapper}}. They do essentially the same thing, but slightly differently. The first three are implemented as a {{FlatMapFunction}} while the latter is implemented as a {{StreamOperator}} (which is more low-level and gives access to state and timers and such).

We should unify this into one wrapper. (With possibly a more concise name...)

In the process of this we should also make sure that we always use a {{DoFnRunner}} via {{DoFnRunners.createDefault}}. This will help reduce bugs such as [BEAM-241].",", , , , "
"   Rename Method,","Apache Apex Runner Like Spark, Flink and GearPump, Apache Apex also does have advantages. Is it possible to have a runner for Apache Apex?",", "
"   Move Class,Extract Method,","Apache Apex Runner Like Spark, Flink and GearPump, Apache Apex also does have advantages. Is it possible to have a runner for Apache Apex?","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,",TextIO should allow specifying a custom delimiter Currently TextIO use {{\r}} {{\n}} or {{\r\n}} or a mix of the two to split a text file into PCollection elements. It might happen that a record is spread across more than one line. In that case we should be able to specify a custom record delimiter to be used in place of the default ones.,"Duplicated Code, Long Method, , "
"   Rename Method,",Make MinLongFn and MaxLongFn mimic SumLongFn and use BinaryCombineLongFn Ditto for the other 'optimized accumulator' combiner functions.,", "
"   Rename Method,Extract Method,","Implement FileIO.write() Design doc: http://s.apache.org/fileio-write

Discussion: https://lists.apache.org/thread.html/cc543556cc709a44ed92262207215eaa0e43a0f573c630b6360d4edc@%3Cdev.beam.apache.org%3E","Duplicated Code, Long Method, , "
"   Rename Method,",BigtableIO should use ValueProviders [https://github.com/apache/beam/pull/2057] is an effort towards BigtableIO templatization. This Issue is a request to get a fully featured template for BigtableIO.,", "
"   Move And Rename Class,Extract Method,","Extract ReifyTimestampsAndWindows from GatherAllPanes The {{ReifyTimestampsAndWindowsFn}} in {{GatherAllPanes}} looks useful for giving {{MapElements}}, {{Filter}}, etc access to window and pane information.

This JIRA represents the proposal to extract a subset of the {{GatherAllPanes}} functionality into a new class {{ReifyTimestampsAndWindows}}, next to {{ReifyTimestamps}} which does similar functionality but with {{TimestampedValue}}

{{GatherAllPanes}} would then rely on the newly extracted public {{PTransform}} for its previous functionality.","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,Move Method,Extract Method,Move Attribute,",Samza runner Apache Samza is a distributed data-processing platform which supports both stream and batch processing. It'll be awesome if we can run BEAM's advanced data transform and multi-language sdks on top of Samza.,"Duplicated Code, Long Method, , , , "
"   Move Method,Move Attribute,","Add Kinesis Write transform Currently KinesisIO only has a Read transform, we need to provide a Write too.",", , , "
"   Rename Method,",Update KinesisIO to use AWS SDK 1.11.255 and KCL 1.8.8 The current version of the AWS SDK that Kinesis uses does not include new regions/AZ from AWS. This update solves this as well as include the most recent fixes on the SDK.,", "
"   Rename Method,","Change Filter#greaterThan, etc. to actually use Filter This is a good starter task.

Right now, [{{Filter#greaterThan}}|https://github.com/apache/incubator-beam/blob/315b3c8e333e5f42730c19e89f856d778ce93cab/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/Filter.java#L134] constructs a new DoFn rather than using {{Filter#byPredicate}}. We should fix this to make it consistent and simpler.

We can also remove deprecated functions in that file, and if possible redundant display data.",", "
"   Rename Method,","Change Filter#greaterThan, etc. to actually use Filter This is a good starter task.

Right now, [{{Filter#greaterThan}}|https://github.com/apache/incubator-beam/blob/315b3c8e333e5f42730c19e89f856d778ce93cab/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/Filter.java#L134] constructs a new DoFn rather than using {{Filter#byPredicate}}. We should fix this to make it consistent and simpler.

We can also remove deprecated functions in that file, and if possible redundant display data.",", "
"   Move Class,Move Method,Move Attribute,","Remove (or merge) Java 8 specific tests module into the main one. The module beam-sdks-java-java8tests has specific tests for some core transforms written in Java 8 syntax. Because of the move to Java 8 probably it doesn't make sense anymore to have this module because all the tests should be Java 8, so the tests in this module should be merged with the main ones, or it should be removed if they are already covered.",", , , "
"   Move Method,Extract Method,","Support encryption for S3FileSystem (SSE-S3, SSE-C and SSE-KMS) We should enable AWS S3 users to use encryption when reading or writing by provide encryption keys or using server side encryption via an algorithm, or a key management system (KMS).. 

 ","Duplicated Code, Long Method, , , "
"   Rename Method,","Make MetricQueryResults and related classes more json-serialization friendly When working on this PR [https://github.com/apache/beam/pull/4548] MetricQueryResults needed to be serialized to be pushed to a metrics sink. As they were it required a custom serializer that just calls the name(), counter(), committed(), attempted() ... methods. MetricQueryResults are so close to be serializable with the default serializer, just need the accessors to be renamed get*, that creating DTO objects with get* methods to just call the non-get methods seems unnecessary.  
So just rename public accessors to get* on the experimental API",", "
"   Rename Method,Inline Method,",Update Flink Runner to Flink 1.5.0 0,", , "
"   Rename Method,",Add tests for Flink DoFnOperator side-input checkpointing 0,", "
"   Rename Method,","Split IOTestPipelineOptions to multiple, test-specific files Currently we have one big IOTestPipelineOptions interface that is used in many IOITs. It contains test specific options that should rather be located next to testing classes, not in a generic file. Let's split this. Additionally, besides separation of concerns,  this will allow adding test-specific @Default and @Required annotations and validate the options better. ",", "
"   Rename Method,","Introducing gcpTempLocation that default to tempLocation Currently, DataflowPipelineOptions.stagingLocation default to tempLocation. And, it requires tempLocation to be a gcs path.
Another case is BigQueryIO uses tempLocation and also requires it to be on gcs.
So, users cannot set tempLocation to a non-gcs path with DataflowRunner or BigQueryIO.

However, tempLocation could be on any file system. For example, WordCount defaults to output to tempLocation.

The proposal is to add gcpTempLocation. And, it defaults to tempLocation if tempLocation is a gcs path.
StagingLocation and BigQueryIO will use gcpTempLocation by default.",", "
"   Rename Class,Rename Method,Extract Method,","Enable partial updates Elasticsearch Expose a configuration option on the {{ElasticsearchIO}} to enable partial updates rather than full document inserts. 

Rationale: We have the case where different pipelines process different categories of information of the target entity (e.g. one for taxonomic processing, another for geospatial processing). A read and merge is not possible inside the batch call, meaning the only way to do it is through a join. The join approach is slow, and also stops the ability to run a single process in isolation (e.g. reprocess the geospatial component of all docs). 

Use of this configuration parameter has to be used in conjunction with controlling the document ID (possible since BEAM-3201) to make sense. 

The client API would include a {{withUseUpdate(...)}} such as: 

{code} 
source.apply( 
ElasticsearchIO.write() 
.withConnectionConfiguration(connectionConfiguration) 
.withIdFn(new ExtractValueFn(""id"")) 
.withUseUpdate(true) 
{code} 

","Duplicated Code, Long Method, , "
"   Rename Method,","Consider enabling spotless java format throughout codebase ""Spotless"" can enforce - and automatically restore - automatic Java formatting. Whenever formatting is off, it tells a user the exact command to fix it. 

It isn't (just) about code layout, it is about automation. We have pretty strict style rules enforced by checkstyle. The most efficient way to fix up a file is with autoformat. But if the autoformat hits a bunch of irrelevant lines, that is annoying for a reviewer and obscures git blame. 

If we enforce autoformat all the time, then it makes sure that autoformatting a particular PR has minimal effects and is always safe to do.",", "
"   Rename Method,Extract Method,","PipelineResult needs waitUntilFinish() and cancel() waitToFinish() and cancel() are two most common operations for users to interact with a started pipeline.

Right now, they are only available in DataflowPipelineJob. But, it is better to move them to the common interface, so people can start implement them in other runners, and runner agnostic code can interact with PipelineResult better.","Duplicated Code, Long Method, , "
"   Rename Method,","Improve IOChannelUtils.resolve() to accept multiple paths at once Currently, IOChannelUtils.resolve() method can only resolve one path against base path. 

It's useful to have another method with arguments that includes one base path and multiple others. The return string will be a directory that start with base path and append rests which are separated by file separator.",", "
"   Move And Rename Class,Rename Method,Move Method,Extract Method,","BigQueryIO.Read reimplemented as BoundedSource BigQueryIO.Read is currently implemented in a hacky way: the DirectPipelineRunner streams all rows in the table or query result directly using the JSON API, in a single-threaded manner.

In contrast, the DataflowPipelineRunner uses an entirely different code path implemented in the Google Cloud Dataflow service. (A BigQuery export job to GCS, followed by a parallel read from GCS).

We need to reimplement BigQueryIO as a BoundedSource in order to support other runners in a scalable way.

I additionally suggest that we revisit the design of the BigQueryIO source in the process. A short list:

* Do not use TableRow as the default value for rows. It could be Map<String, Object> with well-defined types, for example, or an Avro GenericRecord. Dropping TableRow will get around a variety of issues with types, fields named 'f', etc., and it will also reduce confusion as we use TableRow objects differently than usual (for good reason).

* We could also directly add support for a RowParser to a user's POJO.

* We should expose TableSchema as a side output from the BigQueryIO.Read.

* Our builders for BigQueryIO.Read are useful and we should keep them. Where possible we should also allow users to provide the JSON objects that configure the underlying intermediate tables, query export, etc. This would let users directly control result flattening, location of intermediate tables, table decorators, etc., and also optimistically let users take advantage of some new BigQuery features without code changes.

* We could use switch between whether we use a BigQuery export + parallel scan vs API read based on factors such as the size of the table at pipeline construction time.","Duplicated Code, Long Method, , , "
"   Move Class,Rename Class,Rename Method,Extract Method,","BigQueryIO.Read reimplemented as BoundedSource BigQueryIO.Read is currently implemented in a hacky way: the DirectPipelineRunner streams all rows in the table or query result directly using the JSON API, in a single-threaded manner.

In contrast, the DataflowPipelineRunner uses an entirely different code path implemented in the Google Cloud Dataflow service. (A BigQuery export job to GCS, followed by a parallel read from GCS).

We need to reimplement BigQueryIO as a BoundedSource in order to support other runners in a scalable way.

I additionally suggest that we revisit the design of the BigQueryIO source in the process. A short list:

* Do not use TableRow as the default value for rows. It could be Map<String, Object> with well-defined types, for example, or an Avro GenericRecord. Dropping TableRow will get around a variety of issues with types, fields named 'f', etc., and it will also reduce confusion as we use TableRow objects differently than usual (for good reason).

* We could also directly add support for a RowParser to a user's POJO.

* We should expose TableSchema as a side output from the BigQueryIO.Read.

* Our builders for BigQueryIO.Read are useful and we should keep them. Where possible we should also allow users to provide the JSON objects that configure the underlying intermediate tables, query export, etc. This would let users directly control result flattening, location of intermediate tables, table decorators, etc., and also optimistically let users take advantage of some new BigQuery features without code changes.

* We could use switch between whether we use a BigQuery export + parallel scan vs API read based on factors such as the size of the table at pipeline construction time.","Duplicated Code, Long Method, , "
"   Move Method,Move Attribute,","Use BigQueryServices abstraction in BigQueryIO There are legacy code that sent request to BigQuery directly.
They should be moved to use BigQueryServices.",", , , "
"   Move Class,Extract Method,",Improve BeamSqlLine unit tests 0,"Duplicated Code, Long Method, , "
"   Move Method,Move Attribute,",Add an integration test for BeamSqlLine Test non group by window queries in Beam SQL Shell tutorial.,", , , "
"   Move And Rename Class,","Generalize FileChecksumMatcher used for all E2E test Refactor WordCountOnSuccessMatcher to be more general so that it can be reused by other tests.

Requirement:
Given input file path (accept glob) and expected checksum, generate checksum of file(s) and verify with expected.",", "
"   Move And Rename Class,","Generalize FileChecksumMatcher used for all E2E test Refactor WordCountOnSuccessMatcher to be more general so that it can be reused by other tests.

Requirement:
Given input file path (accept glob) and expected checksum, generate checksum of file(s) and verify with expected.",", "
"   Rename Method,Extract Method,","Support ES 6.x for ElasticsearchIO Elasticsearch has released 6.3.2 but ElasticsearchIO only supports 2x-5.x. 
We should support ES 6.x for ElasticsearchIO. 
https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html 
https://github.com/apache/beam/blob/master/sdks/java/io/elasticsearch/src/main/java/org/apache/beam/sdk/io/elasticsearch/ElasticsearchIO.java","Duplicated Code, Long Method, , "
"   Move And Rename Class,Rename Class,Move Method,Extract Method,Move Attribute,","PubSubIO: reimplement in Java PubSubIO is currently only partially implemented in Java: the DirectPipelineRunner uses a non-scalable API in a single-threaded manner.

In contrast, the DataflowPipelineRunner uses an entirely different code path implemented in the Google Cloud Dataflow service.

We need to reimplement PubSubIO in Java in order to support other runners in a scalable way.

Additionally, we can take this opportunity to add new features:

* getting timestamp from an arbitrary lambda in arbitrary formats rather than from a message attribute in only 2 formats.
* exposing metadata and attributes in the elements produced by PubSubIO.Read
* setting metadata and attributes in the messages written by PubSubIO.Write
","Duplicated Code, Long Method, , , , "
"   Move And Rename Class,","Pipelines and their executions naming changes The purpose of the changes is to clarify the differences between the two, have
consensus between runners, and unify the implementation.

Current states:
* PipelineOptions.appName defaults to mainClass name
* DataflowPipelineOptions.jobName defaults to appName+user+datetime
* FlinkPipelineOptions.jobName defaults to appName+user+datetime

Proposal:
1. Replace PipelineOptions.appName with PipelineOptions.pipelineName.
* It is the user-visible name for a specific graph.
* default to mainClass name.
* Use cases: Find all executions of a pipeline
2. Add jobName to top level PipelineOptions.
* It is the unique name for an execution
* defaults to pipelineName + user + datetime + random Integer
* Use cases:
-- Finding all executions by USER_A between TIME_X and TIME_Y
-- Naming resources created by the execution. for example:",", "
"   Move And Rename Class,","Pipelines and their executions naming changes The purpose of the changes is to clarify the differences between the two, have
consensus between runners, and unify the implementation.

Current states:
* PipelineOptions.appName defaults to mainClass name
* DataflowPipelineOptions.jobName defaults to appName+user+datetime
* FlinkPipelineOptions.jobName defaults to appName+user+datetime

Proposal:
1. Replace PipelineOptions.appName with PipelineOptions.pipelineName.
* It is the user-visible name for a specific graph.
* default to mainClass name.
* Use cases: Find all executions of a pipeline
2. Add jobName to top level PipelineOptions.
* It is the unique name for an execution
* defaults to pipelineName + user + datetime + random Integer
* Use cases:
-- Finding all executions by USER_A between TIME_X and TIME_Y
-- Naming resources created by the execution. for example:",", "
"   Rename Method,","Support Dynamic PipelineOptions During the graph construction phase, the given SDK generates an initial
execution graph for the program. At execution time, this graph is
executed, either locally or by a service. Currently, Beam only supports
parameterization at graph construction time. Both Flink and Spark supply
functionality that allows a pre-compiled job to be run without SDK
interaction with updated runtime parameters.

In its current incarnation, Dataflow can read values of PipelineOptions at
job submission time, but this requires the presence of an SDK to properly
encode these values into the job. We would like to build a common layer
into the Beam model so that these dynamic options can be properly provided
to jobs.

Please see
https://docs.google.com/document/d/1I-iIgWDYasb7ZmXbGBHdok_IK1r1YAJ90JG5Fz0_28o/edit
for the high-level model, and
https://docs.google.com/document/d/17I7HeNQmiIfOJi0aI70tgGMMkOSgGi8ZUH-MOnFatZ8/edit
for
the specific API proposal.
",", "
"   Move And Rename Class,Move Method,",Move mock classes to sql src/main 0,", , "
"   Rename Method,","Portable Flink support for maxBundleSize/maxBundleMillis The portable runner needs to support larger bundles in streaming mode. Currently every element is a separate bundle, which is very inefficient due to the per bundle SDK worker overhead. The old Java SDK runner already supports these parameters.",", "
"   Rename Method,",Allow registering UDF with the same method name but different argument list 0,", "
"   Rename Method,Extract Method,","Switch from IOChannelFactory to FileSystems Right now, FileBasedSource and FileBasedSink communication is mediated by IOChannelFactory. There are a number of issues:

* Global configuration -- e.g., all 'gs://' URIs use the same credentials. This should be per-source/per-sink/etc.
* Supported APIs -- currently IOChannelFactory is in the ""non-public API"" util package and subject to change. We need users to be able to add new backends ('s3://', 'hdfs://', etc.) directly, without fear that they will be broken.
* Per-backend features: e.g., creating buckets in GCS/s3, setting expiration time, etc.

Updates:
Design docs posted on dev@ list:
Part 1: IOChannelFactory Redesign: 
https://docs.google.com/document/d/11TdPyZ9_zmjokhNWM3Id-XJsVG3qel2lhdKTknmZ_7M/edit#

Part 2: Configurable BeamFileSystem:
https://docs.google.com/document/d/1-7vo9nLRsEEzDGnb562PuL4q9mUiq_ZVpCAiyyJw8p8/edit#heading=h.p3gc3colc2cs","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Switch from IOChannelFactory to FileSystems Right now, FileBasedSource and FileBasedSink communication is mediated by IOChannelFactory. There are a number of issues:

* Global configuration -- e.g., all 'gs://' URIs use the same credentials. This should be per-source/per-sink/etc.
* Supported APIs -- currently IOChannelFactory is in the ""non-public API"" util package and subject to change. We need users to be able to add new backends ('s3://', 'hdfs://', etc.) directly, without fear that they will be broken.
* Per-backend features: e.g., creating buckets in GCS/s3, setting expiration time, etc.

Updates:
Design docs posted on dev@ list:
Part 1: IOChannelFactory Redesign: 
https://docs.google.com/document/d/11TdPyZ9_zmjokhNWM3Id-XJsVG3qel2lhdKTknmZ_7M/edit#

Part 2: Configurable BeamFileSystem:
https://docs.google.com/document/d/1-7vo9nLRsEEzDGnb562PuL4q9mUiq_ZVpCAiyyJw8p8/edit#heading=h.p3gc3colc2cs","Duplicated Code, Long Method, , "
"   Move Method,Move Attribute,","Switch from IOChannelFactory to FileSystems Right now, FileBasedSource and FileBasedSink communication is mediated by IOChannelFactory. There are a number of issues:

* Global configuration -- e.g., all 'gs://' URIs use the same credentials. This should be per-source/per-sink/etc.
* Supported APIs -- currently IOChannelFactory is in the ""non-public API"" util package and subject to change. We need users to be able to add new backends ('s3://', 'hdfs://', etc.) directly, without fear that they will be broken.
* Per-backend features: e.g., creating buckets in GCS/s3, setting expiration time, etc.

Updates:
Design docs posted on dev@ list:
Part 1: IOChannelFactory Redesign: 
https://docs.google.com/document/d/11TdPyZ9_zmjokhNWM3Id-XJsVG3qel2lhdKTknmZ_7M/edit#

Part 2: Configurable BeamFileSystem:
https://docs.google.com/document/d/1-7vo9nLRsEEzDGnb562PuL4q9mUiq_ZVpCAiyyJw8p8/edit#heading=h.p3gc3colc2cs",", , , "
"   Move Class,Rename Method,Move Method,Extract Method,Inline Method,","Switch from IOChannelFactory to FileSystems Right now, FileBasedSource and FileBasedSink communication is mediated by IOChannelFactory. There are a number of issues:

* Global configuration -- e.g., all 'gs://' URIs use the same credentials. This should be per-source/per-sink/etc.
* Supported APIs -- currently IOChannelFactory is in the ""non-public API"" util package and subject to change. We need users to be able to add new backends ('s3://', 'hdfs://', etc.) directly, without fear that they will be broken.
* Per-backend features: e.g., creating buckets in GCS/s3, setting expiration time, etc.

Updates:
Design docs posted on dev@ list:
Part 1: IOChannelFactory Redesign: 
https://docs.google.com/document/d/11TdPyZ9_zmjokhNWM3Id-XJsVG3qel2lhdKTknmZ_7M/edit#

Part 2: Configurable BeamFileSystem:
https://docs.google.com/document/d/1-7vo9nLRsEEzDGnb562PuL4q9mUiq_ZVpCAiyyJw8p8/edit#heading=h.p3gc3colc2cs","Duplicated Code, Long Method, , , , "
"   Rename Method,","Switch from IOChannelFactory to FileSystems Right now, FileBasedSource and FileBasedSink communication is mediated by IOChannelFactory. There are a number of issues:

* Global configuration -- e.g., all 'gs://' URIs use the same credentials. This should be per-source/per-sink/etc.
* Supported APIs -- currently IOChannelFactory is in the ""non-public API"" util package and subject to change. We need users to be able to add new backends ('s3://', 'hdfs://', etc.) directly, without fear that they will be broken.
* Per-backend features: e.g., creating buckets in GCS/s3, setting expiration time, etc.

Updates:
Design docs posted on dev@ list:
Part 1: IOChannelFactory Redesign: 
https://docs.google.com/document/d/11TdPyZ9_zmjokhNWM3Id-XJsVG3qel2lhdKTknmZ_7M/edit#

Part 2: Configurable BeamFileSystem:
https://docs.google.com/document/d/1-7vo9nLRsEEzDGnb562PuL4q9mUiq_ZVpCAiyyJw8p8/edit#heading=h.p3gc3colc2cs",", "
"   Rename Method,",SimpleStreamingWordCountTest does not properly test fixed windows {{org.apache.beam.runners.spark.translation.streaming.SimpleStreamingWordCountTest}} does not properly test {{FixedWindows}}.,", "
"   Rename Method,","Add public TypedPValue.setTypeDescriptor This would give fairly pithy answers to StackOverflow questions sometimes.

When choosing between .getOutputCoder() and .getOutputTypeDescriptor() for a transform/DoFn we often choose the type, so the coder registry can do its thing.

This would also give a similar choice between .setCoder(...) and .setTypeDescriptor(...).

And anyhow we have the intention of removing our practice of the ""*Internal"" suffix, so this one might be most easily solved by making it public.",", "
"   Rename Method,",Use AutoValue and deal with Document instead of String in MongoDbIO 0,", "
"   Rename Class,Inline Method,",Use AutoValue and deal with Document instead of String in MongoDbIO 0,", , "
"   Move And Rename Class,Rename Method,Move Method,Move Attribute,","Use composition over inheritance in spark StreamingEvaluationContext if two contexts are necessary. After working on PR: https://github.com/apache/incubator-beam/pull/1096 ,
I felt it is easy to forget updating spark streaming context with current inheritance.

And, having a single EvaluationContext to support both streaming and batch is even better.",", , , "
"   Rename Class,Rename Method,Extract Method,","Validate PipelineOptions Default annotation It shouldn't allow @Override with @Default annotation, for example the following is broken:
interface A {
@Default.Integer(1)
Integer getFoo();
void setFoo();
}

interface B extends A {
@Default.Integer(-1)
@Override
Integer getFoo();
}

It is broken, because PipelineOptions default values are lazily evaluated. And, it will depends on which one of the two following operations happen first:
options.as(A.class) and options.as(B.class)

If users want to change the default value, users should do setFoo(...) explicitly.

It shouldn't allow adding Default annotation as well.","Duplicated Code, Long Method, , "
"   Rename Method,","Support GroupByKey directly Currently the SparkRunner supports GroupByKey via override with GroupByKeyViaGroupByKeyOnly, make it support GroupByKey directly.",", "
"   Rename Method,",Use New DoFn Directly in Flink Runner 0,", "
"   Rename Method,",Use New DoFn Directly in Flink Runner 0,", "
"   Move Method,Move Attribute,","Make ApiSurfaceTest detect the java package/module under test {{ApiSurfaceTest}} in the {{sdks/java/core}} is the class responsible for protecting our public API surface.

This test walks the public signatures of all modules and explicitly verifies that everything is on a whitelist. This is how we control what dependencies we expose to our users, so that Beam can keep a tight, stable API surface.

Today you must indicate what Java package to scan, such as `org.apache.beam.sdk`. It would be nice for this to be automatically determined.",", , , "
"   Move Method,Move Attribute,","Make ApiSurfaceTest detect the java package/module under test {{ApiSurfaceTest}} in the {{sdks/java/core}} is the class responsible for protecting our public API surface.

This test walks the public signatures of all modules and explicitly verifies that everything is on a whitelist. This is how we control what dependencies we expose to our users, so that Beam can keep a tight, stable API surface.

Today you must indicate what Java package to scan, such as `org.apache.beam.sdk`. It would be nice for this to be automatically determined.",", , , "
"   Rename Method,","ProxyManager enhancements As brett pointed out, the DefaultProxyManager needs some refactoring especially on these points:

- remove cache period
- get() ignores the return of getRemoteFile()
- make the proxy check for the checksum and metadata files first, before parsing the path for an artifact
- getArtifactFile should use artifact.setFile() and not return the file anymore.
- temp should be setup to deleteOnExit()
- rename copyTempToTarget to moveTempToTarget
- rename prepare/release checksums to prepare/release ChecksumListeners
- use FileUtils.read when applicable",", "
"   Rename Method,","Search Usability Taken from Bretts email to the list
{noformat}
- [ ] improve the search results page
- [ ] remove metadata files
- [ ] merge versions in search results
- [ ] for snapshots, just show SNAPSHOT, not timestamps
- [ ] show hits in the results (this may not be possible or needed with better results, however)

- [ ] existing JIRA complaints
- [ ] MRM-732 (tokenizing)
- [ ] MRM-495 (weighting)
- [ ] MRM-609 (windows bug - may be fixed)
- [ ] MRM-933 (hit count, pagination completely busted)

- [ ] advanced search
- [ ] improve appearance and flexibility, maybe change to ""add term"" buttons on the default search
- [ ] class/package search is still flaky
- [ ] might be the analyzer rules, etc. for splitting on '.'

- [ ] browse improvements
- [ ] artifact version list should show basic shared project information rather than having to drill into one version
- [ ] snapshot should go to a page that shows a list of versions
(go to latest, but list previous snapshots)
{noformat}",", "
"   Pull Up Method,Pull Up Attribute,","Log configuration and repository changes made. Log the changes made to the configurations and repositories.

Added logging to the following operations:
- purge (artifacts being removed)
- repository scanning configurations
- database scanning configurations
- add, edit, and delete of managed and remote repositories
- add and delete repository groups (including add and delete of repository from group)

Changes are logged in archiva-audit.log.",", Duplicated Code, Duplicated Code, "
"   Rename Method,Extract Method,","reduce memory used by indexing process the ArchivaIndexerTask (and other scheduling tasks) use quite a bit of memory per file (particularly the name, but also other fields that are unused)","Duplicated Code, Long Method, , "
"   Rename Method,",introduce a metadata content repository API see MRM-1025 for justification,", "
"   Rename Class,Rename Method,",introduce a metadata content repository API see MRM-1025 for justification,", "
"   Extract Method,Inline Method,",introduce a metadata content repository API see MRM-1025 for justification,"Duplicated Code, Long Method, , , "
"   Rename Class,Extract Interface,",introduce a metadata content repository API see MRM-1025 for justification,", Large Class, "
"   Rename Method,","migrate Archiva's ""browse"" functionality to use the metadata content repository API the first steps to removing direct interaction with the database - see MRM-1025 for more information.

A test of the API's implementation will be that we can utilise it directly from the webapp action classes without introducing additional business logic, and such that we can comfortably use it in the same way from the XMLRPC module. However, we also do not want presentation-related logic (or at least the organisation of the information on screen) in the repository implementation. If that is needed, a simpler replacement abstraction for RepositoryBrowsing may be appropriate.",", "
"   Rename Method,Inline Method,","remove dependency-graph in favour of the maven-dependency-tree library the dependency graph module is only used to render the dependency tree on the artifact information pages. It currently reproduces a lot of Maven functionality, and is heavily tied to the archiva-repository-layer and archiva-model.

We should instead use the standard Maven library for handling Maven dependency trees. If we later encounter different types of trees, we can provide alternative implementations. It's unlikely something as generic is needed (though the code is still there to be revisited in SVN if the case arises)",", , "
"   Rename Class,Rename Method,Move Method,Extract Method,","migrate repository statistics to the metadata content repository currently, after every scan, repository statistics are persisted to the database. These could easily be stored in the metadata repository. While at present we will keep the ""per-scan"" approach, in the future this might better be represented as a regularly stored snapshot / built in versioning of the metadata. It could potentially be adding this information on the fly instead of at scan by adding a callback in the repository API when an artifact / project is created - however this will be approached later.

When migrating the functionality, it should be offered as a standalone plugin (the only dependencies should be in the webapp that need to render the information and trigger some cleanup).

The following changes will be needed:
- save the statistics in ArchivaRepositoryScanningTaskExecutor / ArchivaRepositoryScanningTaskExecutorTest
- reset statistics in EditManagedRepositoryAction / EditManagedRepositoryActionTest when location changes
- query for a prior successful scan in RepositoryArchivaTaskScheduler
- retrieve last scan info in RepositoryAction and update corresponding JSP to display information correctly
- generate report action to query scan statistics between given dates and aggregate
","Duplicated Code, Long Method, , , "
"   Inline Method,Move Attribute,","migrate repository statistics to the metadata content repository currently, after every scan, repository statistics are persisted to the database. These could easily be stored in the metadata repository. While at present we will keep the ""per-scan"" approach, in the future this might better be represented as a regularly stored snapshot / built in versioning of the metadata. It could potentially be adding this information on the fly instead of at scan by adding a callback in the repository API when an artifact / project is created - however this will be approached later.

When migrating the functionality, it should be offered as a standalone plugin (the only dependencies should be in the webapp that need to render the information and trigger some cleanup).

The following changes will be needed:
- save the statistics in ArchivaRepositoryScanningTaskExecutor / ArchivaRepositoryScanningTaskExecutorTest
- reset statistics in EditManagedRepositoryAction / EditManagedRepositoryActionTest when location changes
- query for a prior successful scan in RepositoryArchivaTaskScheduler
- retrieve last scan info in RepositoryAction and update corresponding JSP to display information correctly
- generate report action to query scan statistics between given dates and aggregate
",", , , "
"   Rename Method,","complete artifact browsing currently some features of the artifact display page are not implemented (links to dependencies, displaying other information from the pom, and so on)",", "
"   Rename Class,Rename Method,Inline Method,","implement alternative or improve repository metadata storage The biggest thing to look at is metadata-repository-file. I threw this together with property files quickly and there's no optimisation or even exception handling. We need to look at the right way to approach this - a more robust implementation of a file system store (properties or xml) is definitely workable, but would need to be combined with something like a Lucene index (as in Archiva 0.9) to make some of the operations fast enough. What I would like to look at instead is using JCR (with file system persistence - not a database!) to see how well it reacts to a lot of operations. As you can tell from the docs, the storage is tailored to living in a hierarchical content repository in whatever form that takes, and the storage is isolated behind an API.",", , "
"   Rename Method,","implement alternative or improve repository metadata storage The biggest thing to look at is metadata-repository-file. I threw this together with property files quickly and there's no optimisation or even exception handling. We need to look at the right way to approach this - a more robust implementation of a file system store (properties or xml) is definitely workable, but would need to be combined with something like a Lucene index (as in Archiva 0.9) to make some of the operations fast enough. What I would like to look at instead is using JCR (with file system persistence - not a database!) to see how well it reacts to a lot of operations. As you can tell from the docs, the storage is tailored to living in a hierarchical content repository in whatever form that takes, and the storage is isolated behind an API.",", "
"   Rename Method,","implement alternative or improve repository metadata storage The biggest thing to look at is metadata-repository-file. I threw this together with property files quickly and there's no optimisation or even exception handling. We need to look at the right way to approach this - a more robust implementation of a file system store (properties or xml) is definitely workable, but would need to be combined with something like a Lucene index (as in Archiva 0.9) to make some of the operations fast enough. What I would like to look at instead is using JCR (with file system persistence - not a database!) to see how well it reacts to a lot of operations. As you can tell from the docs, the storage is tailored to living in a hierarchical content repository in whatever form that takes, and the storage is isolated behind an API.",", "
"   Move Class,Extract Superclass,Extract Method,","implement alternative or improve repository metadata storage The biggest thing to look at is metadata-repository-file. I threw this together with property files quickly and there's no optimisation or even exception handling. We need to look at the right way to approach this - a more robust implementation of a file system store (properties or xml) is definitely workable, but would need to be combined with something like a Lucene index (as in Archiva 0.9) to make some of the operations fast enough. What I would like to look at instead is using JCR (with file system persistence - not a database!) to see how well it reacts to a lot of operations. As you can tell from the docs, the storage is tailored to living in a hierarchical content repository in whatever form that takes, and the storage is isolated behind an API.","Duplicated Code, Long Method, , Duplicated Code, Large Class, "
"   Rename Method,Extract Method,","implement alternative or improve repository metadata storage The biggest thing to look at is metadata-repository-file. I threw this together with property files quickly and there's no optimisation or even exception handling. We need to look at the right way to approach this - a more robust implementation of a file system store (properties or xml) is definitely workable, but would need to be combined with something like a Lucene index (as in Archiva 0.9) to make some of the operations fast enough. What I would like to look at instead is using JCR (with file system persistence - not a database!) to see how well it reacts to a lot of operations. As you can tell from the docs, the storage is tailored to living in a hierarchical content repository in whatever form that takes, and the storage is isolated behind an API.","Duplicated Code, Long Method, , "
"   Move And Rename Class,Rename Method,","Add simple 'CRUD' pages for project-level metadata along with a ""generic metadata"" plugin See for more details:
http://mail-archives.apache.org/mod_mbox/archiva-dev/201003.mbox/%3c70CA8566-051F-4CDA-A09D-622641AD8548@apache.org%3e",", "
"   Extract Method,Inline Method,","Add simple 'CRUD' pages for project-level metadata along with a ""generic metadata"" plugin See for more details:
http://mail-archives.apache.org/mod_mbox/archiva-dev/201003.mbox/%3c70CA8566-051F-4CDA-A09D-622641AD8548@apache.org%3e","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Add simple 'CRUD' pages for project-level metadata along with a ""generic metadata"" plugin See for more details:
http://mail-archives.apache.org/mod_mbox/archiva-dev/201003.mbox/%3c70CA8566-051F-4CDA-A09D-622641AD8548@apache.org%3e","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,",complete the proxy interface the current proxy interface is not well integrated and the tests are failing. Complete this integration.,"Duplicated Code, Long Method, , "
"   Rename Method,Pull Up Method,Extract Method,Inline Method,Pull Up Attribute,",complete the proxy interface the current proxy interface is not well integrated and the tests are failing. Complete this integration.,"Duplicated Code, Long Method, , , Duplicated Code, Duplicated Code, "
"   Rename Method,Inline Method,",complete the proxy interface the current proxy interface is not well integrated and the tests are failing. Complete this integration.,", , "
"   Rename Method,",complete the proxy interface the current proxy interface is not well integrated and the tests are failing. Complete this integration.,", "
"   Rename Method,",complete the proxy interface the current proxy interface is not well integrated and the tests are failing. Complete this integration.,", "
"   Rename Method,",complete the proxy interface the current proxy interface is not well integrated and the tests are failing. Complete this integration.,", "
"   Rename Class,Rename Method,Move Method,Inline Method,Move Attribute,",complete the proxy interface the current proxy interface is not well integrated and the tests are failing. Complete this integration.,", , , , "
"   Rename Method,",complete the proxy interface the current proxy interface is not well integrated and the tests are failing. Complete this integration.,", "
"   Rename Method,",Web services for repository merging or artifact promotion I think CRUD for managed repositories should be a pre-requisite of this.,", "
"   Rename Method,Inline Method,","remove use of plexus-spring remove all use of plexus annotations to @Inject annotations.
and don't use anymore plexus-spring.
done in redback",", , "
"   Rename Method,Extract Method,","remove use of plexus-spring remove all use of plexus annotations to @Inject annotations.
and don't use anymore plexus-spring.
done in redback","Duplicated Code, Long Method, , "
"   Pull Up Method,Move Method,",Expose Archiva services trough REST 0,", , Duplicated Code, "
"   Rename Class,Rename Method,",Expose Archiva services trough REST 0,", "
"   Rename Method,",Expose Archiva services trough REST 0,", "
"   Move Class,Rename Method,Move Method,Extract Method,Move Attribute,","Add a new module for repository administration management Currently most part of the logic regarding repositories administration is done in webapp part and duplicate in xmlrpc.
As we are adding some rest services, in order to avoid an other duplication, the logic will be moved in a new module archiva-repository-admin","Duplicated Code, Long Method, , , , "
"   Rename Method,","Add a new module for repository administration management Currently most part of the logic regarding repositories administration is done in webapp part and duplicate in xmlrpc.
As we are adding some rest services, in order to avoid an other duplication, the logic will be moved in a new module archiva-repository-admin",", "
"   Pull Up Method,Move Method,Pull Up Attribute,Move Attribute,","downloading (optionnaly) remote index to display remote artifacts in search results if a managed repo has remote repos, artifacts are available but not display in a search result.
* configure an optionnel remote index url
* running this download in a configurable schedule",", , , Duplicated Code, Duplicated Code, "
"   Push Down Method,Push Down Attribute,","downloading (optionnaly) remote index to display remote artifacts in search results if a managed repo has remote repos, artifacts are available but not display in a search result.
* configure an optionnel remote index url
* running this download in a configurable schedule",", , , "
"   Extract Method,Inline Method,","when used as a maven1 proxy, Archiva should handle relocation from maven2 poms When a maven1 client asks for /servletapi/jars/servletapi-2.4.jar, Archiva converts path to the maven2 location of this artifact. As maven1 has no relocation support, the jar is required in the repo. 
Archiva can be more that a proxy : download the artifact POM, read relocation infos, and return the relocated jar.

attached Patch add a new ""applyRelocation"" to DefaultProxyManager.
I've tried this code with the servletapi example, but it may be bad designed as I just discovered maven / archiva APIs.","Duplicated Code, Long Method, , , "
"   Pull Up Method,Pull Up Attribute,",add rest method to delete artifact. a rest call to delete an artifact in a repository.,", Duplicated Code, Duplicated Code, "
"   Rename Method,Extract Method,Inline Method,","ability to have multiple reports currently the reports are all aggregated into one representation and execution. It should be possible to have separate groups, with just the health ones tied to the indexing and the others run on demand.","Duplicated Code, Long Method, , , "
"   Rename Method,",ldap configuration editable with the ui. 0,", "
"   Rename Method,",path of merged index for group configurable 0,", "
"   Rename Method,","build merged index for groups with a cron schedule the current merged index for groups is generated ""on the fly"" (with some caching to avoid generation on each request).
We can add a cron schedule to generate it.",", "
"   Rename Method,","repogroup merged index ttl configurable per repository group currently this ttl is system property value.
It must be configurable per repository group.",", "
"   Move Method,Move Attribute,",Add xml-rpc search 0,", , , "
"   Rename Method,",Upload (deploy) artifacts to a repository - via a web form (not using wagon) The web interface should allow to upload artifacts to the repository. For M1 one could just ftp the artifacts as neededm but with M2 having to go through the file:deploy plugin is a pain. Archiva could help a lot here,", "
"   Move Method,Move Attribute,","Update archiva to latest plexus appserver ( + container etc) The attached patch updates plexus-appserver and all related dependencies. Tested and working here through the archiva-plexus-runtime.
I also altered the context path in archiva-plexus-application from '/' to '/archiva' to be in-line with continuum.",", , , "
"   Rename Class,Rename Method,Extract Method,Inline Method,","Repository purge feature for snapshots We need a way to purge a repository of snapshots older than a certain date, (optionally retaining the most recent one) and fixing the metadata.

","Duplicated Code, Long Method, , , "
"   Rename Method,Pull Up Method,","caching repository query interface need to be able to query the repository to see if artifacts exist, what versions are available, etc. This needs to interoperate with the indexing, and where applicable should cache information, balancing the need to keep a low memory overhead but avoid repetitive disk reads on metadata files.",", Duplicated Code, "
"   Rename Method,Extract Method,","Create missing tests for repository purge Comments from MRM-294:

A general thought here too, for later: it might be worth reviewing the exceptions that can occur in *Purge and see if we can recover better from each rather than bubbling it

Tests:

* I think you can remove many of the components from the test XML files where the default suffice (just keep the registry and jdo factory)
* the tests have a lot of bolierplate that can probably be turned into methods that generate test data

Missing tests:

* no tests for the consumer or the Released Snapshots purge
* days old test is only testing by file age - it should also test the metadata-driven snapshots

A general thought for Archiva in the long term, too... setting up the database to test this was probably a pain. We should have stub implementations of the indexer and dao's to avoid it.","Duplicated Code, Long Method, , "
"   Extract Method,Pull Up Attribute,","Create missing tests for repository purge Comments from MRM-294:

A general thought here too, for later: it might be worth reviewing the exceptions that can occur in *Purge and see if we can recover better from each rather than bubbling it

Tests:

* I think you can remove many of the components from the test XML files where the default suffice (just keep the registry and jdo factory)
* the tests have a lot of bolierplate that can probably be turned into methods that generate test data

Missing tests:

* no tests for the consumer or the Released Snapshots purge
* days old test is only testing by file age - it should also test the metadata-driven snapshots

A general thought for Archiva in the long term, too... setting up the database to test this was probably a pain. We should have stub implementations of the indexer and dao's to avoid it.","Duplicated Code, Long Method, , Duplicated Code, "
"   Rename Method,","Virtual repositories or repository grouping A number of managed repositories can be grouped together with that group having only one url. So you only need to specify that url in the settings.xml file and when Archiva receives a request via that url, it would look for that artifact from the repositories belonging to that group. 

More details are dicussed here:
http://www.nabble.com/Archiva-1.1-Roadmap-td15262645.html#a15263879",", "
"   Rename Method,","Virtual repositories or repository grouping A number of managed repositories can be grouped together with that group having only one url. So you only need to specify that url in the settings.xml file and when Archiva receives a request via that url, it would look for that artifact from the repositories belonging to that group. 

More details are dicussed here:
http://www.nabble.com/Archiva-1.1-Roadmap-td15262645.html#a15263879",", "
"   Rename Method,Move Method,Move Attribute,","Improve RSS feed generation - add check to control file size of generated RSS feeds
- make sure that the when the client requests the RSS, it gets the right headers to not try and update it if it hasn't changed for performance
- get the urls of the feeds from the rss feed request

More info in: http://www.nabble.com/Re%3A-svn-commit%3A-r645833---in--archiva-trunk%3A-archiva-jetty-src-main-conf-jetty.xml-archiva-modules-archiva-web-archiva-rss-src-main-java-org-apache-archiva-rss-processor-NewArtifactsRssFeedProcessor.java-td16562163.html",", , , "
"   Rename Method,Extract Method,","Improve RSS feed generation - add check to control file size of generated RSS feeds
- make sure that the when the client requests the RSS, it gets the right headers to not try and update it if it hasn't changed for performance
- get the urls of the feeds from the rss feed request

More info in: http://www.nabble.com/Re%3A-svn-commit%3A-r645833---in--archiva-trunk%3A-archiva-jetty-src-main-conf-jetty.xml-archiva-modules-archiva-web-archiva-rss-src-main-java-org-apache-archiva-rss-processor-NewArtifactsRssFeedProcessor.java-td16562163.html","Duplicated Code, Long Method, , "
"   Rename Method,",Support inclusion of pom file when deploying an artifact via the web upload form Currently the web upload form only supports pom generation when uploading M2 artifacts.,", "
"   Rename Class,Move And Rename Class,Rename Method,Move Method,Inline Method,Move Attribute,","Removal of Archiva-Webdav implementation in favor of Jackrabbit-webdav This patch removes plexus-webdav in favor of Jackrabbit's webdav servlet implementation. It is not yet 100% completed and tested. HTTP GET and PUT should work correctly.

The following needs to happen before integration:

1) New Jackrabbit classes need to be correctly unit tested.
2) Webdav properties need to be implemented
3) Testing of common webdav clients - Mac OS X, Windows, Wagon-Dav, etc",", , , , "
"   Move And Rename Class,Move Method,",aggregate indices for repository groups 0,", , "
"   Rename Method,Extract Method,","Remove VersionedReference/ProjectReference/ArtifactReference from RepositoryProxyConnectors Remove VersionedReference/ProjectReference/ArtifactReference from RepositoryProxyConnectors.

Converting from request paths to VersionedReference/ProjectReference/ArtifactReference is not always 100% correct.","Duplicated Code, Long Method, , "
"   Rename Method,","create a second index with a subset of information, and have it compressed required for the eclipse plugin. Will need to follow up with Jason on exactly what data is required.",", "
"   Rename Method,","Remove dependency to DS Currently the jcr install requires the declarative services. In order to have minimal dependencies, an activator in combination with service trackers should be used instead.",", "
"   Rename Method,","Replace MimeTypeService (commons/mime) by [Sling]ServletContext registered as a Service The commons/mime project with the MimeTypeService has been introduced to be able to provide MIME type mapping to the Content Loader in the former sling-content-jcr bundle and a ServletContext to access the MIME type mapping was not available. Extensibility has been reached by defining a MimeTypeProvider service, which might be implemented by other bundles to enhance MIME type mapping in the MimeTypeService.

With the new Sling API we defined a new ServletResolver interface, which in real Sling resolves resource types to servlets registered as OSGi service. These servlets have to be initialized with a ServletConfig and a ServletContext. If we want to provide ServletResolver implementations outside the core project (which I strive to), we have to have a way to provide the ServletContext from the main Sling servlet to the ServletResolver.

So I propose ...

* to have the main sling servlet (SlingMainServlet) register the ServletContext as a javax.servlet.ServletContext service which would be available to all intersted parties.

* to remove the commons/mime project as MIME type mapping may be used by accessing the ServletContext service

* drop the MimeTypeResovler interface functionality completely. To extend the registered ServletContext service with new mappings, the ServletContext service will be a ManagedService, whose configuration will be able to configure MIME type mappings.",", "
"   Rename Method,","Replace MimeTypeService (commons/mime) by [Sling]ServletContext registered as a Service The commons/mime project with the MimeTypeService has been introduced to be able to provide MIME type mapping to the Content Loader in the former sling-content-jcr bundle and a ServletContext to access the MIME type mapping was not available. Extensibility has been reached by defining a MimeTypeProvider service, which might be implemented by other bundles to enhance MIME type mapping in the MimeTypeService.

With the new Sling API we defined a new ServletResolver interface, which in real Sling resolves resource types to servlets registered as OSGi service. These servlets have to be initialized with a ServletConfig and a ServletContext. If we want to provide ServletResolver implementations outside the core project (which I strive to), we have to have a way to provide the ServletContext from the main Sling servlet to the ServletResolver.

So I propose ...

* to have the main sling servlet (SlingMainServlet) register the ServletContext as a javax.servlet.ServletContext service which would be available to all intersted parties.

* to remove the commons/mime project as MIME type mapping may be used by accessing the ServletContext service

* drop the MimeTypeResovler interface functionality completely. To extend the registered ServletContext service with new mappings, the ServletContext service will be a ManagedService, whose configuration will be able to configure MIME type mappings.",", "
"   Move Class,Move And Rename Class,Move Method,Extract Method,Move Attribute,","Use smaller orderable tasks in jcrinstall's osgi installer See proposal at http://markmail.org/message/a6xx4dawsokl6lpx . 

This should help improve bundle management as discussed in http://markmail.org/message/ld6tkz6fdseknntx","Duplicated Code, Long Method, , , , "
"   Move Class,Move And Rename Class,Move Method,Extract Method,Move Attribute,","Use smaller orderable tasks in jcrinstall's osgi installer See proposal at http://markmail.org/message/a6xx4dawsokl6lpx . 

This should help improve bundle management as discussed in http://markmail.org/message/ld6tkz6fdseknntx","Duplicated Code, Long Method, , , , "
"   Rename Method,","Recompile jsps on modifications and avoid periodic check Currently the jsp script handler checks a jsp for modifications when the jsp is called. This check has a configurable time intervall.
With the latest changes we have resource events and therefore could recheck the jsps on events and avoid the per request check completly.",", "
"   Rename Method,","Recompile jsps on modifications and avoid periodic check Currently the jsp script handler checks a jsp for modifications when the jsp is called. This check has a configurable time intervall.
With the latest changes we have resource events and therefore could recheck the jsps on events and avoid the per request check completly.",", "
"   Rename Method,",Use commons dynamic class loader The jsp scripting should use the new commons class loader instead of doing a dynamic import package * and using the repository class loader.,", "
"   Rename Method,",Use commons dynamic class loader The jsp scripting should use the new commons class loader instead of doing a dynamic import package * and using the repository class loader.,", "
"   Rename Method,",Use new commons dynamic class loader instead of dynamic import package * The new commons dynamic class loader provides a more robust way of dynamic class loading than relying on the dynamic import package *,", "
"   Rename Method,",Use new commons dynamic class loader instead of dynamic import package * The new commons dynamic class loader provides a more robust way of dynamic class loading than relying on the dynamic import package *,", "
"   Rename Method,","Replace Resource.getRawData and .getObject methods by better API David brought up an issue on the dev list [1] regarding the Resource.getRawData() method. In short, David suggests to replace the getRawData method with a signature, which more closely reflects the definition of Sling as a web application framework for JCR. The general consesus on the list is that the getRawData() method is badly named and that we should have a method which shows the JCR integration yet does not tie the API too much into JCR.

So I propose the following:

> Remove the getRawData() and getObject() methods from the Resource interface
> Add new interfaces NodeResource and ObjectResource:

// resources backed by JCR nodes
public interface NodeResource extends Resource {
Node getNode();
}

// resources mapped using JCR OCM for example
public interface ObjectResource extends Resource {
Object getObject();
}

This way, we have the Resource interfaces completely storage-agnostic and provide for a wide range of extensions, such as an URLResource, which may be backed by a URL such as an entry in an OSGi bundle.

[1] http://www.mail-archive.com/sling-dev@incubator.apache.org/msg00906.html",", "
"   Rename Method,","Replace Resource.getRawData and .getObject methods by better API David brought up an issue on the dev list [1] regarding the Resource.getRawData() method. In short, David suggests to replace the getRawData method with a signature, which more closely reflects the definition of Sling as a web application framework for JCR. The general consesus on the list is that the getRawData() method is badly named and that we should have a method which shows the JCR integration yet does not tie the API too much into JCR.

So I propose the following:

> Remove the getRawData() and getObject() methods from the Resource interface
> Add new interfaces NodeResource and ObjectResource:

// resources backed by JCR nodes
public interface NodeResource extends Resource {
Node getNode();
}

// resources mapped using JCR OCM for example
public interface ObjectResource extends Resource {
Object getObject();
}

This way, we have the Resource interfaces completely storage-agnostic and provide for a wide range of extensions, such as an URLResource, which may be backed by a URL such as an entry in an OSGi bundle.

[1] http://www.mail-archive.com/sling-dev@incubator.apache.org/msg00906.html",", "
"   Rename Method,","Replace Resource.getRawData and .getObject methods by better API David brought up an issue on the dev list [1] regarding the Resource.getRawData() method. In short, David suggests to replace the getRawData method with a signature, which more closely reflects the definition of Sling as a web application framework for JCR. The general consesus on the list is that the getRawData() method is badly named and that we should have a method which shows the JCR integration yet does not tie the API too much into JCR.

So I propose the following:

> Remove the getRawData() and getObject() methods from the Resource interface
> Add new interfaces NodeResource and ObjectResource:

// resources backed by JCR nodes
public interface NodeResource extends Resource {
Node getNode();
}

// resources mapped using JCR OCM for example
public interface ObjectResource extends Resource {
Object getObject();
}

This way, we have the Resource interfaces completely storage-agnostic and provide for a wide range of extensions, such as an URLResource, which may be backed by a URL such as an entry in an OSGi bundle.

[1] http://www.mail-archive.com/sling-dev@incubator.apache.org/msg00906.html",", "
"   Rename Method,","Replace Resource.getRawData and .getObject methods by better API David brought up an issue on the dev list [1] regarding the Resource.getRawData() method. In short, David suggests to replace the getRawData method with a signature, which more closely reflects the definition of Sling as a web application framework for JCR. The general consesus on the list is that the getRawData() method is badly named and that we should have a method which shows the JCR integration yet does not tie the API too much into JCR.

So I propose the following:

> Remove the getRawData() and getObject() methods from the Resource interface
> Add new interfaces NodeResource and ObjectResource:

// resources backed by JCR nodes
public interface NodeResource extends Resource {
Node getNode();
}

// resources mapped using JCR OCM for example
public interface ObjectResource extends Resource {
Object getObject();
}

This way, we have the Resource interfaces completely storage-agnostic and provide for a wide range of extensions, such as an URLResource, which may be backed by a URL such as an entry in an OSGi bundle.

[1] http://www.mail-archive.com/sling-dev@incubator.apache.org/msg00906.html",", "
"   Rename Method,","Replace Resource.getRawData and .getObject methods by better API David brought up an issue on the dev list [1] regarding the Resource.getRawData() method. In short, David suggests to replace the getRawData method with a signature, which more closely reflects the definition of Sling as a web application framework for JCR. The general consesus on the list is that the getRawData() method is badly named and that we should have a method which shows the JCR integration yet does not tie the API too much into JCR.

So I propose the following:

> Remove the getRawData() and getObject() methods from the Resource interface
> Add new interfaces NodeResource and ObjectResource:

// resources backed by JCR nodes
public interface NodeResource extends Resource {
Node getNode();
}

// resources mapped using JCR OCM for example
public interface ObjectResource extends Resource {
Object getObject();
}

This way, we have the Resource interfaces completely storage-agnostic and provide for a wide range of extensions, such as an URLResource, which may be backed by a URL such as an entry in an OSGi bundle.

[1] http://www.mail-archive.com/sling-dev@incubator.apache.org/msg00906.html",", "
"   Rename Method,Extract Method,","Framework update is unstable The Framework API specifies that calling Bundle.update(InputStream) should update the framework.

In the Sling launcher we implemented this functionality by storing the framework.jar file in the filesystem (along the sling.properties file) and creating a class loader from that jar file to launch the framework and thus Sling. This works perfectly and has the added benefit to make sure the framework is really isolated from the environment (something which is not always guaranteed by certain servlet containers).

The problem with the current implementation is two-fold:

(1) The URLClassLoader loads the jar file using a JAR URL, which - by default - uses caching. As a consequence replacing the existing jar file with a new one of the same name causes resources loaded through the class loader (Class.getResource[AsStream], ClassLoader.getResource[AsStream]) to actually come from the old JAR file instead of the new one.

(2) Replacing a JAR file which has been used by a class loader is not always possible. Particularly on Windows systems, the files which are still opened by a process cannot be removed and thus cannot be replaced. On Unix systems, the situation is different because removing a file does not really delete it but just removes the directory entry and thus enables to create a new file with the same name.

Both problems can be fixed for good by using generational jar file names:

* On startup all jar file generations are checked and all files except the most recent one is removed. The most recent
jar file is renamed to the base name also used on initial startup

* On system bundle update a new generation jar file is created and the new framework is launched from the new
generation jar file.

The drawback of this solution is, that a live update is not possible because the fixed part launcher has to be replaced. This is less of a problem in a servlet container environment (using the Sling web app launcher) because in that situation the web can simply be replaced.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Framework update is unstable The Framework API specifies that calling Bundle.update(InputStream) should update the framework.

In the Sling launcher we implemented this functionality by storing the framework.jar file in the filesystem (along the sling.properties file) and creating a class loader from that jar file to launch the framework and thus Sling. This works perfectly and has the added benefit to make sure the framework is really isolated from the environment (something which is not always guaranteed by certain servlet containers).

The problem with the current implementation is two-fold:

(1) The URLClassLoader loads the jar file using a JAR URL, which - by default - uses caching. As a consequence replacing the existing jar file with a new one of the same name causes resources loaded through the class loader (Class.getResource[AsStream], ClassLoader.getResource[AsStream]) to actually come from the old JAR file instead of the new one.

(2) Replacing a JAR file which has been used by a class loader is not always possible. Particularly on Windows systems, the files which are still opened by a process cannot be removed and thus cannot be replaced. On Unix systems, the situation is different because removing a file does not really delete it but just removes the directory entry and thus enables to create a new file with the same name.

Both problems can be fixed for good by using generational jar file names:

* On startup all jar file generations are checked and all files except the most recent one is removed. The most recent
jar file is renamed to the base name also used on initial startup

* On system bundle update a new generation jar file is created and the new framework is launched from the new
generation jar file.

The drawback of this solution is, that a live update is not possible because the fixed part launcher has to be replaced. This is less of a problem in a servlet container environment (using the Sling web app launcher) because in that situation the web can simply be replaced.","Duplicated Code, Long Method, , "
"   Move Class,Move Method,Move Attribute,",Update Script System to be JSR-223 Compatible Currently sling and microsling use a custom scripting framework. This framework should be updated to be jsr-223 compatible.,", , , "
"   Move Class,Move Method,Move Attribute,",Update Script System to be JSR-223 Compatible Currently sling and microsling use a custom scripting framework. This framework should be updated to be jsr-223 compatible.,", , , "
"   Rename Class,Rename Method,",Update Script System to be JSR-223 Compatible Currently sling and microsling use a custom scripting framework. This framework should be updated to be jsr-223 compatible.,", "
"   Move Method,Extract Method,",Update Script System to be JSR-223 Compatible Currently sling and microsling use a custom scripting framework. This framework should be updated to be jsr-223 compatible.,"Duplicated Code, Long Method, , , "
"   Move Class,Rename Method,Inline Method,","Script resolution should consider partial selector string Currently the Sling DefaultSlingScriptResolver class only considers the full selector string (if one is available) for finding a script. Actually, it should gradually cut off elements from the selector string until a script is found or the complete selector string has been cut off.",", , "
"   Move Class,Rename Method,Inline Method,","Script resolution should consider partial selector string Currently the Sling DefaultSlingScriptResolver class only considers the full selector string (if one is available) for finding a script. Actually, it should gradually cut off elements from the selector string until a script is found or the complete selector string has been cut off.",", , "
"   Rename Method,","Provide ResourceResolver mapping information in a ConfigurationPrinter Currently the resource resolver mapping configuraiton is only available from the ""Jcr Resource Resolver"" page. It would be helpful for support purposes to have this information available as part of the configuration status.",", "
"   Rename Method,Extract Method,","Add logout method to Authenticator With the Sling Engine 2.0.4 the Authenticator interface has been introduced to support a generic way to have a user authenticated. This allows for an authentication agnostic way to force a user to login.

The drawback of the current solution is, that neither authentication handlers nor the Authenticator interface provide APi to logout a user again. This should be fixed as follows:

* Add an Authenticator.logout() method which logs out a user in a similar way the login method logs a user in
* Add a new AuthenticationHandler2 interface extending the AuthenticationHandler interface and providing a dropAuthentication method
which mirrors the AuthenticationHandler.requestAuthentication method.
* Add a LogoutServlet calling Authenticator.logout in a similar manner as the LoginServlet calls the login method

Authentication handlers supporting logging out just implement the AuthenticationHandler2 interface while still registering as a plain AuthenticationHandler. The Authenticator implementation in the Sling Engine bundle identifies the authentication handlers correctly to call or to not call the dropAuthentication method.","Duplicated Code, Long Method, , "
"   Rename Method,","ResourceProviderEntry uses iterators rather than maps, this becomes expensive with apps with many servlets. The ResourceProviderEntry uses iterators over many servlets, this is probably Ok for Resource resolution but when it comes to servlet resolution many are tested and this can be expensive especially if there are many servlets. IMHO, the class should be refactored to use trees of maps.

This has been discussed on list, the intention is to create a contrib version to explore this further, not wanting to impact the active version in trunk.",", "
"   Rename Method,Move Method,","ResourceProviderEntry uses iterators rather than maps, this becomes expensive with apps with many servlets. The ResourceProviderEntry uses iterators over many servlets, this is probably Ok for Resource resolution but when it comes to servlet resolution many are tested and this can be expensive especially if there are many servlets. IMHO, the class should be refactored to use trees of maps.

This has been discussed on list, the intention is to create a contrib version to explore this further, not wanting to impact the active version in trunk.",", , "
"   Rename Method,Move Method,","ResourceProviderEntry uses iterators rather than maps, this becomes expensive with apps with many servlets. The ResourceProviderEntry uses iterators over many servlets, this is probably Ok for Resource resolution but when it comes to servlet resolution many are tested and this can be expensive especially if there are many servlets. IMHO, the class should be refactored to use trees of maps.

This has been discussed on list, the intention is to create a contrib version to explore this further, not wanting to impact the active version in trunk.",", , "
"   Move Method,Move Attribute,","Provide helper class for simpler unit testing of sling code When writing unit tests that do a bit of integration in Sling, you often need the following:

- a JCR repository
- registered node types
- a JCR-based resource resolver
- adapter managers (to support all the adaptTo methods in your code)

This covers most of what has to be done to run unit tests for services without an OSGi container running. I have written such a helper class for our product and would like to contribute it to Sling, especially to write unit tests for SLING-1131. Patch will follow.
",", , , "
"   Move Method,Move Attribute,","Provide helper class for simpler unit testing of sling code When writing unit tests that do a bit of integration in Sling, you often need the following:

- a JCR repository
- registered node types
- a JCR-based resource resolver
- adapter managers (to support all the adaptTo methods in your code)

This covers most of what has to be done to run unit tests for services without an OSGi container running. I have written such a helper class for our product and would like to contribute it to Sling, especially to write unit tests for SLING-1131. Patch will follow.
",", , , "
"   Extract Superclass,Extract Interface,Extract Method,","Allow uploading JSON files to create content structures Currently uploading a JSON file will just create the file node.

On the other hand it would be useful if uploading to a node with a request extension of JSON, the JSON would be unpacked and handled as if it would be a modification request with the JSON data being the content to store.

This would be similar to JSON upload supported by CouchDB.","Duplicated Code, Long Method, , Duplicated Code, Large Class, Large Class, "
"   Rename Method,","Extend Resource interface and provide AbstractResource base class Currently the Resource interface has only bare bones API to access its own local attributes, like getPath() or getResourceType().

Accessing the resource in the context of its parent or its children is not currently possible and doing so requires getting the resource resolver out of the resource and asking the resource resolver.

For convenience, we should add the following methods:

getParent() -- returns the parent resource (same as ResourceUtil.getParent(this))
getName() -- returns the name of the resource (same as ResourceUtil.getName(this))
listChildren() -- same as getResourceResolver().listChildren(this)
getChild(String) -- same as getResourceResolver().getResource(this, path)
isResourceType(String) -- same as ResourceUtil.isA(this, String)

The new AbstractResource class will implement these methods as indicated.

Implementors of the Resource interface are then advised to actually extend from the AbstractResource interface, which in the future will provide default implementations of any methods added to the Resource interface, if it makes sense.",", "
"   Pull Up Method,Pull Up Attribute,","Extend Resource interface and provide AbstractResource base class Currently the Resource interface has only bare bones API to access its own local attributes, like getPath() or getResourceType().

Accessing the resource in the context of its parent or its children is not currently possible and doing so requires getting the resource resolver out of the resource and asking the resource resolver.

For convenience, we should add the following methods:

getParent() -- returns the parent resource (same as ResourceUtil.getParent(this))
getName() -- returns the name of the resource (same as ResourceUtil.getName(this))
listChildren() -- same as getResourceResolver().listChildren(this)
getChild(String) -- same as getResourceResolver().getResource(this, path)
isResourceType(String) -- same as ResourceUtil.isA(this, String)

The new AbstractResource class will implement these methods as indicated.

Implementors of the Resource interface are then advised to actually extend from the AbstractResource interface, which in the future will provide default implementations of any methods added to the Resource interface, if it makes sense.",", Duplicated Code, Duplicated Code, "
"   Move Class,Move Method,Move Attribute,","Move NodeType management from JcrContentHelper (sling-content-jcr) to AbstractSlingRepository (sling-jackrabbit-api) Currently, the sling-content-jcr bundle has a ContentManagerFactory implementation (the JcrContentHelper, used to access Sling ContentManager instances), which listens for bundles being registered and registers node types registered in the bundles and loads initial content stored in those bundles. Additionally, this class cares to setup the Sling ContentManager instances used to access the OCM functionality.

While working on the sling-event bundle, we discovered a race condition with node type registration and repository use. To overcome this race condition, we propose to move the node type registration part from the ContentManagerFactory implementation to the AbstractSlingRepository class in the sling-jackrabbit-api bundle. This class is the base class for services registered to provide the repository. As such, this class has contact to the repository before the service is registered or made available to the users of the repository.

Some background information on the race condition: The JcrContentHelper is implemented as an OSGi Component and requires a repository and as such will only be activated when the repository is available. The AbstractRepositoryEventHandler in the sling-event bundle also requires a repository and is also implemented as an OSGi component. When now both components are waiting for a repository to become available, the AbstractRepositoryEventHandler may be activated before the JcrContentHelper is activated and therefore, the node types required by the AbstractRepositoryEventHandler are not available yet as the JcrContentHandler was not able yet to register them ...
",", , , "
"   Rename Method,","Add branding for the Apache Felix Web Console In my whiteboard I have a prototyp branding bundle for the Apache Felix Web Console [1]. Now that the web console support for branding has been published, we should propagate this and included it in the base set of our bundles.

[1] https://svn.apache.org/repos/asf/sling/whiteboard/fmeschbe/webconsolebranding",", "
"   Rename Method,","Add branding for the Apache Felix Web Console In my whiteboard I have a prototyp branding bundle for the Apache Felix Web Console [1]. Now that the web console support for branding has been published, we should propagate this and included it in the base set of our bundles.

[1] https://svn.apache.org/repos/asf/sling/whiteboard/fmeschbe/webconsolebranding",", "
"   Rename Method,Push Down Method,Extract Method,Push Down Attribute,","Redesign thread pool management The current thread pool management has some drawbacks:
- the api contract is slightly confusing, e.g. create with a new configuration could be called and this new configuration is completly ignored
- no reference counting of thread pools - thread pools are only shutdown when the bundle is shutdown
- thread pools are not manageable through config admin","Duplicated Code, Long Method, , , , "
"   Move Method,Extract Method,",Implement ResourceResolverFactory concept See http://cwiki.apache.org/SLING/add-resourceresolverfactory-service-interface.html,"Duplicated Code, Long Method, , , "
"   Rename Method,",Remove direct dependency to web console The new web console provides a way to define plugins without a direct dependency to the web console.,", "
"   Rename Method,",Remove direct dependency to web console The new web console provides a way to define plugins without a direct dependency to the web console.,", "
"   Rename Class,Move Attribute,","Utilities for bundle version comparison, handling both jar files and Bundle objects For SLING-1273 I need to compare versions of the launchpad base jar.

As this comparison logic is reusable, I'll create utility classes in the org.apache.sling.commons.osgi bundle.",", , "
"   Rename Class,Move Attribute,","Utilities for bundle version comparison, handling both jar files and Bundle objects For SLING-1273 I need to compare versions of the launchpad base jar.

As this comparison logic is reusable, I'll create utility classes in the org.apache.sling.commons.osgi bundle.",", , "
"   Rename Method,","Rename methods of (new) AuthenticationHandler interface Currently the AuthenticationHandler interface defines the following methods:

authenticate - extract credentials from request
requestAuthentication - ask client for credentials
dropAuthentication - forget about current credentials

The authenticate and requestAuthentication names are historic and date back to some internal code at the time where uthenticationHandler did not exist yet.

IMHO these names are not that good. And since we are defining new API anyway, this might probaby be a good time to rename the methods.

extractCredentials - extract credentials from request
requestCredentials - ask client for credentials
dropCredentials - forget about current credentials

See also the discussion at http://markmail.org/thread/bocbtx2q5js4i2gf",", "
"   Rename Method,","Rename methods of (new) AuthenticationHandler interface Currently the AuthenticationHandler interface defines the following methods:

authenticate - extract credentials from request
requestAuthentication - ask client for credentials
dropAuthentication - forget about current credentials

The authenticate and requestAuthentication names are historic and date back to some internal code at the time where uthenticationHandler did not exist yet.

IMHO these names are not that good. And since we are defining new API anyway, this might probaby be a good time to rename the methods.

extractCredentials - extract credentials from request
requestCredentials - ask client for credentials
dropCredentials - forget about current credentials

See also the discussion at http://markmail.org/thread/bocbtx2q5js4i2gf",", "
"   Rename Method,","Rename methods of (new) AuthenticationHandler interface Currently the AuthenticationHandler interface defines the following methods:

authenticate - extract credentials from request
requestAuthentication - ask client for credentials
dropAuthentication - forget about current credentials

The authenticate and requestAuthentication names are historic and date back to some internal code at the time where uthenticationHandler did not exist yet.

IMHO these names are not that good. And since we are defining new API anyway, this might probaby be a good time to rename the methods.

extractCredentials - extract credentials from request
requestCredentials - ask client for credentials
dropCredentials - forget about current credentials

See also the discussion at http://markmail.org/thread/bocbtx2q5js4i2gf",", "
"   Rename Method,","Allow bundles to contribute values to the script bindings As I described here: http://markmail.org/message/cjsjywo3rsgfujks, I'd like to see a way for bundles to contribute script binding values.

The proposed interface is:

public interface SlingScriptBindingValuesProvider { void addBindings(Bindings bindings); } 

The Bindings object will be made read-only via a facade.",", "
"   Rename Method,Move Method,Inline Method,Move Attribute,","Include jackrabbit classloader code to adjust it for Sling needs We use the jackrabbit classloader 1.5 - starting with version 1.6 this code has been dropped from jackrabbit. Therefore it is no longer maintained there.
In addition we don't need all the features and using our own source for this code could improve the classloader.
Therefore I will copy over the code and start modifying it.",", , , , "
"   Rename Method,Move Method,Inline Method,Move Attribute,","Include jackrabbit classloader code to adjust it for Sling needs We use the jackrabbit classloader 1.5 - starting with version 1.6 this code has been dropped from jackrabbit. Therefore it is no longer maintained there.
In addition we don't need all the features and using our own source for this code could improve the classloader.
Therefore I will copy over the code and start modifying it.",", , , , "
"   Rename Method,","Auto-reconnect to the JCR Repository if it is not found or lost Affects the jackrabbit-client bundle accessing a ""remote"" repository. Currently the respective component just tries to access the repository and gives up, if it fails.

This should be enhanced to:

* retry in configurable intervals whether the repository is now available
* find out that a repository may have gone and try to reconnect in the same configurable intervals

Care must be taken in the second case to drop (close ?) any sessions still connected with the old disappeared repository.",", "
"   Rename Method,","Auto-reconnect to the JCR Repository if it is not found or lost Affects the jackrabbit-client bundle accessing a ""remote"" repository. Currently the respective component just tries to access the repository and gives up, if it fails.

This should be enhanced to:

* retry in configurable intervals whether the repository is now available
* find out that a repository may have gone and try to reconnect in the same configurable intervals

Care must be taken in the second case to drop (close ?) any sessions still connected with the old disappeared repository.",", "
"   Rename Method,","Auto-reconnect to the JCR Repository if it is not found or lost Affects the jackrabbit-client bundle accessing a ""remote"" repository. Currently the respective component just tries to access the repository and gives up, if it fails.

This should be enhanced to:

* retry in configurable intervals whether the repository is now available
* find out that a repository may have gone and try to reconnect in the same configurable intervals

Care must be taken in the second case to drop (close ?) any sessions still connected with the old disappeared repository.",", "
"   Rename Method,","Auto-reconnect to the JCR Repository if it is not found or lost Affects the jackrabbit-client bundle accessing a ""remote"" repository. Currently the respective component just tries to access the repository and gives up, if it fails.

This should be enhanced to:

* retry in configurable intervals whether the repository is now available
* find out that a repository may have gone and try to reconnect in the same configurable intervals

Care must be taken in the second case to drop (close ?) any sessions still connected with the old disappeared repository.",", "
"   Rename Method,Push Down Method,Inline Method,Push Down Attribute,",Launchpad Plugin should be able to load additional bundle defs from a file 0,", , , , "
"   Rename Method,Push Down Method,Inline Method,Push Down Attribute,",Launchpad Plugin should be able to load additional bundle defs from a file 0,", , , , "
"   Move Method,Extract Method,Move Attribute,","Path Based Resource Type Provider only works for nt:unstructured nodes The samples/path-based-rtp module only handles nt:unstructured nodes, it would be useful to optionally support other node types.

This can be done by extending the configuration strings as follows:

/content:2 
applies to nt:unstructured nodes under /content, and uses the second path element as the resource type

/content:3:nt:file 
applies to nt:file nodes under /content, and uses the third path element as the resource type

/content:2:(nt:*) 
applies to node that match the supplied regular expression (in parentheses) and uses the second path element as the resource type

","Duplicated Code, Long Method, , , , "
"   Move Method,Extract Method,Move Attribute,","Path Based Resource Type Provider only works for nt:unstructured nodes The samples/path-based-rtp module only handles nt:unstructured nodes, it would be useful to optionally support other node types.

This can be done by extending the configuration strings as follows:

/content:2 
applies to nt:unstructured nodes under /content, and uses the second path element as the resource type

/content:3:nt:file 
applies to nt:file nodes under /content, and uses the third path element as the resource type

/content:2:(nt:*) 
applies to node that match the supplied regular expression (in parentheses) and uses the second path element as the resource type

","Duplicated Code, Long Method, , , , "
"   Extract Method,Move Attribute,",Add a component which allows bundles to configure the session returned by SlingRepository discussion in http://markmail.org/thread/umuk7beuisp6zoqs,"Duplicated Code, Long Method, , , "
"   Move Method,Extract Method,Move Attribute,","Limit the number of parallel jobs Currently jobs can either be processed in parallel or in a series (per topic). However, if parallel processing is used, as many jobs as are available are processed in parallel and there is no limit.
We should change the meaning of the parallel property to:
- false: no parallel processing
- a positiv number N : parallel processing with max N jobs in parallel
- anything else: parallel processing

Currently it is:
- false: no parallel processing
- anything else: parallel processing
","Duplicated Code, Long Method, , , , "
"   Move Method,Extract Method,Move Attribute,","Break out ScriptResolver implementation into separate project The Sling ScriptResovler interface is currently implemented in the sling/core project. To be able to more flexible handle script resolution, this implementation should be broken out into its own project.","Duplicated Code, Long Method, , , , "
"   Rename Method,Move Method,Extract Method,","Align OpenID authentiction handler with new Commons Auth The OpenID authentication handler contains much of today's Commons Auth support for login forms and feedback provisioning.

This should be refactored to make the authentication handler simpler and align it with the Commons Auth functionality","Duplicated Code, Long Method, , , "
"   Rename Method,","Recompile java scripts on modification and avoid periodic checks Currently the java script handler checks a java file for modifications when the script is called. This check has a configurable time intervall.
With the latest changes we have resource events and therefore could recheck the sources on events and avoid the per request check completly.",", "
"   Rename Method,","Recompile java scripts on modification and avoid periodic checks Currently the java script handler checks a java file for modifications when the script is called. This check has a configurable time intervall.
With the latest changes we have resource events and therefore could recheck the sources on events and avoid the per request check completly.",", "
"   Rename Method,",Web Console Plugin should be a configuration printer The scripting core registers a web console plugin to display the available script engines; this should rather be a configuration printer for the web console,", "
"   Rename Method,",Web Console Plugin should be a configuration printer The scripting core registers a web console plugin to display the available script engines; this should rather be a configuration printer for the web console,", "
"   Rename Method,Extract Method,","DefaultScriptResolver should also listen to ScriptEngineFactory services Currently the DefaultScriptResolver only considers bundles with a javax.script.ScriptEngineFactory SPI file. Some factories might be registered as services as they require more setup than would be available when using the instantiation through SPI API.

Therefore the DefaultScriptResolver should also consider ScriptEngineFactory services.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","DefaultScriptResolver should also listen to ScriptEngineFactory services Currently the DefaultScriptResolver only considers bundles with a javax.script.ScriptEngineFactory SPI file. Some factories might be registered as services as they require more setup than would be available when using the instantiation through SPI API.

Therefore the DefaultScriptResolver should also consider ScriptEngineFactory services.","Duplicated Code, Long Method, , "
"   Extract Method,Inline Method,","Add replaceAccessControlEntry method to AccessControlUtil ModifyAceServlet and DefaultContentCreator both have a need to merge new privileges for a given principal to an existing resource ACL. Doing so involves some rather complex logic which is easy to get wrong, and in the Sakai 3 project, we found that quite a few service developers needed the same functionality. This patch moves the functionality to a shared utility method to eliminate any tempatations to copy-and-paste (or worse, rewrite incorrectly).

Besides consolidating the logic (and removing it from ModifyAceServlet and DefaultContentCreator), this patch introduces a couple of other changes:

* The ACE merge is more conservative. SLING-997 broke apart specified and existing aggregated privileges and then tried to recombine them into possibly new combinations. This patch instead maintains exactly what the client specified and (when possible) what was already there, but does not create any new aggregates of its own. This better matches Jackrabbit's default behavior, should minimize client surprises, and eliminates a subtle bug: any candidate aggregate privilege needs to be checked for ""isAbstract()"".
* The ModifyAceServlet JavaDoc is corrected and expanded.
* Some bad logging format is fixed.
","Duplicated Code, Long Method, , , "
"   Rename Method,","Compact Syntax for ESP expressions in HTML attributes The current syntax for ESP expressions ( <%= value %>) parallels the JSP syntax, but is hard to read when it is used in HTML attributes (<a href=""<%=link %>""><%=name %> </a>). I propose an syntax addition that allows inline ESP expressions in HTML in a more compact way and that is less intrusive to the XML structure. Basis for the syntax were JSP Expression language and XSLT. The above example would be rephrased in the new syntax as: (<a href=""{link}""><%=name %> </a>).",", "
"   Rename Method,","Compact Syntax for ESP expressions in HTML attributes The current syntax for ESP expressions ( <%= value %>) parallels the JSP syntax, but is hard to read when it is used in HTML attributes (<a href=""<%=link %>""><%=name %> </a>). I propose an syntax addition that allows inline ESP expressions in HTML in a more compact way and that is less intrusive to the XML structure. Basis for the syntax were JSP Expression language and XSLT. The above example would be rephrased in the new syntax as: (<a href=""{link}""><%=name %> </a>).",", "
"   Rename Method,Extract Method,","Clean up compiler API and use classloading infrastructure The current interface of the commons compiler is unnecessary complicated and does not use all of the features of the commons classloading infrastructure.

We can:
- remove the CompilerEnvironment interface - this can be handled internally
- remove the ClassWriter interface - we have the ClassLoaderWriter interface in the commons classloader
- change the options interface to extend a map - this allows us to add new options without changing interfaces/api
- the compile unit interface can be changed to
CompileUnit {
InputStream getSource();
String getMainTypeName();
}
This simplifies the integration with the rest of sling which is resource based.
The JavaCompiler interface then just takes an array of compile units, an error handler and the options


","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Clean up compiler API and use classloading infrastructure The current interface of the commons compiler is unnecessary complicated and does not use all of the features of the commons classloading infrastructure.

We can:
- remove the CompilerEnvironment interface - this can be handled internally
- remove the ClassWriter interface - we have the ClassLoaderWriter interface in the commons classloader
- change the options interface to extend a map - this allows us to add new options without changing interfaces/api
- the compile unit interface can be changed to
CompileUnit {
InputStream getSource();
String getMainTypeName();
}
This simplifies the integration with the rest of sling which is resource based.
The JavaCompiler interface then just takes an array of compile units, an error handler and the options


","Duplicated Code, Long Method, , "
"   Move And Rename Class,Rename Method,","provide number of processed requests 
For status monitoring it would be useful to have the number of total processed requests since restart of the JVM. Is it possible to have a ""statistics"" component which could provide such numbers?

(The ""manual"" way to do it would be writing a very simple RequestFilter which just increments a counter and expose this object as OSGI component.)",", "
"   Move And Rename Class,Rename Method,","provide number of processed requests 
For status monitoring it would be useful to have the number of total processed requests since restart of the JVM. Is it possible to have a ""statistics"" component which could provide such numbers?

(The ""manual"" way to do it would be writing a very simple RequestFilter which just increments a counter and expose this object as OSGI component.)",", "
"   Move Method,Extract Method,Move Attribute,","Commons Log should not export OSGi Configuration Admin package but have a dynamic dependency Currently the Commons Log module exports the OSGi Configuration Admin package to be able to register ManagedService[Factory] services. This is probably very bad style but has been done to have the Log mechanism configurable as soon as the Configuration Admin service is registered.

There is a better solution to this problem though:
* The OSGi Configuration Admin package is imported using DynamicImport-Package. This allows for
dynamic wiring only when the API is used.
* The ManagedService[Factory] services are registered as ServiceFactory services. This means,
that the actual Configuration Admin API (ManagedService[Factory] interfaces and ConfigurationException)
need only be wired, when these services are actually accessed -- which in turn is the case only
when the ConfigurationAdmin service is registered and starts working.

Thus we solve the exact problem: Commons Log can be active and will be Configuration Admin configurable as soon as the ConfigurationAdmin service is active.","Duplicated Code, Long Method, , , , "
"   Rename Method,","Create the Sling Launchpad, based on microsling-core code Following up on the ""[RT] Shall we merge microsling into Sling?"" [1] and ""µsling 2.0 requirements"" [2] threads on sling-dev, we need to merge microsling into Sling.

Here the are requirements as discussed in [2] (taking into account Felix's comment about WebDAV and Michael's comment about switching to other JCR repositories):

µsling 2.0 is a preconfigured instance of Sling, meant to allow web developers to test drive Sling by building scripted web and REST applications backed by a JCR repository.

The µsling 2.0 distribution only requires a Java 5 VM to run, no installation is needed. Fifteen minutes should be enough to start µsling and understand the basic concepts, based on self-guiding examples. µsling should ideally be delivered as a single runnable jar file.

Java programming is not required to build web and REST applications with µsling 2.0: both server-side and client-side javascript code and presentation templates can be used to process HTTP requests. Other scripting and templating languages (JSP and BSF-supported ones) can be plugged in easily.

The µjax ""application protocol"" and client-side javascript ""JCR proxy"" library make it easy to write powerful Ajaxish JCR-based applications with µsling 2.0.

µsling 2.0 is built on the same codebase as Sling, it's only a specific configuration of Sling.

All µsling 2.0 features are available in Sling applications, as long as they are enabled in the Sling configuration.

Sling (and µsling, as it runs the same core code) uses OSGi to modularize the framework, but µsling does not require any OSGI skills, and makes OSGI largely invisible to beginners.

All Sling features and modules can also be activated in a µsling 2.0 instance, by installing and activating the required OSGi bundles.

µsling 2.0 passes all the integration tests of the existing microsling test suite (SVN revision 605206), with minor adaptations where needed.

µsling 2.0 includes a WebDAV server module to make it easy to copy scripts into the JCR repository.

[1] http://markmail.org/message/2s7agnu5kklti6da
[2] http://markmail.org/message/atbjzjjp2wflkotb",", "
"   Move Class,Move Method,","Create the Sling Launchpad, based on microsling-core code Following up on the ""[RT] Shall we merge microsling into Sling?"" [1] and ""µsling 2.0 requirements"" [2] threads on sling-dev, we need to merge microsling into Sling.

Here the are requirements as discussed in [2] (taking into account Felix's comment about WebDAV and Michael's comment about switching to other JCR repositories):

µsling 2.0 is a preconfigured instance of Sling, meant to allow web developers to test drive Sling by building scripted web and REST applications backed by a JCR repository.

The µsling 2.0 distribution only requires a Java 5 VM to run, no installation is needed. Fifteen minutes should be enough to start µsling and understand the basic concepts, based on self-guiding examples. µsling should ideally be delivered as a single runnable jar file.

Java programming is not required to build web and REST applications with µsling 2.0: both server-side and client-side javascript code and presentation templates can be used to process HTTP requests. Other scripting and templating languages (JSP and BSF-supported ones) can be plugged in easily.

The µjax ""application protocol"" and client-side javascript ""JCR proxy"" library make it easy to write powerful Ajaxish JCR-based applications with µsling 2.0.

µsling 2.0 is built on the same codebase as Sling, it's only a specific configuration of Sling.

All µsling 2.0 features are available in Sling applications, as long as they are enabled in the Sling configuration.

Sling (and µsling, as it runs the same core code) uses OSGi to modularize the framework, but µsling does not require any OSGI skills, and makes OSGI largely invisible to beginners.

All Sling features and modules can also be activated in a µsling 2.0 instance, by installing and activating the required OSGi bundles.

µsling 2.0 passes all the integration tests of the existing microsling test suite (SVN revision 605206), with minor adaptations where needed.

µsling 2.0 includes a WebDAV server module to make it easy to copy scripts into the JCR repository.

[1] http://markmail.org/message/2s7agnu5kklti6da
[2] http://markmail.org/message/atbjzjjp2wflkotb",", , "
"   Rename Method,","Create the Sling Launchpad, based on microsling-core code Following up on the ""[RT] Shall we merge microsling into Sling?"" [1] and ""µsling 2.0 requirements"" [2] threads on sling-dev, we need to merge microsling into Sling.

Here the are requirements as discussed in [2] (taking into account Felix's comment about WebDAV and Michael's comment about switching to other JCR repositories):

µsling 2.0 is a preconfigured instance of Sling, meant to allow web developers to test drive Sling by building scripted web and REST applications backed by a JCR repository.

The µsling 2.0 distribution only requires a Java 5 VM to run, no installation is needed. Fifteen minutes should be enough to start µsling and understand the basic concepts, based on self-guiding examples. µsling should ideally be delivered as a single runnable jar file.

Java programming is not required to build web and REST applications with µsling 2.0: both server-side and client-side javascript code and presentation templates can be used to process HTTP requests. Other scripting and templating languages (JSP and BSF-supported ones) can be plugged in easily.

The µjax ""application protocol"" and client-side javascript ""JCR proxy"" library make it easy to write powerful Ajaxish JCR-based applications with µsling 2.0.

µsling 2.0 is built on the same codebase as Sling, it's only a specific configuration of Sling.

All µsling 2.0 features are available in Sling applications, as long as they are enabled in the Sling configuration.

Sling (and µsling, as it runs the same core code) uses OSGi to modularize the framework, but µsling does not require any OSGI skills, and makes OSGI largely invisible to beginners.

All Sling features and modules can also be activated in a µsling 2.0 instance, by installing and activating the required OSGi bundles.

µsling 2.0 passes all the integration tests of the existing microsling test suite (SVN revision 605206), with minor adaptations where needed.

µsling 2.0 includes a WebDAV server module to make it easy to copy scripts into the JCR repository.

[1] http://markmail.org/message/2s7agnu5kklti6da
[2] http://markmail.org/message/atbjzjjp2wflkotb",", "
"   Rename Method,","Create the Sling Launchpad, based on microsling-core code Following up on the ""[RT] Shall we merge microsling into Sling?"" [1] and ""µsling 2.0 requirements"" [2] threads on sling-dev, we need to merge microsling into Sling.

Here the are requirements as discussed in [2] (taking into account Felix's comment about WebDAV and Michael's comment about switching to other JCR repositories):

µsling 2.0 is a preconfigured instance of Sling, meant to allow web developers to test drive Sling by building scripted web and REST applications backed by a JCR repository.

The µsling 2.0 distribution only requires a Java 5 VM to run, no installation is needed. Fifteen minutes should be enough to start µsling and understand the basic concepts, based on self-guiding examples. µsling should ideally be delivered as a single runnable jar file.

Java programming is not required to build web and REST applications with µsling 2.0: both server-side and client-side javascript code and presentation templates can be used to process HTTP requests. Other scripting and templating languages (JSP and BSF-supported ones) can be plugged in easily.

The µjax ""application protocol"" and client-side javascript ""JCR proxy"" library make it easy to write powerful Ajaxish JCR-based applications with µsling 2.0.

µsling 2.0 is built on the same codebase as Sling, it's only a specific configuration of Sling.

All µsling 2.0 features are available in Sling applications, as long as they are enabled in the Sling configuration.

Sling (and µsling, as it runs the same core code) uses OSGi to modularize the framework, but µsling does not require any OSGI skills, and makes OSGI largely invisible to beginners.

All Sling features and modules can also be activated in a µsling 2.0 instance, by installing and activating the required OSGi bundles.

µsling 2.0 passes all the integration tests of the existing microsling test suite (SVN revision 605206), with minor adaptations where needed.

µsling 2.0 includes a WebDAV server module to make it easy to copy scripts into the JCR repository.

[1] http://markmail.org/message/2s7agnu5kklti6da
[2] http://markmail.org/message/atbjzjjp2wflkotb",", "
"   Rename Method,","Create the Sling Launchpad, based on microsling-core code Following up on the ""[RT] Shall we merge microsling into Sling?"" [1] and ""µsling 2.0 requirements"" [2] threads on sling-dev, we need to merge microsling into Sling.

Here the are requirements as discussed in [2] (taking into account Felix's comment about WebDAV and Michael's comment about switching to other JCR repositories):

µsling 2.0 is a preconfigured instance of Sling, meant to allow web developers to test drive Sling by building scripted web and REST applications backed by a JCR repository.

The µsling 2.0 distribution only requires a Java 5 VM to run, no installation is needed. Fifteen minutes should be enough to start µsling and understand the basic concepts, based on self-guiding examples. µsling should ideally be delivered as a single runnable jar file.

Java programming is not required to build web and REST applications with µsling 2.0: both server-side and client-side javascript code and presentation templates can be used to process HTTP requests. Other scripting and templating languages (JSP and BSF-supported ones) can be plugged in easily.

The µjax ""application protocol"" and client-side javascript ""JCR proxy"" library make it easy to write powerful Ajaxish JCR-based applications with µsling 2.0.

µsling 2.0 is built on the same codebase as Sling, it's only a specific configuration of Sling.

All µsling 2.0 features are available in Sling applications, as long as they are enabled in the Sling configuration.

Sling (and µsling, as it runs the same core code) uses OSGi to modularize the framework, but µsling does not require any OSGI skills, and makes OSGI largely invisible to beginners.

All Sling features and modules can also be activated in a µsling 2.0 instance, by installing and activating the required OSGi bundles.

µsling 2.0 passes all the integration tests of the existing microsling test suite (SVN revision 605206), with minor adaptations where needed.

µsling 2.0 includes a WebDAV server module to make it easy to copy scripts into the JCR repository.

[1] http://markmail.org/message/2s7agnu5kklti6da
[2] http://markmail.org/message/atbjzjjp2wflkotb",", "
"   Rename Method,","Create the Sling Launchpad, based on microsling-core code Following up on the ""[RT] Shall we merge microsling into Sling?"" [1] and ""µsling 2.0 requirements"" [2] threads on sling-dev, we need to merge microsling into Sling.

Here the are requirements as discussed in [2] (taking into account Felix's comment about WebDAV and Michael's comment about switching to other JCR repositories):

µsling 2.0 is a preconfigured instance of Sling, meant to allow web developers to test drive Sling by building scripted web and REST applications backed by a JCR repository.

The µsling 2.0 distribution only requires a Java 5 VM to run, no installation is needed. Fifteen minutes should be enough to start µsling and understand the basic concepts, based on self-guiding examples. µsling should ideally be delivered as a single runnable jar file.

Java programming is not required to build web and REST applications with µsling 2.0: both server-side and client-side javascript code and presentation templates can be used to process HTTP requests. Other scripting and templating languages (JSP and BSF-supported ones) can be plugged in easily.

The µjax ""application protocol"" and client-side javascript ""JCR proxy"" library make it easy to write powerful Ajaxish JCR-based applications with µsling 2.0.

µsling 2.0 is built on the same codebase as Sling, it's only a specific configuration of Sling.

All µsling 2.0 features are available in Sling applications, as long as they are enabled in the Sling configuration.

Sling (and µsling, as it runs the same core code) uses OSGi to modularize the framework, but µsling does not require any OSGI skills, and makes OSGI largely invisible to beginners.

All Sling features and modules can also be activated in a µsling 2.0 instance, by installing and activating the required OSGi bundles.

µsling 2.0 passes all the integration tests of the existing microsling test suite (SVN revision 605206), with minor adaptations where needed.

µsling 2.0 includes a WebDAV server module to make it easy to copy scripts into the JCR repository.

[1] http://markmail.org/message/2s7agnu5kklti6da
[2] http://markmail.org/message/atbjzjjp2wflkotb",", "
"   Move Class,Move And Rename Class,","Create the Sling Launchpad, based on microsling-core code Following up on the ""[RT] Shall we merge microsling into Sling?"" [1] and ""µsling 2.0 requirements"" [2] threads on sling-dev, we need to merge microsling into Sling.

Here the are requirements as discussed in [2] (taking into account Felix's comment about WebDAV and Michael's comment about switching to other JCR repositories):

µsling 2.0 is a preconfigured instance of Sling, meant to allow web developers to test drive Sling by building scripted web and REST applications backed by a JCR repository.

The µsling 2.0 distribution only requires a Java 5 VM to run, no installation is needed. Fifteen minutes should be enough to start µsling and understand the basic concepts, based on self-guiding examples. µsling should ideally be delivered as a single runnable jar file.

Java programming is not required to build web and REST applications with µsling 2.0: both server-side and client-side javascript code and presentation templates can be used to process HTTP requests. Other scripting and templating languages (JSP and BSF-supported ones) can be plugged in easily.

The µjax ""application protocol"" and client-side javascript ""JCR proxy"" library make it easy to write powerful Ajaxish JCR-based applications with µsling 2.0.

µsling 2.0 is built on the same codebase as Sling, it's only a specific configuration of Sling.

All µsling 2.0 features are available in Sling applications, as long as they are enabled in the Sling configuration.

Sling (and µsling, as it runs the same core code) uses OSGi to modularize the framework, but µsling does not require any OSGI skills, and makes OSGI largely invisible to beginners.

All Sling features and modules can also be activated in a µsling 2.0 instance, by installing and activating the required OSGi bundles.

µsling 2.0 passes all the integration tests of the existing microsling test suite (SVN revision 605206), with minor adaptations where needed.

µsling 2.0 includes a WebDAV server module to make it easy to copy scripts into the JCR repository.

[1] http://markmail.org/message/2s7agnu5kklti6da
[2] http://markmail.org/message/atbjzjjp2wflkotb",", "
"   Rename Method,","Create the Sling Launchpad, based on microsling-core code Following up on the ""[RT] Shall we merge microsling into Sling?"" [1] and ""µsling 2.0 requirements"" [2] threads on sling-dev, we need to merge microsling into Sling.

Here the are requirements as discussed in [2] (taking into account Felix's comment about WebDAV and Michael's comment about switching to other JCR repositories):

µsling 2.0 is a preconfigured instance of Sling, meant to allow web developers to test drive Sling by building scripted web and REST applications backed by a JCR repository.

The µsling 2.0 distribution only requires a Java 5 VM to run, no installation is needed. Fifteen minutes should be enough to start µsling and understand the basic concepts, based on self-guiding examples. µsling should ideally be delivered as a single runnable jar file.

Java programming is not required to build web and REST applications with µsling 2.0: both server-side and client-side javascript code and presentation templates can be used to process HTTP requests. Other scripting and templating languages (JSP and BSF-supported ones) can be plugged in easily.

The µjax ""application protocol"" and client-side javascript ""JCR proxy"" library make it easy to write powerful Ajaxish JCR-based applications with µsling 2.0.

µsling 2.0 is built on the same codebase as Sling, it's only a specific configuration of Sling.

All µsling 2.0 features are available in Sling applications, as long as they are enabled in the Sling configuration.

Sling (and µsling, as it runs the same core code) uses OSGi to modularize the framework, but µsling does not require any OSGI skills, and makes OSGI largely invisible to beginners.

All Sling features and modules can also be activated in a µsling 2.0 instance, by installing and activating the required OSGi bundles.

µsling 2.0 passes all the integration tests of the existing microsling test suite (SVN revision 605206), with minor adaptations where needed.

µsling 2.0 includes a WebDAV server module to make it easy to copy scripts into the JCR repository.

[1] http://markmail.org/message/2s7agnu5kklti6da
[2] http://markmail.org/message/atbjzjjp2wflkotb",", "
"   Rename Method,","Create the Sling Launchpad, based on microsling-core code Following up on the ""[RT] Shall we merge microsling into Sling?"" [1] and ""µsling 2.0 requirements"" [2] threads on sling-dev, we need to merge microsling into Sling.

Here the are requirements as discussed in [2] (taking into account Felix's comment about WebDAV and Michael's comment about switching to other JCR repositories):

µsling 2.0 is a preconfigured instance of Sling, meant to allow web developers to test drive Sling by building scripted web and REST applications backed by a JCR repository.

The µsling 2.0 distribution only requires a Java 5 VM to run, no installation is needed. Fifteen minutes should be enough to start µsling and understand the basic concepts, based on self-guiding examples. µsling should ideally be delivered as a single runnable jar file.

Java programming is not required to build web and REST applications with µsling 2.0: both server-side and client-side javascript code and presentation templates can be used to process HTTP requests. Other scripting and templating languages (JSP and BSF-supported ones) can be plugged in easily.

The µjax ""application protocol"" and client-side javascript ""JCR proxy"" library make it easy to write powerful Ajaxish JCR-based applications with µsling 2.0.

µsling 2.0 is built on the same codebase as Sling, it's only a specific configuration of Sling.

All µsling 2.0 features are available in Sling applications, as long as they are enabled in the Sling configuration.

Sling (and µsling, as it runs the same core code) uses OSGi to modularize the framework, but µsling does not require any OSGI skills, and makes OSGI largely invisible to beginners.

All Sling features and modules can also be activated in a µsling 2.0 instance, by installing and activating the required OSGi bundles.

µsling 2.0 passes all the integration tests of the existing microsling test suite (SVN revision 605206), with minor adaptations where needed.

µsling 2.0 includes a WebDAV server module to make it easy to copy scripts into the JCR repository.

[1] http://markmail.org/message/2s7agnu5kklti6da
[2] http://markmail.org/message/atbjzjjp2wflkotb",", "
"   Rename Method,","Create the Sling Launchpad, based on microsling-core code Following up on the ""[RT] Shall we merge microsling into Sling?"" [1] and ""µsling 2.0 requirements"" [2] threads on sling-dev, we need to merge microsling into Sling.

Here the are requirements as discussed in [2] (taking into account Felix's comment about WebDAV and Michael's comment about switching to other JCR repositories):

µsling 2.0 is a preconfigured instance of Sling, meant to allow web developers to test drive Sling by building scripted web and REST applications backed by a JCR repository.

The µsling 2.0 distribution only requires a Java 5 VM to run, no installation is needed. Fifteen minutes should be enough to start µsling and understand the basic concepts, based on self-guiding examples. µsling should ideally be delivered as a single runnable jar file.

Java programming is not required to build web and REST applications with µsling 2.0: both server-side and client-side javascript code and presentation templates can be used to process HTTP requests. Other scripting and templating languages (JSP and BSF-supported ones) can be plugged in easily.

The µjax ""application protocol"" and client-side javascript ""JCR proxy"" library make it easy to write powerful Ajaxish JCR-based applications with µsling 2.0.

µsling 2.0 is built on the same codebase as Sling, it's only a specific configuration of Sling.

All µsling 2.0 features are available in Sling applications, as long as they are enabled in the Sling configuration.

Sling (and µsling, as it runs the same core code) uses OSGi to modularize the framework, but µsling does not require any OSGI skills, and makes OSGI largely invisible to beginners.

All Sling features and modules can also be activated in a µsling 2.0 instance, by installing and activating the required OSGi bundles.

µsling 2.0 passes all the integration tests of the existing microsling test suite (SVN revision 605206), with minor adaptations where needed.

µsling 2.0 includes a WebDAV server module to make it easy to copy scripts into the JCR repository.

[1] http://markmail.org/message/2s7agnu5kklti6da
[2] http://markmail.org/message/atbjzjjp2wflkotb",", "
"   Rename Method,","Create the Sling Launchpad, based on microsling-core code Following up on the ""[RT] Shall we merge microsling into Sling?"" [1] and ""µsling 2.0 requirements"" [2] threads on sling-dev, we need to merge microsling into Sling.

Here the are requirements as discussed in [2] (taking into account Felix's comment about WebDAV and Michael's comment about switching to other JCR repositories):

µsling 2.0 is a preconfigured instance of Sling, meant to allow web developers to test drive Sling by building scripted web and REST applications backed by a JCR repository.

The µsling 2.0 distribution only requires a Java 5 VM to run, no installation is needed. Fifteen minutes should be enough to start µsling and understand the basic concepts, based on self-guiding examples. µsling should ideally be delivered as a single runnable jar file.

Java programming is not required to build web and REST applications with µsling 2.0: both server-side and client-side javascript code and presentation templates can be used to process HTTP requests. Other scripting and templating languages (JSP and BSF-supported ones) can be plugged in easily.

The µjax ""application protocol"" and client-side javascript ""JCR proxy"" library make it easy to write powerful Ajaxish JCR-based applications with µsling 2.0.

µsling 2.0 is built on the same codebase as Sling, it's only a specific configuration of Sling.

All µsling 2.0 features are available in Sling applications, as long as they are enabled in the Sling configuration.

Sling (and µsling, as it runs the same core code) uses OSGi to modularize the framework, but µsling does not require any OSGI skills, and makes OSGI largely invisible to beginners.

All Sling features and modules can also be activated in a µsling 2.0 instance, by installing and activating the required OSGi bundles.

µsling 2.0 passes all the integration tests of the existing microsling test suite (SVN revision 605206), with minor adaptations where needed.

µsling 2.0 includes a WebDAV server module to make it easy to copy scripts into the JCR repository.

[1] http://markmail.org/message/2s7agnu5kklti6da
[2] http://markmail.org/message/atbjzjjp2wflkotb",", "
"   Rename Method,","Create the Sling Launchpad, based on microsling-core code Following up on the ""[RT] Shall we merge microsling into Sling?"" [1] and ""µsling 2.0 requirements"" [2] threads on sling-dev, we need to merge microsling into Sling.

Here the are requirements as discussed in [2] (taking into account Felix's comment about WebDAV and Michael's comment about switching to other JCR repositories):

µsling 2.0 is a preconfigured instance of Sling, meant to allow web developers to test drive Sling by building scripted web and REST applications backed by a JCR repository.

The µsling 2.0 distribution only requires a Java 5 VM to run, no installation is needed. Fifteen minutes should be enough to start µsling and understand the basic concepts, based on self-guiding examples. µsling should ideally be delivered as a single runnable jar file.

Java programming is not required to build web and REST applications with µsling 2.0: both server-side and client-side javascript code and presentation templates can be used to process HTTP requests. Other scripting and templating languages (JSP and BSF-supported ones) can be plugged in easily.

The µjax ""application protocol"" and client-side javascript ""JCR proxy"" library make it easy to write powerful Ajaxish JCR-based applications with µsling 2.0.

µsling 2.0 is built on the same codebase as Sling, it's only a specific configuration of Sling.

All µsling 2.0 features are available in Sling applications, as long as they are enabled in the Sling configuration.

Sling (and µsling, as it runs the same core code) uses OSGi to modularize the framework, but µsling does not require any OSGI skills, and makes OSGI largely invisible to beginners.

All Sling features and modules can also be activated in a µsling 2.0 instance, by installing and activating the required OSGi bundles.

µsling 2.0 passes all the integration tests of the existing microsling test suite (SVN revision 605206), with minor adaptations where needed.

µsling 2.0 includes a WebDAV server module to make it easy to copy scripts into the JCR repository.

[1] http://markmail.org/message/2s7agnu5kklti6da
[2] http://markmail.org/message/atbjzjjp2wflkotb",", "
"   Rename Method,","Create the Sling Launchpad, based on microsling-core code Following up on the ""[RT] Shall we merge microsling into Sling?"" [1] and ""µsling 2.0 requirements"" [2] threads on sling-dev, we need to merge microsling into Sling.

Here the are requirements as discussed in [2] (taking into account Felix's comment about WebDAV and Michael's comment about switching to other JCR repositories):

µsling 2.0 is a preconfigured instance of Sling, meant to allow web developers to test drive Sling by building scripted web and REST applications backed by a JCR repository.

The µsling 2.0 distribution only requires a Java 5 VM to run, no installation is needed. Fifteen minutes should be enough to start µsling and understand the basic concepts, based on self-guiding examples. µsling should ideally be delivered as a single runnable jar file.

Java programming is not required to build web and REST applications with µsling 2.0: both server-side and client-side javascript code and presentation templates can be used to process HTTP requests. Other scripting and templating languages (JSP and BSF-supported ones) can be plugged in easily.

The µjax ""application protocol"" and client-side javascript ""JCR proxy"" library make it easy to write powerful Ajaxish JCR-based applications with µsling 2.0.

µsling 2.0 is built on the same codebase as Sling, it's only a specific configuration of Sling.

All µsling 2.0 features are available in Sling applications, as long as they are enabled in the Sling configuration.

Sling (and µsling, as it runs the same core code) uses OSGi to modularize the framework, but µsling does not require any OSGI skills, and makes OSGI largely invisible to beginners.

All Sling features and modules can also be activated in a µsling 2.0 instance, by installing and activating the required OSGi bundles.

µsling 2.0 passes all the integration tests of the existing microsling test suite (SVN revision 605206), with minor adaptations where needed.

µsling 2.0 includes a WebDAV server module to make it easy to copy scripts into the JCR repository.

[1] http://markmail.org/message/2s7agnu5kklti6da
[2] http://markmail.org/message/atbjzjjp2wflkotb",", "
"   Rename Method,","Create the Sling Launchpad, based on microsling-core code Following up on the ""[RT] Shall we merge microsling into Sling?"" [1] and ""µsling 2.0 requirements"" [2] threads on sling-dev, we need to merge microsling into Sling.

Here the are requirements as discussed in [2] (taking into account Felix's comment about WebDAV and Michael's comment about switching to other JCR repositories):

µsling 2.0 is a preconfigured instance of Sling, meant to allow web developers to test drive Sling by building scripted web and REST applications backed by a JCR repository.

The µsling 2.0 distribution only requires a Java 5 VM to run, no installation is needed. Fifteen minutes should be enough to start µsling and understand the basic concepts, based on self-guiding examples. µsling should ideally be delivered as a single runnable jar file.

Java programming is not required to build web and REST applications with µsling 2.0: both server-side and client-side javascript code and presentation templates can be used to process HTTP requests. Other scripting and templating languages (JSP and BSF-supported ones) can be plugged in easily.

The µjax ""application protocol"" and client-side javascript ""JCR proxy"" library make it easy to write powerful Ajaxish JCR-based applications with µsling 2.0.

µsling 2.0 is built on the same codebase as Sling, it's only a specific configuration of Sling.

All µsling 2.0 features are available in Sling applications, as long as they are enabled in the Sling configuration.

Sling (and µsling, as it runs the same core code) uses OSGi to modularize the framework, but µsling does not require any OSGI skills, and makes OSGI largely invisible to beginners.

All Sling features and modules can also be activated in a µsling 2.0 instance, by installing and activating the required OSGi bundles.

µsling 2.0 passes all the integration tests of the existing microsling test suite (SVN revision 605206), with minor adaptations where needed.

µsling 2.0 includes a WebDAV server module to make it easy to copy scripts into the JCR repository.

[1] http://markmail.org/message/2s7agnu5kklti6da
[2] http://markmail.org/message/atbjzjjp2wflkotb",", "
"   Rename Method,","Create the Sling Launchpad, based on microsling-core code Following up on the ""[RT] Shall we merge microsling into Sling?"" [1] and ""µsling 2.0 requirements"" [2] threads on sling-dev, we need to merge microsling into Sling.

Here the are requirements as discussed in [2] (taking into account Felix's comment about WebDAV and Michael's comment about switching to other JCR repositories):

µsling 2.0 is a preconfigured instance of Sling, meant to allow web developers to test drive Sling by building scripted web and REST applications backed by a JCR repository.

The µsling 2.0 distribution only requires a Java 5 VM to run, no installation is needed. Fifteen minutes should be enough to start µsling and understand the basic concepts, based on self-guiding examples. µsling should ideally be delivered as a single runnable jar file.

Java programming is not required to build web and REST applications with µsling 2.0: both server-side and client-side javascript code and presentation templates can be used to process HTTP requests. Other scripting and templating languages (JSP and BSF-supported ones) can be plugged in easily.

The µjax ""application protocol"" and client-side javascript ""JCR proxy"" library make it easy to write powerful Ajaxish JCR-based applications with µsling 2.0.

µsling 2.0 is built on the same codebase as Sling, it's only a specific configuration of Sling.

All µsling 2.0 features are available in Sling applications, as long as they are enabled in the Sling configuration.

Sling (and µsling, as it runs the same core code) uses OSGi to modularize the framework, but µsling does not require any OSGI skills, and makes OSGI largely invisible to beginners.

All Sling features and modules can also be activated in a µsling 2.0 instance, by installing and activating the required OSGi bundles.

µsling 2.0 passes all the integration tests of the existing microsling test suite (SVN revision 605206), with minor adaptations where needed.

µsling 2.0 includes a WebDAV server module to make it easy to copy scripts into the JCR repository.

[1] http://markmail.org/message/2s7agnu5kklti6da
[2] http://markmail.org/message/atbjzjjp2wflkotb",", "
"   Move Method,Move Attribute,",Expose the ScriptEngineManager managed by Sling I'd like to add a new service interface called ScriptEngineManagerFactory to scripting.api and have SlingScriptAdapterFactory implement it. This will allow other infrastucture bundles to use javax.script engines.,", , , "
"   Extract Interface,Move Method,Move Attribute,","Upgrade GWT Extension to 2.0.3 Current gwt contrib modules use 1.4.60, which is pretty old.",", , , Large Class, "
"   Rename Method,Move Method,",switch SlingAuthenticator to use ResourceResolverFactory API rather than JCR directory 0,", , "
"   Rename Method,","Allow access to Node and Property Methods on ScriptableNode and ScriptableProperty I would like to propose access to all jcr Node methods from ScriptableNode and access to the jcr Property.

I recently wanted to access the Property.getLength() method from a .esp script and didn't find a good way starting
out from my very convenient ScriptableNode.

This discussion already talks about a similar issue:
http://www.mail-archive.com/sling-dev@incubator.apache.org/msg01481.html

So ideally somthing like an automatic getter mapping that I know from earlier rhino projects would mean 
that I could access the same information through for example prop.length or prop.getLength().

I think it would be great if all jcr Property and Node methods would be exposed, otherwise we are hiding jcr features from
the script user. I think maybe the solution also requires a ScriptableProperty.

WDYT?",", "
"   Rename Method,","Allow access to Node and Property Methods on ScriptableNode and ScriptableProperty I would like to propose access to all jcr Node methods from ScriptableNode and access to the jcr Property.

I recently wanted to access the Property.getLength() method from a .esp script and didn't find a good way starting
out from my very convenient ScriptableNode.

This discussion already talks about a similar issue:
http://www.mail-archive.com/sling-dev@incubator.apache.org/msg01481.html

So ideally somthing like an automatic getter mapping that I know from earlier rhino projects would mean 
that I could access the same information through for example prop.length or prop.getLength().

I think it would be great if all jcr Property and Node methods would be exposed, otherwise we are hiding jcr features from
the script user. I think maybe the solution also requires a ScriptableProperty.

WDYT?",", "
"   Rename Method,","Allow access to Node and Property Methods on ScriptableNode and ScriptableProperty I would like to propose access to all jcr Node methods from ScriptableNode and access to the jcr Property.

I recently wanted to access the Property.getLength() method from a .esp script and didn't find a good way starting
out from my very convenient ScriptableNode.

This discussion already talks about a similar issue:
http://www.mail-archive.com/sling-dev@incubator.apache.org/msg01481.html

So ideally somthing like an automatic getter mapping that I know from earlier rhino projects would mean 
that I could access the same information through for example prop.length or prop.getLength().

I think it would be great if all jcr Property and Node methods would be exposed, otherwise we are hiding jcr features from
the script user. I think maybe the solution also requires a ScriptableProperty.

WDYT?",", "
"   Rename Method,","Allow access to Node and Property Methods on ScriptableNode and ScriptableProperty I would like to propose access to all jcr Node methods from ScriptableNode and access to the jcr Property.

I recently wanted to access the Property.getLength() method from a .esp script and didn't find a good way starting
out from my very convenient ScriptableNode.

This discussion already talks about a similar issue:
http://www.mail-archive.com/sling-dev@incubator.apache.org/msg01481.html

So ideally somthing like an automatic getter mapping that I know from earlier rhino projects would mean 
that I could access the same information through for example prop.length or prop.getLength().

I think it would be great if all jcr Property and Node methods would be exposed, otherwise we are hiding jcr features from
the script user. I think maybe the solution also requires a ScriptableProperty.

WDYT?",", "
"   Extract Method,Inline Method,","UserManager permissions manipulation services API It would be nice if the jackrabbit.usermanager bundle exposed OSGI service(s) that maps exactly the functionalities of the REST services, so that one can use their features also in a programmatic way. 

This can be useful if an application has to manage users and groups without having an explicit request object (ex: from an EventListener), or in the case a user has to manipulate his account (in this case he doesn't have an administrative account, so his requests are not permitted to modify users). Also i think that, in certain situations, it could be just cleaner and simpler to write a servlet or script that directly invoke the methods, instead of find the way to invoke the REST services.

I think a simple but exaustive way to achieve this can be the direct mapping of the REST services described in http://sling.apache.org/site/managing-users-and-groups-jackrabbitusermanager.html and http://sling.apache.org/site/managing-permissions-jackrabbitaccessmanager.html, only using well-known JCR classes.
For example, obtaining the users list could be as simple as getting the UserManager OSGI service and invoking a method like ""public NodeIterator listUsers()"", changing a permission could be achieved by getting the AccessManager OSGI service and invoking a method like ""public void modifyPermission(String node_path, String principalId, String privilege_name, String privilege_value, String order)"", and so on...

Perhaps the best way to standardize these services is a dedicated API that formalizes the underlying concepts (ex: User, Group, Privilege and NodeAccessControl... if you think it's the case, i could propose my own...), but i think the simple REST services mapping could already be a nice (and ready-to-go) feature for developers...

","Duplicated Code, Long Method, , , "
"   Extract Method,Inline Method,","UserManager permissions manipulation services API It would be nice if the jackrabbit.usermanager bundle exposed OSGI service(s) that maps exactly the functionalities of the REST services, so that one can use their features also in a programmatic way. 

This can be useful if an application has to manage users and groups without having an explicit request object (ex: from an EventListener), or in the case a user has to manipulate his account (in this case he doesn't have an administrative account, so his requests are not permitted to modify users). Also i think that, in certain situations, it could be just cleaner and simpler to write a servlet or script that directly invoke the methods, instead of find the way to invoke the REST services.

I think a simple but exaustive way to achieve this can be the direct mapping of the REST services described in http://sling.apache.org/site/managing-users-and-groups-jackrabbitusermanager.html and http://sling.apache.org/site/managing-permissions-jackrabbitaccessmanager.html, only using well-known JCR classes.
For example, obtaining the users list could be as simple as getting the UserManager OSGI service and invoking a method like ""public NodeIterator listUsers()"", changing a permission could be achieved by getting the AccessManager OSGI service and invoking a method like ""public void modifyPermission(String node_path, String principalId, String privilege_name, String privilege_value, String order)"", and so on...

Perhaps the best way to standardize these services is a dedicated API that formalizes the underlying concepts (ex: User, Group, Privilege and NodeAccessControl... if you think it's the case, i could propose my own...), but i think the simple REST services mapping could already be a nice (and ready-to-go) feature for developers...

","Duplicated Code, Long Method, , , "
"   Rename Method,","OSGi events should contain a userID when possible. We're building a system that tracks the user's movements in the system.
One of the things we're tracking is the OSGi events that are emitted by Sling. These don't contain a userid however.
Since this all hapens in the JcrResourceListener which is a JCR Observation Listener, getting the userID is fairly straightforward.

I'll attach a patch that includes this functionality.

AFAICT this is the only location that emits user generated events, if I missed a spot I'd be happy to try and patch that one as well.",", "
"   Rename Method,","OSGi events should contain a userID when possible. We're building a system that tracks the user's movements in the system.
One of the things we're tracking is the OSGi events that are emitted by Sling. These don't contain a userid however.
Since this all hapens in the JcrResourceListener which is a JCR Observation Listener, getting the userID is fairly straightforward.

I'll attach a patch that includes this functionality.

AFAICT this is the only location that emits user generated events, if I missed a spot I'd be happy to try and patch that one as well.",", "
"   Rename Method,Move Method,Extract Method,Move Attribute,","SlingServlet service For SLING-550 I need to call the SlingMainServlet outside of the web container's request/response cycle.

I'll attach a patch that introduces a new SlingServlet interface that SlingMainServlet implements.","Duplicated Code, Long Method, , , , "
"   Rename Method,Move Method,Extract Method,Move Attribute,","SlingServlet service For SLING-550 I need to call the SlingMainServlet outside of the web container's request/response cycle.

I'll attach a patch that introduces a new SlingServlet interface that SlingMainServlet implements.","Duplicated Code, Long Method, , , , "
"   Rename Method,Pull Up Method,Pull Up Attribute,","create servlet to get effective access control list /path.acl.json -> returns the declared ACL, if any
/path.eacl.json -> returns the effective ACL",", Duplicated Code, Duplicated Code, "
"   Rename Method,Pull Up Method,Pull Up Attribute,","create servlet to get effective access control list /path.acl.json -> returns the declared ACL, if any
/path.eacl.json -> returns the effective ACL",", Duplicated Code, Duplicated Code, "
"   Rename Method,Extract Method,","Expose subset of authentication info properties as ResourceResolver attributes The properties of the authentication info (or credentials map) provided to the ResoureResolverFactory to create a new ResourceResolver should be exposed by the ResourceResolver.

Exceptions apply to any passwords or JCR Credentials or other identifiable sensitive information, which should of course not be exposed.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Default JSON rendering with selectable depth As discussed on list [1], the default JSON rendering should work as follows:

.../mynode.0.json -> returns the node and its properties

.../mynode.1.json -> returns the node, it properties and the direct child nodes and their properties
.../mynode.2.json -> same, but two levels deep in the child nodes hierarchy
...
.../mynode.infinity.json -> returns the entire subtree.

and .../mynode.json behaves like .../mynode.0.json

[1] http://mail-archives.apache.org/mod_mbox/incubator-sling-dev/200801.mbox/%3c227048280801090142x32b75287ifc06ca901b329ed0@mail.gmail.com%3e","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Default JSON rendering with selectable depth As discussed on list [1], the default JSON rendering should work as follows:

.../mynode.0.json -> returns the node and its properties

.../mynode.1.json -> returns the node, it properties and the direct child nodes and their properties
.../mynode.2.json -> same, but two levels deep in the child nodes hierarchy
...
.../mynode.infinity.json -> returns the entire subtree.

and .../mynode.json behaves like .../mynode.0.json

[1] http://mail-archives.apache.org/mod_mbox/incubator-sling-dev/200801.mbox/%3c227048280801090142x32b75287ifc06ca901b329ed0@mail.gmail.com%3e","Duplicated Code, Long Method, , "
"   Rename Method,","Change the OSGi installer interface As discussed in the mailing list:

the current OSGi installer has three methods:
- registerResources : which is used to register all resources from the
installer client like jcr install; this is usually invoked on startup
- addResource : adds a resource during runtime
- removeResource : removes a resource during runtime

The api is simple, fine and sufficient. However there is a small glitch
here. If a client detects several changes at once, like a set of bundles
is removed or updated, it has to call addResource or removeResource for
each change separately. The OSGi installer could run a install cycle
inbetween those method calls. Causing a part of this process to be done
in the first cycle and the other part in the second cycle - now as OSGi
has a dynamic nature this isn't a problem after two cycles everything is
installed as expected. But still I have the feeling that it would be
nicer, if the client could submit several changes add once, so maybe
instead of having a addResource and removeResource method, just a
updateResources([] addedResources, [] removedResources).
",", "
"   Extract Superclass,Pull Up Method,Pull Up Attribute,","Move SlingAdaptable from adapter bundle to api The SlingAdaptable - which is the default implementation for Adaptable - is current in the adapter bundle. The implementation contains some caching.
As the API should not depend on other Sling bundles, AbstractResource can't extend this implementation. In addition if one wants to use the SlingAdaptable together
with the Sling API it needs several bundles.

We can clean this up, by :
- adding SlingAdaptable to the API
- deprecate SlingAdaptable in the adapter bundle and let it extend the API version
- make AbstractResource extend the new SlingAdaptable
- move the AdapterManager code from AbstractResource to the new SlingAdaptable",", Duplicated Code, Large Class, Duplicated Code, Duplicated Code, "
"   Move Class,Extract Superclass,Rename Method,Move Method,Extract Method,Move Attribute,","Register internal post operations as services for consumption by servlets other than the SlingPostServlet As discussed in [1] it would be useful to have the internal operations of the Sling POST Servlet available as services for other bundles to reuse.

[1] http://markmail.org/message/a7vrtyhictf7tv4m","Duplicated Code, Long Method, , , , Duplicated Code, Large Class, "
"   Rename Method,Pull Up Method,Extract Method,Inline Method,","Add state management for resources Currently there is no state management, so it is hard to tell if a resource has been installed, should be installed, uninstalled etc.
In some situations this leads to endless loops where something is tried over and over again - although nothing needs to be done anymore or can't be done.

If we add proper state management to the resources, the installer knows what needs to be done and can act accordingly","Duplicated Code, Long Method, , , Duplicated Code, "
"   Rename Method,Extract Method,","Add state management for resources Currently there is no state management, so it is hard to tell if a resource has been installed, should be installed, uninstalled etc.
In some situations this leads to endless loops where something is tried over and over again - although nothing needs to be done anymore or can't be done.

If we add proper state management to the resources, the installer knows what needs to be done and can act accordingly","Duplicated Code, Long Method, , "
"   Move Method,Inline Method,Move Attribute,","Add state management for resources Currently there is no state management, so it is hard to tell if a resource has been installed, should be installed, uninstalled etc.
In some situations this leads to endless loops where something is tried over and over again - although nothing needs to be done anymore or can't be done.

If we add proper state management to the resources, the installer knows what needs to be done and can act accordingly",", , , , "
"   Pull Up Method,Extract Method,Pull Up Attribute,","Add state management for resources Currently there is no state management, so it is hard to tell if a resource has been installed, should be installed, uninstalled etc.
In some situations this leads to endless loops where something is tried over and over again - although nothing needs to be done anymore or can't be done.

If we add proper state management to the resources, the installer knows what needs to be done and can act accordingly","Duplicated Code, Long Method, , Duplicated Code, Duplicated Code, "
"   Move Method,Inline Method,Move Attribute,","Add state management for resources Currently there is no state management, so it is hard to tell if a resource has been installed, should be installed, uninstalled etc.
In some situations this leads to endless loops where something is tried over and over again - although nothing needs to be done anymore or can't be done.

If we add proper state management to the resources, the installer knows what needs to be done and can act accordingly",", , , , "
"   Rename Class,Inline Method,","Improvements to event bundle Currently the eventing does not store events immediately into the repository in order to quickly respond back to the event admin. (This is in order to not get blacklisted). The incoming event is put into a queue and picked up from there.
It would be safer to have a separate write queue which is independent from the processing queue to write the event as soon into the repository as possible.
",", , "
"   Rename Method,Extract Method,","Improvements to event bundle Currently the eventing does not store events immediately into the repository in order to quickly respond back to the event admin. (This is in order to not get blacklisted). The incoming event is put into a queue and picked up from there.
It would be safer to have a separate write queue which is independent from the processing queue to write the event as soon into the repository as possible.
","Duplicated Code, Long Method, , "
"   Rename Class,Inline Method,","Improvements to event bundle Currently the eventing does not store events immediately into the repository in order to quickly respond back to the event admin. (This is in order to not get blacklisted). The incoming event is put into a queue and picked up from there.
It would be safer to have a separate write queue which is independent from the processing queue to write the event as soon into the repository as possible.
",", , "
"   Push Down Method,Push Down Attribute,","Improvements to event bundle Currently the eventing does not store events immediately into the repository in order to quickly respond back to the event admin. (This is in order to not get blacklisted). The incoming event is put into a queue and picked up from there.
It would be safer to have a separate write queue which is independent from the processing queue to write the event as soon into the repository as possible.
",", , , "
"   Push Down Method,Push Down Attribute,","Improvements to event bundle Currently the eventing does not store events immediately into the repository in order to quickly respond back to the event admin. (This is in order to not get blacklisted). The incoming event is put into a queue and picked up from there.
It would be safer to have a separate write queue which is independent from the processing queue to write the event as soon into the repository as possible.
",", , , "
"   Rename Method,Extract Method,","Improvements to event bundle Currently the eventing does not store events immediately into the repository in order to quickly respond back to the event admin. (This is in order to not get blacklisted). The incoming event is put into a queue and picked up from there.
It would be safer to have a separate write queue which is independent from the processing queue to write the event as soon into the repository as possible.
","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,",ResourceResolver should not go up the path for GET requests SLING-117 changes need to be ported to Sling,"Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,",ResourceResolver should not go up the path for GET requests SLING-117 changes need to be ported to Sling,"Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Make Sling POST Servlet dynamically configurable Currently the SlingPostServlet is a regaular DS 1.0 component which is stopped and restarted for updated OSGi configuration. This gives a certain amount of down-time during configuration update and also places some load on the system.

To make configuration updates as transparent and low latency as possible, it would probably be good support the new DS 1.0 functionality for dynamic configuration update using the ""modify"" method.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Make Sling POST Servlet dynamically configurable Currently the SlingPostServlet is a regaular DS 1.0 component which is stopped and restarted for updated OSGi configuration. This gives a certain amount of down-time during configuration update and also places some load on the system.

To make configuration updates as transparent and low latency as possible, it would probably be good support the new DS 1.0 functionality for dynamic configuration update using the ""modify"" method.","Duplicated Code, Long Method, , "
"   Rename Method,Inline Method,Move Attribute,","Remove direct dependency to web console by using new configuration printer support With the latest web console, it is possible to implement a configuration printer for the web console which does not use the interface but is invoked via reflection.",", , , "
"   Rename Method,Inline Method,Move Attribute,","Remove direct dependency to web console by using new configuration printer support With the latest web console, it is possible to implement a configuration printer for the web console which does not use the interface but is invoked via reflection.",", , , "
"   Rename Method,Inline Method,","Remove direct dependency to web console by using new configuration printer support With the latest web console, it is possible to implement a configuration printer for the web console which does not use the interface but is invoked via reflection.
",", , "
"   Rename Method,Inline Method,","Remove direct dependency to web console by using new configuration printer support With the latest web console, it is possible to implement a configuration printer for the web console which does not use the interface but is invoked via reflection.
",", , "
"   Rename Method,","Make SlingException a RuntimeException and derive all exceptions of SlingException Assuming lazy consensus to the changes proposed in the mail thread ""Rethinking Exceptions in Sling"" [1], the changes from the whiteboard [2] should be applied.

[1] http://www.mail-archive.com/sling-dev@incubator.apache.org/msg01520.html
[2] http://svn.apache.org/repos/asf/incubator/sling/whiteboard/fmeschbe/effective_exceptions/api",", "
"   Move And Rename Class,","Make SlingException a RuntimeException and derive all exceptions of SlingException Assuming lazy consensus to the changes proposed in the mail thread ""Rethinking Exceptions in Sling"" [1], the changes from the whiteboard [2] should be applied.

[1] http://www.mail-archive.com/sling-dev@incubator.apache.org/msg01520.html
[2] http://svn.apache.org/repos/asf/incubator/sling/whiteboard/fmeschbe/effective_exceptions/api",", "
"   Move Method,Move Attribute,","Status information should be stored outside the bundle data directory Currently the status information is stored inside the private bundle data area of the installer core bundle - this is fine as long as the core bundle itself is not updated. As soon as it is updated, all this data is lost. Most of the data can be recalculated but the information if a bundle or config has been installed through the installer or through any other way is lost.
Therefore we should store the data outside of the bundle private data (like config admin does)
",", , , "
"   Move Class,Move And Rename Class,",Make installers pluggable Currently the osgi installer only supports bundles and configurations - we should add service interfaces to allow installation of other resources like deployment packages etc.,", "
"   Move And Rename Class,Extract Interface,","Allow resource transformer for processing installable resources In some cases the installable resource is not directly the installable end product, for example if a jar is dropped into the installer which is not a bundle a service could bundlize this jar.
",", Large Class, "
"   Rename Class,Rename Method,","Allow resource transformer for processing installable resources In some cases the installable resource is not directly the installable end product, for example if a jar is dropped into the installer which is not a bundle a service could bundlize this jar.
",", "
"   Move And Rename Class,Extract Interface,","Allow resource transformer for processing installable resources In some cases the installable resource is not directly the installable end product, for example if a jar is dropped into the installer which is not a bundle a service could bundlize this jar.
",", Large Class, "
"   Rename Method,Move Method,Extract Method,","Improve the (internal) resource handling The current resource handling has some drawbacks - one is that a resource is always copied into the local data store even if it did not change; another one is that integrating new concepts like the resource transformer etc is difficult and error prone.

The first thing we should change is the resource types, currently we have CONFIG and BUNDLE - this assumes that the client knows what resources it is providing - however it is not the task of the client to decide this; in many cases the client has even not the knowledge about it. Therefore we should introduce two new resource types PROPERTIES and FILE.
However, if the client really knows what it is dealing with, it can use one of the two new types OSGI_BUNDLE or OSGI_CONFIG - we introduce these new types, to be more compatible - the old constants CONFIG and BUNDLE will be deprecated and aliases for PROPERTIES and FILE!

When a new resource provider is registering itself or an update of resources takes place, merging of resources should be done immediately - the sync first does some general sanity checks on the incoming data and then uses URL and digest of the data to check for updates/removes. If a resource with the same URL and digest is already available, it is assumed to be the same and no further processing is required! This avoids unnecessary copies.

We further decide between incoming resource type (usually PROPERTIES or FILE) and processing resource type. The new resource transformer services are responsible for this transformation (maybe in combination with data transformation). A resource transformer can omit one or more resources with resource type and data.
If a resource has a processing resource type, this resource is used to be processed by the osgi installer task factories. As long as the resource has no processing resource type, it is not processed.","Duplicated Code, Long Method, , , "
"   Rename Method,Move Method,Extract Method,","Improve the (internal) resource handling The current resource handling has some drawbacks - one is that a resource is always copied into the local data store even if it did not change; another one is that integrating new concepts like the resource transformer etc is difficult and error prone.

The first thing we should change is the resource types, currently we have CONFIG and BUNDLE - this assumes that the client knows what resources it is providing - however it is not the task of the client to decide this; in many cases the client has even not the knowledge about it. Therefore we should introduce two new resource types PROPERTIES and FILE.
However, if the client really knows what it is dealing with, it can use one of the two new types OSGI_BUNDLE or OSGI_CONFIG - we introduce these new types, to be more compatible - the old constants CONFIG and BUNDLE will be deprecated and aliases for PROPERTIES and FILE!

When a new resource provider is registering itself or an update of resources takes place, merging of resources should be done immediately - the sync first does some general sanity checks on the incoming data and then uses URL and digest of the data to check for updates/removes. If a resource with the same URL and digest is already available, it is assumed to be the same and no further processing is required! This avoids unnecessary copies.

We further decide between incoming resource type (usually PROPERTIES or FILE) and processing resource type. The new resource transformer services are responsible for this transformation (maybe in combination with data transformation). A resource transformer can omit one or more resources with resource type and data.
If a resource has a processing resource type, this resource is used to be processed by the osgi installer task factories. As long as the resource has no processing resource type, it is not processed.","Duplicated Code, Long Method, , , "
"   Rename Class,Extract Method,","Improve the (internal) resource handling The current resource handling has some drawbacks - one is that a resource is always copied into the local data store even if it did not change; another one is that integrating new concepts like the resource transformer etc is difficult and error prone.

The first thing we should change is the resource types, currently we have CONFIG and BUNDLE - this assumes that the client knows what resources it is providing - however it is not the task of the client to decide this; in many cases the client has even not the knowledge about it. Therefore we should introduce two new resource types PROPERTIES and FILE.
However, if the client really knows what it is dealing with, it can use one of the two new types OSGI_BUNDLE or OSGI_CONFIG - we introduce these new types, to be more compatible - the old constants CONFIG and BUNDLE will be deprecated and aliases for PROPERTIES and FILE!

When a new resource provider is registering itself or an update of resources takes place, merging of resources should be done immediately - the sync first does some general sanity checks on the incoming data and then uses URL and digest of the data to check for updates/removes. If a resource with the same URL and digest is already available, it is assumed to be the same and no further processing is required! This avoids unnecessary copies.

We further decide between incoming resource type (usually PROPERTIES or FILE) and processing resource type. The new resource transformer services are responsible for this transformation (maybe in combination with data transformation). A resource transformer can omit one or more resources with resource type and data.
If a resource has a processing resource type, this resource is used to be processed by the osgi installer task factories. As long as the resource has no processing resource type, it is not processed.","Duplicated Code, Long Method, , "
"   Rename Method,",Remove dependency to web console Since some versions the web console provides a way to register plugins without directly depending on the web console api - we should use this as well,", "
"   Rename Method,",Remove dependency to web console Since some versions the web console provides a way to register plugins without directly depending on the web console api - we should use this as well,", "
"   Rename Method,","Launchpad installer should not depend on SCR The current launchpad installer implementation requires SCR - we should rather implement an activator with a service listener. As soon as the osgi installer service and the resource provider is available, this service should start it's action",", "
"   Rename Method,","Launchpad installer should not depend on SCR The current launchpad installer implementation requires SCR - we should rather implement an activator with a service listener. As soon as the osgi installer service and the resource provider is available, this service should start it's action",", "
"   Move Class,Move Method,Move Attribute,","Support for server-side JUnit tests, injected in a running Sling instance I've been working on a prototype JUnit extension framework [1] that allows JUnit tests to be injected in a Sling instance, and executed via a test runner servlet.

Tests are not yet OSGi-aware in the first prototype, but they are dynamically injected as exported classes in a bundle that points to them with a Test-Package header. Apart from that they are 100% normal JUnit3 or 4 tests.

Next step is to use annotations to inject services in the tests, I'm thinking of something like

@TestReference
SlingRepository repository

And maybe

@Test
@OptionalTest(requiredService=repository)
public void someTest()...

which would ignore the test if the repository service is not present.

The prototype consists of two bundles: ""extension"" which is the test detection and runner services, and ""testbundle"" which provides a few example tests. To play with it, install the two bundles and access the test servlet at /system/sling/junit/

[1] http://svn.apache.org/repos/asf/sling/whiteboard/bdelacretaz/junit",", , , "
"   Move Class,Move Method,Extract Method,Move Attribute,","Support for server-side JUnit tests, injected in a running Sling instance I've been working on a prototype JUnit extension framework [1] that allows JUnit tests to be injected in a Sling instance, and executed via a test runner servlet.

Tests are not yet OSGi-aware in the first prototype, but they are dynamically injected as exported classes in a bundle that points to them with a Test-Package header. Apart from that they are 100% normal JUnit3 or 4 tests.

Next step is to use annotations to inject services in the tests, I'm thinking of something like

@TestReference
SlingRepository repository

And maybe

@Test
@OptionalTest(requiredService=repository)
public void someTest()...

which would ignore the test if the repository service is not present.

The prototype consists of two bundles: ""extension"" which is the test detection and runner services, and ""testbundle"" which provides a few example tests. To play with it, install the two bundles and access the test servlet at /system/sling/junit/

[1] http://svn.apache.org/repos/asf/sling/whiteboard/bdelacretaz/junit","Duplicated Code, Long Method, , , , "
"   Rename Method,","Support for server-side JUnit tests, injected in a running Sling instance I've been working on a prototype JUnit extension framework [1] that allows JUnit tests to be injected in a Sling instance, and executed via a test runner servlet.

Tests are not yet OSGi-aware in the first prototype, but they are dynamically injected as exported classes in a bundle that points to them with a Test-Package header. Apart from that they are 100% normal JUnit3 or 4 tests.

Next step is to use annotations to inject services in the tests, I'm thinking of something like

@TestReference
SlingRepository repository

And maybe

@Test
@OptionalTest(requiredService=repository)
public void someTest()...

which would ignore the test if the repository service is not present.

The prototype consists of two bundles: ""extension"" which is the test detection and runner services, and ""testbundle"" which provides a few example tests. To play with it, install the two bundles and access the test servlet at /system/sling/junit/

[1] http://svn.apache.org/repos/asf/sling/whiteboard/bdelacretaz/junit",", "
"   Move Method,Extract Method,","Persist configuration changes not made through the installer Currently, config and bundle changes done through other ways than the installer (like web console) are not persisted - sometimes creating confusion and making changes for example in a clustered env more error prone.

The providers should register a write back hook and the installer core detects changes and invokes the correct hook to persist the configuration","Duplicated Code, Long Method, , , "
"   Rename Method,","Extend ResourceResolver to make it more flexible As a result of defining a virtual resource tree (SLING-197) we have a need to modify the ResourceResolver API in two respects:

(1) Add ResourceResolver.resolve(String absPath)

This method behaves exactly as ResourceResolver.resolve(HttpServletRequest) except, that the latter method may make use of additional request properties, such as request headers or parameters while the resolve(String) method only has the string to work on.

Currently the resolve(HttpServletRequest) method does nothing more than use the HttpServletRequest.getPathInfo() to resolve the resource, thus both implementations would actually be equivalent.

The absPath argument is an absolute path. Resolution fails for relative paths.


(2) Support relative paths in ResourceResolver.getResource(String path)

Currently this method is defined to throw a SlingException if the path is relative. This should be changed such that the ResourceResolver applies some search path logic to find a resource with the given relative path

The search path logic is comparable to how *nix systems use the PATH environment variable.

This method may then be used by multiple users such as Servlet/Script resolution.


(3) Add ResourceResolver.map(String) method

This method applies the reverse mappings of the ResourceResolver.resolve(String absPath) method to return a path suitable for both resolver() methods. This allows for the creation of link paths for resources.
",", "
"   Rename Method,","Extend ResourceResolver to make it more flexible As a result of defining a virtual resource tree (SLING-197) we have a need to modify the ResourceResolver API in two respects:

(1) Add ResourceResolver.resolve(String absPath)

This method behaves exactly as ResourceResolver.resolve(HttpServletRequest) except, that the latter method may make use of additional request properties, such as request headers or parameters while the resolve(String) method only has the string to work on.

Currently the resolve(HttpServletRequest) method does nothing more than use the HttpServletRequest.getPathInfo() to resolve the resource, thus both implementations would actually be equivalent.

The absPath argument is an absolute path. Resolution fails for relative paths.


(2) Support relative paths in ResourceResolver.getResource(String path)

Currently this method is defined to throw a SlingException if the path is relative. This should be changed such that the ResourceResolver applies some search path logic to find a resource with the given relative path

The search path logic is comparable to how *nix systems use the PATH environment variable.

This method may then be used by multiple users such as Servlet/Script resolution.


(3) Add ResourceResolver.map(String) method

This method applies the reverse mappings of the ResourceResolver.resolve(String absPath) method to return a path suitable for both resolver() methods. This allows for the creation of link paths for resources.
",", "
"   Move And Rename Class,","JUnitServlet should be replaceable The JUnitServlet of the testing/junit/core module should be replaceable, which means:

-Factoring out the actual test selection, listing and execution logic in a separate service
-Allowing for disabling the servlet or mounting it under a different path, by configuration",", "
"   Rename Class,Move Method,Move Attribute,","JUnitServlet should be replaceable The JUnitServlet of the testing/junit/core module should be replaceable, which means:

-Factoring out the actual test selection, listing and execution logic in a separate service
-Allowing for disabling the servlet or mounting it under a different path, by configuration",", , , "
"   Move And Rename Class,","JUnitServlet should be replaceable The JUnitServlet of the testing/junit/core module should be replaceable, which means:

-Factoring out the actual test selection, listing and execution logic in a separate service
-Allowing for disabling the servlet or mounting it under a different path, by configuration",", "
"   Rename Method,","JUnitServlet should be replaceable The JUnitServlet of the testing/junit/core module should be replaceable, which means:

-Factoring out the actual test selection, listing and execution logic in a separate service
-Allowing for disabling the servlet or mounting it under a different path, by configuration",", "
"   Move Class,Extract Method,","Move properties support to own class Currently the OsgiUtil class has two purposes : support for property/configuration handling and support for event creation.
When embedding this class in another bundle, this bundle requires an import to the event package although it is never really used.

Therefore we should split the OsgiUtil class in several classes - each with a specific purpose. The OsgiUtil implementation will then just call the methods from the new classes.","Duplicated Code, Long Method, , "
"   Move Class,Extract Method,","Move properties support to own class Currently the OsgiUtil class has two purposes : support for property/configuration handling and support for event creation.
When embedding this class in another bundle, this bundle requires an import to the event package although it is never really used.

Therefore we should split the OsgiUtil class in several classes - each with a specific purpose. The OsgiUtil implementation will then just call the methods from the new classes.","Duplicated Code, Long Method, , "
"   Rename Method,","When creating the initial configuration use relative path names for the repository configuration and home The Jackrabbit server creates a configuration on initial startup if none is available from the Configuration Admin service. This configuration will contain absolute path names to the repository configuration file as well as the repository home directory. This makes it hard to relocate the Sling Home folder because the configuration will still point to the old location.

Instead the default configuration (unless overwritten by some framework property) should use relative path names to be resolved against sling.home once the repository is started/created.",", "
"   Move Class,Rename Method,Move Method,Extract Method,Move Attribute,","Provide a Sling servlet to run server-side JUnit tests The testing/junit/core module currently uses a plain servlet that runs outside of the Sling request cycle.

However, some tests depend on custom Sling Filters, for example, and as such need to run in the Sling request cycle.

To keep the junit/core module reusable outside of Sling, I'll implement a second servlet, that will be active only in a Sling environment. The existing plain servlet can be disabled by configuration if needed.
","Duplicated Code, Long Method, , , , "
"   Move Class,Move Attribute,","Provide a Sling servlet to run server-side JUnit tests The testing/junit/core module currently uses a plain servlet that runs outside of the Sling request cycle.

However, some tests depend on custom Sling Filters, for example, and as such need to run in the Sling request cycle.

To keep the junit/core module reusable outside of Sling, I'll implement a second servlet, that will be active only in a Sling environment. The existing plain servlet can be disabled by configuration if needed.
",", , "
"   Move Class,Move Method,Extract Method,Inline Method,Move Attribute,","Replace file logger with logback Rather than doing the work internally of writing log messages to disk, the internal log mechanism should use logback to handle the heavy lifting. This also adds the ability to add appenders and other nifty logback features.","Duplicated Code, Long Method, , , , , "
"   Rename Method,","Clean up content data and request data handling ContentData and RequestData have both a dispose() method which actually is not really used. We could completly remove them
In addition we can remove the stack of ContentData objects from RequestData by handling this locally in the SlingRequestProcessImpl#dispatch method

This further simplifies the data objects.",", "
"   Rename Method,","Clean up content data and request data handling ContentData and RequestData have both a dispose() method which actually is not really used. We could completly remove them
In addition we can remove the stack of ContentData objects from RequestData by handling this locally in the SlingRequestProcessImpl#dispatch method

This further simplifies the data objects.",", "
"   Rename Method,","Properly provide locale inheritance support The ResourceBundle class is intended to be used with inheritance of resources from parent resource bundles. Thus the ResourceBundle.getObject(String) is implemented along these lines:

Object obj = handleGetObject(key);
if (obj == null) {
if (parent != null) {
obj = parent.getObject(key);
}
if (obj == null) {
throw new MissingResourceException(...)
}
}
return obj;

The JcrResourceBundle.handleObject(String key) method is implemented like this:

Object value = // get from the repository resources
return (value == null) ? key : value;

That is the key is returned if there is no value. This, though, breaks the inheritance chain intended by the ResourceBundle.getObject(String) method. We should fix this along these lines:

* The JcrResourceBundleProvider provides a root ResourceBundle implementation as follows:
- handleObject(String key) always returns the key
- getLocale() returns an empty Locale
- getKeys() always returns an empty enumeration
* The root ResourceBundle is used as the parent for all JcrResourceBundle instances which have no Locale induced parent
* JcrResourceBundle.handleGetObject(String key) only returns the value for the given locale

This way we can keep the guarantee that getObject(String key) method of a ResourceBundle provided by the JcrResourceBundleProvider never throws a MissingResourceException.",", "
"   Rename Method,","Properly provide locale inheritance support The ResourceBundle class is intended to be used with inheritance of resources from parent resource bundles. Thus the ResourceBundle.getObject(String) is implemented along these lines:

Object obj = handleGetObject(key);
if (obj == null) {
if (parent != null) {
obj = parent.getObject(key);
}
if (obj == null) {
throw new MissingResourceException(...)
}
}
return obj;

The JcrResourceBundle.handleObject(String key) method is implemented like this:

Object value = // get from the repository resources
return (value == null) ? key : value;

That is the key is returned if there is no value. This, though, breaks the inheritance chain intended by the ResourceBundle.getObject(String) method. We should fix this along these lines:

* The JcrResourceBundleProvider provides a root ResourceBundle implementation as follows:
- handleObject(String key) always returns the key
- getLocale() returns an empty Locale
- getKeys() always returns an empty enumeration
* The root ResourceBundle is used as the parent for all JcrResourceBundle instances which have no Locale induced parent
* JcrResourceBundle.handleGetObject(String key) only returns the value for the given locale

This way we can keep the guarantee that getObject(String key) method of a ResourceBundle provided by the JcrResourceBundleProvider never throws a MissingResourceException.",", "
"   Rename Class,Move And Rename Class,Rename Method,",Upgrade Groovy to 1.8 Groovy 1.8 has been released. We should upgrade.,", "
"   Push Down Method,Push Down Attribute,","Add support for partial bundle lists A project should be able to define a bundle list which then can be applied as a unit to some other bundle list.

See http://markmail.org/message/qvil2wqdkh26zbhs for more information",", , , "
"   Move Method,Move Attribute,","Apply some validation to requested redirects after authentication Currently the DefaultAuthenticationFeedbackHandler.handleRedirect and AbstractAuthenticationHandler.sendRedirect methods do not apply any validity checks on the requested redirect target.

We should apply some checks to ensure a valid target is accessible within the Sling application. If the target is not valid, the methods would redirect to the servlet context root path -- obeying the contract for redirecting the client but not necessairily to the desired target. In any case an ERROR level message is written to the log indicating why the redirect target is not being honoured.

This check should be made available to AuthenticationHandler implementations such that they may apply checks to their own redirects.",", , , "
"   Move Method,Move Attribute,","Apply some validation to requested redirects after authentication Currently the DefaultAuthenticationFeedbackHandler.handleRedirect and AbstractAuthenticationHandler.sendRedirect methods do not apply any validity checks on the requested redirect target.

We should apply some checks to ensure a valid target is accessible within the Sling application. If the target is not valid, the methods would redirect to the servlet context root path -- obeying the contract for redirecting the client but not necessairily to the desired target. In any case an ERROR level message is written to the log indicating why the redirect target is not being honoured.

This check should be made available to AuthenticationHandler implementations such that they may apply checks to their own redirects.",", , , "
"   Rename Class,Move Method,Extract Method,Move Attribute,","ujax post servlet should respond with status page instead of default redirect it is desirable that the ujax post serlvet responds with a status page rather than with a default redirect.
this allows clients (javascript+formpost or ajax based) to react better on the modifications.

will provide a patch for the post servlet that responds with that status page. the old redirect behavior can still be achieved by sending a ujax:redirect=""*"" input parameter.","Duplicated Code, Long Method, , , , "
"   Rename Class,Move Method,Extract Method,Move Attribute,","ujax post servlet should respond with status page instead of default redirect it is desirable that the ujax post serlvet responds with a status page rather than with a default redirect.
this allows clients (javascript+formpost or ajax based) to react better on the modifications.

will provide a patch for the post servlet that responds with that status page. the old redirect behavior can still be achieved by sending a ujax:redirect=""*"" input parameter.","Duplicated Code, Long Method, , , , "
"   Rename Method,","ujax post servlet should respond with status page instead of default redirect it is desirable that the ujax post serlvet responds with a status page rather than with a default redirect.
this allows clients (javascript+formpost or ajax based) to react better on the modifications.

will provide a patch for the post servlet that responds with that status page. the old redirect behavior can still be achieved by sending a ujax:redirect=""*"" input parameter.",", "
"   Move Method,Extract Method,Move Attribute,",Support write back of configurations The file installer should support writing back changed configurations,"Duplicated Code, Long Method, , , , "
"   Move Method,Extract Method,Move Attribute,",Support write back of configurations The file installer should support writing back changed configurations,"Duplicated Code, Long Method, , , , "
"   Rename Method,Extract Method,","Leverage Framework interface to better control framework startup During startup the bootstrap installer may install, update or uninstall system extension fragments which require the framework to be restarted for the action to properly complete.

The current implementation solves the problem like this:

- initialize the framework (Framework.init())
- start the framework into startlevel 1 (Framework.start())
- call bootstrap installer for install, update, uninstall
- check whether a restart is required:
- if yes: restart
- otherwise: set start level to originally requested start level

This is clumsy and dilutes the startup. Particularly the FRAMEWORK_STARTED event is fired without the framework startup to actually have completed.

To fix this we can solve this easily using the Framework interface like this:

Framework tmpFramework = createFramework(notifiable, logger, props);
init(tmpFramework);
if (new BootstrapInstaller(tmpFramework.getBundleContext(), logger, resourceProvider).install()) {
init(tmpFramework);
}
tmpFramework.start();
this.framework = tmpFramework;","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Leverage Framework interface to better control framework startup During startup the bootstrap installer may install, update or uninstall system extension fragments which require the framework to be restarted for the action to properly complete.

The current implementation solves the problem like this:

- initialize the framework (Framework.init())
- start the framework into startlevel 1 (Framework.start())
- call bootstrap installer for install, update, uninstall
- check whether a restart is required:
- if yes: restart
- otherwise: set start level to originally requested start level

This is clumsy and dilutes the startup. Particularly the FRAMEWORK_STARTED event is fired without the framework startup to actually have completed.

To fix this we can solve this easily using the Framework interface like this:

Framework tmpFramework = createFramework(notifiable, logger, props);
init(tmpFramework);
if (new BootstrapInstaller(tmpFramework.getBundleContext(), logger, resourceProvider).install()) {
init(tmpFramework);
}
tmpFramework.start();
this.framework = tmpFramework;","Duplicated Code, Long Method, , "
"   Pull Up Method,Pull Up Attribute,","Provide a resource bundle for all requests Currently only if a request is served by the Sling Main Servlet, the resource bundle provider and the i18n filter is called.
However, if the request is not directly served by Sling we could still support i18n:

We define a new interface RequestLocaleResolver which just takes a HttpServletRequest - this service can get the resource resolver from the request attribute (see authentication support). As the HttpServletRequest has no method to get a resource bundle we store the resource bundle as a request attribute.
To reduce the overhead, we only create the bundle on request, so only if the request attribute is queried we lazily create the resource bundle

If a RequestLocaleResolver is available it has precedence of a LocaleResolver - in addition we deprecate the LocaleResolver",", Duplicated Code, Duplicated Code, "
"   Pull Up Method,Pull Up Attribute,","Provide a resource bundle for all requests Currently only if a request is served by the Sling Main Servlet, the resource bundle provider and the i18n filter is called.
However, if the request is not directly served by Sling we could still support i18n:

We define a new interface RequestLocaleResolver which just takes a HttpServletRequest - this service can get the resource resolver from the request attribute (see authentication support). As the HttpServletRequest has no method to get a resource bundle we store the resource bundle as a request attribute.
To reduce the overhead, we only create the bundle on request, so only if the request attribute is queried we lazily create the resource bundle

If a RequestLocaleResolver is available it has precedence of a LocaleResolver - in addition we deprecate the LocaleResolver",", Duplicated Code, Duplicated Code, "
"   Rename Method,","Add a listener to the installer We should add a listener to the installer which allows to keep track of the actions done by the installer.
As a first step, we could notify the start of an installation cycle, the suspension of the installer and a processed resource",", "
"   Rename Method,","Add a listener to the installer We should add a listener to the installer which allows to keep track of the actions done by the installer.
As a first step, we could notify the start of an installation cycle, the suspension of the installer and a processed resource",", "
"   Rename Method,","Allow for better configuration of sling home folder Currently a few files and folders in the ${sling.home} folder can be more freely located than others:

* The org.apache.sling.launchpad.base.jar file is expected inside ${sling.home}, hardcoded
* The sling.properties file is expected inside ${sling.home}, hardcoded
* The start folder containing initial bundles to install is expected inside ${sling.home}, hardcoded
* The Felix framework cache location is configured by the org.osgi.framework.storage property
* The location of Configuration Admin configuration is configured by the felix.cm.dir property
* The location for File Installer (of the OSGi Install facility) is configured by the sling.installer.dir property

To have more flexibility, two new properties should be added:

* sling.properties (default: ${sling.home}/sling.properties) -- Provides the path and name of the sling.properties file contains the configurable properties used for starting Sling and then becoming Framework properties accessible with the BundleContext.getProperty(String) method. This property itself is also available as a Framework property and will be set to the default value by the Sling Launcher if not already set. The property must not contain a reference to another property but if it is not an absolute path, it is resolved against ${sling.home}.

* sling.launchpad (default: ${sling.home}) Defines the location of Sling Launchpad related files and folders. At the moment this is the location of the org.apache.sling.launchpad.base.jar library which provides the OSGi Framework and the startup folder which takes bundles to install on framework startup. The property must not contain a reference to another property but if it is not an absolute path, it is resolved against ${sling.home}.

The defaults are defined such, that they account for backwards compatibility.

From a little code inspection, probably the following areas have to be updated:
* The Sling class must ensure the property values
* The Sling class (loadConfigProperties method) must be modified to read the sling.properties file from the location indicated by the ${sling.properties} property
* The Launcher class must be modified to look for and place the launcher jar in the ${sling.launchpad} folder (instead of ${sling.home})
* The BootstrapInstaller class must be modified to expect bundles in the ${sling.launchpad}/startup folder",", "
"   Rename Method,","add an SPI interface for injecting custom/alternate PostResponse implementations This was discussed in SLING-2156, but not necessarily tied to that issue.

Basically, it should be possible to implement an interface which can produce an implementation of the PostResponse interface.",", "
"   Rename Method,","add an SPI interface for injecting custom/alternate PostResponse implementations This was discussed in SLING-2156, but not necessarily tied to that issue.

Basically, it should be possible to implement an interface which can produce an implementation of the PostResponse interface.",", "
"   Rename Method,Move Method,Extract Method,","Improve support for embedding the Sling Launcher in Java applications The only way to embed the Sling launcher today is to build an array of command line options and call the static Main.main(String[] args) method from the application. This is kind of weird and only allows a small subset of configuration properties to be easily supplied by the calling application.

In addition the handling of command line options is currently split between the Main and the MainDelagate class: The Main class implements the usage functionality (for the -h command line option) while the MainDelegate class implements the actual conversion of command line options to internal configuration property. This should be improved in that the support for real command lines should be consolidated in the Main class and the map of configuration properties supplied to the MainDelagate class should only contain actual configuration properties.","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Improve JcrResourceResolver#resolve performance when big number of vanityPath are present At the moment the performance of JcrResourceResolver#resolve is tight with the number of sling:vanityPath present in the repository.
Large number of vanityPath means large response time specially in the worse case scenario (namely huge number of vanityPath and request that doesn't match any vanityPath) but also in the average cases.
Sling currently employs generic regexps also for vanityPath, but since the regex behind a vanityPath is well know there is room for optimization.
I'll attach a graphs that shows the situation and a potential patch.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Improve JcrResourceResolver#resolve performance when big number of vanityPath are present At the moment the performance of JcrResourceResolver#resolve is tight with the number of sling:vanityPath present in the repository.
Large number of vanityPath means large response time specially in the worse case scenario (namely huge number of vanityPath and request that doesn't match any vanityPath) but also in the average cases.
Sling currently employs generic regexps also for vanityPath, but since the regex behind a vanityPath is well know there is room for optimization.
I'll attach a graphs that shows the situation and a potential patch.","Duplicated Code, Long Method, , "
"   Move Method,Extract Method,",create an AbstractMavenLifecycleParticipant implementation to add artifacts from the bundle list to the dependency list The capability created here: http://jira.codehaus.org/browse/MNG-4224 should allow us to add non-pom dependencies from an external bundle list file into the dependency list for the project BEFORE the execution order is calculated.,"Duplicated Code, Long Method, , , "
"   Move Method,Extract Method,",create an AbstractMavenLifecycleParticipant implementation to add artifacts from the bundle list to the dependency list The capability created here: http://jira.codehaus.org/browse/MNG-4224 should allow us to add non-pom dependencies from an external bundle list file into the dependency list for the project BEFORE the execution order is calculated.,"Duplicated Code, Long Method, , , "
"   Move Method,Extract Method,","Implement generic retry mechanism Currently, if a task fails it is retried over and over again which basically is kind of an endless loop.
However, if between two retries nothing has changed, it is most likely that a retry is failing again. We have some basic code for bundle handling, but this is a) tied to bundles, b) not reusable, and c) not working perfectly.

Instead we should come up with a generic mechanism for retries - a task should signal that it want's to be retried. In addition we need a notification mechanism which notifies the installer to retry something, e.g. a service listening for bundle events would notify the installer each time a bundle event has occurred etc.","Duplicated Code, Long Method, , , "
"   Move Method,Extract Method,","Implement generic retry mechanism Currently, if a task fails it is retried over and over again which basically is kind of an endless loop.
However, if between two retries nothing has changed, it is most likely that a retry is failing again. We have some basic code for bundle handling, but this is a) tied to bundles, b) not reusable, and c) not working perfectly.

Instead we should come up with a generic mechanism for retries - a task should signal that it want's to be retried. In addition we need a notification mechanism which notifies the installer to retry something, e.g. a service listening for bundle events would notify the installer each time a bundle event has occurred etc.","Duplicated Code, Long Method, , , "
"   Pull Up Method,Move Method,Extract Method,Move Attribute,","Updating a fragment with a different version but the same content does not work If you update a bundle through a JCR package, it is only updated in Apache Felix if the actual content has changed. It is not enough to have the version number changed. This is a problem, because usually in multimodule maven builds I increase the version for all contained bundles although they might not have changed in that release. Unfortunately the new version is not deployed, therefore the Webconsole still shows the old version.
The same problem applies to Bundle Fragments.","Duplicated Code, Long Method, , , , Duplicated Code, "
"   Extract Method,Inline Method,","Provide support for versioned resources Currently, a TransformationResult can only set an id and arbitrary attributes. There is currently no support for a specific version attribute which in turn can be picked up by the core to compare resources.","Duplicated Code, Long Method, , , "
"   Extract Method,Inline Method,","Provide support for versioned resources Currently, a TransformationResult can only set an id and arbitrary attributes. There is currently no support for a specific version attribute which in turn can be picked up by the core to compare resources.","Duplicated Code, Long Method, , , "
"   Move Method,Move Attribute,",Add getters and setters to defined entries For ease of use the SlingBindings and ResourceMetadata classes should be extended with explicit getters and setters for the defined map entries.,", , , "
"   Move Method,Extract Method,Move Attribute,",Add getters and setters to defined entries For ease of use the SlingBindings and ResourceMetadata classes should be extended with explicit getters and setters for the defined map entries.,"Duplicated Code, Long Method, , , , "
"   Rename Method,","JcrResourceListener: Asynchronously post Events to EventAdmin Currently the JcrResourceListener posts events to the OSGi EventAdmin as it processes the JCR events.

This may create a considerable delay on the repository observation queue processing because each call to the EventAdmin.postEvent must immediately extract the appropriate EventHandler services from the service registry. During JCR Event processing, we should only generate the OSGi events and post pone actual posting of events to a separate thread.",", "
"   Move Method,Extract Method,Move Attribute,","Detect startup mode Currently the bootstrap installer detects if a new startup contains newer bundles and installs/updates them accordingly

We could generalize this and provide a startup mode detection which is able to distinguish between a fresh startup (install), a simple restart or an update.","Duplicated Code, Long Method, , , , "
"   Move Method,Extract Method,Move Attribute,","Detect startup mode Currently the bootstrap installer detects if a new startup contains newer bundles and installs/updates them accordingly

We could generalize this and provide a startup mode detection which is able to distinguish between a fresh startup (install), a simple restart or an update.","Duplicated Code, Long Method, , , , "
"   Rename Class,Move Class,","New Startup Features With SLING-2372 we have now a mechanism to decide what kind of startup currently is processed

We could leverage this to create new functionality:

Enhance lauchpad api with the startup mode, the possibility to register listener which are informed about the startup mode and progress.
Another feature would be to not directly go to the beginning start level of the framework, but stop at an intermediate level on installs and updates and from there increase the start level one by one.
This makes updates and installs much smoother

For more information I'll write an email to the dev list :)",", "
"   Move Class,Extract Method,Move Attribute,","Make RepositoryTestBase's repository usable from JUnit 4.x style tests Tests currently need to inherit from that class to get a TestRepository, which forces them to use the old JUnit3.x style.

I'll add a static method to that class, that JUnit 4.x tests can use to get a repository.","Duplicated Code, Long Method, , , "
"   Move Method,Move Attribute,","Separate request/access logging from the Sling Engine Currently the request and access log entries are generated in the Sling Engine bundle. The consequence of this is, that only requests going through the Sling Main Servlet are actually logged.

The fix is to hook the request and access logging infrastructure into the serlvet container as a Servlet API Filter such as to run it as early and late as possible and more importantly on a global level to catch all requests.

In addition, since this filter has nothing to do with the Sling Engine, it would make sense to create a bundle in the commons area along with the commons/log and commons/logservice bundles.
",", , , "
"   Move Method,Move Attribute,","Separate request/access logging from the Sling Engine Currently the request and access log entries are generated in the Sling Engine bundle. The consequence of this is, that only requests going through the Sling Main Servlet are actually logged.

The fix is to hook the request and access logging infrastructure into the serlvet container as a Servlet API Filter such as to run it as early and late as possible and more importantly on a global level to catch all requests.

In addition, since this filter has nothing to do with the Sling Engine, it would make sense to create a bundle in the commons area along with the commons/log and commons/logservice bundles.
",", , , "
"   Move Class,Move And Rename Class,Move Method,Extract Method,Move Attribute,","Make ResourceResolverFactory independent from JCR As discussed on http://markmail.org/thread/wp6cghi5nqprpusn we would like to create a ResourceResolverFactory implementation that is not dependent on JCR. 
This will make it easier to create custom ResourceResolverFactories, where things like domain mappings, vanity paths and resource provider resolution could will not have to be re-implemented.

A whiteboard area for this has been created at http://svn.apache.org/repos/asf/sling/whiteboard/resourceresolverfactory","Duplicated Code, Long Method, , , , "
"   Move Class,Move And Rename Class,Move Method,Extract Method,Move Attribute,","Make ResourceResolverFactory independent from JCR As discussed on http://markmail.org/thread/wp6cghi5nqprpusn we would like to create a ResourceResolverFactory implementation that is not dependent on JCR. 
This will make it easier to create custom ResourceResolverFactories, where things like domain mappings, vanity paths and resource provider resolution could will not have to be re-implemented.

A whiteboard area for this has been created at http://svn.apache.org/repos/asf/sling/whiteboard/resourceresolverfactory","Duplicated Code, Long Method, , , , "
"   Rename Method,Move Method,Inline Method,Move Attribute,","Make ResourceResolverFactory independent from JCR As discussed on http://markmail.org/thread/wp6cghi5nqprpusn we would like to create a ResourceResolverFactory implementation that is not dependent on JCR. 
This will make it easier to create custom ResourceResolverFactories, where things like domain mappings, vanity paths and resource provider resolution could will not have to be re-implemented.

A whiteboard area for this has been created at http://svn.apache.org/repos/asf/sling/whiteboard/resourceresolverfactory",", , , , "
"   Move Method,Move Attribute,","Logging Panel not always provided The ""Sling Log Support"" Web Console page has a problem when the log bundle is started without Servlet API wiring. In this case the panel is not available and can only be made available upon rewiring the log bundle.

Fix:
- Register plugin as a ServiceFactory to lazily instantiate the class when required/used
- dynamically import the servlet API to lazily wire when required
",", , , "
"   Extract Method,Move Attribute,","Installer start event should already be sent when resources are provisioned Currently the installer reports started/suspended events from the run loop. When new resources arrive from a provider, the preparation of those might take some time (like copying them to the file system etc.) in this case, the started event is reported way later than the resources are provided. It would make more sense to report the started event right when new resources arrive or resources are removed","Duplicated Code, Long Method, , , "
"   Extract Method,Move Attribute,","Installer start event should already be sent when resources are provisioned Currently the installer reports started/suspended events from the run loop. When new resources arrive from a provider, the preparation of those might take some time (like copying them to the file system etc.) in this case, the started event is reported way later than the resources are provided. It would make more sense to report the started event right when new resources arrive or resources are removed","Duplicated Code, Long Method, , , "
"   Rename Class,Move Method,Extract Method,Inline Method,Move Attribute,","ClassLoaderWriter should provide class loader for loading written classes/resources As a follow up to SLING-2445 the ClassLoaderWriter should be enhanced to return a class loader which can be used to load the dynamically loaded classes written through this writer.
This writer should use the dynamic class loader as a parent and implement the DynamicClassLoader interface which allows to check if the class loader is still current.

The returned classloader should not be cached by clients, they should just get the class loader each time they require one. The writer ensures that always a fresh loader is returned.

The java and jsp scripting should use this class loader instead.","Duplicated Code, Long Method, , , , , "
"   Rename Class,Move Method,Extract Method,Inline Method,Move Attribute,","ClassLoaderWriter should provide class loader for loading written classes/resources As a follow up to SLING-2445 the ClassLoaderWriter should be enhanced to return a class loader which can be used to load the dynamically loaded classes written through this writer.
This writer should use the dynamic class loader as a parent and implement the DynamicClassLoader interface which allows to check if the class loader is still current.

The returned classloader should not be cached by clients, they should just get the class loader each time they require one. The writer ensures that always a fresh loader is returned.

The java and jsp scripting should use this class loader instead.","Duplicated Code, Long Method, , , , , "
"   Rename Method,Move Method,Extract Method,Inline Method,","ClassLoaderWriter should provide class loader for loading written classes/resources As a follow up to SLING-2445 the ClassLoaderWriter should be enhanced to return a class loader which can be used to load the dynamically loaded classes written through this writer.
This writer should use the dynamic class loader as a parent and implement the DynamicClassLoader interface which allows to check if the class loader is still current.

The returned classloader should not be cached by clients, they should just get the class loader each time they require one. The writer ensures that always a fresh loader is returned.

The java and jsp scripting should use this class loader instead.","Duplicated Code, Long Method, , , , "
"   Rename Method,Move Method,Extract Method,Inline Method,","ClassLoaderWriter should provide class loader for loading written classes/resources As a follow up to SLING-2445 the ClassLoaderWriter should be enhanced to return a class loader which can be used to load the dynamically loaded classes written through this writer.
This writer should use the dynamic class loader as a parent and implement the DynamicClassLoader interface which allows to check if the class loader is still current.

The returned classloader should not be cached by clients, they should just get the class loader each time they require one. The writer ensures that always a fresh loader is returned.

The java and jsp scripting should use this class loader instead.","Duplicated Code, Long Method, , , , "
"   Rename Method,","Replace Resource.getResourceProvider() by Resource.getResourceResolver() Currently the Resource provides access to the ResourceProvider which created the resource. This is not really practical and probably not correct at all because the ResourceProvider is something operating behind the scenes on behalf of the ResourceResolver. Thefore this method should be replaced by a method providing access to the ResourceResolver causing the Resource object to be created by the ResourceProvider.

Changes due:

Resource:
+ getResourceResolver()
- getResourceProvider()

ResourceProvider:
+ add ResourceResolver arguments to all methods creating Resource instances",", "
"   Rename Method,","Replace Resource.getResourceProvider() by Resource.getResourceResolver() Currently the Resource provides access to the ResourceProvider which created the resource. This is not really practical and probably not correct at all because the ResourceProvider is something operating behind the scenes on behalf of the ResourceResolver. Thefore this method should be replaced by a method providing access to the ResourceResolver causing the Resource object to be created by the ResourceProvider.

Changes due:

Resource:
+ getResourceResolver()
- getResourceProvider()

ResourceProvider:
+ add ResourceResolver arguments to all methods creating Resource instances",", "
"   Rename Method,Extract Method,","Replace Resource.getResourceProvider() by Resource.getResourceResolver() Currently the Resource provides access to the ResourceProvider which created the resource. This is not really practical and probably not correct at all because the ResourceProvider is something operating behind the scenes on behalf of the ResourceResolver. Thefore this method should be replaced by a method providing access to the ResourceResolver causing the Resource object to be created by the ResourceProvider.

Changes due:

Resource:
+ getResourceResolver()
- getResourceProvider()

ResourceProvider:
+ add ResourceResolver arguments to all methods creating Resource instances","Duplicated Code, Long Method, , "
"   Rename Method,","Replace Resource.getResourceProvider() by Resource.getResourceResolver() Currently the Resource provides access to the ResourceProvider which created the resource. This is not really practical and probably not correct at all because the ResourceProvider is something operating behind the scenes on behalf of the ResourceResolver. Thefore this method should be replaced by a method providing access to the ResourceResolver causing the Resource object to be created by the ResourceProvider.

Changes due:

Resource:
+ getResourceResolver()
- getResourceProvider()

ResourceProvider:
+ add ResourceResolver arguments to all methods creating Resource instances",", "
"   Move Method,Extract Method,Move Attribute,","Allow mapping nodes to internet domains Sling should support hosting multiple domains, with different JCR roots.
E.g.:
http://www.domain1.com could map to /content/domain1.com
http://www.domain2.com could map to /content/domain2.com

While developing a website, the fully qualified domain might not be available. Ideally, the mapping could be configured in a flexible way. One option would be to maintain a set of regular expressions to match against URLs. Each regexp would then match to a path in the JCR.","Duplicated Code, Long Method, , , , "
"   Move Method,Extract Method,Move Attribute,","Improve package refresh behaviour Right now when packages are installed or updated, there is a refresh on all bundles.
We could improve this by collecting all changed/installed bundles resp host bundles for fragments and just refresh those","Duplicated Code, Long Method, , , , "
"   Move Method,Extract Method,Move Attribute,","Improve package refresh behaviour Right now when packages are installed or updated, there is a refresh on all bundles.
We could improve this by collecting all changed/installed bundles resp host bundles for fragments and just refresh those","Duplicated Code, Long Method, , , , "
"   Pull Up Method,Pull Up Attribute,","Improve internal task handling Currently internal task handling like the restarting of bundles or package refreshs are not handled properly, especially they do not survice a restart of the installer.

I guess the easiest way would be to create artifical resources for this and handle them by special task factories. This would persists these tasks ootb and would also allow to add arbitrary information to each task",", Duplicated Code, Duplicated Code, "
"   Pull Up Method,Pull Up Attribute,","Improve internal task handling Currently internal task handling like the restarting of bundles or package refreshs are not handled properly, especially they do not survice a restart of the installer.

I guess the easiest way would be to create artifical resources for this and handle them by special task factories. This would persists these tasks ootb and would also allow to add arbitrary information to each task",", Duplicated Code, Duplicated Code, "
"   Extract Method,Inline Method,","Implement CRUD based on resources We need full CRUD support based on resources. In general we need some api which allows to create, update and delete resources. The method call have to be delegated to the underlying resource providers.","Duplicated Code, Long Method, , , "
"   Rename Method,","Implement CRUD based on resources We need full CRUD support based on resources. In general we need some api which allows to create, update and delete resources. The method call have to be delegated to the underlying resource providers.",", "
"   Rename Method,Inline Method,","Implement CRUD based on resources We need full CRUD support based on resources. In general we need some api which allows to create, update and delete resources. The method call have to be delegated to the underlying resource providers.",", , "
"   Rename Method,Inline Method,","Implement CRUD based on resources We need full CRUD support based on resources. In general we need some api which allows to create, update and delete resources. The method call have to be delegated to the underlying resource providers.",", , "
"   Rename Method,","Launchpad war should optionally use an external Repository, without requiring a change of bundles It would be useful to allow the Launchpad to use an external Repository (accessed via JNDI or RMI), without having to modify the war file or load/unload bundles.

I'll search for a solution along these lines:

1) Launchpad includes both the jackrabbit-server (embedded repository) and jackrabbit-client (access to external repositories via JNDI or RMI) bundles, but by default the jackrabbit-client does not provide a Repository.

2) At startup, the Sling class searches the classpath and/or environment for additional configuration properties

3) A specific configuration property prevents the jackrabbit-server bundle from providing a Repository, and lets the jackrabbit-client provide it.

In this way, the web container could be setup in advance to define which Repository to use, and new releases of the launchpad war file could be dropped in without requiring any configuration or war file changes.",", "
"   Rename Method,Move Method,Move Attribute,","Installer should detect if installer bundle is refreshed by another bundle update If e.g. the logging bundle is updated through the OSGi installer, this causes the logging bundle to be refreshed which in turn causes the OSGi installer to be restarted.
However as the OSGi installer is not aware of this fact, it is waiting for a package refresh event, and is holding references in an async thread. This is turn sometimes causes the bundle stop/start of the installer bundle to fail.

Therefore the OSGi installer should detect this situation and stop itself - the same as it already does for updating itself.
With the R4.3 wiring api, this is easily to detect; without it, it gets harder - we should have an implementation which detects whether R4.3 is available and uses this functionality. If its not available it should use a fallback",", , , "
"   Move Method,Extract Method,Move Attribute,","ResourceResolverFactory should only be available if specific ResourceProvider/Factories are registered The ResourceResolverFactory should only be available if a configurable set of providers is available-
We should add a string array property whose value can either be a pid or a filter expression (starting with a '(')
Only if for all configured values a matching provder/factory is available a RRFactory will be registered","Duplicated Code, Long Method, , , , "
"   Move Method,Extract Method,Move Attribute,","ResourceResolverFactory should only be available if specific ResourceProvider/Factories are registered The ResourceResolverFactory should only be available if a configurable set of providers is available-
We should add a string array property whose value can either be a pid or a filter expression (starting with a '(')
Only if for all configured values a matching provder/factory is available a RRFactory will be registered","Duplicated Code, Long Method, , , , "
"   Rename Method,","Improvement for the Sling performance tools Added a few improvements to the Sling performance testing tools:
1. Added the possibility to run each test method in a class as a performance test; until now a test was represented by a class (for each new test you had to add a new java class)
2. Added a PerformanceTest annotation that is used to discover the test methods in a java class
3. Added the possibility to provide with the PerformanceTest annotation a few configuration parameters for the test like the warmup time, run time , warm up invocations or run invocations; by default the warmuptime and runtime are used but a user can choose not to count on the time but to prefer setting the number of test invocations that he wants to be made during the test run
4. Created a new maven project that contains only the sling performance tests and left the framework(tool) related part in the base project as we would like to use the framework for performance testing in other projects also
5. Added the possibility to have parameters sent to test suite object, added a before suite and after suite method",", "
"   Rename Method,","Improvement for the Sling performance tools Added a few improvements to the Sling performance testing tools:
1. Added the possibility to run each test method in a class as a performance test; until now a test was represented by a class (for each new test you had to add a new java class)
2. Added a PerformanceTest annotation that is used to discover the test methods in a java class
3. Added the possibility to provide with the PerformanceTest annotation a few configuration parameters for the test like the warmup time, run time , warm up invocations or run invocations; by default the warmuptime and runtime are used but a user can choose not to count on the time but to prefer setting the number of test invocations that he wants to be made during the test run
4. Created a new maven project that contains only the sling performance tests and left the framework(tool) related part in the base project as we would like to use the framework for performance testing in other projects also
5. Added the possibility to have parameters sent to test suite object, added a before suite and after suite method",", "
"   Rename Method,","Improvement for the Sling performance tools Added a few improvements to the Sling performance testing tools:
1. Added the possibility to run each test method in a class as a performance test; until now a test was represented by a class (for each new test you had to add a new java class)
2. Added a PerformanceTest annotation that is used to discover the test methods in a java class
3. Added the possibility to provide with the PerformanceTest annotation a few configuration parameters for the test like the warmup time, run time , warm up invocations or run invocations; by default the warmuptime and runtime are used but a user can choose not to count on the time but to prefer setting the number of test invocations that he wants to be made during the test run
4. Created a new maven project that contains only the sling performance tests and left the framework(tool) related part in the base project as we would like to use the framework for performance testing in other projects also
5. Added the possibility to have parameters sent to test suite object, added a before suite and after suite method",", "
"   Move Class,Move Method,Move Attribute,","Provide job queues as JMX beans 
There should be an interface which one can query to get information about the status of a service implementing this interface.


eg.

public interface HealthCheckable {

public int getStatus();

}

For the return value for this method we could use:

static int OK = 0;
static int WARNING = 1;
static int CRITICAL = 2;
static int UNKNOWN = 3;

(these are the values which Nagios uses as return values for its plugins, see http://nagiosplug.sourceforge.net/developer-guidelines.html#AEN76).

The decision what value is returned is delegated to the service, so maybe they need to have some configuration to define the points, where a ""OK"" becomes ""WARNING"".


Via OSGI whiteboard pattern we can collect then all services providing status information and calculate an overall status of the system. 

",", , , "
"   Rename Method,","Use repository.url.override.property in jackrabbit-client bundle SLING-254 implements a mechanism where the jackrabbit-server bundle uses a system property named ""repository.url.override.property""
to override its configuration.

The same mechanism should be implemented in the jackrabbit-client bundle, to allow people to create Sling-based webapps that can be dropped in web containers and use a container-provided repository without requiring webapp configuration changes.",", "
"   Rename Method,","Use repository.url.override.property in jackrabbit-client bundle SLING-254 implements a mechanism where the jackrabbit-server bundle uses a system property named ""repository.url.override.property""
to override its configuration.

The same mechanism should be implemented in the jackrabbit-client bundle, to allow people to create Sling-based webapps that can be dropped in web containers and use a container-provided repository without requiring webapp configuration changes.",", "
"   Move Method,Extract Method,Move Attribute,","[Tooling] Logging framework for Slingclipse We need a Logging framework for Slingclipse.
I see two options at the moment:

- using a log framework as SLF4J logger or other similar
- using the embedded Eclipse logging framework","Duplicated Code, Long Method, , , , "
"   Move Method,Extract Method,Move Attribute,","[Tooling] Logging framework for Slingclipse We need a Logging framework for Slingclipse.
I see two options at the moment:

- using a log framework as SLF4J logger or other similar
- using the embedded Eclipse logging framework","Duplicated Code, Long Method, , , , "
"   Move Method,Move Attribute,","Manually trigger sync on files/directories We should be able to manually publish a subtree of content, as opposed to the sync happening in the background. One use case is where the auto-sync is disabled and the user only wants to publish the changes manually.",", , , "
"   Move Method,Move Attribute,","Manually trigger sync on files/directories We should be able to manually publish a subtree of content, as opposed to the sync happening in the background. One use case is where the auto-sync is disabled and the user only wants to publish the changes manually.",", , , "
"   Rename Method,","Web console plugin for tenant management Extending the support for Tenant API, we need to add a console plugin to create/remove tenants.
We also need a pluggable support to register tenant setup handler, so that various modules/implementations can be plugged in.",", "
"   Rename Method,","Web console plugin for tenant management Extending the support for Tenant API, we need to add a console plugin to create/remove tenants.
We also need a pluggable support to register tenant setup handler, so that various modules/implementations can be plugged in.",", "
"   Rename Method,Extract Method,","resource access security service for resource providers without backing ACLs Adding a minmal resource access gate as discussed in [1].
First step is to define the API interface and a minimal implementation which allows to define READ access (rest of CRUD can follow later)

[1] http://markmail.org/thread/4ctczoiy533tquyl","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","resource access security service for resource providers without backing ACLs Adding a minmal resource access gate as discussed in [1].
First step is to define the API interface and a minimal implementation which allows to define READ access (rest of CRUD can follow later)

[1] http://markmail.org/thread/4ctczoiy533tquyl","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","resource access security service for resource providers without backing ACLs Adding a minmal resource access gate as discussed in [1].
First step is to define the API interface and a minimal implementation which allows to define READ access (rest of CRUD can follow later)

[1] http://markmail.org/thread/4ctczoiy533tquyl","Duplicated Code, Long Method, , "
"   Move Class,Rename Class,Move Method,Move Attribute,","resource access security service for resource providers without backing ACLs Adding a minmal resource access gate as discussed in [1].
First step is to define the API interface and a minimal implementation which allows to define READ access (rest of CRUD can follow later)

[1] http://markmail.org/thread/4ctczoiy533tquyl",", , , "
"   Rename Method,","resource access security service for resource providers without backing ACLs Adding a minmal resource access gate as discussed in [1].
First step is to define the API interface and a minimal implementation which allows to define READ access (rest of CRUD can follow later)

[1] http://markmail.org/thread/4ctczoiy533tquyl",", "
"   Rename Method,Move Method,Extract Method,Move Attribute,","Define TenantManager API Tenants currently can only be administered (create, update, remove) through the Web Console. In addition the TenantProvider service interface allows for looking tenants up (read).

For administrative purposes it would be good to have a TenantManager service interface which allows for these administrative tasks. Something like:

public interface TenantManager extends TenantProvider {
Tenant create(String tenantId, Map<String, Object> properties);
void setProperty(Tenant tenant, String name, Object value);
void remove(Tenant tenant);
}","Duplicated Code, Long Method, , , , "
"   Rename Method,Move Method,Extract Method,Move Attribute,","Define TenantManager API Tenants currently can only be administered (create, update, remove) through the Web Console. In addition the TenantProvider service interface allows for looking tenants up (read).

For administrative purposes it would be good to have a TenantManager service interface which allows for these administrative tasks. Something like:

public interface TenantManager extends TenantProvider {
Tenant create(String tenantId, Map<String, Object> properties);
void setProperty(Tenant tenant, String name, Object value);
void remove(Tenant tenant);
}","Duplicated Code, Long Method, , , , "
"   Extract Superclass,Pull Up Method,Pull Up Attribute,","Add Resource Type inheritance As discussed on the dev-list [1], resource type inheritance for script/servlet resolution should be implemented as follows:

* define an optional property sling:resourceSuperType which may take a resource type used as 
the super type of a resource type. This property is stored at the node where the scripts
are located for the resource type or at the node itself. (see below)

* the resourceSuperType of a property resource is the resourceSuperType of the parent node
plus the property name.

* define a sling:ResourceSuperType mixin node type, which defines the sling:resourceType
property for it to be attachable to any node.

* Add Resource.getResourceSuperType() returning the super type of the resource type or
null if no such super type exists.

* Add support to the Servlet Resolver to resolve scripts not just for the resource type but also
for the super type (if defined). This super type resolution takes place before falling back
default scripts !


Sample:

/
+--- types
+--- type1
+--- type2
+--- sling:resourceSuperType = type1
+--- type3
+--- content
+--- en
+--- sling:resourceType = type2
+--- en
+--- sling:resourceType = type2
+--- sling:resourceSuperType = type3

The resource super types are defined as follows:
/types/type1 --> no super type
/types/type2 --> type1, due to sling:resoureSuperType property
/content/en --> type1, due to sling:resourceSuperType property in types/type2
/conent/de --> type3, due to sling:resourceSuperType property


[1] http://www.mail-archive.com/sling-dev@incubator.apache.org/msg02365.html",", Duplicated Code, Large Class, Duplicated Code, Duplicated Code, "
"   Extract Superclass,Pull Up Method,Pull Up Attribute,","Add Resource Type inheritance As discussed on the dev-list [1], resource type inheritance for script/servlet resolution should be implemented as follows:

* define an optional property sling:resourceSuperType which may take a resource type used as 
the super type of a resource type. This property is stored at the node where the scripts
are located for the resource type or at the node itself. (see below)

* the resourceSuperType of a property resource is the resourceSuperType of the parent node
plus the property name.

* define a sling:ResourceSuperType mixin node type, which defines the sling:resourceType
property for it to be attachable to any node.

* Add Resource.getResourceSuperType() returning the super type of the resource type or
null if no such super type exists.

* Add support to the Servlet Resolver to resolve scripts not just for the resource type but also
for the super type (if defined). This super type resolution takes place before falling back
default scripts !


Sample:

/
+--- types
+--- type1
+--- type2
+--- sling:resourceSuperType = type1
+--- type3
+--- content
+--- en
+--- sling:resourceType = type2
+--- en
+--- sling:resourceType = type2
+--- sling:resourceSuperType = type3

The resource super types are defined as follows:
/types/type1 --> no super type
/types/type2 --> type1, due to sling:resoureSuperType property
/content/en --> type1, due to sling:resourceSuperType property in types/type2
/conent/de --> type3, due to sling:resourceSuperType property


[1] http://www.mail-archive.com/sling-dev@incubator.apache.org/msg02365.html",", Duplicated Code, Large Class, Duplicated Code, Duplicated Code, "
"   Rename Method,","Make ResourceMetadata read-only when delivered to client code As recently discussed in the mailing list, ResourceMetadata is an object which provides additional metadata information about a resource but is not intended to be changed by client code.

As ResourceMetadata extends from (Hash)Map it is read/write by default and might potentially be changed by client code.
We should update the API docs that this object is read-only and also enforce it in our implementation.

It seems so far no one is changing the ResourceMetadata after it has left the resource resolver, therefore we can make it read-only after it is returned by the resource resolver.",", "
"   Move Class,Move Method,","Optionally run Sling on Apache OAK It would be nice to have a runMode of Sling that runs on top of Apache OAK [0]


[0] http://jackrabbit.apache.org/oak/",", , "
"   Rename Method,","Optionally run Sling on Apache OAK It would be nice to have a runMode of Sling that runs on top of Apache OAK [0]


[0] http://jackrabbit.apache.org/oak/",", "
"   Move Class,Move And Rename Class,Extract Interface,Rename Method,","Simplify the Sling (aka Component) API JIRA issue to track simplification of the Sling (aka Component) API.

See http://www.mail-archive.com/sling-dev@incubator.apache.org/msg00177.html for the discussion on the mailing list.",", Large Class, "
"   Move Class,Rename Class,Move Method,","Simplify the Sling (aka Component) API JIRA issue to track simplification of the Sling (aka Component) API.

See http://www.mail-archive.com/sling-dev@incubator.apache.org/msg00177.html for the discussion on the mailing list.",", , "
"   Move Class,Rename Class,Move Method,","Simplify the Sling (aka Component) API JIRA issue to track simplification of the Sling (aka Component) API.

See http://www.mail-archive.com/sling-dev@incubator.apache.org/msg00177.html for the discussion on the mailing list.",", , "
"   Move Class,Move And Rename Class,Extract Interface,Rename Method,","Simplify the Sling (aka Component) API JIRA issue to track simplification of the Sling (aka Component) API.

See http://www.mail-archive.com/sling-dev@incubator.apache.org/msg00177.html for the discussion on the mailing list.",", Large Class, "
"   Rename Method,","Extensible Sling system health checking tool I have created a prototype at https://github.com/bdelacretaz/muppet-prototype that we might want to move to our contrib folder.

Muppet (it's like a Puppet, but different (*)) allows you to check the health of a system by defining rules that (out of the box) verify things like the presence of specific OSGi bundles, JMX MBeans values, JUnit tests execution (including scriptable ones thanks to the Sling testing tools), correct disabling of default Sling credentials, etc.

New rule types can be defined by adding RuleBuilder OSGi services, there are several examples in this initial code.

I'll add a how-to for this initial version here. 

Known issues are:
-The output does not indicate the value that causes a rule to fail
-The servlet output is not JSON yet
-Tags on rules would be nice to be able to run just the performance or security rules for example
-A rule for checking OSGi configuration parameters would be useful.

(*) credits to Joerg Hoh for that one, as well as inspiration in https://github.com/joerghoh/cq5-healthcheck",", "
"   Move Class,Move Method,Move Attribute,","Extensible Sling system health checking tool I have created a prototype at https://github.com/bdelacretaz/muppet-prototype that we might want to move to our contrib folder.

Muppet (it's like a Puppet, but different (*)) allows you to check the health of a system by defining rules that (out of the box) verify things like the presence of specific OSGi bundles, JMX MBeans values, JUnit tests execution (including scriptable ones thanks to the Sling testing tools), correct disabling of default Sling credentials, etc.

New rule types can be defined by adding RuleBuilder OSGi services, there are several examples in this initial code.

I'll add a how-to for this initial version here. 

Known issues are:
-The output does not indicate the value that causes a rule to fail
-The servlet output is not JSON yet
-Tags on rules would be nice to be able to run just the performance or security rules for example
-A rule for checking OSGi configuration parameters would be useful.

(*) credits to Joerg Hoh for that one, as well as inspiration in https://github.com/joerghoh/cq5-healthcheck",", , , "
"   Move And Rename Class,Extract Method,","Extensible Sling system health checking tool I have created a prototype at https://github.com/bdelacretaz/muppet-prototype that we might want to move to our contrib folder.

Muppet (it's like a Puppet, but different (*)) allows you to check the health of a system by defining rules that (out of the box) verify things like the presence of specific OSGi bundles, JMX MBeans values, JUnit tests execution (including scriptable ones thanks to the Sling testing tools), correct disabling of default Sling credentials, etc.

New rule types can be defined by adding RuleBuilder OSGi services, there are several examples in this initial code.

I'll add a how-to for this initial version here. 

Known issues are:
-The output does not indicate the value that causes a rule to fail
-The servlet output is not JSON yet
-Tags on rules would be nice to be able to run just the performance or security rules for example
-A rule for checking OSGi configuration parameters would be useful.

(*) credits to Joerg Hoh for that one, as well as inspiration in https://github.com/joerghoh/cq5-healthcheck","Duplicated Code, Long Method, , "
"   Rename Method,","Extensible Sling system health checking tool I have created a prototype at https://github.com/bdelacretaz/muppet-prototype that we might want to move to our contrib folder.

Muppet (it's like a Puppet, but different (*)) allows you to check the health of a system by defining rules that (out of the box) verify things like the presence of specific OSGi bundles, JMX MBeans values, JUnit tests execution (including scriptable ones thanks to the Sling testing tools), correct disabling of default Sling credentials, etc.

New rule types can be defined by adding RuleBuilder OSGi services, there are several examples in this initial code.

I'll add a how-to for this initial version here. 

Known issues are:
-The output does not indicate the value that causes a rule to fail
-The servlet output is not JSON yet
-Tags on rules would be nice to be able to run just the performance or security rules for example
-A rule for checking OSGi configuration parameters would be useful.

(*) credits to Joerg Hoh for that one, as well as inspiration in https://github.com/joerghoh/cq5-healthcheck",", "
"   Move Class,Move And Rename Class,Rename Method,Move Method,","Extensible Sling system health checking tool I have created a prototype at https://github.com/bdelacretaz/muppet-prototype that we might want to move to our contrib folder.

Muppet (it's like a Puppet, but different (*)) allows you to check the health of a system by defining rules that (out of the box) verify things like the presence of specific OSGi bundles, JMX MBeans values, JUnit tests execution (including scriptable ones thanks to the Sling testing tools), correct disabling of default Sling credentials, etc.

New rule types can be defined by adding RuleBuilder OSGi services, there are several examples in this initial code.

I'll add a how-to for this initial version here. 

Known issues are:
-The output does not indicate the value that causes a rule to fail
-The servlet output is not JSON yet
-Tags on rules would be nice to be able to run just the performance or security rules for example
-A rule for checking OSGi configuration parameters would be useful.

(*) credits to Joerg Hoh for that one, as well as inspiration in https://github.com/joerghoh/cq5-healthcheck",", , "
"   Rename Method,","Extensible Sling system health checking tool I have created a prototype at https://github.com/bdelacretaz/muppet-prototype that we might want to move to our contrib folder.

Muppet (it's like a Puppet, but different (*)) allows you to check the health of a system by defining rules that (out of the box) verify things like the presence of specific OSGi bundles, JMX MBeans values, JUnit tests execution (including scriptable ones thanks to the Sling testing tools), correct disabling of default Sling credentials, etc.

New rule types can be defined by adding RuleBuilder OSGi services, there are several examples in this initial code.

I'll add a how-to for this initial version here. 

Known issues are:
-The output does not indicate the value that causes a rule to fail
-The servlet output is not JSON yet
-Tags on rules would be nice to be able to run just the performance or security rules for example
-A rule for checking OSGi configuration parameters would be useful.

(*) credits to Joerg Hoh for that one, as well as inspiration in https://github.com/joerghoh/cq5-healthcheck",", "
"   Rename Method,","Extensible Sling system health checking tool I have created a prototype at https://github.com/bdelacretaz/muppet-prototype that we might want to move to our contrib folder.

Muppet (it's like a Puppet, but different (*)) allows you to check the health of a system by defining rules that (out of the box) verify things like the presence of specific OSGi bundles, JMX MBeans values, JUnit tests execution (including scriptable ones thanks to the Sling testing tools), correct disabling of default Sling credentials, etc.

New rule types can be defined by adding RuleBuilder OSGi services, there are several examples in this initial code.

I'll add a how-to for this initial version here. 

Known issues are:
-The output does not indicate the value that causes a rule to fail
-The servlet output is not JSON yet
-Tags on rules would be nice to be able to run just the performance or security rules for example
-A rule for checking OSGi configuration parameters would be useful.

(*) credits to Joerg Hoh for that one, as well as inspiration in https://github.com/joerghoh/cq5-healthcheck",", "
"   Rename Method,Extract Method,","discovery.impl: a resource based implementation of the discovery.api This ticket is about contributing a resource based implementation of the discovery api (see [0]) named discovery.impl to Sling. The implementation is attached as a .tar.gz - its md5 hash is d8891e5401114b2a629d3ff01044a1d6


Short description of the discovery.impl:

The discovery.impl is an out-of-the-box implementation of the discovery.api using standard features of Sling. The discovery.api provides a view of a topology consisting of a number of individual sling-instances. The instances are loosely coupled, except for being part of the topology they do not implicitly or necessarily share anything else. For those instances though that form a cluster - ie when connected to the same repository - the api has an abstraction called ClusterView.

The discovery.impl uses two mechanisms for discovering other instances:

* it stores information about the local instance at a unique location in the repository. Thus allowing other instances that access the same repository to see and recognize each other.
* it connects to 'remote' instances via a plain HTTP POST, announcing the instances that it can see, and getting back the instances of the counterpart

All of the above is done regularly using a heart-beat - thus allowing to get a view of the currently live instances.

The discovery.api additionally supports leader-election within a cluster: it ensures that one and only one instance is elected leader and stays leader until it disappears/shuts down/dies. The discovery.impl uses repository-based voting between the instances of a cluster to establish a common 'cluster view'. Based on an established view, the discovery.impl is then able to deterministically elect one of the instances of the view as the leader (namely the one with the lowest 'id').

Also, to support the PropertyProvider concept of the discovery.api, the properties of each instance are propagated to the other instances using the heartbeat as piggyback (either via the repository or via HTTP POSTs for remote instances).

To get an idea of the discovery.impl build and add and start the two bundles (org.apache.sling.discovery.api and org.apache.sling.discovery.impl) to your sling installation and open the browser to the provided, simplistic 'topology webconsole' at

http://localhost:4502/system/console/topology


Please let me know if anything needs further explanation, details. Looking forward to having this included in Sling!


Cheers,
Stefan
--
[0] http://svn.apache.org/repos/asf/sling/trunk/contrib/extensions/discovery/api","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,","discovery.impl: a resource based implementation of the discovery.api This ticket is about contributing a resource based implementation of the discovery api (see [0]) named discovery.impl to Sling. The implementation is attached as a .tar.gz - its md5 hash is d8891e5401114b2a629d3ff01044a1d6


Short description of the discovery.impl:

The discovery.impl is an out-of-the-box implementation of the discovery.api using standard features of Sling. The discovery.api provides a view of a topology consisting of a number of individual sling-instances. The instances are loosely coupled, except for being part of the topology they do not implicitly or necessarily share anything else. For those instances though that form a cluster - ie when connected to the same repository - the api has an abstraction called ClusterView.

The discovery.impl uses two mechanisms for discovering other instances:

* it stores information about the local instance at a unique location in the repository. Thus allowing other instances that access the same repository to see and recognize each other.
* it connects to 'remote' instances via a plain HTTP POST, announcing the instances that it can see, and getting back the instances of the counterpart

All of the above is done regularly using a heart-beat - thus allowing to get a view of the currently live instances.

The discovery.api additionally supports leader-election within a cluster: it ensures that one and only one instance is elected leader and stays leader until it disappears/shuts down/dies. The discovery.impl uses repository-based voting between the instances of a cluster to establish a common 'cluster view'. Based on an established view, the discovery.impl is then able to deterministically elect one of the instances of the view as the leader (namely the one with the lowest 'id').

Also, to support the PropertyProvider concept of the discovery.api, the properties of each instance are propagated to the other instances using the heartbeat as piggyback (either via the repository or via HTTP POSTs for remote instances).

To get an idea of the discovery.impl build and add and start the two bundles (org.apache.sling.discovery.api and org.apache.sling.discovery.impl) to your sling installation and open the browser to the provided, simplistic 'topology webconsole' at

http://localhost:4502/system/console/topology


Please let me know if anything needs further explanation, details. Looking forward to having this included in Sling!


Cheers,
Stefan
--
[0] http://svn.apache.org/repos/asf/sling/trunk/contrib/extensions/discovery/api",", "
"   Rename Method,Extract Method,","discovery.impl: a resource based implementation of the discovery.api This ticket is about contributing a resource based implementation of the discovery api (see [0]) named discovery.impl to Sling. The implementation is attached as a .tar.gz - its md5 hash is d8891e5401114b2a629d3ff01044a1d6


Short description of the discovery.impl:

The discovery.impl is an out-of-the-box implementation of the discovery.api using standard features of Sling. The discovery.api provides a view of a topology consisting of a number of individual sling-instances. The instances are loosely coupled, except for being part of the topology they do not implicitly or necessarily share anything else. For those instances though that form a cluster - ie when connected to the same repository - the api has an abstraction called ClusterView.

The discovery.impl uses two mechanisms for discovering other instances:

* it stores information about the local instance at a unique location in the repository. Thus allowing other instances that access the same repository to see and recognize each other.
* it connects to 'remote' instances via a plain HTTP POST, announcing the instances that it can see, and getting back the instances of the counterpart

All of the above is done regularly using a heart-beat - thus allowing to get a view of the currently live instances.

The discovery.api additionally supports leader-election within a cluster: it ensures that one and only one instance is elected leader and stays leader until it disappears/shuts down/dies. The discovery.impl uses repository-based voting between the instances of a cluster to establish a common 'cluster view'. Based on an established view, the discovery.impl is then able to deterministically elect one of the instances of the view as the leader (namely the one with the lowest 'id').

Also, to support the PropertyProvider concept of the discovery.api, the properties of each instance are propagated to the other instances using the heartbeat as piggyback (either via the repository or via HTTP POSTs for remote instances).

To get an idea of the discovery.impl build and add and start the two bundles (org.apache.sling.discovery.api and org.apache.sling.discovery.impl) to your sling installation and open the browser to the provided, simplistic 'topology webconsole' at

http://localhost:4502/system/console/topology


Please let me know if anything needs further explanation, details. Looking forward to having this included in Sling!


Cheers,
Stefan
--
[0] http://svn.apache.org/repos/asf/sling/trunk/contrib/extensions/discovery/api","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,","discovery.impl: a resource based implementation of the discovery.api This ticket is about contributing a resource based implementation of the discovery api (see [0]) named discovery.impl to Sling. The implementation is attached as a .tar.gz - its md5 hash is d8891e5401114b2a629d3ff01044a1d6


Short description of the discovery.impl:

The discovery.impl is an out-of-the-box implementation of the discovery.api using standard features of Sling. The discovery.api provides a view of a topology consisting of a number of individual sling-instances. The instances are loosely coupled, except for being part of the topology they do not implicitly or necessarily share anything else. For those instances though that form a cluster - ie when connected to the same repository - the api has an abstraction called ClusterView.

The discovery.impl uses two mechanisms for discovering other instances:

* it stores information about the local instance at a unique location in the repository. Thus allowing other instances that access the same repository to see and recognize each other.
* it connects to 'remote' instances via a plain HTTP POST, announcing the instances that it can see, and getting back the instances of the counterpart

All of the above is done regularly using a heart-beat - thus allowing to get a view of the currently live instances.

The discovery.api additionally supports leader-election within a cluster: it ensures that one and only one instance is elected leader and stays leader until it disappears/shuts down/dies. The discovery.impl uses repository-based voting between the instances of a cluster to establish a common 'cluster view'. Based on an established view, the discovery.impl is then able to deterministically elect one of the instances of the view as the leader (namely the one with the lowest 'id').

Also, to support the PropertyProvider concept of the discovery.api, the properties of each instance are propagated to the other instances using the heartbeat as piggyback (either via the repository or via HTTP POSTs for remote instances).

To get an idea of the discovery.impl build and add and start the two bundles (org.apache.sling.discovery.api and org.apache.sling.discovery.impl) to your sling installation and open the browser to the provided, simplistic 'topology webconsole' at

http://localhost:4502/system/console/topology


Please let me know if anything needs further explanation, details. Looking forward to having this included in Sling!


Cheers,
Stefan
--
[0] http://svn.apache.org/repos/asf/sling/trunk/contrib/extensions/discovery/api",", "
"   Rename Class,Rename Method,","Make Function/Tag Names More Consistent Right now there are a few names which are not consistent between the Tags, EL Functions and Sling API. This should be corrected to allow more easy usage.",", "
"   Rename Class,Rename Method,","Make Function/Tag Names More Consistent Right now there are a few names which are not consistent between the Tags, EL Functions and Sling API. This should be corrected to allow more easy usage.",", "
"   Rename Method,Extract Method,","Simplify handling of multiple references (post processors, node name generators, post operations) The current handling of the multiple references to post processors, node name generators and post operations is based on old DS specifications and requires a lot of additional syncing and handling.

With newer DS versions we can get the service and it's properties during the bind/unbind methods without any additional method calls - this simplifies the handling of such references as we don't have to wait in bind until the component is activated and all the delayed handling can be removed","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Simplify handling of multiple references (post processors, node name generators, post operations) The current handling of the multiple references to post processors, node name generators and post operations is based on old DS specifications and requires a lot of additional syncing and handling.

With newer DS versions we can get the service and it's properties during the bind/unbind methods without any additional method calls - this simplifies the handling of such references as we don't have to wait in bind until the component is activated and all the delayed handling can be removed","Duplicated Code, Long Method, , "
"   Rename Method,","Replace administrative login by service-based login From the start Sling tried to solve the problem of providing services access to the repository and resource tree without having to hard code and configure any passwords. This was done first with the SlingRepository.loginAdministrative and later with the ResourceResolverFactory.getAdministrativeResourceResolver methods.

Over time this mechanism proved to be the hammer to hit all nails. Particularly these methods while truly useful have the disadvantage of providing full administrative privileges to services where just some specific kind of privilege would be enough.

For example for the JSP compiler it would be enough to be able to read the JSP source scripts and write the Java classes out to the JSP compiler's target location. Other access is not required. Similarly to manage users user management privileges are enough and no access to /content is really required.

To solve this problem a new API for Service Authentication has been proposed at https://cwiki.apache.org/confluence/display/SLING/Service+Authentication. The prototype of which is implemented in http://svn.apache.org/repos/asf/sling/whiteboard/fmeschbe/deprecate_login_administrative.

This issue is about merging the prototype code back into trunk and thus fully implementing the feature.",", "
"   Rename Method,","Replace administrative login by service-based login From the start Sling tried to solve the problem of providing services access to the repository and resource tree without having to hard code and configure any passwords. This was done first with the SlingRepository.loginAdministrative and later with the ResourceResolverFactory.getAdministrativeResourceResolver methods.

Over time this mechanism proved to be the hammer to hit all nails. Particularly these methods while truly useful have the disadvantage of providing full administrative privileges to services where just some specific kind of privilege would be enough.

For example for the JSP compiler it would be enough to be able to read the JSP source scripts and write the Java classes out to the JSP compiler's target location. Other access is not required. Similarly to manage users user management privileges are enough and no access to /content is really required.

To solve this problem a new API for Service Authentication has been proposed at https://cwiki.apache.org/confluence/display/SLING/Service+Authentication. The prototype of which is implemented in http://svn.apache.org/repos/asf/sling/whiteboard/fmeschbe/deprecate_login_administrative.

This issue is about merging the prototype code back into trunk and thus fully implementing the feature.",", "
"   Rename Method,","Replace administrative login by service-based login From the start Sling tried to solve the problem of providing services access to the repository and resource tree without having to hard code and configure any passwords. This was done first with the SlingRepository.loginAdministrative and later with the ResourceResolverFactory.getAdministrativeResourceResolver methods.

Over time this mechanism proved to be the hammer to hit all nails. Particularly these methods while truly useful have the disadvantage of providing full administrative privileges to services where just some specific kind of privilege would be enough.

For example for the JSP compiler it would be enough to be able to read the JSP source scripts and write the Java classes out to the JSP compiler's target location. Other access is not required. Similarly to manage users user management privileges are enough and no access to /content is really required.

To solve this problem a new API for Service Authentication has been proposed at https://cwiki.apache.org/confluence/display/SLING/Service+Authentication. The prototype of which is implemented in http://svn.apache.org/repos/asf/sling/whiteboard/fmeschbe/deprecate_login_administrative.

This issue is about merging the prototype code back into trunk and thus fully implementing the feature.",", "
"   Rename Method,","Replace administrative login by service-based login From the start Sling tried to solve the problem of providing services access to the repository and resource tree without having to hard code and configure any passwords. This was done first with the SlingRepository.loginAdministrative and later with the ResourceResolverFactory.getAdministrativeResourceResolver methods.

Over time this mechanism proved to be the hammer to hit all nails. Particularly these methods while truly useful have the disadvantage of providing full administrative privileges to services where just some specific kind of privilege would be enough.

For example for the JSP compiler it would be enough to be able to read the JSP source scripts and write the Java classes out to the JSP compiler's target location. Other access is not required. Similarly to manage users user management privileges are enough and no access to /content is really required.

To solve this problem a new API for Service Authentication has been proposed at https://cwiki.apache.org/confluence/display/SLING/Service+Authentication. The prototype of which is implemented in http://svn.apache.org/repos/asf/sling/whiteboard/fmeschbe/deprecate_login_administrative.

This issue is about merging the prototype code back into trunk and thus fully implementing the feature.",", "
"   Rename Method,","Replace administrative login by service-based login From the start Sling tried to solve the problem of providing services access to the repository and resource tree without having to hard code and configure any passwords. This was done first with the SlingRepository.loginAdministrative and later with the ResourceResolverFactory.getAdministrativeResourceResolver methods.

Over time this mechanism proved to be the hammer to hit all nails. Particularly these methods while truly useful have the disadvantage of providing full administrative privileges to services where just some specific kind of privilege would be enough.

For example for the JSP compiler it would be enough to be able to read the JSP source scripts and write the Java classes out to the JSP compiler's target location. Other access is not required. Similarly to manage users user management privileges are enough and no access to /content is really required.

To solve this problem a new API for Service Authentication has been proposed at https://cwiki.apache.org/confluence/display/SLING/Service+Authentication. The prototype of which is implemented in http://svn.apache.org/repos/asf/sling/whiteboard/fmeschbe/deprecate_login_administrative.

This issue is about merging the prototype code back into trunk and thus fully implementing the feature.",", "
"   Rename Method,Extract Method,","Specify nodetype for node creation It would be nice to specify the node type for new nodes when they are created through the post servlet.

Perhaps by specifying a node type parameter (like ujax:nodetype)?","Duplicated Code, Long Method, , "
"   Rename Method,","Improve processing performance if job is processed locally Currently the job processing is completely observation based: even if a job is added locally and also processed locally, this is not immediately put into the queue. This creates an unnecessary delay between adding a job and processing it which can easily be avoided by directly putting the job into the queue.",", "
"   Rename Method,","Improve processing performance if job is processed locally Currently the job processing is completely observation based: even if a job is added locally and also processed locally, this is not immediately put into the queue. This creates an unnecessary delay between adding a job and processing it which can easily be avoided by directly putting the job into the queue.",", "
"   Rename Method,Extract Method,","Create simpler but more flexible scheduler service api The current api of the scheduler service has two problems:
- each time we add a new feature, we have more or less to duplicate the methods and add new methods with exactly this new feature
- most of the methods throw checked and unchecked exceptions although this is usually useless for the client","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Create simpler but more flexible scheduler service api The current api of the scheduler service has two problems:
- each time we add a new feature, we have more or less to duplicate the methods and add new methods with exactly this new feature
- most of the methods throw checked and unchecked exceptions although this is usually useless for the client","Duplicated Code, Long Method, , "
"   Rename Class,Extract Method,",[Tooling] show content of .content.xml in project explorer Irrespective of the chosen serialization the content of .content.xml should be shown in the tree structure of the project explorer. This can be achieved using a Navigator Content Extension (NCE) (which is part of the Common Navigator Framework (CNF) of eclipse).,"Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,",[Tooling] show content of .content.xml in project explorer Irrespective of the chosen serialization the content of .content.xml should be shown in the tree structure of the project explorer. This can be achieved using a Navigator Content Extension (NCE) (which is part of the Common Navigator Framework (CNF) of eclipse).,"Duplicated Code, Long Method, , "
"   Rename Class,Extract Method,",[Tooling] show content of .content.xml in project explorer Irrespective of the chosen serialization the content of .content.xml should be shown in the tree structure of the project explorer. This can be achieved using a Navigator Content Extension (NCE) (which is part of the Common Navigator Framework (CNF) of eclipse).,"Duplicated Code, Long Method, , "
"   Move Method,Move Attribute,","Improve Karaf integration tests - separate testing support and test into own packages and jars
- use Maven Failsafe Plugin for tests
- update Pax Exam to 4.9.1
- enable test (remove Ignore annotation)
- increase timeouts to 300000 ms
- test features",", , , "
"   Rename Method,","Improve Karaf integration tests - separate testing support and test into own packages and jars
- use Maven Failsafe Plugin for tests
- update Pax Exam to 4.9.1
- enable test (remove Ignore annotation)
- increase timeouts to 300000 ms
- test features",", "
"   Move Class,Move And Rename Class,","Improve Karaf integration tests - separate testing support and test into own packages and jars
- use Maven Failsafe Plugin for tests
- update Pax Exam to 4.9.1
- enable test (remove Ignore annotation)
- increase timeouts to 300000 ms
- test features",", "
"   Move Method,Extract Method,Move Attribute,","Immutable HealthCheck Results As discussed on list, I'll change the Result class to be immutable and allow for both single-value and log-based results:

Result is immutable.

Result has two constructors, one that takes a Status and a message String and one that takes a ResultLog, and sets the Result status to ResultLog.getStatus().

The ResultLog is a list of messages, each with a Status and a message String. It's getStatus() method returns the highest Status that was added to it.","Duplicated Code, Long Method, , , , "
"   Move Method,Move Attribute,","Immutable HealthCheck Results As discussed on list, I'll change the Result class to be immutable and allow for both single-value and log-based results:

Result is immutable.

Result has two constructors, one that takes a Status and a message String and one that takes a ResultLog, and sets the Result status to ResultLog.getStatus().

The ResultLog is a list of messages, each with a Status and a message String. It's getStatus() method returns the highest Status that was added to it.",", , , "
"   Move Method,Extract Method,Move Attribute,","Immutable HealthCheck Results As discussed on list, I'll change the Result class to be immutable and allow for both single-value and log-based results:

Result is immutable.

Result has two constructors, one that takes a Status and a message String and one that takes a ResultLog, and sets the Result status to ResultLog.getStatus().

The ResultLog is a list of messages, each with a Status and a message String. It's getStatus() method returns the highest Status that was added to it.","Duplicated Code, Long Method, , , , "
"   Rename Method,","Use service properties for HC meta data and improve JMX registration As discussed in the mailing list, we can simplify the health check api by using service properties for all meta data of a HC.
In addition, the jmx registration bridge needs updates on how to handle if two HC services are using the same name for registration


Mail threads:

http://mail-archives.us.apache.org/mod_mbox/sling-dev/201308.mbox/%3CCAKkCf4r89JbsVP_-RQK=R304wBL8gqpzAq4Rw3Z9sijWbBo8Yg@mail.gmail.com%3E
http://mail-archives.us.apache.org/mod_mbox/sling-dev/201308.mbox/%3CCAKkCf4q79NO26Va542s_QghoF1F1UvsjNyLs6as_2xQczRecpg@mail.gmail.com%3E",", "
"   Rename Method,","Use service properties for HC meta data and improve JMX registration As discussed in the mailing list, we can simplify the health check api by using service properties for all meta data of a HC.
In addition, the jmx registration bridge needs updates on how to handle if two HC services are using the same name for registration


Mail threads:

http://mail-archives.us.apache.org/mod_mbox/sling-dev/201308.mbox/%3CCAKkCf4r89JbsVP_-RQK=R304wBL8gqpzAq4Rw3Z9sijWbBo8Yg@mail.gmail.com%3E
http://mail-archives.us.apache.org/mod_mbox/sling-dev/201308.mbox/%3CCAKkCf4q79NO26Va542s_QghoF1F1UvsjNyLs6as_2xQczRecpg@mail.gmail.com%3E",", "
"   Rename Method,","Use service properties for HC meta data and improve JMX registration As discussed in the mailing list, we can simplify the health check api by using service properties for all meta data of a HC.
In addition, the jmx registration bridge needs updates on how to handle if two HC services are using the same name for registration


Mail threads:

http://mail-archives.us.apache.org/mod_mbox/sling-dev/201308.mbox/%3CCAKkCf4r89JbsVP_-RQK=R304wBL8gqpzAq4Rw3Z9sijWbBo8Yg@mail.gmail.com%3E
http://mail-archives.us.apache.org/mod_mbox/sling-dev/201308.mbox/%3CCAKkCf4q79NO26Va542s_QghoF1F1UvsjNyLs6as_2xQczRecpg@mail.gmail.com%3E",", "
"   Rename Class,Rename Method,","Improve Karaf Features - break down sling-karaf feature into more fine-grained features for better reusability
- add descriptions -and versions-
- add more features
- update integration tests
- update Pax Exam to latest
- update Karaf to 4.x
- update Maven Failsafe Plugin to 2.17
- -add workaround for KARAF-1972-
- update README
- cleanup",", "
"   Rename Method,Pull Up Attribute,","Support for progress tracking of jobs and keeping jobs For long-running jobs, it would be useful to have some means to track progress, which can be shown in a console for the user. This should include the following:

* ETA
* Completeness value computed from (optional, defaults to 1.0) max and current value (e.g. 42% or 23/100)
* Log output stream for detailed progress information
* Failure reason in case job failed

AFAICS this requires a few changes to the existing implementation:

* Jobs need additional support for setting properties, e.g. max and current progress value
* Jobs need to be kept at least for a while after they completed/failed to give access to failure information/log stream",", Duplicated Code, "
"   Rename Method,Extract Method,","Support for progress tracking of jobs and keeping jobs For long-running jobs, it would be useful to have some means to track progress, which can be shown in a console for the user. This should include the following:

* ETA
* Completeness value computed from (optional, defaults to 1.0) max and current value (e.g. 42% or 23/100)
* Log output stream for detailed progress information
* Failure reason in case job failed

AFAICS this requires a few changes to the existing implementation:

* Jobs need additional support for setting properties, e.g. max and current progress value
* Jobs need to be kept at least for a while after they completed/failed to give access to failure information/log stream","Duplicated Code, Long Method, , "
"   Rename Method,Pull Up Attribute,","Support for progress tracking of jobs and keeping jobs For long-running jobs, it would be useful to have some means to track progress, which can be shown in a console for the user. This should include the following:

* ETA
* Completeness value computed from (optional, defaults to 1.0) max and current value (e.g. 42% or 23/100)
* Log output stream for detailed progress information
* Failure reason in case job failed

AFAICS this requires a few changes to the existing implementation:

* Jobs need additional support for setting properties, e.g. max and current progress value
* Jobs need to be kept at least for a while after they completed/failed to give access to failure information/log stream",", Duplicated Code, "
"   Rename Method,Inline Method,","Support for progress tracking of jobs and keeping jobs For long-running jobs, it would be useful to have some means to track progress, which can be shown in a console for the user. This should include the following:

* ETA
* Completeness value computed from (optional, defaults to 1.0) max and current value (e.g. 42% or 23/100)
* Log output stream for detailed progress information
* Failure reason in case job failed

AFAICS this requires a few changes to the existing implementation:

* Jobs need additional support for setting properties, e.g. max and current progress value
* Jobs need to be kept at least for a while after they completed/failed to give access to failure information/log stream",", , "
"   Rename Method,Extract Method,","Support for progress tracking of jobs and keeping jobs For long-running jobs, it would be useful to have some means to track progress, which can be shown in a console for the user. This should include the following:

* ETA
* Completeness value computed from (optional, defaults to 1.0) max and current value (e.g. 42% or 23/100)
* Log output stream for detailed progress information
* Failure reason in case job failed

AFAICS this requires a few changes to the existing implementation:

* Jobs need additional support for setting properties, e.g. max and current progress value
* Jobs need to be kept at least for a while after they completed/failed to give access to failure information/log stream","Duplicated Code, Long Method, , "
"   Rename Method,","Support for progress tracking of jobs and keeping jobs For long-running jobs, it would be useful to have some means to track progress, which can be shown in a console for the user. This should include the following:

* ETA
* Completeness value computed from (optional, defaults to 1.0) max and current value (e.g. 42% or 23/100)
* Log output stream for detailed progress information
* Failure reason in case job failed

AFAICS this requires a few changes to the existing implementation:

* Jobs need additional support for setting properties, e.g. max and current progress value
* Jobs need to be kept at least for a while after they completed/failed to give access to failure information/log stream",", "
"   Move Class,Rename Class,Move Method,Extract Method,Move Attribute,","make UjaxHtmlResponse public usable the current ujax html response can only by used by the UjaxPostServlet. It would be useful if it can also be used by other classes.
","Duplicated Code, Long Method, , , , "
"   Move Class,Rename Class,Move Method,Extract Method,Move Attribute,","make UjaxHtmlResponse public usable the current ujax html response can only by used by the UjaxPostServlet. It would be useful if it can also be used by other classes.
","Duplicated Code, Long Method, , , , "
"   Move Method,Extract Method,",[Tooling] support (auto-) deploy of content-bundles Similar to SLING-3009 in Eclipse it should be possible to (auto-) deploy a content-bundle easily using the configured launchpad-server. This feature will eventually integrate with SLING-2985,"Duplicated Code, Long Method, , , "
"   Move Method,Extract Method,Move Attribute,","[Tooling] add whitelabel support for slingclipse For the slingclipse plugin, at certain locations, it should be possible to 'brand' the plugins. Eg add custom icons, labels etc","Duplicated Code, Long Method, , , , "
"   Move Method,Move Attribute,","[Tooling] add whitelabel support for slingclipse For the slingclipse plugin, at certain locations, it should be possible to 'brand' the plugins. Eg add custom icons, labels etc",", , , "
"   Move Method,Move Attribute,","[Tooling] add whitelabel support for slingclipse For the slingclipse plugin, at certain locations, it should be possible to 'brand' the plugins. Eg add custom icons, labels etc",", , , "
"   Move Method,Extract Method,Move Attribute,","[Tooling] add whitelabel support for slingclipse For the slingclipse plugin, at certain locations, it should be possible to 'brand' the plugins. Eg add custom icons, labels etc","Duplicated Code, Long Method, , , , "
"   Rename Method,","Enable Logback to use OSGi services for Filters and TurboFilters Currently the Logback integration enabled use of OSGi services as Appenders [1]. It would be helpful if it can also enabled using OSGi services for Logback Filters and TurboFilters [2]. 

The filters can be used to precisely extract required logs for certain flow and prove to be very useful for debugging complex issues

[1] https://github.com/chetanmeh/sling-logback#appenders-and-whiteboard-pattern
[2] http://logback.qos.ch/manual/filters.html",", "
"   Rename Class,Rename Method,","Repository: API needs a single method to create a node and set its non-binary properties The Repository add/update methods are as follows:

{code}
Command<Void> newAddNodeCommand(FileInfo fileInfo); 
Command<Void> newUpdateContentNodeCommand(FileInfo fileInfo, ResourceProxy resourceProxy);
{code}

with the add command not knowing the jcr:primaryType of the node to create, and guessing one of nt:file or nt:folder. The unified command ( newAddOrUpdateNodeCommand ? ) could possibly receive only the ResourceProxy, with a repository path set, and that would be enough information.

Both vlt and resource-based implementations need to be adjusted after the API change.",", "
"   Rename Method,","Improve Authentication Handling The following scheme should improve the authentication handling:

Anonymous user accesses content he's not allowed to access
- During resource resolution a access control exception is thrown
- This exception should be catch at this point and a request for authentication should be send back

Authenticated user accesses content he's not allowed to access
- During resource resolution a access control exception is thrown
- a 404 should be send back",", "
"   Rename Method,","Improve Authentication Handling The following scheme should improve the authentication handling:

Anonymous user accesses content he's not allowed to access
- During resource resolution a access control exception is thrown
- This exception should be catch at this point and a request for authentication should be send back

Authenticated user accesses content he's not allowed to access
- During resource resolution a access control exception is thrown
- a 404 should be send back",", "
"   Rename Class,Rename Method,Extract Method,","Record loaded content to not reload inadvertedly. Currently, the content loader always reloads content indicated as initial content from the bundles if the repository does not contain the respective content. Sometimes it may be desirable to remove the content from the repository and not get the content reloaded on bundle (or system) restart.

To prevent such content reload, the content loader should take note of loaded content and not try to reload content, which is marked as ""loaded"", regardless of whether the actual content (still) exists or not.","Duplicated Code, Long Method, , "
"   Extract Method,Move Attribute,","Allow installing/updating the install support bundle from the servers view Currently the only way to install the support/install bundle is if we're creating a new server when creating a new bundle. this option should be available from the server editor page.

Implementation plan:

* Extract an OSGiClient and make it part of API/Core
* Create an OSGiClientFactory and make it a SCR component+service
* Make access to the archetype resources public, probably as part of a separate bundle ( a follow-up bug should make this bundle contain all embedded resources )
* Implement bundle install in the server page, maybe with special logic for SNAPSHOTs","Duplicated Code, Long Method, , , "
"   Rename Method,Move Method,Extract Method,","Allow installing/updating the install support bundle from the servers view Currently the only way to install the support/install bundle is if we're creating a new server when creating a new bundle. this option should be available from the server editor page.

Implementation plan:

* Extract an OSGiClient and make it part of API/Core
* Create an OSGiClientFactory and make it a SCR component+service
* Make access to the archetype resources public, probably as part of a separate bundle ( a follow-up bug should make this bundle contain all embedded resources )
* Implement bundle install in the server page, maybe with special logic for SNAPSHOTs","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Provide a way to schedule jobs The current way of scheduling jobs is not very user friendly: it requires to send an event via event admin. We should add a java api to create scheduled job creation.

In addition, we could add a new queue type: maintenance queue which is an order queue with the difference of running jobs exactly at the point of time when they are created. So a scheduled maintenance job is executed exactly at the scheduled time - unless another job is currently running.

Apart from starting such jobs, stopping a job would be another nice addition","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Provide a way to schedule jobs The current way of scheduling jobs is not very user friendly: it requires to send an event via event admin. We should add a java api to create scheduled job creation.

In addition, we could add a new queue type: maintenance queue which is an order queue with the difference of running jobs exactly at the point of time when they are created. So a scheduled maintenance job is executed exactly at the scheduled time - unless another job is currently running.

Apart from starting such jobs, stopping a job would be another nice addition","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Provide a way to schedule jobs The current way of scheduling jobs is not very user friendly: it requires to send an event via event admin. We should add a java api to create scheduled job creation.

In addition, we could add a new queue type: maintenance queue which is an order queue with the difference of running jobs exactly at the point of time when they are created. So a scheduled maintenance job is executed exactly at the scheduled time - unless another job is currently running.

Apart from starting such jobs, stopping a job would be another nice addition","Duplicated Code, Long Method, , "
"   Rename Class,Extract Interface,Rename Method,Move Method,Extract Method,","Provide a way to schedule jobs The current way of scheduling jobs is not very user friendly: it requires to send an event via event admin. We should add a java api to create scheduled job creation.

In addition, we could add a new queue type: maintenance queue which is an order queue with the difference of running jobs exactly at the point of time when they are created. So a scheduled maintenance job is executed exactly at the scheduled time - unless another job is currently running.

Apart from starting such jobs, stopping a job would be another nice addition","Duplicated Code, Long Method, , , Large Class, "
"   Rename Method,","Provide a way to schedule jobs The current way of scheduling jobs is not very user friendly: it requires to send an event via event admin. We should add a java api to create scheduled job creation.

In addition, we could add a new queue type: maintenance queue which is an order queue with the difference of running jobs exactly at the point of time when they are created. So a scheduled maintenance job is executed exactly at the scheduled time - unless another job is currently running.

Apart from starting such jobs, stopping a job would be another nice addition",", "
"   Rename Method,","Provide a way to schedule jobs The current way of scheduling jobs is not very user friendly: it requires to send an event via event admin. We should add a java api to create scheduled job creation.

In addition, we could add a new queue type: maintenance queue which is an order queue with the difference of running jobs exactly at the point of time when they are created. So a scheduled maintenance job is executed exactly at the scheduled time - unless another job is currently running.

Apart from starting such jobs, stopping a job would be another nice addition",", "
"   Rename Method,","Provide a way to schedule jobs The current way of scheduling jobs is not very user friendly: it requires to send an event via event admin. We should add a java api to create scheduled job creation.

In addition, we could add a new queue type: maintenance queue which is an order queue with the difference of running jobs exactly at the point of time when they are created. So a scheduled maintenance job is executed exactly at the scheduled time - unless another job is currently running.

Apart from starting such jobs, stopping a job would be another nice addition",", "
"   Move Method,Move Attribute,","Provide a way to schedule jobs The current way of scheduling jobs is not very user friendly: it requires to send an event via event admin. We should add a java api to create scheduled job creation.

In addition, we could add a new queue type: maintenance queue which is an order queue with the difference of running jobs exactly at the point of time when they are created. So a scheduled maintenance job is executed exactly at the scheduled time - unless another job is currently running.

Apart from starting such jobs, stopping a job would be another nice addition",", , , "
"   Move Method,Move Attribute,","Provide a way to schedule jobs The current way of scheduling jobs is not very user friendly: it requires to send an event via event admin. We should add a java api to create scheduled job creation.

In addition, we could add a new queue type: maintenance queue which is an order queue with the difference of running jobs exactly at the point of time when they are created. So a scheduled maintenance job is executed exactly at the scheduled time - unless another job is currently running.

Apart from starting such jobs, stopping a job would be another nice addition",", , , "
"   Rename Class,Extract Interface,Rename Method,Move Method,Extract Method,","Provide a way to schedule jobs The current way of scheduling jobs is not very user friendly: it requires to send an event via event admin. We should add a java api to create scheduled job creation.

In addition, we could add a new queue type: maintenance queue which is an order queue with the difference of running jobs exactly at the point of time when they are created. So a scheduled maintenance job is executed exactly at the scheduled time - unless another job is currently running.

Apart from starting such jobs, stopping a job would be another nice addition","Duplicated Code, Long Method, , , Large Class, "
"   Rename Method,Extract Method,","Provide a way to schedule jobs The current way of scheduling jobs is not very user friendly: it requires to send an event via event admin. We should add a java api to create scheduled job creation.

In addition, we could add a new queue type: maintenance queue which is an order queue with the difference of running jobs exactly at the point of time when they are created. So a scheduled maintenance job is executed exactly at the scheduled time - unless another job is currently running.

Apart from starting such jobs, stopping a job would be another nice addition","Duplicated Code, Long Method, , "
"   Move Method,Move Attribute,","Implement support for Feature Flags/Toggles in Sling It would be nice if sling provide support for feature flags (also called toggles) pattern [1].

I am thinking the implementation could provide the following.
1) Integrate an existing framework such as togglz [2] or implement something similar with UI/Configuration to toggle features.
2) Create a jcr property (sling:Feature) which can be added to resource type nodes. Then some conditional logic in <sling:include>, and get and post servlets determines if the resource should be rendered if its resource type contains such property.

[1] http://en.wikipedia.org/wiki/Feature_toggle
[2] http://www.togglz.org/",", , , "
"   Move Method,Move Attribute,","Implement support for Feature Flags/Toggles in Sling It would be nice if sling provide support for feature flags (also called toggles) pattern [1].

I am thinking the implementation could provide the following.
1) Integrate an existing framework such as togglz [2] or implement something similar with UI/Configuration to toggle features.
2) Create a jcr property (sling:Feature) which can be added to resource type nodes. Then some conditional logic in <sling:include>, and get and post servlets determines if the resource should be rendered if its resource type contains such property.

[1] http://en.wikipedia.org/wiki/Feature_toggle
[2] http://www.togglz.org/",", , , "
"   Move And Rename Class,Rename Class,Rename Method,Move Method,Inline Method,Move Attribute,","Job state and related enumerations This is a follow up from SLING-3028 based on comments by Stefan Seifert:
I find the enum name Job.JobType not ideal, because it does not stand of a type but for a state of the job. But there is a JobState enum in the consumer API package already.
I find the enum and class names JobState and JobStatus in the consumer package not ideal, because they do not stand for a state, but for a job result.
",", , , , "
"   Move Class,Pull Up Method,Move Method,Move Attribute,","Add new simple project content wizard We should include a new wizards which creates a simple content projects. The project would simply create a faceted project with the according facet set and set any additional type specific properties (eg with content it would create a jcr_root directory). Although this wizard would do very trivial tasks, IMO this will help users kickstart projects and get into the mindset of slingclipse.

The initial content should be something like
{code}
.
├── jcr_root
│   ├── content
│   └───── example
└── META-INF
└── vault
├── config.xml
├── filter.xml
└── settings.xml
{code}",", , , Duplicated Code, "
"   Move Class,Pull Up Method,Move Method,Move Attribute,","Add new simple project content wizard We should include a new wizards which creates a simple content projects. The project would simply create a faceted project with the according facet set and set any additional type specific properties (eg with content it would create a jcr_root directory). Although this wizard would do very trivial tasks, IMO this will help users kickstart projects and get into the mindset of slingclipse.

The initial content should be something like
{code}
.
├── jcr_root
│   ├── content
│   └───── example
└── META-INF
└── vault
├── config.xml
├── filter.xml
└── settings.xml
{code}",", , , Duplicated Code, "
"   Move Class,Move Attribute,","Enable Logback ChangeLevelDispatcher by default if JUL Integration is enabled With Logback its possible to use ChangeLevelDispatcher [1] to minimize the performance impact. Currently one needs to explicitly enable in logback.xml via

{code:xml}
<configuration>
<contextListener class=""ch.qos.logback.classic.jul.LevelChangePropagator""/>
</configuration>
{code}

It would be better if Logback Integration logic adds this listener on its own if the {{org.apache.sling.commons.log.julenabled}} is set to true without requiring explicit user effort to tweak Logback xml config

[1] http://logback.qos.ch/manual/configuration.html#LevelChangePropagator",", , "
"   Rename Method,","Rename µjax to ""sling client library"" As discussed recently [1], reducing the number of names for the different sling components should help people make sense of it.

The idea is to rename the µjax stuff to ""sling client library"", and I suppose the UjaxPostServlet can be renamed to SlingPostServlet?

There were some discussions about using ""catapult"" for the name, but that's so good a name that we'll keep it in store for later ;-)

[1] http://markmail.org/message/5yeuwlbsj6m7da6o",", "
"   Rename Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Implement suggestions from SLING-3223 in Sling replication Umbrella issue to track implementation of improvements for Sling Replication as suggested in SLING-3223 comments.
Main ones refer to:
- more tests
- fix javadoc / typos
- avoid AdapterFactory for installing packages
- fix action names
- refactor AuthenticationHandlers
","Duplicated Code, Long Method, , , , , "
"   Rename Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Implement suggestions from SLING-3223 in Sling replication Umbrella issue to track implementation of improvements for Sling Replication as suggested in SLING-3223 comments.
Main ones refer to:
- more tests
- fix javadoc / typos
- avoid AdapterFactory for installing packages
- fix action names
- refactor AuthenticationHandlers
","Duplicated Code, Long Method, , , , , "
"   Rename Method,","Make Sling imports dynamic Currently the imports to sling auth core and sling api resource are mandatory, this means the security provider is only active if Sling is running.
These imports should rather be dynamic as the provider should also run if only the repository is available",", "
"   Rename Method,","Make Sling imports dynamic Currently the imports to sling auth core and sling api resource are mandatory, this means the security provider is only active if Sling is running.
These imports should rather be dynamic as the provider should also run if only the repository is available",", "
"   Rename Method,Extract Method,","Provide a HealthCheckExecutor service Goals:
* Be able to get an overall (aggregated) result as quickly as possible (ideally <2sec)
* Whenever possible, return most current results (e.g. for a memory check)
* Provide a declarative way for async checks (async checks should be the exception though) 

Approach
* Run checks in parallel
* Make sure long running (or even stuck) checks are timed out
* If a health check must run asynchronously (because its execution time cannot be optimized), it should be enough to just specify a service property (e.g. ""hc.async"").

See also
http://apache-sling.73963.n3.nabble.com/Health-Check-Improvements-td4029330.html#a4029402
http://apache-sling.73963.n3.nabble.com/Health-checks-execution-service-td4028477.html","Duplicated Code, Long Method, , "
"   Move Method,Inline Method,Move Attribute,","Provide a HealthCheckExecutor service Goals:
* Be able to get an overall (aggregated) result as quickly as possible (ideally <2sec)
* Whenever possible, return most current results (e.g. for a memory check)
* Provide a declarative way for async checks (async checks should be the exception though) 

Approach
* Run checks in parallel
* Make sure long running (or even stuck) checks are timed out
* If a health check must run asynchronously (because its execution time cannot be optimized), it should be enough to just specify a service property (e.g. ""hc.async"").

See also
http://apache-sling.73963.n3.nabble.com/Health-Check-Improvements-td4029330.html#a4029402
http://apache-sling.73963.n3.nabble.com/Health-checks-execution-service-td4028477.html",", , , , "
"   Extract Interface,Rename Method,","Provide a HealthCheckExecutor service Goals:
* Be able to get an overall (aggregated) result as quickly as possible (ideally <2sec)
* Whenever possible, return most current results (e.g. for a memory check)
* Provide a declarative way for async checks (async checks should be the exception though) 

Approach
* Run checks in parallel
* Make sure long running (or even stuck) checks are timed out
* If a health check must run asynchronously (because its execution time cannot be optimized), it should be enough to just specify a service property (e.g. ""hc.async"").

See also
http://apache-sling.73963.n3.nabble.com/Health-Check-Improvements-td4029330.html#a4029402
http://apache-sling.73963.n3.nabble.com/Health-checks-execution-service-td4028477.html",", Large Class, "
"   Move Method,Extract Method,Move Attribute,","Provide a HealthCheckExecutor service Goals:
* Be able to get an overall (aggregated) result as quickly as possible (ideally <2sec)
* Whenever possible, return most current results (e.g. for a memory check)
* Provide a declarative way for async checks (async checks should be the exception though) 

Approach
* Run checks in parallel
* Make sure long running (or even stuck) checks are timed out
* If a health check must run asynchronously (because its execution time cannot be optimized), it should be enough to just specify a service property (e.g. ""hc.async"").

See also
http://apache-sling.73963.n3.nabble.com/Health-Check-Improvements-td4029330.html#a4029402
http://apache-sling.73963.n3.nabble.com/Health-checks-execution-service-td4028477.html","Duplicated Code, Long Method, , , , "
"   Rename Method,Extract Method,","Provide a HealthCheckExecutor service Goals:
* Be able to get an overall (aggregated) result as quickly as possible (ideally <2sec)
* Whenever possible, return most current results (e.g. for a memory check)
* Provide a declarative way for async checks (async checks should be the exception though) 

Approach
* Run checks in parallel
* Make sure long running (or even stuck) checks are timed out
* If a health check must run asynchronously (because its execution time cannot be optimized), it should be enough to just specify a service property (e.g. ""hc.async"").

See also
http://apache-sling.73963.n3.nabble.com/Health-Check-Improvements-td4029330.html#a4029402
http://apache-sling.73963.n3.nabble.com/Health-checks-execution-service-td4028477.html","Duplicated Code, Long Method, , "
"   Move Method,Inline Method,Move Attribute,","Provide a HealthCheckExecutor service Goals:
* Be able to get an overall (aggregated) result as quickly as possible (ideally <2sec)
* Whenever possible, return most current results (e.g. for a memory check)
* Provide a declarative way for async checks (async checks should be the exception though) 

Approach
* Run checks in parallel
* Make sure long running (or even stuck) checks are timed out
* If a health check must run asynchronously (because its execution time cannot be optimized), it should be enough to just specify a service property (e.g. ""hc.async"").

See also
http://apache-sling.73963.n3.nabble.com/Health-Check-Improvements-td4029330.html#a4029402
http://apache-sling.73963.n3.nabble.com/Health-checks-execution-service-td4028477.html",", , , , "
"   Move Method,Extract Method,Move Attribute,","Provide a HealthCheckExecutor service Goals:
* Be able to get an overall (aggregated) result as quickly as possible (ideally <2sec)
* Whenever possible, return most current results (e.g. for a memory check)
* Provide a declarative way for async checks (async checks should be the exception though) 

Approach
* Run checks in parallel
* Make sure long running (or even stuck) checks are timed out
* If a health check must run asynchronously (because its execution time cannot be optimized), it should be enough to just specify a service property (e.g. ""hc.async"").

See also
http://apache-sling.73963.n3.nabble.com/Health-Check-Improvements-td4029330.html#a4029402
http://apache-sling.73963.n3.nabble.com/Health-checks-execution-service-td4028477.html","Duplicated Code, Long Method, , , , "
"   Extract Interface,Rename Method,","Provide a HealthCheckExecutor service Goals:
* Be able to get an overall (aggregated) result as quickly as possible (ideally <2sec)
* Whenever possible, return most current results (e.g. for a memory check)
* Provide a declarative way for async checks (async checks should be the exception though) 

Approach
* Run checks in parallel
* Make sure long running (or even stuck) checks are timed out
* If a health check must run asynchronously (because its execution time cannot be optimized), it should be enough to just specify a service property (e.g. ""hc.async"").

See also
http://apache-sling.73963.n3.nabble.com/Health-Check-Improvements-td4029330.html#a4029402
http://apache-sling.73963.n3.nabble.com/Health-checks-execution-service-td4028477.html",", Large Class, "
"   Move Method,Inline Method,Move Attribute,","Provide a HealthCheckExecutor service Goals:
* Be able to get an overall (aggregated) result as quickly as possible (ideally <2sec)
* Whenever possible, return most current results (e.g. for a memory check)
* Provide a declarative way for async checks (async checks should be the exception though) 

Approach
* Run checks in parallel
* Make sure long running (or even stuck) checks are timed out
* If a health check must run asynchronously (because its execution time cannot be optimized), it should be enough to just specify a service property (e.g. ""hc.async"").

See also
http://apache-sling.73963.n3.nabble.com/Health-Check-Improvements-td4029330.html#a4029402
http://apache-sling.73963.n3.nabble.com/Health-checks-execution-service-td4028477.html",", , , , "
"   Rename Method,Extract Method,","Provide a HealthCheckExecutor service Goals:
* Be able to get an overall (aggregated) result as quickly as possible (ideally <2sec)
* Whenever possible, return most current results (e.g. for a memory check)
* Provide a declarative way for async checks (async checks should be the exception though) 

Approach
* Run checks in parallel
* Make sure long running (or even stuck) checks are timed out
* If a health check must run asynchronously (because its execution time cannot be optimized), it should be enough to just specify a service property (e.g. ""hc.async"").

See also
http://apache-sling.73963.n3.nabble.com/Health-Check-Improvements-td4029330.html#a4029402
http://apache-sling.73963.n3.nabble.com/Health-checks-execution-service-td4028477.html","Duplicated Code, Long Method, , "
"   Rename Method,Pull Up Method,Move Method,Extract Method,Pull Up Attribute,Move Attribute,","Leverage improved observation support from Oak OAK-1120 introduces better support for observation, which could be used by Sling. For example JcrResourceListener could be rewritten leveraging Oak's Observer. Since Oak observers already run on background threads further decoupling (like it is currently done) is not necessary. This makes it unnecessary to queue potentially a lot of events in Sling. Since neither Oak there does queue events (they are generated by need) this will probably greatly improve scalability in the face of many events. 

Furthermore OSGi filters could be passed down and translated to Oak such that filtering is done much closer to the source of the events. 

Finally instead of using a centralised event dispatcher (like JcrResourceListener currently is) it would be better to install a dedicated Observer for each OSGi event listener since dispatching is already handled by Oak and thread pooling (i.e. assigning threads for dispatching call backs to observers) can be controlled through Sling's thread pool support (*). This has the further advantage of making individual stats available for the listeners through JMX.

(*) Register an OakExecutor backed by e.g. a Sling thread pool and it will be picked up by Oak.","Duplicated Code, Long Method, , , , Duplicated Code, Duplicated Code, "
"   Rename Method,Pull Up Method,Move Method,Extract Method,Pull Up Attribute,Move Attribute,","Leverage improved observation support from Oak OAK-1120 introduces better support for observation, which could be used by Sling. For example JcrResourceListener could be rewritten leveraging Oak's Observer. Since Oak observers already run on background threads further decoupling (like it is currently done) is not necessary. This makes it unnecessary to queue potentially a lot of events in Sling. Since neither Oak there does queue events (they are generated by need) this will probably greatly improve scalability in the face of many events. 

Furthermore OSGi filters could be passed down and translated to Oak such that filtering is done much closer to the source of the events. 

Finally instead of using a centralised event dispatcher (like JcrResourceListener currently is) it would be better to install a dedicated Observer for each OSGi event listener since dispatching is already handled by Oak and thread pooling (i.e. assigning threads for dispatching call backs to observers) can be controlled through Sling's thread pool support (*). This has the further advantage of making individual stats available for the listeners through JMX.

(*) Register an OakExecutor backed by e.g. a Sling thread pool and it will be picked up by Oak.","Duplicated Code, Long Method, , , , Duplicated Code, Duplicated Code, "
"   Rename Method,Extract Method,",Allow flushing of external caching systems Allow signaling over HTTP to external caching systems that a content hierarchy has changed.,"Duplicated Code, Long Method, , "
"   Rename Method,","Provide annotation-driven approach to create Model objects I've been working on a model factory approach in my whiteboard and think it is at the state to be formally named Sling Models and moved into extensions.

http://svn.apache.org/repos/asf/sling/whiteboard/justin/yamf/

Code would be repackage and renamed where appropriate.

See
https://cwiki.apache.org/confluence/display/SLING/YAMF+-+Yet+Another+Model+Factory",", "
"   Rename Method,","Provide annotation-driven approach to create Model objects I've been working on a model factory approach in my whiteboard and think it is at the state to be formally named Sling Models and moved into extensions.

http://svn.apache.org/repos/asf/sling/whiteboard/justin/yamf/

Code would be repackage and renamed where appropriate.

See
https://cwiki.apache.org/confluence/display/SLING/YAMF+-+Yet+Another+Model+Factory",", "
"   Rename Method,",Remove packages after being processed After a replication package it is used cleanup the resources it occupied.,", "
"   Rename Method,",Remove packages after being processed After a replication package it is used cleanup the resources it occupied.,", "
"   Rename Method,Pull Up Attribute,","Add ability to replicate to multiple enpoints Add ability to replicate to multiple endpoints in order to support broadcasting in a clustered environment.
The implementation should also have a configurable strategy (broadcast to all or just to one in the cluster).
It should support also polling from multiple sources.
",", Duplicated Code, "
"   Rename Method,Pull Up Attribute,","Add ability to replicate to multiple enpoints Add ability to replicate to multiple endpoints in order to support broadcasting in a clustered environment.
The implementation should also have a configurable strategy (broadcast to all or just to one in the cluster).
It should support also polling from multiple sources.
",", Duplicated Code, "
"   Rename Method,Move Method,","Simplify Feature Flags API The Feature Flags API currently is quite complex involving a helper object making query for FeatureFlag status quite complex. Also, setting the current context is clumsy with (currently) two filters.

I propose a few changes:

(a) Remove ClientContext object. Checking for feature enablement should be as simple as just calling ""Features.isEnabled(featureName)"". Currently the curent ClientContext has to be retrieved and then the isEnabled method being called there.

(b) Feature flag values are egerly evaluated. This should be done on demand making the context setup much quicker and light-weight. For added performance the evaluation results can still be cached.

(c) The duplicate filter can be removed by code directly grabbing the ResourceResolver from the request attribute just like the SlingMainServlet does (thanks CarstenZ for the hint).

(d) We don't currently need the explicit ""context"" management because the servlet filter does that directly using internal API.

(e) I wonder, whether we need all these feature accessors in the Features service, some look like convenience methods, particularly the one of getFeatureNames and getFeatures.",", , "
"   Rename Method,Move Method,","Simplify Feature Flags API The Feature Flags API currently is quite complex involving a helper object making query for FeatureFlag status quite complex. Also, setting the current context is clumsy with (currently) two filters.

I propose a few changes:

(a) Remove ClientContext object. Checking for feature enablement should be as simple as just calling ""Features.isEnabled(featureName)"". Currently the curent ClientContext has to be retrieved and then the isEnabled method being called there.

(b) Feature flag values are egerly evaluated. This should be done on demand making the context setup much quicker and light-weight. For added performance the evaluation results can still be cached.

(c) The duplicate filter can be removed by code directly grabbing the ResourceResolver from the request attribute just like the SlingMainServlet does (thanks CarstenZ for the hint).

(d) We don't currently need the explicit ""context"" management because the servlet filter does that directly using internal API.

(e) I wonder, whether we need all these feature accessors in the Features service, some look like convenience methods, particularly the one of getFeatureNames and getFeatures.",", , "
"   Rename Class,Move Method,Move Attribute,","Expose OSGI configuration via HTTP We need a safe way to expose OSGI configuration via HTTP.

Requirements:
- all configs for a certain factory should be manageable
- they should have associated JCR nodes that contain the config properties
- only configs that are available through ConfigurationAdmin should be available
- the HTTP urls should have friendly names
- (Optional) the implementation should be general enough to be used for other configs other than replication if needed

For example: a configuration with name publish for org.apache.sling.replication.agent.impl.ReplicationAgentServiceFactory
should be mapped to /etc/replication/agent/publish


Problems with current implementation of JCR nodes created by JCR installed:
- Configuration files are read and created from /apps/.../config or /libs/.../config, and there is no easy way to determine which are active in the ConfigurationAdmin
- There is no way to restrict a repository path to create only configuration from a specified factory (making it unusable with relaxed ACLs)
- The url of a configuration is unfriendly (it contains the fully qualified name of the factory)
- The node types are not homogenous making it hard to use in a client application (some are nt:file, some are sling:OsgiConfig)


",", , , "
"   Rename Class,Move Method,Move Attribute,","Expose OSGI configuration via HTTP We need a safe way to expose OSGI configuration via HTTP.

Requirements:
- all configs for a certain factory should be manageable
- they should have associated JCR nodes that contain the config properties
- only configs that are available through ConfigurationAdmin should be available
- the HTTP urls should have friendly names
- (Optional) the implementation should be general enough to be used for other configs other than replication if needed

For example: a configuration with name publish for org.apache.sling.replication.agent.impl.ReplicationAgentServiceFactory
should be mapped to /etc/replication/agent/publish


Problems with current implementation of JCR nodes created by JCR installed:
- Configuration files are read and created from /apps/.../config or /libs/.../config, and there is no easy way to determine which are active in the ConfigurationAdmin
- There is no way to restrict a repository path to create only configuration from a specified factory (making it unusable with relaxed ACLs)
- The url of a configuration is unfriendly (it contains the fully qualified name of the factory)
- The node types are not homogenous making it hard to use in a client application (some are nt:file, some are sling:OsgiConfig)


",", , , "
"   Rename Method,Extract Method,","Expose the import queue for reverse replication through ReplicationAgent interface The replication agent should have 3 main queues (the request queue, the transport queue and the response queue). These queues should be accessible and manageable through ReplicationAgent interface.

This issue relates to the implementation of the response queue inside the ReplicationAgent and making it accessible through ReplicationAgent.getQueue API. The response queue is the queue where reverse replication on author stores packages from publish.

","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Expose the import queue for reverse replication through ReplicationAgent interface The replication agent should have 3 main queues (the request queue, the transport queue and the response queue). These queues should be accessible and manageable through ReplicationAgent interface.

This issue relates to the implementation of the response queue inside the ReplicationAgent and making it accessible through ReplicationAgent.getQueue API. The response queue is the queue where reverse replication on author stores packages from publish.

","Duplicated Code, Long Method, , "
"   Rename Method,","Bootdelegation support for third party library Requirement:
1) Bootdelegate third party library in SLing OSGI framework.
2) Bootdelegated classes and resources should be looked up from configured third party jars only. Similar classes from parent classloaders should not leak into system. This case arises usually in Web deployment scenario. 

Infact we want to bootdelegate RSA library and at the same time prevent interference from parent classloaders in Web deployment model.

Approach:

1) We extend SlingLauncherClassloader to scan a predefined path (say <launchpadhome>/lib/etx) for jar files and add them to its classpath. This is done at startup. SlingLauncherClassloader will try to load class from its classpath before trying parent classloaders. Currently it is done for org.apache.sling.launchpad.base.jar. 
2) The packages can be configured for bootdelegation using sling properties:
sling.bootdelegation.class.<any bootdelegated class>=<bootdelegated package>",", "
"   Rename Method,","Bootdelegation support for third party library Requirement:
1) Bootdelegate third party library in SLing OSGI framework.
2) Bootdelegated classes and resources should be looked up from configured third party jars only. Similar classes from parent classloaders should not leak into system. This case arises usually in Web deployment scenario. 

Infact we want to bootdelegate RSA library and at the same time prevent interference from parent classloaders in Web deployment model.

Approach:

1) We extend SlingLauncherClassloader to scan a predefined path (say <launchpadhome>/lib/etx) for jar files and add them to its classpath. This is done at startup. SlingLauncherClassloader will try to load class from its classpath before trying parent classloaders. Currently it is done for org.apache.sling.launchpad.base.jar. 
2) The packages can be configured for bootdelegation using sling properties:
sling.bootdelegation.class.<any bootdelegated class>=<bootdelegated package>",", "
"   Rename Method,","Reduce memory footprint of JcrResourceListener JcrResourceListener.onEvent(EventIterator) keeps references to all Event instances in the passed iterator. This has turned out as a memory bottleneck e.g. in the scenario where a large sub-tree is copied and NODE_ADDED events for all nodes in that sub-tree are generated. 

Furthermore Oak will be able to handle arbitrarily large transactions (Session.save). In such cases the EventIterator might contains millions of events.",", "
"   Rename Method,","Reduce memory footprint of JcrResourceListener JcrResourceListener.onEvent(EventIterator) keeps references to all Event instances in the passed iterator. This has turned out as a memory bottleneck e.g. in the scenario where a large sub-tree is copied and NODE_ADDED events for all nodes in that sub-tree are generated. 

Furthermore Oak will be able to handle arbitrarily large transactions (Session.save). In such cases the EventIterator might contains millions of events.",", "
"   Rename Method,","Reduce memory footprint of JcrResourceListener JcrResourceListener.onEvent(EventIterator) keeps references to all Event instances in the passed iterator. This has turned out as a memory bottleneck e.g. in the scenario where a large sub-tree is copied and NODE_ADDED events for all nodes in that sub-tree are generated. 

Furthermore Oak will be able to handle arbitrarily large transactions (Session.save). In such cases the EventIterator might contains millions of events.",", "
"   Rename Method,","Reduce memory footprint of JcrResourceListener JcrResourceListener.onEvent(EventIterator) keeps references to all Event instances in the passed iterator. This has turned out as a memory bottleneck e.g. in the scenario where a large sub-tree is copied and NODE_ADDED events for all nodes in that sub-tree are generated. 

Furthermore Oak will be able to handle arbitrarily large transactions (Session.save). In such cases the EventIterator might contains millions of events.",", "
"   Rename Method,Extract Method,","introduce back-off strategy for topology connector frequency Currently topology heartbeats are sent every 15 or 30 sec, which might seem a lot – especially as they were way too chatty (which is fixed now with SLING-3377). The suggestion by [~fmeschbe] is to lower this heartbeat frequency.

The main reason for having a high heartbeat frequency is quicker failure detection – but it's obviously a trade-off as it increases load.

Here's a proposal for how to tackle this:

* introduce two different sets of heartbeats, one for repository and one for connectors
* the repository ones would remain at the current frequency (suggested default: 30sec interval, 60sec timeout). The idea is that we would want to detect crashes within a cluster rather quickly, more quickly than in the topology in general.
* the connectors would get a back-off behavior, where initially the values are the same (30sec/60sec) but then they send out less frequent heartbeats over time, reaching a max (eg 5min). This would have to be controlled by the receiving side, ie both sides of the connector have to agree that interval and timeout are the same.","Duplicated Code, Long Method, , "
"   Move Class,Rename Method,Move Method,Move Attribute,","Simplify AbstractSlingRepository implementation With the introduction of the SlingRepository.loginService method the existing setup of the AbstractSlingRepository became quite complex in that it hacks in a SlingRepository proxy to be able to register the SlingRepository as a service and implement the new method.

An additional problem of the AbstractSlingRepository class is that it expects the implementation to be implemented using Declarative Services. While this was simple and easy in the beginning it created a runtime dependency which does not go well with the OSGi framework.

So, I propose to create a new couple of (abstract) classes which simplify the setup and implementation of SlingRepository services.

Another ""feature"" of the original AbstractSlingRepository base class was access to ""foreign"" repositories as well as repository pinging which turns out to be functionality not being usefull in an abstract base class. Rather this would be something in an actual implementation which knows how to deal with such pre-existing ""foreign"" repository instances.",", , , "
"   Move Class,Rename Method,Move Method,Move Attribute,","Simplify AbstractSlingRepository implementation With the introduction of the SlingRepository.loginService method the existing setup of the AbstractSlingRepository became quite complex in that it hacks in a SlingRepository proxy to be able to register the SlingRepository as a service and implement the new method.

An additional problem of the AbstractSlingRepository class is that it expects the implementation to be implemented using Declarative Services. While this was simple and easy in the beginning it created a runtime dependency which does not go well with the OSGi framework.

So, I propose to create a new couple of (abstract) classes which simplify the setup and implementation of SlingRepository services.

Another ""feature"" of the original AbstractSlingRepository base class was access to ""foreign"" repositories as well as repository pinging which turns out to be functionality not being usefull in an abstract base class. Rather this would be something in an actual implementation which knows how to deal with such pre-existing ""foreign"" repository instances.",", , , "
"   Move Method,Move Attribute,","Simplify AbstractSlingRepository implementation With the introduction of the SlingRepository.loginService method the existing setup of the AbstractSlingRepository became quite complex in that it hacks in a SlingRepository proxy to be able to register the SlingRepository as a service and implement the new method.

An additional problem of the AbstractSlingRepository class is that it expects the implementation to be implemented using Declarative Services. While this was simple and easy in the beginning it created a runtime dependency which does not go well with the OSGi framework.

So, I propose to create a new couple of (abstract) classes which simplify the setup and implementation of SlingRepository services.

Another ""feature"" of the original AbstractSlingRepository base class was access to ""foreign"" repositories as well as repository pinging which turns out to be functionality not being usefull in an abstract base class. Rather this would be something in an actual implementation which knows how to deal with such pre-existing ""foreign"" repository instances.",", , , "
"   Rename Method,Pull Up Method,Pull Up Attribute,","Simplify AbstractSlingRepository implementation With the introduction of the SlingRepository.loginService method the existing setup of the AbstractSlingRepository became quite complex in that it hacks in a SlingRepository proxy to be able to register the SlingRepository as a service and implement the new method.

An additional problem of the AbstractSlingRepository class is that it expects the implementation to be implemented using Declarative Services. While this was simple and easy in the beginning it created a runtime dependency which does not go well with the OSGi framework.

So, I propose to create a new couple of (abstract) classes which simplify the setup and implementation of SlingRepository services.

Another ""feature"" of the original AbstractSlingRepository base class was access to ""foreign"" repositories as well as repository pinging which turns out to be functionality not being usefull in an abstract base class. Rather this would be something in an actual implementation which knows how to deal with such pre-existing ""foreign"" repository instances.",", Duplicated Code, Duplicated Code, "
"   Move Method,Move Attribute,","Simplify AbstractSlingRepository implementation With the introduction of the SlingRepository.loginService method the existing setup of the AbstractSlingRepository became quite complex in that it hacks in a SlingRepository proxy to be able to register the SlingRepository as a service and implement the new method.

An additional problem of the AbstractSlingRepository class is that it expects the implementation to be implemented using Declarative Services. While this was simple and easy in the beginning it created a runtime dependency which does not go well with the OSGi framework.

So, I propose to create a new couple of (abstract) classes which simplify the setup and implementation of SlingRepository services.

Another ""feature"" of the original AbstractSlingRepository base class was access to ""foreign"" repositories as well as repository pinging which turns out to be functionality not being usefull in an abstract base class. Rather this would be something in an actual implementation which knows how to deal with such pre-existing ""foreign"" repository instances.",", , , "
"   Rename Method,Pull Up Method,Pull Up Attribute,","Simplify AbstractSlingRepository implementation With the introduction of the SlingRepository.loginService method the existing setup of the AbstractSlingRepository became quite complex in that it hacks in a SlingRepository proxy to be able to register the SlingRepository as a service and implement the new method.

An additional problem of the AbstractSlingRepository class is that it expects the implementation to be implemented using Declarative Services. While this was simple and easy in the beginning it created a runtime dependency which does not go well with the OSGi framework.

So, I propose to create a new couple of (abstract) classes which simplify the setup and implementation of SlingRepository services.

Another ""feature"" of the original AbstractSlingRepository base class was access to ""foreign"" repositories as well as repository pinging which turns out to be functionality not being usefull in an abstract base class. Rather this would be something in an actual implementation which knows how to deal with such pre-existing ""foreign"" repository instances.",", Duplicated Code, Duplicated Code, "
"   Rename Method,Extract Method,Inline Method,","[discovery] reduce write activity of discovery heartbeats As discussed on the list at [0] there are ways to reduce the #writes to the repository for topology connector announcements. This ticket is about 1) and 2) of mentioned thread. Here again for completeness sake:

1) store the announcement's lastHeartbeat value (ie created value) as a separate property instead of as part of the json. Plus only update the json if anything has changed.
2) don't store the announcement's lastHeartbeat (or created) at all - but instead change the logic such that an announcement is valid as long the container-instance (the instance which has received and stored the announcement) is alive. This increases reaction time slightly (in the worst case x2..) in case the containing instance and some instance which is part of the announcement crash within the same heartbeat interval.

[0] - http://markmail.org/thread/2ev5sy3b3mr5klc5","Duplicated Code, Long Method, , , "
"   Rename Method,","Support multiple bundles for jarWebSupport The Launchpad Plugin allows to define the Http Service implementation to be specified with the jarWebSupport property. This property currently only supports a single bundle.

Now, the Felix Jetty Http Service implementation has been refactored (FELIX-4427) to not export the API anymore. Thus the Http Service API and the Servlet API have to be provided by one or more additional bundles.

Hence the jarWebSupport property should be extended to support multiple bundles.",", "
"   Rename Method,","Support multiple bundles for jarWebSupport The Launchpad Plugin allows to define the Http Service implementation to be specified with the jarWebSupport property. This property currently only supports a single bundle.

Now, the Felix Jetty Http Service implementation has been refactored (FELIX-4427) to not export the API anymore. Thus the Http Service API and the Servlet API have to be provided by one or more additional bundles.

Hence the jarWebSupport property should be extended to support multiple bundles.",", "
"   Rename Method,Extract Method,","Improve Parameter Support As discussed on the dev list [1], Sling's request parameter support should be extended and improved:

* Provide list of request parameters in the order specified in the request (the Servlet API does not define the order and servlet containers may or may not respect the order in the parameter map; but there is no official API to get the list of request parameters in the actual order as defined on the request)
* Support Servlet API 3.0 style multipart/form-data requests
* Make sure request parameters are uniformely supported on all servlet conainers with respect to character encoding and size restrictions.

As a corollary to Servlet API 3 support for request parameters, we might also want to extend the ServletContext API to implement the Servlet API 3 methods (even though, most will just throw)

[1] http://sling.markmail.org/thread/ysz5sxpb6wkuelzd","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Improve Parameter Support As discussed on the dev list [1], Sling's request parameter support should be extended and improved:

* Provide list of request parameters in the order specified in the request (the Servlet API does not define the order and servlet containers may or may not respect the order in the parameter map; but there is no official API to get the list of request parameters in the actual order as defined on the request)
* Support Servlet API 3.0 style multipart/form-data requests
* Make sure request parameters are uniformely supported on all servlet conainers with respect to character encoding and size restrictions.

As a corollary to Servlet API 3 support for request parameters, we might also want to extend the ServletContext API to implement the Servlet API 3 methods (even though, most will just throw)

[1] http://sling.markmail.org/thread/ysz5sxpb6wkuelzd","Duplicated Code, Long Method, , "
"   Move Method,Move Attribute,","Enable WebDAV to the Sling default workspace through Sling Currently, the jcr/webdav module provides WebDAV through the SimpleWebDavServlet in its own servlet context (configurable, default is /dav). This default support requires including the name of the workspace to access in the URL.

Additional support for WebDAV should be added which allows WebDAV access to the default workspace used by Sling through the Sling Main Servlet. As such, a Sling Servlet is to be registered which implements WebDAV.",", , , "
"   Move Method,Move Attribute,","Enable WebDAV to the Sling default workspace through Sling Currently, the jcr/webdav module provides WebDAV through the SimpleWebDavServlet in its own servlet context (configurable, default is /dav). This default support requires including the name of the workspace to access in the URL.

Additional support for WebDAV should be added which allows WebDAV access to the default workspace used by Sling through the Sling Main Servlet. As such, a Sling Servlet is to be registered which implements WebDAV.",", , , "
"   Move Method,Move Attribute,",ResourceAccessSecurity does not secure access for update operations ResourceAccessSecurity should use gates registered for update operations in order to secure access to modifiable value maps.,", , , "
"   Rename Method,","GET /foo/*.html should return the equivalent of an empty Node Being discussed on sling-dev in the ""GET the magic star"" thread [1].

[1] http://www.mail-archive.com/sling-dev@incubator.apache.org/msg03603.html",", "
"   Rename Method,","GET /foo/*.html should return the equivalent of an empty Node Being discussed on sling-dev in the ""GET the magic star"" thread [1].

[1] http://www.mail-archive.com/sling-dev@incubator.apache.org/msg03603.html",", "
"   Rename Class,Extract Superclass,Rename Method,",Add Sling Replication ITs for all the exposed resources Provide integration tests for all the resources exposed by Sling Replication to check which ones are exposed given a certain runMode and possibly how to restrict access to them.,", Duplicated Code, Large Class, "
"   Rename Class,Extract Method,",Add Sling Replication ITs for all the exposed resources Provide integration tests for all the resources exposed by Sling Replication to check which ones are exposed given a certain runMode and possibly how to restrict access to them.,"Duplicated Code, Long Method, , "
"   Rename Class,Extract Superclass,Rename Method,",Add Sling Replication ITs for all the exposed resources Provide integration tests for all the resources exposed by Sling Replication to check which ones are exposed given a certain runMode and possibly how to restrict access to them.,", Duplicated Code, Large Class, "
"   Rename Class,Extract Method,",Add Sling Replication ITs for all the exposed resources Provide integration tests for all the resources exposed by Sling Replication to check which ones are exposed given a certain runMode and possibly how to restrict access to them.,"Duplicated Code, Long Method, , "
"   Extract Interface,Extract Method,","Facilitate writing of integration tests for multiple instances Facilitate writing of integration tests for multiple instances. This is needed for better testing features like replication or topology discovery.

Create a base class for such tests and a way to configure multiple instances and also sample integration test.

","Duplicated Code, Long Method, , Large Class, "
"   Rename Method,","Facilitate writing of integration tests for multiple instances Facilitate writing of integration tests for multiple instances. This is needed for better testing features like replication or topology discovery.

Create a base class for such tests and a way to configure multiple instances and also sample integration test.

",", "
"   Rename Method,","Make CompositeHealthCheck use HealthCheckExecutor for parallel execution As the CompositeHealthCheck is used fairly heavily in the well-known product ([1] & [2]) it would be good to make the parallel execution available for the CompositeHealthCheck as well (this would also be in line with the web console that is already using the HealthCheckExecutor). 

The attached patch 
- uses the HealthCheckExecutor (shortening the implementation of the CompositeHealthCheck.execute() for quite a bit)
- needs to detect cycles in the configuration in a different way (the ThreadLocal does not work anymore)
- comes with a unit test that tests both the execution and the cycle detection

[1] http://localhost:4502/system/sling/monitoring/mbeans/org/apache/sling/healthcheck/HealthCheck/systemchecks.json
[2] http://localhost:4502/libs/granite/operations/content/hr.html/system/sling/monitoring/mbeans/org/apache/sling/healthcheck/HealthCheck/securitychecks",", "
"   Rename Method,","Make CompositeHealthCheck use HealthCheckExecutor for parallel execution As the CompositeHealthCheck is used fairly heavily in the well-known product ([1] & [2]) it would be good to make the parallel execution available for the CompositeHealthCheck as well (this would also be in line with the web console that is already using the HealthCheckExecutor). 

The attached patch 
- uses the HealthCheckExecutor (shortening the implementation of the CompositeHealthCheck.execute() for quite a bit)
- needs to detect cycles in the configuration in a different way (the ThreadLocal does not work anymore)
- comes with a unit test that tests both the execution and the cycle detection

[1] http://localhost:4502/system/sling/monitoring/mbeans/org/apache/sling/healthcheck/HealthCheck/systemchecks.json
[2] http://localhost:4502/libs/granite/operations/content/hr.html/system/sling/monitoring/mbeans/org/apache/sling/healthcheck/HealthCheck/securitychecks",", "
"   Rename Method,",Add ability to configure OSGi components from crank files Users should be able to specify OSGi configs inside .crank.txt files.,", "
"   Rename Method,",Add ability to configure OSGi components from crank files Users should be able to specify OSGi configs inside .crank.txt files.,", "
"   Extract Method,Move Attribute,",separate node name and content type in content loader extend API (node name and content type are currently taken from file name),"Duplicated Code, Long Method, , , "
"   Extract Method,Move Attribute,",separate node name and content type in content loader extend API (node name and content type are currently taken from file name),"Duplicated Code, Long Method, , , "
"   Rename Method,",Run FindBugs (and fix accordingly) on Sling Replication code Sling Replication code has grown over time and I'd like to a) fix current problems as reported by FindBugs b) add a reporting entry to the pom.xml in order to be continuously able to check eventual problems.,", "
"   Rename Method,",Run FindBugs (and fix accordingly) on Sling Replication code Sling Replication code has grown over time and I'd like to a) fix current problems as reported by FindBugs b) add a reporting entry to the pom.xml in order to be continuously able to check eventual problems.,", "
"   Rename Method,","Allow importing content from arbitrary locations The current import wizard only processes the whole project. This is problematic for large projects where import would take a lot of time. Also, at times a developer makes a small change in the repository and wants to sync it back to the workspace.

To support these scenarios we need to allow importing content from arbitrary locations under the content sync root.",", "
"   Rename Method,","Make RepositoryFactory statefull, allow a Repository to be stopped At the moment we have a stateless RepositoryFactory, ie it only provides newRepository() which creates a new repository each time. This was fine as long as the repository was stateless - but with the introduction of the NodeTypeRegistry for example, the repository initialization is now also heavy-weight.",", "
"   Rename Method,","Make RepositoryFactory statefull, allow a Repository to be stopped At the moment we have a stateless RepositoryFactory, ie it only provides newRepository() which creates a new repository each time. This was fine as long as the repository was stateless - but with the introduction of the NodeTypeRegistry for example, the repository initialization is now also heavy-weight.",", "
"   Rename Method,","Allow performance tests to report custom class name to the report logger allow performance tests to overwrite the class name passed in by the junit runner to the report logger. This is useful for test factories and the like.

SLING-3294 breaks SLING-2727 because it forces a toString method on the test class. The proposed solution allows this only if the test case implements IdentifiableClassName#testClassName()

",", "
"   Rename Method,","Allow performance tests to report custom class name to the report logger allow performance tests to overwrite the class name passed in by the junit runner to the report logger. This is useful for test factories and the like.

SLING-3294 breaks SLING-2727 because it forces a toString method on the test class. The proposed solution allows this only if the test case implements IdentifiableClassName#testClassName()

",", "
"   Rename Class,Rename Method,Move Method,Move Attribute,",Remove embedded Tomcat Juli jar by overriding LogFactory which delegates to Slf4j DataSource provider bundle currently embed the Tomcat Juli jar ~38 kb (required by Tomcat JDBC). This can be easily removed by inlining and overriding the org.apache.juli.logging.LogFactory implementation which delegates to Slf4j,", , , "
"   Rename Class,Rename Method,Move Method,Move Attribute,",Remove embedded Tomcat Juli jar by overriding LogFactory which delegates to Slf4j DataSource provider bundle currently embed the Tomcat Juli jar ~38 kb (required by Tomcat JDBC). This can be easily removed by inlining and overriding the org.apache.juli.logging.LogFactory implementation which delegates to Slf4j,", , , "
"   Extract Method,Inline Method,","Sling console: keep console history since start Just as an idea: currently the SlingConsoleEventListener throws away events if the console is not showing (==null). Even though it might not be standard, but IMHO it could be useful if the listener keeps the history (with a limit..) and shows it as soon as the console is started. The reasoning is: you don't always start the console right away at startup - but likely when you run into issues - at which point you don't have any history (yet/anymore).","Duplicated Code, Long Method, , , "
"   Extract Method,Inline Method,","Sling console: keep console history since start Just as an idea: currently the SlingConsoleEventListener throws away events if the console is not showing (==null). Even though it might not be standard, but IMHO it could be useful if the listener keeps the history (with a limit..) and shows it as soon as the console is started. The reasoning is: you don't always start the console right away at startup - but likely when you run into issues - at which point you don't have any history (yet/anymore).","Duplicated Code, Long Method, , , "
"   Move And Rename Class,","Debug based on bundles deployed on the server During an offline discussion with [~mpetria] the following item came up: when debugging a Sling application with a large number of bundles, it's hard to synchronise sources between the workspace and the server. It would be good to be able to debug the application based on the information that we have about the running bundles ( e.g. if they are Maven artifacts we can retrieve the sources and associated them to the project ).

Not trivial but worth looking into.",", "
"   Move And Rename Class,","Debug based on bundles deployed on the server During an offline discussion with [~mpetria] the following item came up: when debugging a Sling application with a large number of bundles, it's hard to synchronise sources between the workspace and the server. It would be good to be able to debug the application based on the information that we have about the running bundles ( e.g. if they are Maven artifacts we can retrieve the sources and associated them to the project ).

Not trivial but worth looking into.",", "
"   Rename Method,","Add SlingHealthCheck annotation Add a SlingHealthCheck annotation (similar to SlingServlet). The metatype property should set to true per default since its probably common to configure healthchecks. Usage examples:

{code:title=Examples|borderStyle=solid}
@SlingHealthCheck(name = BundlesStartedCheck.HC_NAME, label = ""Apache Sling Health Check - "" + BundlesStartedCheck.HC_NAME, description = ""Checks whether all bundles are started."", tags = ""osgi"")

@SlingHealthCheck(name = DiskSpaceCheck.HC_NAME, label = ""Apache Sling Health Check - "" + DiskSpaceCheck.HC_NAME,
description = ""Checks whether enough disk space is available."", tags = ""resources"", configurationFactory = true,
configurationPolicy = ConfigurationPolicy.REQUIRE)
{code}",", "
"   Rename Method,","Add SlingHealthCheck annotation Add a SlingHealthCheck annotation (similar to SlingServlet). The metatype property should set to true per default since its probably common to configure healthchecks. Usage examples:

{code:title=Examples|borderStyle=solid}
@SlingHealthCheck(name = BundlesStartedCheck.HC_NAME, label = ""Apache Sling Health Check - "" + BundlesStartedCheck.HC_NAME, description = ""Checks whether all bundles are started."", tags = ""osgi"")

@SlingHealthCheck(name = DiskSpaceCheck.HC_NAME, label = ""Apache Sling Health Check - "" + DiskSpaceCheck.HC_NAME,
description = ""Checks whether enough disk space is available."", tags = ""resources"", configurationFactory = true,
configurationPolicy = ConfigurationPolicy.REQUIRE)
{code}",", "
"   Rename Method,","Add SlingHealthCheck annotation Add a SlingHealthCheck annotation (similar to SlingServlet). The metatype property should set to true per default since its probably common to configure healthchecks. Usage examples:

{code:title=Examples|borderStyle=solid}
@SlingHealthCheck(name = BundlesStartedCheck.HC_NAME, label = ""Apache Sling Health Check - "" + BundlesStartedCheck.HC_NAME, description = ""Checks whether all bundles are started."", tags = ""osgi"")

@SlingHealthCheck(name = DiskSpaceCheck.HC_NAME, label = ""Apache Sling Health Check - "" + DiskSpaceCheck.HC_NAME,
description = ""Checks whether enough disk space is available."", tags = ""resources"", configurationFactory = true,
configurationPolicy = ConfigurationPolicy.REQUIRE)
{code}",", "
"   Rename Method,","Add SlingHealthCheck annotation Add a SlingHealthCheck annotation (similar to SlingServlet). The metatype property should set to true per default since its probably common to configure healthchecks. Usage examples:

{code:title=Examples|borderStyle=solid}
@SlingHealthCheck(name = BundlesStartedCheck.HC_NAME, label = ""Apache Sling Health Check - "" + BundlesStartedCheck.HC_NAME, description = ""Checks whether all bundles are started."", tags = ""osgi"")

@SlingHealthCheck(name = DiskSpaceCheck.HC_NAME, label = ""Apache Sling Health Check - "" + DiskSpaceCheck.HC_NAME,
description = ""Checks whether enough disk space is available."", tags = ""resources"", configurationFactory = true,
configurationPolicy = ConfigurationPolicy.REQUIRE)
{code}",", "
"   Rename Method,Extract Method,","Checkin versionable nodes in initial content As discussed in [1] a bundle developer should be able to decide wether a versionable node in his initial content should be checked in after the content is imported or not.

[1] http://www.mail-archive.com/sling-dev@incubator.apache.org/msg03804.html","Duplicated Code, Long Method, , "
"   Rename Method,","Refine 'connected' state of a (vlt) repository, cache node types after disconnection properly Currently the ServerUtil.getDefaultRepository always returns a valid repository with which you can do getNodeTypeRegistry() and that in turn connects to the server and loads the node types. This seemed convenient.

But the problem is, it doesn't give the user any control when to connect to the server and when not. And it will result in connection errors at places where the user maybe didn't intend or know that a connection would be done.

Hence a new, simpler schema:
* when the server is stopped, no 'repository connection' is being established, including for the node type registry
** hence, with a stopped server, the node type registry can be null - hence some actions require adjustments for that situation
* when the server is started ('connected to'), the repository is connected and the node type registry loaded
** at this stage the node type registry can be used for various actions, including code completion, property type display
* when the server is stopped again, the node type registry is cached and still provided to the various actions (without any server interaction going on though)

This should be more intuitive and make initial content-browsing in 'offline mode' simpler",", "
"   Rename Method,","Refine 'connected' state of a (vlt) repository, cache node types after disconnection properly Currently the ServerUtil.getDefaultRepository always returns a valid repository with which you can do getNodeTypeRegistry() and that in turn connects to the server and loads the node types. This seemed convenient.

But the problem is, it doesn't give the user any control when to connect to the server and when not. And it will result in connection errors at places where the user maybe didn't intend or know that a connection would be done.

Hence a new, simpler schema:
* when the server is stopped, no 'repository connection' is being established, including for the node type registry
** hence, with a stopped server, the node type registry can be null - hence some actions require adjustments for that situation
* when the server is started ('connected to'), the repository is connected and the node type registry loaded
** at this stage the node type registry can be used for various actions, including code completion, property type display
* when the server is stopped again, the node type registry is cached and still provided to the various actions (without any server interaction going on though)

This should be more intuitive and make initial content-browsing in 'offline mode' simpler",", "
"   Rename Method,Move Method,Extract Method,Inline Method,","Sling Models: Allow caller to deal with exceptions Currently due to the specification of the adaptTo-method to return null if adaptation is not possible, the caller is not notified about any exceptions (because they are caught within the ModelAdapterFactory).

This is e.g. necessary to deal with validation exceptions properly (i.e. required field injection not possible). The problem was also discussed briefly in http://apache-sling.73963.n3.nabble.com/Silng-Models-Validation-Framework-td4033411.html.

All exceptions either being thrown by the 
@PostConstruct method or caused by the field/method injection are not propagated but basically swallowed by Sling Models.

It would be great to be able to catch those exceptions either in the view or in a servlet filter. I think it should be possible to throw unchecked exceptions in the ModelAdapterFactory.getFactory() method if it is requested (i.e. through a global OSGi configuration flag for Sling Models).
WDYT?
Would you accept such a patch or do you think this breaks the API (also compare with https://issues.apache.org/jira/browse/SLING-2712?focusedCommentId=13561516&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13561516).

If it does not work through the adaptTo, SlingModels should provide an alternative way of instanciating models (and propagating exceptions), although this is kind of tricky, because it internally relies on adaptTo as well (e.g. in https://github.com/apache/sling/blob/trunk/bundles/extensions/models/impl/src/main/java/org/apache/sling/models/impl/ModelAdapterFactory.java#L647)","Duplicated Code, Long Method, , , , "
"   Rename Method,Move Method,Extract Method,Inline Method,","Sling Models: Allow caller to deal with exceptions Currently due to the specification of the adaptTo-method to return null if adaptation is not possible, the caller is not notified about any exceptions (because they are caught within the ModelAdapterFactory).

This is e.g. necessary to deal with validation exceptions properly (i.e. required field injection not possible). The problem was also discussed briefly in http://apache-sling.73963.n3.nabble.com/Silng-Models-Validation-Framework-td4033411.html.

All exceptions either being thrown by the 
@PostConstruct method or caused by the field/method injection are not propagated but basically swallowed by Sling Models.

It would be great to be able to catch those exceptions either in the view or in a servlet filter. I think it should be possible to throw unchecked exceptions in the ModelAdapterFactory.getFactory() method if it is requested (i.e. through a global OSGi configuration flag for Sling Models).
WDYT?
Would you accept such a patch or do you think this breaks the API (also compare with https://issues.apache.org/jira/browse/SLING-2712?focusedCommentId=13561516&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13561516).

If it does not work through the adaptTo, SlingModels should provide an alternative way of instanciating models (and propagating exceptions), although this is kind of tricky, because it internally relies on adaptTo as well (e.g. in https://github.com/apache/sling/blob/trunk/bundles/extensions/models/impl/src/main/java/org/apache/sling/models/impl/ModelAdapterFactory.java#L647)","Duplicated Code, Long Method, , , , "
"   Rename Method,Extract Method,Inline Method,","Sling Models: Allow caller to deal with exceptions Currently due to the specification of the adaptTo-method to return null if adaptation is not possible, the caller is not notified about any exceptions (because they are caught within the ModelAdapterFactory).

This is e.g. necessary to deal with validation exceptions properly (i.e. required field injection not possible). The problem was also discussed briefly in http://apache-sling.73963.n3.nabble.com/Silng-Models-Validation-Framework-td4033411.html.

All exceptions either being thrown by the 
@PostConstruct method or caused by the field/method injection are not propagated but basically swallowed by Sling Models.

It would be great to be able to catch those exceptions either in the view or in a servlet filter. I think it should be possible to throw unchecked exceptions in the ModelAdapterFactory.getFactory() method if it is requested (i.e. through a global OSGi configuration flag for Sling Models).
WDYT?
Would you accept such a patch or do you think this breaks the API (also compare with https://issues.apache.org/jira/browse/SLING-2712?focusedCommentId=13561516&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13561516).

If it does not work through the adaptTo, SlingModels should provide an alternative way of instanciating models (and propagating exceptions), although this is kind of tricky, because it internally relies on adaptTo as well (e.g. in https://github.com/apache/sling/blob/trunk/bundles/extensions/models/impl/src/main/java/org/apache/sling/models/impl/ModelAdapterFactory.java#L647)","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,Inline Method,","Sling Models: Allow caller to deal with exceptions Currently due to the specification of the adaptTo-method to return null if adaptation is not possible, the caller is not notified about any exceptions (because they are caught within the ModelAdapterFactory).

This is e.g. necessary to deal with validation exceptions properly (i.e. required field injection not possible). The problem was also discussed briefly in http://apache-sling.73963.n3.nabble.com/Silng-Models-Validation-Framework-td4033411.html.

All exceptions either being thrown by the 
@PostConstruct method or caused by the field/method injection are not propagated but basically swallowed by Sling Models.

It would be great to be able to catch those exceptions either in the view or in a servlet filter. I think it should be possible to throw unchecked exceptions in the ModelAdapterFactory.getFactory() method if it is requested (i.e. through a global OSGi configuration flag for Sling Models).
WDYT?
Would you accept such a patch or do you think this breaks the API (also compare with https://issues.apache.org/jira/browse/SLING-2712?focusedCommentId=13561516&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13561516).

If it does not work through the adaptTo, SlingModels should provide an alternative way of instanciating models (and propagating exceptions), although this is kind of tricky, because it internally relies on adaptTo as well (e.g. in https://github.com/apache/sling/blob/trunk/bundles/extensions/models/impl/src/main/java/org/apache/sling/models/impl/ModelAdapterFactory.java#L647)","Duplicated Code, Long Method, , , "
"   Rename Class,Move Class,Rename Method,Extract Method,","Sling Models: Add support for constructor dependency injection Currently, Sling Models only supports dependency injection for fields (or interface getter methods), but not for constructor arguments. This ticket is for discussing what this constructor dependency injection should support, and perhaps finally provide a patch to implement it.

This is somewhat related to SLING-3715 for class-based dependency injection, because this would come in especially handy for constructor injection.","Duplicated Code, Long Method, , "
"   Move Class,Rename Method,Extract Method,","Sling Models: Add support for constructor dependency injection Currently, Sling Models only supports dependency injection for fields (or interface getter methods), but not for constructor arguments. This ticket is for discussing what this constructor dependency injection should support, and perhaps finally provide a patch to implement it.

This is somewhat related to SLING-3715 for class-based dependency injection, because this would come in especially handy for constructor injection.","Duplicated Code, Long Method, , "
"   Move And Rename Class,","JsonRendererServlet should provide a way to list children as an array The default JSON renderer prints all children of a node as another key-value attribute. But browsers do not respect the order printed in the response of object attributes (since the standard does not describe this).

In a lot of cases I continue writing my own servlets to print the children as a json array which respects the order. Imo the default renderer should provide a selector to change the output and print children in an array.

PR: https://github.com/apache/sling/pull/23",", "
"   Move And Rename Class,","JsonRendererServlet should provide a way to list children as an array The default JSON renderer prints all children of a node as another key-value attribute. But browsers do not respect the order printed in the response of object attributes (since the standard does not describe this).

In a lot of cases I continue writing my own servlets to print the children as a json array which respects the order. Imo the default renderer should provide a selector to change the output and print children in an array.

PR: https://github.com/apache/sling/pull/23",", "
"   Rename Method,Move Method,Inline Method,","Use the same flow of operations for forward and reverse replication Current replication code treats differently the forward and reverse replication. Forward replication creates a content package, adds it to a queue and transports it to the other instance while reverse replication creates a dummy POLL package, transports it to the other instance, retrieves the result queues it and then installs it in the current instance.

The current flow for reverse replication complicates the code structure and can be simplified by using three main entities:

1. Package Importers: can import(install) a replication package
1.a ReplicationPackageImporter
1.b ReplicationPackageImporterServlet bound to replication/importer resource type
1.c http://localhost:4502/libs/sling/replication/importers.json

2. Package Exporters - can export (create) a replication package 
2.a ReplicationPackageExporter 
2.b ReplicationPackageExporterServlet bound to replication/exporter resource type
2.c http://localhost:4502/libs/sling/replication/exporters.json

3. Replication Agents - coordinate the interaction between an exporter and an importer using the following flow: exports a package, adds it to a queue, and the imports the package. 
3.a ReplicationAgent 
3.b ReplicationAgentServlet bound to replication/agent resource type
3.c http://localhost:4502/libs/sling/replication/agents.json

Basically for forward replication the exporter is local and the importer is remote while for reverse replication the difference is that the exporter is remote and the importer is local.




",", , , "
"   Rename Method,Move Method,Inline Method,","Use the same flow of operations for forward and reverse replication Current replication code treats differently the forward and reverse replication. Forward replication creates a content package, adds it to a queue and transports it to the other instance while reverse replication creates a dummy POLL package, transports it to the other instance, retrieves the result queues it and then installs it in the current instance.

The current flow for reverse replication complicates the code structure and can be simplified by using three main entities:

1. Package Importers: can import(install) a replication package
1.a ReplicationPackageImporter
1.b ReplicationPackageImporterServlet bound to replication/importer resource type
1.c http://localhost:4502/libs/sling/replication/importers.json

2. Package Exporters - can export (create) a replication package 
2.a ReplicationPackageExporter 
2.b ReplicationPackageExporterServlet bound to replication/exporter resource type
2.c http://localhost:4502/libs/sling/replication/exporters.json

3. Replication Agents - coordinate the interaction between an exporter and an importer using the following flow: exports a package, adds it to a queue, and the imports the package. 
3.a ReplicationAgent 
3.b ReplicationAgentServlet bound to replication/agent resource type
3.c http://localhost:4502/libs/sling/replication/agents.json

Basically for forward replication the exporter is local and the importer is remote while for reverse replication the difference is that the exporter is remote and the importer is local.




",", , , "
"   Rename Method,Move Method,Inline Method,Move Attribute,","Use the same flow of operations for forward and reverse replication Current replication code treats differently the forward and reverse replication. Forward replication creates a content package, adds it to a queue and transports it to the other instance while reverse replication creates a dummy POLL package, transports it to the other instance, retrieves the result queues it and then installs it in the current instance.

The current flow for reverse replication complicates the code structure and can be simplified by using three main entities:

1. Package Importers: can import(install) a replication package
1.a ReplicationPackageImporter
1.b ReplicationPackageImporterServlet bound to replication/importer resource type
1.c http://localhost:4502/libs/sling/replication/importers.json

2. Package Exporters - can export (create) a replication package 
2.a ReplicationPackageExporter 
2.b ReplicationPackageExporterServlet bound to replication/exporter resource type
2.c http://localhost:4502/libs/sling/replication/exporters.json

3. Replication Agents - coordinate the interaction between an exporter and an importer using the following flow: exports a package, adds it to a queue, and the imports the package. 
3.a ReplicationAgent 
3.b ReplicationAgentServlet bound to replication/agent resource type
3.c http://localhost:4502/libs/sling/replication/agents.json

Basically for forward replication the exporter is local and the importer is remote while for reverse replication the difference is that the exporter is remote and the importer is local.




",", , , , "
"   Rename Method,Move Method,Extract Method,","Use the same flow of operations for forward and reverse replication Current replication code treats differently the forward and reverse replication. Forward replication creates a content package, adds it to a queue and transports it to the other instance while reverse replication creates a dummy POLL package, transports it to the other instance, retrieves the result queues it and then installs it in the current instance.

The current flow for reverse replication complicates the code structure and can be simplified by using three main entities:

1. Package Importers: can import(install) a replication package
1.a ReplicationPackageImporter
1.b ReplicationPackageImporterServlet bound to replication/importer resource type
1.c http://localhost:4502/libs/sling/replication/importers.json

2. Package Exporters - can export (create) a replication package 
2.a ReplicationPackageExporter 
2.b ReplicationPackageExporterServlet bound to replication/exporter resource type
2.c http://localhost:4502/libs/sling/replication/exporters.json

3. Replication Agents - coordinate the interaction between an exporter and an importer using the following flow: exports a package, adds it to a queue, and the imports the package. 
3.a ReplicationAgent 
3.b ReplicationAgentServlet bound to replication/agent resource type
3.c http://localhost:4502/libs/sling/replication/agents.json

Basically for forward replication the exporter is local and the importer is remote while for reverse replication the difference is that the exporter is remote and the importer is local.




","Duplicated Code, Long Method, , , "
"   Rename Method,Move Method,Extract Method,","Use the same flow of operations for forward and reverse replication Current replication code treats differently the forward and reverse replication. Forward replication creates a content package, adds it to a queue and transports it to the other instance while reverse replication creates a dummy POLL package, transports it to the other instance, retrieves the result queues it and then installs it in the current instance.

The current flow for reverse replication complicates the code structure and can be simplified by using three main entities:

1. Package Importers: can import(install) a replication package
1.a ReplicationPackageImporter
1.b ReplicationPackageImporterServlet bound to replication/importer resource type
1.c http://localhost:4502/libs/sling/replication/importers.json

2. Package Exporters - can export (create) a replication package 
2.a ReplicationPackageExporter 
2.b ReplicationPackageExporterServlet bound to replication/exporter resource type
2.c http://localhost:4502/libs/sling/replication/exporters.json

3. Replication Agents - coordinate the interaction between an exporter and an importer using the following flow: exports a package, adds it to a queue, and the imports the package. 
3.a ReplicationAgent 
3.b ReplicationAgentServlet bound to replication/agent resource type
3.c http://localhost:4502/libs/sling/replication/agents.json

Basically for forward replication the exporter is local and the importer is remote while for reverse replication the difference is that the exporter is remote and the importer is local.




","Duplicated Code, Long Method, , , "
"   Rename Class,Rename Method,","Add simple JavaScript (Rhino) Wrappers for Version and VersionHistory interfaces SLING-382 reported an issue handling the JCR Version and VersionHistory objects, which were wrapped as JCR Nodes. The cause is that there are no wrappers for these interfaces but as both Version and VersionHistory extend Node, the Node wrapper was used. This prevents using the Version and VersionHistory methods.

SLING-382 implemented a workaround for this. I think the correct solution would be to add simple wrappers for the Version and VersionHistory objects which extend from the ScriptableNode wrapper and thus provide the Version and VersionHistory methods as well as the Node methods.",", "
"   Rename Class,Rename Method,","Add simple JavaScript (Rhino) Wrappers for Version and VersionHistory interfaces SLING-382 reported an issue handling the JCR Version and VersionHistory objects, which were wrapped as JCR Nodes. The cause is that there are no wrappers for these interfaces but as both Version and VersionHistory extend Node, the Node wrapper was used. This prevents using the Version and VersionHistory methods.

SLING-382 implemented a workaround for this. I think the correct solution would be to add simple wrappers for the Version and VersionHistory objects which extend from the ScriptableNode wrapper and thus provide the Version and VersionHistory methods as well as the Node methods.",", "
"   Rename Class,Move Attribute,","Cleanup ReplicationAgent interface and ReplicationAgentConfiguration ReplicationAgent should have a minimal interface. More specifically a client should be able to schedule a ReplicationRequest and to visualize the ReplicationQueues.
If we decide that synchronous execution is needed then that should be properly implemented as the current version is not synchronous. 

Also, agent configuration is particular to the SimpleReplicationAgent implementation. It is true that it is the single implementation that we have but it cannot be an API unless we decide on a fixed set of properties that all agent implementations should have. Either way configurations are currently accessible through the custom resource provider.

",", , "
"   Rename Class,Move Attribute,","Cleanup ReplicationAgent interface and ReplicationAgentConfiguration ReplicationAgent should have a minimal interface. More specifically a client should be able to schedule a ReplicationRequest and to visualize the ReplicationQueues.
If we decide that synchronous execution is needed then that should be properly implemented as the current version is not synchronous. 

Also, agent configuration is particular to the SimpleReplicationAgent implementation. It is true that it is the single implementation that we have but it cannot be an API unless we decide on a fixed set of properties that all agent implementations should have. Either way configurations are currently accessible through the custom resource provider.

",", , "
"   Rename Method,Inline Method,","JcrNodeResource takes too long and initializes too much too soon In a performance test expected to reflect reasonably real-world conditions (50 concurrent users of a mixed load 'forum' type application) I found org.apache.sling.jcr.resource.internal.helper.jcr.JcrNodeResource.JcrNodeResource(ResourceResolver, Node, ClassLoader) taking more than 20% of time used. The majority of this time was spent in setting the resource metadata and to a lesser extent the resource type.

Because the metadata especially is not often accessed and even the resource type is not always accessed, delaying these initializations led to a noticeable performance improvement.

The attached patch delays resourcetype lookup and metadata lookups until needed.

",", , "
"   Rename Method,Inline Method,","JcrNodeResource takes too long and initializes too much too soon In a performance test expected to reflect reasonably real-world conditions (50 concurrent users of a mixed load 'forum' type application) I found org.apache.sling.jcr.resource.internal.helper.jcr.JcrNodeResource.JcrNodeResource(ResourceResolver, Node, ClassLoader) taking more than 20% of time used. The majority of this time was spent in setting the resource metadata and to a lesser extent the resource type.

Because the metadata especially is not often accessed and even the resource type is not always accessed, delaying these initializations led to a noticeable performance improvement.

The attached patch delays resourcetype lookup and metadata lookups until needed.

",", , "
"   Rename Method,Inline Method,","JcrNodeResource takes too long and initializes too much too soon In a performance test expected to reflect reasonably real-world conditions (50 concurrent users of a mixed load 'forum' type application) I found org.apache.sling.jcr.resource.internal.helper.jcr.JcrNodeResource.JcrNodeResource(ResourceResolver, Node, ClassLoader) taking more than 20% of time used. The majority of this time was spent in setting the resource metadata and to a lesser extent the resource type.

Because the metadata especially is not often accessed and even the resource type is not always accessed, delaying these initializations led to a noticeable performance improvement.

The attached patch delays resourcetype lookup and metadata lookups until needed.

",", , "
"   Move Method,Move Attribute,","Sling Models - max recursion depth should be configurable Currently, the maximum recursion depth is hard set at 20 in ModelAdapterFactory. This should be made configurable.",", , , "
"   Move Method,Inline Method,","Configure continous event polling using exclusively the rule string ReplicateOnQueueEventRule can trigger POLL requests by using server side events from a remote endpoint. Initially it obtained the configuration by inspecting the configuration of a transport handler. However that introduced a tight coupling just for obtaining some pieces of configuration which can be provided directly to the rule.
{code}
remote trigger on {url} with user {user} and password {password}
{code}",", , , "
"   Move Method,Inline Method,","Configure continous event polling using exclusively the rule string ReplicateOnQueueEventRule can trigger POLL requests by using server side events from a remote endpoint. Initially it obtained the configuration by inspecting the configuration of a transport handler. However that introduced a tight coupling just for obtaining some pieces of configuration which can be provided directly to the rule.
{code}
remote trigger on {url} with user {user} and password {password}
{code}",", , , "
"   Rename Class,Extract Method,","Method and Constructor injection should support adapting list elements Currently, a List<Adaptable> object is converted to a List<somethingelse> where necessary, but this only happens for Field injection. This should be universal.","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,Move Attribute,","Add replication agent factory that can create an agent and its components from a single configuration file Currently the SimpleReplicationAgentFactory can only configure the ReplicationAgent itself, not the subcomponents. In order to make the management of agents simpler we need a way to fully configure an agent in one configuration file.

We will retain also the possibility of referencing other osgi services.


",", , "
"   Rename Class,Rename Method,Move Attribute,","Add replication agent factory that can create an agent and its components from a single configuration file Currently the SimpleReplicationAgentFactory can only configure the ReplicationAgent itself, not the subcomponents. In order to make the management of agents simpler we need a way to fully configure an agent in one configuration file.

We will retain also the possibility of referencing other osgi services.


",", , "
"   Move And Rename Class,Rename Method,","Access content for replication on behalf of the user that triggered the replication Currently the content is accessed via an administrative session. We need to pass a ResourceResolver via all APIs to ensure that the content is accessed only be users that have the right.

For rule triggered requests the actions should be done on the behalf of a replication-service-user.",", "
"   Move And Rename Class,Rename Method,","Access content for replication on behalf of the user that triggered the replication Currently the content is accessed via an administrative session. We need to pass a ResourceResolver via all APIs to ensure that the content is accessed only be users that have the right.

For rule triggered requests the actions should be done on the behalf of a replication-service-user.",", "
"   Rename Method,","Access content for replication on behalf of the user that triggered the replication Currently the content is accessed via an administrative session. We need to pass a ResourceResolver via all APIs to ensure that the content is accessed only be users that have the right.

For rule triggered requests the actions should be done on the behalf of a replication-service-user.",", "
"   Rename Method,","Replication transport authenticators should hide the credentials The transport authentication credentials should not be present in the configuration of an agent as it is currently (see SLING-3898).

",", "
"   Rename Method,","Replication transport authenticators should hide the credentials The transport authentication credentials should not be present in the configuration of an agent as it is currently (see SLING-3898).

",", "
"   Move And Rename Class,Move Class,Rename Method,Move Method,Extract Method,Move Attribute,","Allow configuration of triggers in agent configuration file Currently only rules are configurable for an agent. However rules have a verbose configuration scheme that is inappropriate for more complex configuration. We should be able to configure directly triggers on agents with key value pairs configuration. (see SLING-3898 for a similar config style)


{code}
""triggers"" : [
""trigger0/type=scheduled"",
""trigger0/action=poll"",
""trigger0/interval=30"",

""trigger1/type=remote"",
""trigger1/endpoints[0]=http://localhost:4503/libs/sling/replication/services/triggers/content-event?3600000"",
""trigger1/authentication.properties[user]=admin"",
""trigger1/authentication.properties[password]=admin"",
""trigger1/endpoints[0]=http://localhost:4503/libs/sling/replication/services/exporters/reverse"",
""trigger1/authenticationFactory/type=service"",
""trigger1/authenticationFactory/name=user""
]
{code}","Duplicated Code, Long Method, , , , "
"   Move And Rename Class,Move Class,Rename Method,Move Method,Extract Method,Move Attribute,","Allow configuration of triggers in agent configuration file Currently only rules are configurable for an agent. However rules have a verbose configuration scheme that is inappropriate for more complex configuration. We should be able to configure directly triggers on agents with key value pairs configuration. (see SLING-3898 for a similar config style)


{code}
""triggers"" : [
""trigger0/type=scheduled"",
""trigger0/action=poll"",
""trigger0/interval=30"",

""trigger1/type=remote"",
""trigger1/endpoints[0]=http://localhost:4503/libs/sling/replication/services/triggers/content-event?3600000"",
""trigger1/authentication.properties[user]=admin"",
""trigger1/authentication.properties[password]=admin"",
""trigger1/endpoints[0]=http://localhost:4503/libs/sling/replication/services/exporters/reverse"",
""trigger1/authenticationFactory/type=service"",
""trigger1/authenticationFactory/name=user""
]
{code}","Duplicated Code, Long Method, , , , "
"   Rename Method,Extract Method,","Remove "":savePrefix"" support The :savePrefix parameter was intended to identify the required prefix to parameter names for them to be considered for content modification.

The use case behind such prefixing is, that we might want to have forms which contain a mix GUI provided parameters (some GUI toolkits seem to add them) and parameters to be used for content update. In this case the content parameters could be prefixed and only those could be considered.

In reality, the only value used for prefixing is ""./"" and the :savePrefix is never used. In fact, the SlingPostServlet's modification operation scans all parameters for some which are prefixed with ""./"" if the :savePrefix is not set and then decides upon using prefixes or not.

Thus, this issue is about the following changes:

* remove :savePrefix parameter support
* the prefix used is hardcoded to be ""./""
* If any parameter with the prefix is found, only parameters starting with
""./"", ""../"" and ""/"" are considered for content update
* If no parameter with the ""./"" prefix is found, all parameters not starting
with the operation parameter prefix "":"" are considered for content
update.","Duplicated Code, Long Method, , "
"   Rename Method,Move Method,Move Attribute,","Refactor ReplicationQueue API to eliminate multiagent queues Currently the queue related API (ReplicationQueueProvider and ReplicationQueueDistributionStrategy) takes as argument the agentName. There is no reason why this should happen as typically a queue provider should be configured for a single agent.


Also ReplicationQueueDistributionStrategy has two methods (add and offer) but only one it is used and there is no clear reason why we should have two.


",", , , "
"   Rename Method,Move Method,Move Attribute,","Refactor ReplicationQueue API to eliminate multiagent queues Currently the queue related API (ReplicationQueueProvider and ReplicationQueueDistributionStrategy) takes as argument the agentName. There is no reason why this should happen as typically a queue provider should be configured for a single agent.


Also ReplicationQueueDistributionStrategy has two methods (add and offer) but only one it is used and there is no clear reason why we should have two.


",", , , "
"   Rename Method,","ResourceProviderEntry Should Poll All Viable ModifyingResourceProviders During creation and deletion Currently the ResourceProviderEntry, during creation or deletion of a resource, will send the creation or deletion request to the first viable ModifyingResourceProvider. This is problematic if multiple ModifyingResourceProviders may be viable for a particular path. Consider the following situation:

I create a Resource Provider which listens to /content such that it can participate in resource listing calls on that path. It however does not necessarily handle all resources under /content, the root JcrResourceProvider is also expected to handle some resources under that path. This works fine for the various get methods, however for modification, the custom Resource Provider always gets chosen first and, if it is not able to create a resource, the creation request does not fall back to less specific providers or the root provider even though those provider might well be able to create the resource. 

The common pattern across many of the ResourceResolver operations is to poll all viable Resource Providers. For example, getResource will iterate over all viable Resource Providers in order of path specificity, stopping only if one of them is able to return a resource. The attached patch implements a similar mechanism for the create and delete commands. 

Create will iterate over all viable ModifyingResourceProviders in order of specificity, stopping if one of them actually produces a Resource. Delete will iterate over all viable ModifyingResourceProviders giving each a crack at deleting the resource.",", "
"   Rename Method,","ResourceProviderEntry Should Poll All Viable ModifyingResourceProviders During creation and deletion Currently the ResourceProviderEntry, during creation or deletion of a resource, will send the creation or deletion request to the first viable ModifyingResourceProvider. This is problematic if multiple ModifyingResourceProviders may be viable for a particular path. Consider the following situation:

I create a Resource Provider which listens to /content such that it can participate in resource listing calls on that path. It however does not necessarily handle all resources under /content, the root JcrResourceProvider is also expected to handle some resources under that path. This works fine for the various get methods, however for modification, the custom Resource Provider always gets chosen first and, if it is not able to create a resource, the creation request does not fall back to less specific providers or the root provider even though those provider might well be able to create the resource. 

The common pattern across many of the ResourceResolver operations is to poll all viable Resource Providers. For example, getResource will iterate over all viable Resource Providers in order of path specificity, stopping only if one of them is able to return a resource. The attached patch implements a similar mechanism for the create and delete commands. 

Create will iterate over all viable ModifyingResourceProviders in order of specificity, stopping if one of them actually produces a Resource. Delete will iterate over all viable ModifyingResourceProviders giving each a crack at deleting the resource.",", "
"   Rename Method,","ResourceProviderEntry Should Poll All Viable ModifyingResourceProviders During creation and deletion Currently the ResourceProviderEntry, during creation or deletion of a resource, will send the creation or deletion request to the first viable ModifyingResourceProvider. This is problematic if multiple ModifyingResourceProviders may be viable for a particular path. Consider the following situation:

I create a Resource Provider which listens to /content such that it can participate in resource listing calls on that path. It however does not necessarily handle all resources under /content, the root JcrResourceProvider is also expected to handle some resources under that path. This works fine for the various get methods, however for modification, the custom Resource Provider always gets chosen first and, if it is not able to create a resource, the creation request does not fall back to less specific providers or the root provider even though those provider might well be able to create the resource. 

The common pattern across many of the ResourceResolver operations is to poll all viable Resource Providers. For example, getResource will iterate over all viable Resource Providers in order of path specificity, stopping only if one of them is able to return a resource. The attached patch implements a similar mechanism for the create and delete commands. 

Create will iterate over all viable ModifyingResourceProviders in order of specificity, stopping if one of them actually produces a Resource. Delete will iterate over all viable ModifyingResourceProviders giving each a crack at deleting the resource.",", "
"   Extract Superclass,Rename Method,","Make dependency to config admin optional The settings bundle requires now config admin, which is a heavy dependency for the simply settings bundle.
We should make this optional and lazy",", Duplicated Code, Large Class, "
"   Extract Superclass,Rename Method,","Make dependency to config admin optional The settings bundle requires now config admin, which is a heavy dependency for the simply settings bundle.
We should make this optional and lazy",", Duplicated Code, Large Class, "
"   Extract Interface,Rename Method,Move Method,Move Attribute,","Simplify dependency management by letting the caller to supply its own implementations Replication core bundle exposes a generic component factory that creates a replication component based on a properties map.

The caller of createComponent can also provide a map of default implementations for some services (this is done by passing a componentProvider).

This allows to define specialized osgi factories that bind directly to services and simplifies dependency management. 

",", , , Large Class, "
"   Extract Interface,Rename Method,Move Method,Move Attribute,","Simplify dependency management by letting the caller to supply its own implementations Replication core bundle exposes a generic component factory that creates a replication component based on a properties map.

The caller of createComponent can also provide a map of default implementations for some services (this is done by passing a componentProvider).

This allows to define specialized osgi factories that bind directly to services and simplifies dependency management. 

",", , , Large Class, "
"   Move Class,Move Method,Move Attribute,","Define proper ContentResolver service Mapping request URI paths to Content objects is currently implemented in the URLMapperFilter, which is registered as a request filter for Sling. Other than that this functionality is not currently usable.

Instead the mapping functionality should be defined in a separate service, which we could name ContentResolver (just like the ComponentResolver which selects the component) . This service may be implemented by a specifialized class. That class might even provide for extensibility.

The URLMapperFilter should be renamed to ContentResolverFilter to match the service name and would be refactored to use the ContentResolver service as available and log appropriate messges if missing. Actually, the filter could be the service itself, but it should be ensured, that the filter would be available even in the absence of a repository to provide meaningfull error handling.",", , , "
"   Rename Class,Extract Interface,Rename Method,Extract Method,","Define proper ContentResolver service Mapping request URI paths to Content objects is currently implemented in the URLMapperFilter, which is registered as a request filter for Sling. Other than that this functionality is not currently usable.

Instead the mapping functionality should be defined in a separate service, which we could name ContentResolver (just like the ComponentResolver which selects the component) . This service may be implemented by a specifialized class. That class might even provide for extensibility.

The URLMapperFilter should be renamed to ContentResolverFilter to match the service name and would be refactored to use the ContentResolver service as available and log appropriate messges if missing. Actually, the filter could be the service itself, but it should be ensured, that the filter would be available even in the absence of a repository to provide meaningfull error handling.","Duplicated Code, Long Method, , Large Class, "
"   Rename Class,Extract Interface,Rename Method,Extract Method,","Define proper ContentResolver service Mapping request URI paths to Content objects is currently implemented in the URLMapperFilter, which is registered as a request filter for Sling. Other than that this functionality is not currently usable.

Instead the mapping functionality should be defined in a separate service, which we could name ContentResolver (just like the ComponentResolver which selects the component) . This service may be implemented by a specifialized class. That class might even provide for extensibility.

The URLMapperFilter should be renamed to ContentResolverFilter to match the service name and would be refactored to use the ContentResolver service as available and log appropriate messges if missing. Actually, the filter could be the service itself, but it should be ensured, that the filter would be available even in the absence of a repository to provide meaningfull error handling.","Duplicated Code, Long Method, , Large Class, "
"   Extract Method,Move Attribute,","Add error marker on the project when content sync root does not exist Various scenarios can lead to a content project not having the content sync root correctly configured:

- moving the directory ( although we could hook in a refactoring participant ... )
- having a non-standard location and then copying the project to a new workspace ( the property is per-workspace, not saved in the .settings directory )

To prevent this and other problems, we should add an error marker on the project if the content sync root does not exist.","Duplicated Code, Long Method, , , "
"   Extract Method,Move Attribute,","Add error marker on the project when content sync root does not exist Various scenarios can lead to a content project not having the content sync root correctly configured:

- moving the directory ( although we could hook in a refactoring participant ... )
- having a non-standard location and then copying the project to a new workspace ( the property is per-workspace, not saved in the .settings directory )

To prevent this and other problems, we should add an error marker on the project if the content sync root does not exist.","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,",Allow ValidationModel nodes in the JCR contain wildcards below children and properties According to https://github.com/apache/sling/blob/trunk/contrib/validation/README.md the nodes in the JCR specifying a validation model must always give an explicit property name or child node name. This is sometimes not possible if the subnodes/properties are generated automatically. Therefore it would be great if the elements directly below {{children}} and {{properties}} would allow wildcards.,"Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,",Allow ValidationModel nodes in the JCR contain wildcards below children and properties According to https://github.com/apache/sling/blob/trunk/contrib/validation/README.md the nodes in the JCR specifying a validation model must always give an explicit property name or child node name. This is sometimes not possible if the subnodes/properties are generated automatically. Therefore it would be great if the elements directly below {{children}} and {{properties}} would allow wildcards.,"Duplicated Code, Long Method, , "
"   Rename Method,","Testing: Donate sling-mock, jcr-mock, osgi-mock implementation donate a at suite of mocking libraries to run OSGi/SCR, JCR and esp. Sling in a simulated ""in-memory"" environment for unit tests, ensuring minimal setup time. it uses either a mocked in-memory JCR, or the resourceresolver-mock implementation that is already part of the sling project. additional convenience features like bulk-loading JSON content and binaries into the simulated resource tree via a content loader makes it easy setting up complex text fixtures for your unit tests.",", "
"   Rename Method,","Testing: Donate sling-mock, jcr-mock, osgi-mock implementation donate a at suite of mocking libraries to run OSGi/SCR, JCR and esp. Sling in a simulated ""in-memory"" environment for unit tests, ensuring minimal setup time. it uses either a mocked in-memory JCR, or the resourceresolver-mock implementation that is already part of the sling project. additional convenience features like bulk-loading JSON content and binaries into the simulated resource tree via a content loader makes it easy setting up complex text fixtures for your unit tests.",", "
"   Extract Method,Move Attribute,","Testing: Donate sling-mock, jcr-mock, osgi-mock implementation donate a at suite of mocking libraries to run OSGi/SCR, JCR and esp. Sling in a simulated ""in-memory"" environment for unit tests, ensuring minimal setup time. it uses either a mocked in-memory JCR, or the resourceresolver-mock implementation that is already part of the sling project. additional convenience features like bulk-loading JSON content and binaries into the simulated resource tree via a content loader makes it easy setting up complex text fixtures for your unit tests.","Duplicated Code, Long Method, , , "
"   Rename Method,","Testing: Donate sling-mock, jcr-mock, osgi-mock implementation donate a at suite of mocking libraries to run OSGi/SCR, JCR and esp. Sling in a simulated ""in-memory"" environment for unit tests, ensuring minimal setup time. it uses either a mocked in-memory JCR, or the resourceresolver-mock implementation that is already part of the sling project. additional convenience features like bulk-loading JSON content and binaries into the simulated resource tree via a content loader makes it easy setting up complex text fixtures for your unit tests.",", "
"   Rename Class,Move And Rename Class,Rename Method,Move Method,Extract Method,","Avoid keeping jobs in memory Currently all jobs for a single instance are hold in memory and put into the queues. This is not optiomal, especially for a large amount of jobs. In addition it makes configuration changes, queue updates etc. more complicated.
We should revert this and let a queue only pick a job from the resource tree when it has a free processing space.","Duplicated Code, Long Method, , , "
"   Rename Class,Move And Rename Class,Rename Method,Move Method,Extract Method,","Avoid keeping jobs in memory Currently all jobs for a single instance are hold in memory and put into the queues. This is not optiomal, especially for a large amount of jobs. In addition it makes configuration changes, queue updates etc. more complicated.
We should revert this and let a queue only pick a job from the resource tree when it has a free processing space.","Duplicated Code, Long Method, , , "
"   Move Class,Rename Method,Move Method,","Avoid keeping jobs in memory Currently all jobs for a single instance are hold in memory and put into the queues. This is not optiomal, especially for a large amount of jobs. In addition it makes configuration changes, queue updates etc. more complicated.
We should revert this and let a queue only pick a job from the resource tree when it has a free processing space.",", , "
"   Rename Method,","Avoid keeping jobs in memory Currently all jobs for a single instance are hold in memory and put into the queues. This is not optiomal, especially for a large amount of jobs. In addition it makes configuration changes, queue updates etc. more complicated.
We should revert this and let a queue only pick a job from the resource tree when it has a free processing space.",", "
"   Move Method,Extract Method,Move Attribute,","Avoid keeping jobs in memory Currently all jobs for a single instance are hold in memory and put into the queues. This is not optiomal, especially for a large amount of jobs. In addition it makes configuration changes, queue updates etc. more complicated.
We should revert this and let a queue only pick a job from the resource tree when it has a free processing space.","Duplicated Code, Long Method, , , , "
"   Move Class,Move And Rename Class,Rename Method,Move Method,Inline Method,Move Attribute,","Avoid keeping jobs in memory Currently all jobs for a single instance are hold in memory and put into the queues. This is not optiomal, especially for a large amount of jobs. In addition it makes configuration changes, queue updates etc. more complicated.
We should revert this and let a queue only pick a job from the resource tree when it has a free processing space.",", , , , "
"   Move Class,Move And Rename Class,Rename Method,Move Method,Inline Method,Move Attribute,","Avoid keeping jobs in memory Currently all jobs for a single instance are hold in memory and put into the queues. This is not optiomal, especially for a large amount of jobs. In addition it makes configuration changes, queue updates etc. more complicated.
We should revert this and let a queue only pick a job from the resource tree when it has a free processing space.",", , , , "
"   Rename Method,","Avoid keeping jobs in memory Currently all jobs for a single instance are hold in memory and put into the queues. This is not optiomal, especially for a large amount of jobs. In addition it makes configuration changes, queue updates etc. more complicated.
We should revert this and let a queue only pick a job from the resource tree when it has a free processing space.",", "
"   Move Method,Move Attribute,","Avoid keeping jobs in memory Currently all jobs for a single instance are hold in memory and put into the queues. This is not optiomal, especially for a large amount of jobs. In addition it makes configuration changes, queue updates etc. more complicated.
We should revert this and let a queue only pick a job from the resource tree when it has a free processing space.",", , , "
"   Move Class,Move Method,Extract Method,Inline Method,Move Attribute,","Avoid keeping jobs in memory Currently all jobs for a single instance are hold in memory and put into the queues. This is not optiomal, especially for a large amount of jobs. In addition it makes configuration changes, queue updates etc. more complicated.
We should revert this and let a queue only pick a job from the resource tree when it has a free processing space.","Duplicated Code, Long Method, , , , , "
"   Rename Method,","Avoid keeping jobs in memory Currently all jobs for a single instance are hold in memory and put into the queues. This is not optiomal, especially for a large amount of jobs. In addition it makes configuration changes, queue updates etc. more complicated.
We should revert this and let a queue only pick a job from the resource tree when it has a free processing space.",", "
"   Move Class,Rename Method,Move Method,","Avoid keeping jobs in memory Currently all jobs for a single instance are hold in memory and put into the queues. This is not optiomal, especially for a large amount of jobs. In addition it makes configuration changes, queue updates etc. more complicated.
We should revert this and let a queue only pick a job from the resource tree when it has a free processing space.",", , "
"   Rename Method,","Add notification when a job is added The job notification cover all cases except when a job is added.
We shoud also clarify that notifications are only send locally and never across a cluster (using remote events)",", "
"   Rename Method,","Add notification when a job is added The job notification cover all cases except when a job is added.
We shoud also clarify that notifications are only send locally and never across a cluster (using remote events)",", "
"   Move Method,Move Attribute,","Drop support PROPERTY_NOTIFICATION_JOB event property The PROPERTY_NOTIFICATION_JOB property for notification events has been deprecated some time now but is still send in the events.
We should remove this property now from the event.",", , , "
"   Move Method,Move Attribute,","Drop support PROPERTY_NOTIFICATION_JOB event property The PROPERTY_NOTIFICATION_JOB property for notification events has been deprecated some time now but is still send in the events.
We should remove this property now from the event.",", , , "
"   Inline Method,Move Attribute,","Clean up code and logging statements We could clean up some unused code based on the refactorings done for the 3.4.0 release.
In addition, some logging statements might have wrong log level or might be missing.",", , , "
"   Inline Method,Move Attribute,","Clean up code and logging statements We could clean up some unused code based on the refactorings done for the 3.4.0 release.
In addition, some logging statements might have wrong log level or might be missing.",", , , "
"   Rename Method,Extract Method,Inline Method,","Use safe ids for replication packages such that an user cannot access random files on disk Currently the filevault packages ids are made out of file paths. We should wrap those packages into something less revealing, like a path into the repository.","Duplicated Code, Long Method, , , "
"   Move Class,Move And Rename Class,Move Method,Extract Method,Move Attribute,","Sling Models: Optimize performance when read sling models annotations i did some first performance tests with a sling application that makes intensive usage of sling models to see where potential hotspots are that cost performance, esp. in area of sling models.

attached is a filtered view of a jprofile session showing only the method calls inside sling models implementation:
[^141028_adaptto_jprofiler_slingmodels.gif]

a good part of performance is spent on inspection the annotations of the sling models classes, a call graph of the first method: [^141028_adaptto_jprofiler_slingmodels_getannotation.gif]

i think this is especially the case because this inspection takes place on each adaptTo() call, although the underlying model class never changes when the OSGi bundle stays in place.

it should be possible to come up with an optimization caching the inspection results (which annotations exist on which fields/methods/types with which parameters), and doing only the injection part on each adaptTo() invocation. if the bundle changes the cache has to be cleared.

i will think about it the next days and perhaps come up with an implementation proposal.
","Duplicated Code, Long Method, , , , "
"   Rename Method,Move Method,Extract Method,","Allow validator to support arbitrary types Currently the {{Validator.validate}} method only act on String values. 
Since type conversion is already built into the ValueMap, it would be good to leverage that and to allow Validator to act on arbitrary types there!

Also the type conversion from ValueMap should be leveraged to do the type check in {{ValidationServiceImpl.validatePropertyValue}} rather than implementing a new thing in {{Type.isValid()}}.","Duplicated Code, Long Method, , , "
"   Rename Method,Move Method,Extract Method,","Allow validator to support arbitrary types Currently the {{Validator.validate}} method only act on String values. 
Since type conversion is already built into the ValueMap, it would be good to leverage that and to allow Validator to act on arbitrary types there!

Also the type conversion from ValueMap should be leveraged to do the type check in {{ValidationServiceImpl.validatePropertyValue}} rather than implementing a new thing in {{Type.isValid()}}.","Duplicated Code, Long Method, , , "
"   Rename Method,Move Method,Extract Method,","Allow validator to support arbitrary types Currently the {{Validator.validate}} method only act on String values. 
Since type conversion is already built into the ValueMap, it would be good to leverage that and to allow Validator to act on arbitrary types there!

Also the type conversion from ValueMap should be leveraged to do the type check in {{ValidationServiceImpl.validatePropertyValue}} rather than implementing a new thing in {{Type.isValid()}}.","Duplicated Code, Long Method, , , "
"   Rename Class,Move Method,Move Attribute,","Support Sling Validation through a new field of the Model annotation The current way of integrating Sling Validation (SLING-2803) with Sling Models is to inject the validation service and then call it within a PostConstruct method (http://www.slideshare.net/raducotescu/apache-sling-generic-validation-framework/16).
This has the drawback that
# the {{ValidationService}} needs to be injected
# a PostConstruct needs to be implemented
# the other injections need to be marked as optional (otherwise the validation is never triggered in case of e.g. missing required valuemap values)

Instead it would be good to support this use case with just an additional field on the annotation {{Model}} which is named {{validate}}. By default this should be {{false}} (to be backwards compatible), but if it is {{true}} the Sling Validation should be called before any values are injected into the model. If validation fails the {{ModelAdapterFactory}} should never instanciate the model and rather return null (or throw a meaningful exception for SLING-3709).
",", , , "
"   Extract Superclass,Move Method,","Introduce ""OsgiContext"" junit rule for OSGi and OsgiContextImpl currently the sling mock SlingContext rule provides access to OSGi context and convenience methods for managing OSGi services in mocks.

these methods should be moved to a new OsgiContext rule and OsgiContextImpl in the osgi mocks, so they can be used in projects which want to use only OSGi but not sling as well.

sling mock context is updated to inherit from the osgi context.",", , Duplicated Code, Large Class, "
"   Move Method,Extract Method,Inline Method,","OSGi Mock: Fail-fast when calling methods requiring SCR metadata and this is not present some features and signature variants of OSGi mocks require SCR metadata to be available in the classpath. currently it is ignored silently if they are missing and the feature may fail without notice (perhaps leading to NPEs e.g. because a dependency is not injected).

this may lead to strange unit test failes esp. when using IDEs that cannot generate SCR metadata via maven plugin, or (like eclipse and m2e) this fails sometimes e.g. when refreshing a project without rebuilding it.","Duplicated Code, Long Method, , , , "
"   Move Method,Extract Method,Inline Method,","OSGi Mock: Fail-fast when calling methods requiring SCR metadata and this is not present some features and signature variants of OSGi mocks require SCR metadata to be available in the classpath. currently it is ignored silently if they are missing and the feature may fail without notice (perhaps leading to NPEs e.g. because a dependency is not injected).

this may lead to strange unit test failes esp. when using IDEs that cannot generate SCR metadata via maven plugin, or (like eclipse and m2e) this fails sometimes e.g. when refreshing a project without rebuilding it.","Duplicated Code, Long Method, , , , "
"   Move Method,Move Attribute,","Add basic code completion for Sightly Now that Sightly is part of Sling proper, it would be interesting to add basic support for auto-completion in HTML files. For starters, we will target well-known {{data-sly-*}} attributes which require no further introspection from us.

AngularEclipse does something similar, see [Angular JS completion proposal declaration|https://github.com/angelozerr/angularjs-eclipse/blob/f4c3848bd346260cee7afbce0181043ddb75d598/org.eclipse.angularjs.ui/plugin.xml#L171-L190] and [Angular JS completion proposal implementation|https://github.com/angelozerr/angularjs-eclipse/blob/f4c3848bd346260cee7afbce0181043ddb75d598/org.eclipse.angularjs.ui/src/org/eclipse/angularjs/ui/contentassist/HTMLAngularTagsCompletionProposalComputer.java]",", , , "
"   Move Method,Move Attribute,","Add basic code completion for Sightly Now that Sightly is part of Sling proper, it would be interesting to add basic support for auto-completion in HTML files. For starters, we will target well-known {{data-sly-*}} attributes which require no further introspection from us.

AngularEclipse does something similar, see [Angular JS completion proposal declaration|https://github.com/angelozerr/angularjs-eclipse/blob/f4c3848bd346260cee7afbce0181043ddb75d598/org.eclipse.angularjs.ui/plugin.xml#L171-L190] and [Angular JS completion proposal implementation|https://github.com/angelozerr/angularjs-eclipse/blob/f4c3848bd346260cee7afbce0181043ddb75d598/org.eclipse.angularjs.ui/src/org/eclipse/angularjs/ui/contentassist/HTMLAngularTagsCompletionProposalComputer.java]",", , , "
"   Move Class,Rename Class,Move Method,Move Attribute,","Merge default GET Servlets into a single one Implementing SLING-387 causes a new script/servlet resolution order whereas the default servlets are handled as if they would be servlets for a resource super type of the resource type of the request resource. As such it may happen that for example a default script registered to handle GET/html requests would selected instead of a script defined for the resource type.

To fix this issue, all regular default GET servlets for html, txt and json are to be merged into a single default servlet registered for the GET method. The actual renderes are still separate classes but the new default servlet arbitrates amongst the renderers itself.",", , , "
"   Move Class,Rename Class,Move Method,Move Attribute,","Merge default GET Servlets into a single one Implementing SLING-387 causes a new script/servlet resolution order whereas the default servlets are handled as if they would be servlets for a resource super type of the resource type of the request resource. As such it may happen that for example a default script registered to handle GET/html requests would selected instead of a script defined for the resource type.

To fix this issue, all regular default GET servlets for html, txt and json are to be merged into a single default servlet registered for the GET method. The actual renderes are still separate classes but the new default servlet arbitrates amongst the renderers itself.",", , , "
"   Move Class,Move Method,Inline Method,Move Attribute,","SlingPostServlet: Refactor to execute one operation at a time The SlingPostServlet is anything but REST-ful. For example, it is possible to have a request for resource /a which at the same might cause a resoure /x/y to be moved to /e/f and much nastier things. Providing an accurate response to such operations and acting upon such responses is almost impossible if not introducing multi-status ...

Therefore, the SlingPostServlet should be simplified as follows:

* The servlet is modified to execute a single operation per request, where an operation may be create/modify, delete, move and copy. The actual operation to execute is indicated by a new parameter "":operation"" which can take the following values:

<unset> or empty string - create/modify request
""delete"" - delete current resource
""move"" - move current resource
""copy"" - copy current resource

In addition, this mechanism could be implemented such, that the actual operation might be extensible.

* All operations act on the current resource - request.getResource(). The delete, move and copy operations fail if the current resource is a non-existing resource.

* The distinction between create and just modify depends on the resource: If the current resource does not exist yet it is created. Special treatment for resource creation happens if the path is
terminated by a slash (as proposed by Carsten and Roy in earlier messages) or by a slash-star (/*, like currently). The name of the newly created node is defined as it is today: using special parameters :name, :nameHint and well-known content such as title.
We keep the /* notation to support requests like /*.print.a4.html.

* Some operations (create, move, copy) handle a parameter "":order"", which defines the ordering relation of the newly created item (this is the same behaviour as in the current implementation)

* The copy and move operation by default fail if an item already exists at the destination. This behaviour may be overwritten by setting the "":replace"" paramter to ""true"". This replaces the current ""replace"" value for the :copyFlags and :moveFlags parameter.

* The copy and move operations require another parameter "":dest"", which is the destination path name of the resource. The operation fails if the parameter is missing.

* The "":redirect"" parameter causes the client to be redirected to the desired target in case the operation was successfull. This is the same behaviour as today.

* The "":status"" parameter causes the HTTP status code to be non-standards-compliant: If the parameter value is ""browser"", that status code is always 200/OK, even in case of failure. Otherwise the status code will reflect the actual status.

* Regardless of the "":status"" parameter value, the response is always the complete run-down of the operation executed with the actual status code and eventual exceptions - unless of course if the client has been redirected after successfull operation and instructed to so by the :redirect parameter. This is the same behaviour as today.

* Run-Down of some status codes expected:

200/OK - if :status==browser or if the operation succeeded
201/CREATED - required by a successful move or copy, when the destination
did not exist yet. Also sent when the current resource was created
404/NOT FOUND - if the current resource is missing for copy, move, delete
412/PRECONDITION FAILED - if the destination for the copy or move
operation exists and the :replace parameter is not set to true
(this is consistent with the WebDAV spec for COPY/MOVE in this
situation).
302/FOUND - aka temporary redirect, if the operation succeeded and the
:redirect parameter is set
500/INTERNAL SERVER ERROR - in case of any processing error,
e.g. an exception being thrown",", , , , "
"   Move Class,Rename Method,Move Method,Inline Method,Move Attribute,","SlingPostServlet: Refactor to execute one operation at a time The SlingPostServlet is anything but REST-ful. For example, it is possible to have a request for resource /a which at the same might cause a resoure /x/y to be moved to /e/f and much nastier things. Providing an accurate response to such operations and acting upon such responses is almost impossible if not introducing multi-status ...

Therefore, the SlingPostServlet should be simplified as follows:

* The servlet is modified to execute a single operation per request, where an operation may be create/modify, delete, move and copy. The actual operation to execute is indicated by a new parameter "":operation"" which can take the following values:

<unset> or empty string - create/modify request
""delete"" - delete current resource
""move"" - move current resource
""copy"" - copy current resource

In addition, this mechanism could be implemented such, that the actual operation might be extensible.

* All operations act on the current resource - request.getResource(). The delete, move and copy operations fail if the current resource is a non-existing resource.

* The distinction between create and just modify depends on the resource: If the current resource does not exist yet it is created. Special treatment for resource creation happens if the path is
terminated by a slash (as proposed by Carsten and Roy in earlier messages) or by a slash-star (/*, like currently). The name of the newly created node is defined as it is today: using special parameters :name, :nameHint and well-known content such as title.
We keep the /* notation to support requests like /*.print.a4.html.

* Some operations (create, move, copy) handle a parameter "":order"", which defines the ordering relation of the newly created item (this is the same behaviour as in the current implementation)

* The copy and move operation by default fail if an item already exists at the destination. This behaviour may be overwritten by setting the "":replace"" paramter to ""true"". This replaces the current ""replace"" value for the :copyFlags and :moveFlags parameter.

* The copy and move operations require another parameter "":dest"", which is the destination path name of the resource. The operation fails if the parameter is missing.

* The "":redirect"" parameter causes the client to be redirected to the desired target in case the operation was successfull. This is the same behaviour as today.

* The "":status"" parameter causes the HTTP status code to be non-standards-compliant: If the parameter value is ""browser"", that status code is always 200/OK, even in case of failure. Otherwise the status code will reflect the actual status.

* Regardless of the "":status"" parameter value, the response is always the complete run-down of the operation executed with the actual status code and eventual exceptions - unless of course if the client has been redirected after successfull operation and instructed to so by the :redirect parameter. This is the same behaviour as today.

* Run-Down of some status codes expected:

200/OK - if :status==browser or if the operation succeeded
201/CREATED - required by a successful move or copy, when the destination
did not exist yet. Also sent when the current resource was created
404/NOT FOUND - if the current resource is missing for copy, move, delete
412/PRECONDITION FAILED - if the destination for the copy or move
operation exists and the :replace parameter is not set to true
(this is consistent with the WebDAV spec for COPY/MOVE in this
situation).
302/FOUND - aka temporary redirect, if the operation succeeded and the
:redirect parameter is set
500/INTERNAL SERVER ERROR - in case of any processing error,
e.g. an exception being thrown",", , , , "
"   Rename Method,","Make Content Loader extensible to support new import formats The current Content Loader supports a basic set of import formats which are identified by extension: {{.xml}}, {{.jcr.xml}}, {{.json}}, {{.jar}} and {{.zip}}.

There is a [user request|http://mail-archives.apache.org/mod_mbox/sling-users/201412.mbox/%3cD0A6198C.20FBEB%25bruce.edge@nextissuemedia.com%3e] to support custom formats like Adobe Folio and [some ideas how to implement|http://mail-archives.apache.org/mod_mbox/sling-users/201412.mbox/%3cA2AB572F-FA83-4AE2-806E-49CCE87B9FBE@adobe.com%3e]:

* we create a new org.apache.sling.contentloader.reader package to be exported at version 1.0
* move the ContentCreator (@ProviderType) and ContentReader (@ConsumerType) to that new package
* convert the BaseImportLoader into a standalone ContentReader service holder used by the DefaultContentImporter and ContentLoaderService components.
* For the ContentReader service interface we define service registration properties for the service to expose the file name extension (and maybe content type) the reader supports.
",", "
"   Move Class,Extract Interface,","Make Content Loader extensible to support new import formats The current Content Loader supports a basic set of import formats which are identified by extension: {{.xml}}, {{.jcr.xml}}, {{.json}}, {{.jar}} and {{.zip}}.

There is a [user request|http://mail-archives.apache.org/mod_mbox/sling-users/201412.mbox/%3cD0A6198C.20FBEB%25bruce.edge@nextissuemedia.com%3e] to support custom formats like Adobe Folio and [some ideas how to implement|http://mail-archives.apache.org/mod_mbox/sling-users/201412.mbox/%3cA2AB572F-FA83-4AE2-806E-49CCE87B9FBE@adobe.com%3e]:

* we create a new org.apache.sling.contentloader.reader package to be exported at version 1.0
* move the ContentCreator (@ProviderType) and ContentReader (@ConsumerType) to that new package
* convert the BaseImportLoader into a standalone ContentReader service holder used by the DefaultContentImporter and ContentLoaderService components.
* For the ContentReader service interface we define service registration properties for the service to expose the file name extension (and maybe content type) the reader supports.
",", Large Class, "
"   Move Class,Extract Interface,","Make Content Loader extensible to support new import formats The current Content Loader supports a basic set of import formats which are identified by extension: {{.xml}}, {{.jcr.xml}}, {{.json}}, {{.jar}} and {{.zip}}.

There is a [user request|http://mail-archives.apache.org/mod_mbox/sling-users/201412.mbox/%3cD0A6198C.20FBEB%25bruce.edge@nextissuemedia.com%3e] to support custom formats like Adobe Folio and [some ideas how to implement|http://mail-archives.apache.org/mod_mbox/sling-users/201412.mbox/%3cA2AB572F-FA83-4AE2-806E-49CCE87B9FBE@adobe.com%3e]:

* we create a new org.apache.sling.contentloader.reader package to be exported at version 1.0
* move the ContentCreator (@ProviderType) and ContentReader (@ConsumerType) to that new package
* convert the BaseImportLoader into a standalone ContentReader service holder used by the DefaultContentImporter and ContentLoaderService components.
* For the ContentReader service interface we define service registration properties for the service to expose the file name extension (and maybe content type) the reader supports.
",", Large Class, "
"   Rename Method,","Make Content Loader extensible to support new import formats The current Content Loader supports a basic set of import formats which are identified by extension: {{.xml}}, {{.jcr.xml}}, {{.json}}, {{.jar}} and {{.zip}}.

There is a [user request|http://mail-archives.apache.org/mod_mbox/sling-users/201412.mbox/%3cD0A6198C.20FBEB%25bruce.edge@nextissuemedia.com%3e] to support custom formats like Adobe Folio and [some ideas how to implement|http://mail-archives.apache.org/mod_mbox/sling-users/201412.mbox/%3cA2AB572F-FA83-4AE2-806E-49CCE87B9FBE@adobe.com%3e]:

* we create a new org.apache.sling.contentloader.reader package to be exported at version 1.0
* move the ContentCreator (@ProviderType) and ContentReader (@ConsumerType) to that new package
* convert the BaseImportLoader into a standalone ContentReader service holder used by the DefaultContentImporter and ContentLoaderService components.
* For the ContentReader service interface we define service registration properties for the service to expose the file name extension (and maybe content type) the reader supports.
",", "
"   Move Class,Move Method,Move Attribute,","Move additional mock classes from servlet-resolver to commons/testing The sling/servlet-resolver module contains Mock classes, which may be of general interest and therefore should be moved to the commons/testing module.",", , , "
"   Extract Method,Inline Method,Move Attribute,","Improve Sightly engine performance Several areas of the engine's implementation can be improved to greatly reduce compilation & rendering times:

* rely on the dynamic classloader's cache rather than compile Use-API POJOs all the time
* reduce the number of repository reads in favour of events for deducing if a Sightly script or Use-API POJO needs to be recompiled
* use a per thread admin resource resolver instead of creating many resolvers in various components during the lifecycle of a request
* optimise the {{RenderContext}} implementation code that handles object methods and fields detection","Duplicated Code, Long Method, , , , "
"   Rename Method,","Improve Sightly JS Provider performance The Sightly JavaScript Use Provider can be optimised such that its JavaScript context is run only for JS Use-API scripts, instead of creating the JS context for every Sightly request. The only downside of such an optimisation is that the values provided by the {{SlyBindingsValuesProvider}} will not be available to be used directly in Sightly scripts, making them available only for JS Use-API objects.",", "
"   Rename Method,","Improve Sightly JS Provider performance The Sightly JavaScript Use Provider can be optimised such that its JavaScript context is run only for JS Use-API scripts, instead of creating the JS context for every Sightly request. The only downside of such an optimisation is that the values provided by the {{SlyBindingsValuesProvider}} will not be available to be used directly in Sightly scripts, making them available only for JS Use-API objects.",", "
"   Rename Method,Extract Method,","Configuration values should be compared by string comparison ConfigUtil.isSameData has some logic which compares two configuration dictionaries:
- if the value from dictionary A is a number, a special handling is done which results in string compare if the value from B is a number as well
- otherwise equals is used

Now, if A contains a string and B contains a number, both representing the same value, the dictionaries are considered to be not the same; the special handling is questionable as it only works in one direction.

I think we can relax this and always do a string compare","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Configuration values should be compared by string comparison ConfigUtil.isSameData has some logic which compares two configuration dictionaries:
- if the value from dictionary A is a number, a special handling is done which results in string compare if the value from B is a number as well
- otherwise equals is used

Now, if A contains a string and B contains a number, both representing the same value, the dictionaries are considered to be not the same; the special handling is questionable as it only works in one direction.

I think we can relax this and always do a string compare","Duplicated Code, Long Method, , "
"   Rename Method,","JSON representation of Calendar values should preserve timezone Im currently doing some things with dates in Sling that involve timezones and I find that the documentation regarding it is not particularly clear.

according to https://sling.apache.org/documentation/bundles/manipulating-content-the-slingpostservlet-servlets-post.html#date-properties
several formats are defined. 

I found that the only format that saves a provided timezone is the ISO8601 format, rest of them relies in a Date object, which does not have timezones. Could this be clearly stated?

Also, the ISO8601 parser is problematic. It relies on the Jackrabbit parser which uses format ""±YYYY-MM-DDThh:mm:ss.SSSTZD"", but according to http://www.w3.org/TR/NOTE-datetime the ISO format does not have milliseconds on it (""SSS""). So it is very hard to find a way to keep the timezone information (I had to dig through the code to figure it out)

Could we please replace ISO8601 with the actual format ""±YYYY-MM-DDThh:mm:ss.SSSTZD"" so it is clearer?",", "
"   Rename Method,","JSON representation of Calendar values should preserve timezone Im currently doing some things with dates in Sling that involve timezones and I find that the documentation regarding it is not particularly clear.

according to https://sling.apache.org/documentation/bundles/manipulating-content-the-slingpostservlet-servlets-post.html#date-properties
several formats are defined. 

I found that the only format that saves a provided timezone is the ISO8601 format, rest of them relies in a Date object, which does not have timezones. Could this be clearly stated?

Also, the ISO8601 parser is problematic. It relies on the Jackrabbit parser which uses format ""±YYYY-MM-DDThh:mm:ss.SSSTZD"", but according to http://www.w3.org/TR/NOTE-datetime the ISO format does not have milliseconds on it (""SSS""). So it is very hard to find a way to keep the timezone information (I had to dig through the code to figure it out)

Could we please replace ISO8601 with the actual format ""±YYYY-MM-DDThh:mm:ss.SSSTZD"" so it is clearer?",", "
"   Rename Method,Extract Method,","Avoid caching JCR property values The support for ValueMap is currently caching the JCR Value objects and also the JCR Property object.
If the value map object is held, this might prevent garbage collection within Oak as the value object holds a reference to the revision.

We should check whether caching is needed at all or if for example we could just cache the value itself but not the JCR Value object","Duplicated Code, Long Method, , "
"   Move And Rename Class,Rename Method,Move Method,Inline Method,","Avoid caching JCR property values The support for ValueMap is currently caching the JCR Value objects and also the JCR Property object.
If the value map object is held, this might prevent garbage collection within Oak as the value object holds a reference to the revision.

We should check whether caching is needed at all or if for example we could just cache the value itself but not the JCR Value object",", , , "
"   Move And Rename Class,Rename Method,Move Method,Inline Method,","Avoid caching JCR property values The support for ValueMap is currently caching the JCR Value objects and also the JCR Property object.
If the value map object is held, this might prevent garbage collection within Oak as the value object holds a reference to the revision.

We should check whether caching is needed at all or if for example we could just cache the value itself but not the JCR Value object",", , , "
"   Move And Rename Class,Rename Method,Move Method,Inline Method,","Avoid caching JCR property values The support for ValueMap is currently caching the JCR Value objects and also the JCR Property object.
If the value map object is held, this might prevent garbage collection within Oak as the value object holds a reference to the revision.

We should check whether caching is needed at all or if for example we could just cache the value itself but not the JCR Value object",", , , "
"   Rename Method,Extract Method,","Avoid caching JCR property values The support for ValueMap is currently caching the JCR Value objects and also the JCR Property object.
If the value map object is held, this might prevent garbage collection within Oak as the value object holds a reference to the revision.

We should check whether caching is needed at all or if for example we could just cache the value itself but not the JCR Value object","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Avoid caching JCR property values The support for ValueMap is currently caching the JCR Value objects and also the JCR Property object.
If the value map object is held, this might prevent garbage collection within Oak as the value object holds a reference to the revision.

We should check whether caching is needed at all or if for example we could just cache the value itself but not the JCR Value object","Duplicated Code, Long Method, , "
"   Rename Class,Extract Method,","Register an osgi service for each available service user In order to only activate osgi components when a service user mapping is available it would be useful to have an osgi service registered for a service user. A component can reference the registered service name and only start when that becomes available.

{code}
@Reference(target=""(name=serviceName)"")
ServiceUserExists userExists;
{code}

","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,Move Method,Move Attribute,",Create individual factories for distribution triggers Currently triggers are created using a generic factory (LocalDistributionTriggerFactory). That generic factory should be split up in several specific factories such that properties can be customized in a meaningful way.,", , , "
"   Rename Class,Rename Method,Move Method,Move Attribute,",Create individual factories for distribution triggers Currently triggers are created using a generic factory (LocalDistributionTriggerFactory). That generic factory should be split up in several specific factories such that properties can be customized in a meaningful way.,", , , "
"   Rename Method,","[Sightly] Reduce the number of repository reads On instances that still use Jackrabbit, repository read operations can become problematic if the system is exposed to a high number of requests. Therefore the Sightly Scripting engine should be optimised to reduce the number of repository reads as much as possible.",", "
"   Rename Method,","[Sightly] Reduce the number of repository reads On instances that still use Jackrabbit, repository read operations can become problematic if the system is exposed to a high number of requests. Therefore the Sightly Scripting engine should be optimised to reduce the number of repository reads as much as possible.",", "
"   Rename Method,Move Method,",Expose distribution log information over http Expose distribution log information over http so that a non admin (operator) can see in UI the distribution log,", , "
"   Rename Method,Move Method,",Expose distribution log information over http Expose distribution log information over http so that a non admin (operator) can see in UI the distribution log,", , "
"   Rename Method,","The use of finalize in ResourceResolver leads to performance issues Currently there is a finalizer implemented for the ResourceResolverImpl (http://svn.apache.org/repos/asf/sling/trunk/bundles/resourceresolver/src/main/java/org/apache/sling/resourceresolver/impl/ResourceResolverImpl.java).

This defers garbage collection. For a more detailed analysis and also some metrics around that have a look at https://issues.apache.org/jira/browse/JCR-2768.
A similar approach like in the patch attached to JCR-2768 should be implemented for the ResourceResolverImpl.",", "
"   Rename Method,","The use of finalize in ResourceResolver leads to performance issues Currently there is a finalizer implemented for the ResourceResolverImpl (http://svn.apache.org/repos/asf/sling/trunk/bundles/resourceresolver/src/main/java/org/apache/sling/resourceresolver/impl/ResourceResolverImpl.java).

This defers garbage collection. For a more detailed analysis and also some metrics around that have a look at https://issues.apache.org/jira/browse/JCR-2768.
A similar approach like in the patch attached to JCR-2768 should be implemented for the ResourceResolverImpl.",", "
"   Rename Method,","The use of finalize in ResourceResolver leads to performance issues Currently there is a finalizer implemented for the ResourceResolverImpl (http://svn.apache.org/repos/asf/sling/trunk/bundles/resourceresolver/src/main/java/org/apache/sling/resourceresolver/impl/ResourceResolverImpl.java).

This defers garbage collection. For a more detailed analysis and also some metrics around that have a look at https://issues.apache.org/jira/browse/JCR-2768.
A similar approach like in the patch attached to JCR-2768 should be implemented for the ResourceResolverImpl.",", "
"   Rename Method,Move Method,","Sling NoSQL Resource Provider for Couchbase we want to create a lightweight sling resource provider for using [Couchbase|http://www.couchbase.com/] as storage backend without the need to have an underlying JCR or Oak infrastructure. this is a similar approach like the [MongoDB resource provider|https://svn.apache.org/repos/asf/sling/trunk/contrib/extensions/mongodb].

as [discussed in the mailing list|http://apache-sling.73963.n3.nabble.com/RT-Sling-Resource-Providers-for-NoSQL-databases-MongoDB-Couchbase-tt4046669.html] it would make sense to create a generic shared codebase for nosql resource providers. this shared module could be used for other NoSQL databases as well.",", , "
"   Rename Method,","Use sling mocks for the validation core tests The org.apache.sling.validation.core bundle has its own utility mocks in https://svn.apache.org/repos/asf/sling/trunk/contrib/extensions/validation/core/src/test/java/org/apache/sling/validation/impl/setup/ . All of these can be replaced by the new Sling mocks.

The benefits would be:

- less maintenance for the validation core project
- more exposure/coverage for the Sling mocks",", "
"   Rename Method,","Use sling mocks for the validation core tests The org.apache.sling.validation.core bundle has its own utility mocks in https://svn.apache.org/repos/asf/sling/trunk/contrib/extensions/validation/core/src/test/java/org/apache/sling/validation/impl/setup/ . All of these can be replaced by the new Sling mocks.

The benefits would be:

- less maintenance for the validation core project
- more exposure/coverage for the Sling mocks",", "
"   Rename Method,Extract Method,",Add a TEST distribution request type Add a TEST distribution request type that should be executed without altering the repository.,"Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,",Add a TEST distribution request type Add a TEST distribution request type that should be executed without altering the repository.,"Duplicated Code, Long Method, , "
"   Rename Method,","Make default bundle location configurable We should make the default bundle location for a new configuration handled by the installer configurable. WIth versions 1.0.x this was null, with 1.1.0 we switched to ""?"".
We should revert the default to null but provide a way to change that default",", "
"   Rename Method,","Make default bundle location configurable We should make the default bundle location for a new configuration handled by the installer configurable. WIth versions 1.0.x this was null, with 1.1.0 we switched to ""?"".
We should revert the default to null but provide a way to change that default",", "
"   Rename Method,","Make default bundle location configurable We should make the default bundle location for a new configuration handled by the installer configurable. WIth versions 1.0.x this was null, with 1.1.0 we switched to ""?"".
We should revert the default to null but provide a way to change that default",", "
"   Pull Up Method,Pull Up Attribute,","Provide Oak features provide features for Oak as we have now for Jackrabbit:
* -{{oak-sling}}-
* {{sling-jcr-oak}}
* {{sling-launchpad-oak}}
* {{sling-launchpad-oak-tar}}
* {{sling-launchpad-oak-mongo}}",", Duplicated Code, Duplicated Code, "
"   Rename Method,","Provide Oak features provide features for Oak as we have now for Jackrabbit:
* -{{oak-sling}}-
* {{sling-jcr-oak}}
* {{sling-launchpad-oak}}
* {{sling-launchpad-oak-tar}}
* {{sling-launchpad-oak-mongo}}",", "
"   Rename Method,","Provide Oak features provide features for Oak as we have now for Jackrabbit:
* -{{oak-sling}}-
* {{sling-jcr-oak}}
* {{sling-launchpad-oak}}
* {{sling-launchpad-oak-tar}}
* {{sling-launchpad-oak-mongo}}",", "
"   Rename Method,","Provide Oak features provide features for Oak as we have now for Jackrabbit:
* -{{oak-sling}}-
* {{sling-jcr-oak}}
* {{sling-launchpad-oak}}
* {{sling-launchpad-oak-tar}}
* {{sling-launchpad-oak-mongo}}",", "
"   Move And Rename Class,Move Attribute,","Sling Mock: MockModelAdapterFactory not compatible with latest Sling Models Impl the current sling mock MockModelAdapterFactory implementation is not compatible with the latest sling models implementation because it does not support InjectAnnotationProcessorFactory2 and StaticInjectAnnotationProcessorFactory interfaces.

a better solution than the current hard-wired mock wiring of injectors would be to enhance the osgi mock and bind/unbind them dynamically, then the new interfaces would be supported as well.",", , "
"   Move And Rename Class,Move Attribute,","Sling Mock: MockModelAdapterFactory not compatible with latest Sling Models Impl the current sling mock MockModelAdapterFactory implementation is not compatible with the latest sling models implementation because it does not support InjectAnnotationProcessorFactory2 and StaticInjectAnnotationProcessorFactory interfaces.

a better solution than the current hard-wired mock wiring of injectors would be to enhance the osgi mock and bind/unbind them dynamically, then the new interfaces would be supported as well.",", , "
"   Move And Rename Class,Move Attribute,","Sling Mock: MockModelAdapterFactory not compatible with latest Sling Models Impl the current sling mock MockModelAdapterFactory implementation is not compatible with the latest sling models implementation because it does not support InjectAnnotationProcessorFactory2 and StaticInjectAnnotationProcessorFactory interfaces.

a better solution than the current hard-wired mock wiring of injectors would be to enhance the osgi mock and bind/unbind them dynamically, then the new interfaces would be supported as well.",", , "
"   Rename Method,Extract Method,","MockJcrResourceResolverFactory should allow to register services dynamically. The {{MockJcrResourceResolverFactory}} class creates it's own mocked OSGi BundleContext and uses it to create a ResourceResolverFactory. Part of the OSGi environment initialization performed in the class is registering OSGi services to be available by the mocked resolver. By default there is only one service - SlingRepository.

However, the more recent versions of ResourceProvider implementations requires other services, eg. PathMapper. Right now there is no way to inject this service into the mocked OSGi container. The {{MockJcrResourceResolverFactory}} should allow to register custom services, so it'll be compatible with the future versions of ResourceResolvers and Providers.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","MockJcrResourceResolverFactory should allow to register services dynamically. The {{MockJcrResourceResolverFactory}} class creates it's own mocked OSGi BundleContext and uses it to create a ResourceResolverFactory. Part of the OSGi environment initialization performed in the class is registering OSGi services to be available by the mocked resolver. By default there is only one service - SlingRepository.

However, the more recent versions of ResourceProvider implementations requires other services, eg. PathMapper. Right now there is no way to inject this service into the mocked OSGi container. The {{MockJcrResourceResolverFactory}} should allow to register custom services, so it'll be compatible with the future versions of ResourceResolvers and Providers.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","MockJcrResourceResolverFactory should allow to register services dynamically. The {{MockJcrResourceResolverFactory}} class creates it's own mocked OSGi BundleContext and uses it to create a ResourceResolverFactory. Part of the OSGi environment initialization performed in the class is registering OSGi services to be available by the mocked resolver. By default there is only one service - SlingRepository.

However, the more recent versions of ResourceProvider implementations requires other services, eg. PathMapper. Right now there is no way to inject this service into the mocked OSGi container. The {{MockJcrResourceResolverFactory}} should allow to register custom services, so it'll be compatible with the future versions of ResourceResolvers and Providers.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","OSGi Mock: Handle dynamic service references updates currently service references are only injected when registering the services. updates after this e.g. when a another service is referenced matching an optional or multiple references this is currently ignored. we should handle this in the mock context as well at least in a basic way to support unit tests.

this is required e.g. to resolve SLING-4434","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","OSGi Mock: Handle dynamic service references updates currently service references are only injected when registering the services. updates after this e.g. when a another service is referenced matching an optional or multiple references this is currently ignored. we should handle this in the mock context as well at least in a basic way to support unit tests.

this is required e.g. to resolve SLING-4434","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,Extract Method,","Reduce the number of controller threads for queue Right now each queue is using a controller thread which is not taken from a pool. WIth a lot of queues this results in a lot of threads - although a queue is only started if there is a job for that queue, this still can be a significant number.","Duplicated Code, Long Method, , "
"   Rename Method,","Reduce the number of controller threads for queue Right now each queue is using a controller thread which is not taken from a pool. WIth a lot of queues this results in a lot of threads - although a queue is only started if there is a job for that queue, this still can be a significant number.",", "
"   Extract Method,Move Attribute,","Refactor JCR installer The jcr installer code is very old and grew a lot over the past years, I think it's time to clean it up a little bit","Duplicated Code, Long Method, , , "
"   Move Method,Extract Method,","Refactor JCR installer The jcr installer code is very old and grew a lot over the past years, I think it's time to clean it up a little bit","Duplicated Code, Long Method, , , "
"   Move Method,Extract Method,","Refactor JCR installer The jcr installer code is very old and grew a lot over the past years, I think it's time to clean it up a little bit","Duplicated Code, Long Method, , , "
"   Rename Method,","Extend SlingPostServlet to support references to existing repository content After SLING-422 an interesting (for some applications important) side effect of the old SlingPostServlet has gone: With the old implementation it was possible to have files uploaded to the repository in one or more requests and and in a follow up request create another content and move the previously uploaded file to the newly created content.

To better illustrate: Assume a CMS where you edit some kind of content. The content is composed of one or more images to be uploaded as image files and a title and a body text field. Now the CMS wants to present a user friendly interface and has implemented some cool file upload dialog, which shows upload progress.

This feature is now used to pre-upload the image files to a temporary location. Upon sending the rest of the content - the title and body text - the image files should of course also be moved from the temporary location to the final destination in the same place as the title and body text.

With the old SlingPostServlet, the image files could be moved by simply including :moveSrc/:moveDst parameter pairs for each image file. With the new SlingPostServlet this is not currently possible.

To make such things possible again, the ModifyOperation of the SlingPostServlet is to be extend such as to recognize special parameters. Similar to the solution proposed in SLING-130 (@ValueFrom suffix to refer to values of another form field), I propose the following parameter name suffixes:

@CopyFrom - Copies the items from the repository locations indicated by the parameter value
@MoveFrom - Moves the items from the repository locations indicated by the parameter value

Example use: To move an image file uploaded previously to ""/tmp/image000.gif"" to the ""image"" child node, the HTML form to submit the content (along with title and text fields) could be defined as:

<form method=""POST"" action=""/some/new/content"">
<input type=""hidden"" name=""image@MoveFrom"" value=""/tmp/image000.gif"" />
<input type=""text"" name=""title"" value=""..."" />
<input type=""text"" name=""text"" value=""..."" />
<submit />
</form>

If the item referred to in a @CopyFrom/@MoveFrom parameter is a node of type nt:file, treatment is special: If the natural type of the destination item is nt:file, the addressed node is simply copied or moved. Otherwise the the jcr:content child node is copied or moved. In case of a move the nt:file node is of course also removed.

Example use (continued): After processing the request defined by the above form, the original item /tmp/image000.gif is gone and the contents is now located in /some/new/content/image.",", "
"   Rename Method,Move Method,Extract Method,Inline Method,","Use a single listener registered for multiple path in JCR installer Sling Jcr installer currently registers one listener per watched folder. On an application like AEM this results in ~150 listeners out of total 210 to belong to Jcr installer.

Recently Jackrabbit introduced support for adding [additional path|https://github.com/apache/jackrabbit/blob/trunk/jackrabbit-api/src/main/java/org/apache/jackrabbit/api/observation/JackrabbitEventFilter.java#L232] to listen to as part of JCR-3745. This can be leveraged by the Jcr installer to avoid registering multiple listeners and instead use one listener.

The above feature can be used in following form

{code}
String[] paths = searchPaths.toArray(new String[]{});

JackrabbitEventFilter eventFilter = new JackrabbitEventFilter()
.setAbsPath(paths[0])
.setEventTypes(Event.NODE_ADDED |
Event.NODE_REMOVED |
Event.NODE_MOVED |
Event.PROPERTY_ADDED |
Event.PROPERTY_CHANGED |
Event.PROPERTY_REMOVED )
.setIsDeep(true)
.setNoLocal(false)
.setNoExternal(true);

if (paths.length > 1) {
eventFilter.setAdditionalPaths(paths);
}

JackrabbitObservationManager observationManager = (JackrabbitObservationManager) adminSession.getWorkspace().getObservationManager();

observationManager.addEventListener(this, eventFilter);
{code}

This would allow more efficient observation processing and avoid putting load on system as Oak currently maintains one queue per listener.","Duplicated Code, Long Method, , , , "
"   Move Method,Move Attribute,","Get array of namespace prefixes only once The JcrModifiableValueMap caches an array of namespace prefixes as this is right now an expensive repository operation.
Instead of doing this in each and every value map, we could do this just once per resource provider.",", , , "
"   Move Method,Move Attribute,","Get array of namespace prefixes only once The JcrModifiableValueMap caches an array of namespace prefixes as this is right now an expensive repository operation.
Instead of doing this in each and every value map, we could do this just once per resource provider.",", , , "
"   Rename Method,","discovery.oak: oak-based discovery implementation When discovery is used in a stack based on jackrabbit oak as the repository, the current way of discoving instances somewhat sounds like duplicating work: oak, or more precisely documentnodestore, itself has a low-level [lease mechanism|http://jackrabbit.apache.org/oak/docs/nodestore/documentmk.html] where it stores information about the cluster nodes including a {{leaseEnd}} indicating at what time others can consider a particular node as dead/crashed. This corresponds pretty much to the discovery.impl heartbeat mechanism. And in a stack which is built ontop of oak-documentMk, we could be making use of this fact and delegate the decision about whether a node in a cluster is alive or not to the oak layer. Also, with OAK-2597 the relevant information: {{ActiveClusterNodes}} is nicely exposed via JMX - so that can become the new source of truth defining the cluster view.

When replacing discovery-owned heartbeats with oak-owned ones, there is one important detail to be watched out for: it can no longer easily be determined from another instance in the cluster, whether it has this new discovery bundle activated or not. Hence it is not given that when a voting happens, that all {{active}} nodes (as reported by oak-documentMk) are actually going to respond. So the 'silent instance due to deactivated discovery bundle' case needs special attention/handling.

Other than that, given the normal case of all {{active}} nodes having the bundle activated, the voting mechanism can stay the same as in discovery.impl. The topology connectors can be treated the same too (by storing announcements to their respective {{/var/discovery/clusterInstances/<slingId>/announcements/<announcerSlingId>}} node. The properties can be handled the same too (by storing to {{/properties}} node. Only thing that gets replaced is the {{heartbeats}}.

Note that in order for such an oak-based discovery.impl this oak-lease mechanism must be very robust (it should be so by its own interest already). However, there are currently a few issues that should probably first be resolved until discovery can be based on this: OAK-2739, OAK-2682 and OAK-2681 are currently known in this area.",", "
"   Rename Method,","discovery.oak: oak-based discovery implementation When discovery is used in a stack based on jackrabbit oak as the repository, the current way of discoving instances somewhat sounds like duplicating work: oak, or more precisely documentnodestore, itself has a low-level [lease mechanism|http://jackrabbit.apache.org/oak/docs/nodestore/documentmk.html] where it stores information about the cluster nodes including a {{leaseEnd}} indicating at what time others can consider a particular node as dead/crashed. This corresponds pretty much to the discovery.impl heartbeat mechanism. And in a stack which is built ontop of oak-documentMk, we could be making use of this fact and delegate the decision about whether a node in a cluster is alive or not to the oak layer. Also, with OAK-2597 the relevant information: {{ActiveClusterNodes}} is nicely exposed via JMX - so that can become the new source of truth defining the cluster view.

When replacing discovery-owned heartbeats with oak-owned ones, there is one important detail to be watched out for: it can no longer easily be determined from another instance in the cluster, whether it has this new discovery bundle activated or not. Hence it is not given that when a voting happens, that all {{active}} nodes (as reported by oak-documentMk) are actually going to respond. So the 'silent instance due to deactivated discovery bundle' case needs special attention/handling.

Other than that, given the normal case of all {{active}} nodes having the bundle activated, the voting mechanism can stay the same as in discovery.impl. The topology connectors can be treated the same too (by storing announcements to their respective {{/var/discovery/clusterInstances/<slingId>/announcements/<announcerSlingId>}} node. The properties can be handled the same too (by storing to {{/properties}} node. Only thing that gets replaced is the {{heartbeats}}.

Note that in order for such an oak-based discovery.impl this oak-lease mechanism must be very robust (it should be so by its own interest already). However, there are currently a few issues that should probably first be resolved until discovery can be based on this: OAK-2739, OAK-2682 and OAK-2681 are currently known in this area.",", "
"   Rename Method,","discovery.oak: oak-based discovery implementation When discovery is used in a stack based on jackrabbit oak as the repository, the current way of discoving instances somewhat sounds like duplicating work: oak, or more precisely documentnodestore, itself has a low-level [lease mechanism|http://jackrabbit.apache.org/oak/docs/nodestore/documentmk.html] where it stores information about the cluster nodes including a {{leaseEnd}} indicating at what time others can consider a particular node as dead/crashed. This corresponds pretty much to the discovery.impl heartbeat mechanism. And in a stack which is built ontop of oak-documentMk, we could be making use of this fact and delegate the decision about whether a node in a cluster is alive or not to the oak layer. Also, with OAK-2597 the relevant information: {{ActiveClusterNodes}} is nicely exposed via JMX - so that can become the new source of truth defining the cluster view.

When replacing discovery-owned heartbeats with oak-owned ones, there is one important detail to be watched out for: it can no longer easily be determined from another instance in the cluster, whether it has this new discovery bundle activated or not. Hence it is not given that when a voting happens, that all {{active}} nodes (as reported by oak-documentMk) are actually going to respond. So the 'silent instance due to deactivated discovery bundle' case needs special attention/handling.

Other than that, given the normal case of all {{active}} nodes having the bundle activated, the voting mechanism can stay the same as in discovery.impl. The topology connectors can be treated the same too (by storing announcements to their respective {{/var/discovery/clusterInstances/<slingId>/announcements/<announcerSlingId>}} node. The properties can be handled the same too (by storing to {{/properties}} node. Only thing that gets replaced is the {{heartbeats}}.

Note that in order for such an oak-based discovery.impl this oak-lease mechanism must be very robust (it should be so by its own interest already). However, there are currently a few issues that should probably first be resolved until discovery can be based on this: OAK-2739, OAK-2682 and OAK-2681 are currently known in this area.",", "
"   Rename Method,","discovery.oak: oak-based discovery implementation When discovery is used in a stack based on jackrabbit oak as the repository, the current way of discoving instances somewhat sounds like duplicating work: oak, or more precisely documentnodestore, itself has a low-level [lease mechanism|http://jackrabbit.apache.org/oak/docs/nodestore/documentmk.html] where it stores information about the cluster nodes including a {{leaseEnd}} indicating at what time others can consider a particular node as dead/crashed. This corresponds pretty much to the discovery.impl heartbeat mechanism. And in a stack which is built ontop of oak-documentMk, we could be making use of this fact and delegate the decision about whether a node in a cluster is alive or not to the oak layer. Also, with OAK-2597 the relevant information: {{ActiveClusterNodes}} is nicely exposed via JMX - so that can become the new source of truth defining the cluster view.

When replacing discovery-owned heartbeats with oak-owned ones, there is one important detail to be watched out for: it can no longer easily be determined from another instance in the cluster, whether it has this new discovery bundle activated or not. Hence it is not given that when a voting happens, that all {{active}} nodes (as reported by oak-documentMk) are actually going to respond. So the 'silent instance due to deactivated discovery bundle' case needs special attention/handling.

Other than that, given the normal case of all {{active}} nodes having the bundle activated, the voting mechanism can stay the same as in discovery.impl. The topology connectors can be treated the same too (by storing announcements to their respective {{/var/discovery/clusterInstances/<slingId>/announcements/<announcerSlingId>}} node. The properties can be handled the same too (by storing to {{/properties}} node. Only thing that gets replaced is the {{heartbeats}}.

Note that in order for such an oak-based discovery.impl this oak-lease mechanism must be very robust (it should be so by its own interest already). However, there are currently a few issues that should probably first be resolved until discovery can be based on this: OAK-2739, OAK-2682 and OAK-2681 are currently known in this area.",", "
"   Rename Method,","discovery.oak: oak-based discovery implementation When discovery is used in a stack based on jackrabbit oak as the repository, the current way of discoving instances somewhat sounds like duplicating work: oak, or more precisely documentnodestore, itself has a low-level [lease mechanism|http://jackrabbit.apache.org/oak/docs/nodestore/documentmk.html] where it stores information about the cluster nodes including a {{leaseEnd}} indicating at what time others can consider a particular node as dead/crashed. This corresponds pretty much to the discovery.impl heartbeat mechanism. And in a stack which is built ontop of oak-documentMk, we could be making use of this fact and delegate the decision about whether a node in a cluster is alive or not to the oak layer. Also, with OAK-2597 the relevant information: {{ActiveClusterNodes}} is nicely exposed via JMX - so that can become the new source of truth defining the cluster view.

When replacing discovery-owned heartbeats with oak-owned ones, there is one important detail to be watched out for: it can no longer easily be determined from another instance in the cluster, whether it has this new discovery bundle activated or not. Hence it is not given that when a voting happens, that all {{active}} nodes (as reported by oak-documentMk) are actually going to respond. So the 'silent instance due to deactivated discovery bundle' case needs special attention/handling.

Other than that, given the normal case of all {{active}} nodes having the bundle activated, the voting mechanism can stay the same as in discovery.impl. The topology connectors can be treated the same too (by storing announcements to their respective {{/var/discovery/clusterInstances/<slingId>/announcements/<announcerSlingId>}} node. The properties can be handled the same too (by storing to {{/properties}} node. Only thing that gets replaced is the {{heartbeats}}.

Note that in order for such an oak-based discovery.impl this oak-lease mechanism must be very robust (it should be so by its own interest already). However, there are currently a few issues that should probably first be resolved until discovery can be based on this: OAK-2739, OAK-2682 and OAK-2681 are currently known in this area.",", "
"   Rename Method,","discovery.oak: oak-based discovery implementation When discovery is used in a stack based on jackrabbit oak as the repository, the current way of discoving instances somewhat sounds like duplicating work: oak, or more precisely documentnodestore, itself has a low-level [lease mechanism|http://jackrabbit.apache.org/oak/docs/nodestore/documentmk.html] where it stores information about the cluster nodes including a {{leaseEnd}} indicating at what time others can consider a particular node as dead/crashed. This corresponds pretty much to the discovery.impl heartbeat mechanism. And in a stack which is built ontop of oak-documentMk, we could be making use of this fact and delegate the decision about whether a node in a cluster is alive or not to the oak layer. Also, with OAK-2597 the relevant information: {{ActiveClusterNodes}} is nicely exposed via JMX - so that can become the new source of truth defining the cluster view.

When replacing discovery-owned heartbeats with oak-owned ones, there is one important detail to be watched out for: it can no longer easily be determined from another instance in the cluster, whether it has this new discovery bundle activated or not. Hence it is not given that when a voting happens, that all {{active}} nodes (as reported by oak-documentMk) are actually going to respond. So the 'silent instance due to deactivated discovery bundle' case needs special attention/handling.

Other than that, given the normal case of all {{active}} nodes having the bundle activated, the voting mechanism can stay the same as in discovery.impl. The topology connectors can be treated the same too (by storing announcements to their respective {{/var/discovery/clusterInstances/<slingId>/announcements/<announcerSlingId>}} node. The properties can be handled the same too (by storing to {{/properties}} node. Only thing that gets replaced is the {{heartbeats}}.

Note that in order for such an oak-based discovery.impl this oak-lease mechanism must be very robust (it should be so by its own interest already). However, there are currently a few issues that should probably first be resolved until discovery can be based on this: OAK-2739, OAK-2682 and OAK-2681 are currently known in this area.",", "
"   Move Method,Extract Method,Move Attribute,","JST scripting engine: render indexable HTML and separate javascript code The JST scripting engine should output a default HTML rendering, meant to be indexed by search engines, with a <script> element that points to a separate javascript resource to render the page.

The idea is that the javascript code will be cached by client browsers, being the same for all resources that have the same sling:resourceType.

The HTML rendering should include a meaningful <title> element, using the value of a property named ""title"" or ""description"", if present, or the node name if not.","Duplicated Code, Long Method, , , , "
"   Move Method,Extract Method,Move Attribute,","JST scripting engine: render indexable HTML and separate javascript code The JST scripting engine should output a default HTML rendering, meant to be indexed by search engines, with a <script> element that points to a separate javascript resource to render the page.

The idea is that the javascript code will be cached by client browsers, being the same for all resources that have the same sling:resourceType.

The HTML rendering should include a meaningful <title> element, using the value of a property named ""title"" or ""description"", if present, or the node name if not.","Duplicated Code, Long Method, , , , "
"   Move Class,Move And Rename Class,","Move HttpTestBase integration testing utility to commons/testing The launchpad HttpTestBase class, and a few related utility classes, should move to the commons/testing module as they are reusable. We can for example reuse them for testing additional scripting engines, without having to include them in the launchpad build.",", "
"   Rename Method,Extract Method,","Periodically perform a full topic scan per instance Once the job handling knows that jobs for a topic exist, it does periodic scans for that topic and is therefore independent from any potential problem of observation.
However, this only works as long as the instance did get at least one observation event for a new topic. A full topic scan periodically would solve this problem.","Duplicated Code, Long Method, , "
"   Move Class,Move And Rename Class,Move Method,Extract Method,Move Attribute,","Decouple scheduled jobs from observation Right now the scheduling of jobs is tied to observation events. If - for whatever reason - the observation events are not sent, the scheduling does not pick up changes.
For the general job handling we solved this by periodic scans - we should use a similar approach for the scheduled jobs","Duplicated Code, Long Method, , , , "
"   Rename Method,","Decouple Sightly from the JCR Compiler The current implementation of the Sightly scripting engine depends on the JCR Compiler for generating Java classes from the Sightly script files. However, the JCR Compiler can be slow on some systems due to JCR's locking mechanisms.

Since Sling also provides the {{org.apache.sling.commons.fsclassloader}}, which implements a faster filesystem-based {{ClassLoaderWriter}}, it would be better to use a more generic approach for generating Java classes.",", "
"   Move Method,Extract Method,","Crankstart should use the Sling Provisioning Model Crankstart should be converted to use the Sling Provisioning Model that was created in the meantime.

This should preserve the following features:

Set Initial classpath, select framework version
Set defaults for variables, override with system properties
Set OSGi framework properties, with variable
Install configs early, as soon as ConfigAdmin is available
OSGi configs with typed values
Guard against multiple factory config creation (CRANKSTART_CONFIG_ID)
Ability to register new startup commands
mvn:protocol is optional, can also use http
log messages during startup","Duplicated Code, Long Method, , , "
"   Rename Method,Inline Method,","Crankstart should use the Sling Provisioning Model Crankstart should be converted to use the Sling Provisioning Model that was created in the meantime.

This should preserve the following features:

Set Initial classpath, select framework version
Set defaults for variables, override with system properties
Set OSGi framework properties, with variable
Install configs early, as soon as ConfigAdmin is available
OSGi configs with typed values
Guard against multiple factory config creation (CRANKSTART_CONFIG_ID)
Ability to register new startup commands
mvn:protocol is optional, can also use http
log messages during startup",", , "
"   Rename Method,Move Method,Extract Method,Move Attribute,","Crankstart should use the Sling Provisioning Model Crankstart should be converted to use the Sling Provisioning Model that was created in the meantime.

This should preserve the following features:

Set Initial classpath, select framework version
Set defaults for variables, override with system properties
Set OSGi framework properties, with variable
Install configs early, as soon as ConfigAdmin is available
OSGi configs with typed values
Guard against multiple factory config creation (CRANKSTART_CONFIG_ID)
Ability to register new startup commands
mvn:protocol is optional, can also use http
log messages during startup","Duplicated Code, Long Method, , , , "
"   Rename Class,Extract Superclass,Extract Method,","Sling Mock: Add ""NONE"" resource resolver type the ""NONE"" resource resolver type should initialize the real sling resource resolver factory implementation, but without registering any resource provider (esp. no JCR and no root provider).

this is useful when testing own resource providers that should be registered as root provider without JCR.","Duplicated Code, Long Method, , Duplicated Code, Large Class, "
"   Rename Method,","New Resource Provider API Mail thread from the mailing list:
http://mail-archives.apache.org/mod_mbox/sling-dev/201505.mbox/%3C555983ED.1080800%40apache.org%3E

Starting mail:
The resource provider API has grown a lot over time and when we started
with it we didn't really think about potential extensions of the api.
Today, each time we add a new feature, we come up with a new marker
interface. There is also the distinction between a resource provider
(singleton/stateless) and the factory (creating stateful providers).
Although the api is not intended to be used by the average resource api
user (it's an extension), we put it in the same package. And there are
more minor things.

Therefore I think it's time to start a new API that is more future proof
and solves the known problems. I've created a draft prototype at [1].

During the performance analysis by Joel he found out that getParent
calls to a resource a pretty expensive as in the end these are string
based. Therefore, e.g. the JCR implementation can't simply call
getParent on a node and wrap it in a resource. Therefore I think we
should add a getParent(Resource) method to the resource resolver and
have a better way to handle this in a resource provider.

Instead of having a resource provider and a resource provider factory,
we define a single ResourceProvider which is a singleton. If this
provider needs authentication and/or needs to keep state per user, the
PROPERTY_AUTHENTICATE needs to be set to true and in this case the
authenticate method is called. This one returns a data object which is
passed in to each and every method. If auth is not required, the method
is not called and null is passed in as the data object.
For authentication, providers do not support login administrative
anymore, just users and service users.

A provider is mounted at a single root - no more support for mounting it
at different path at the same time; and a provider always owns the root.
So if a provider does not return a resource for a given path, no other
provider is asked. This allows for improved implementations and resource
resolving. If we decided that we need this for compatibility we can
solve it differently.

Instead of using marker interface, we define the ResourceProvider as an
abstract class. This allows us to add new methods without breaking
existing providers.

Each method gets a ResolveContext, containing the resource resolver,
the previously mentioned state data object and other things, e.g. the
parameter support recently added to the resource resolving. In the
future we can pass in additional data without breaking the interface.

Apart from that the resource provider is similar to the aggregation of
the already existing marker interfaces. There are two exceptions,
observation and query which I'll handle in different emails.

[1]
https://svn.apache.org/repos/asf/sling/whiteboard/cziegeler/api-v3/src/main/java/org/apache/sling/api/resource/provider/",", "
"   Rename Method,","New Resource Provider API Mail thread from the mailing list:
http://mail-archives.apache.org/mod_mbox/sling-dev/201505.mbox/%3C555983ED.1080800%40apache.org%3E

Starting mail:
The resource provider API has grown a lot over time and when we started
with it we didn't really think about potential extensions of the api.
Today, each time we add a new feature, we come up with a new marker
interface. There is also the distinction between a resource provider
(singleton/stateless) and the factory (creating stateful providers).
Although the api is not intended to be used by the average resource api
user (it's an extension), we put it in the same package. And there are
more minor things.

Therefore I think it's time to start a new API that is more future proof
and solves the known problems. I've created a draft prototype at [1].

During the performance analysis by Joel he found out that getParent
calls to a resource a pretty expensive as in the end these are string
based. Therefore, e.g. the JCR implementation can't simply call
getParent on a node and wrap it in a resource. Therefore I think we
should add a getParent(Resource) method to the resource resolver and
have a better way to handle this in a resource provider.

Instead of having a resource provider and a resource provider factory,
we define a single ResourceProvider which is a singleton. If this
provider needs authentication and/or needs to keep state per user, the
PROPERTY_AUTHENTICATE needs to be set to true and in this case the
authenticate method is called. This one returns a data object which is
passed in to each and every method. If auth is not required, the method
is not called and null is passed in as the data object.
For authentication, providers do not support login administrative
anymore, just users and service users.

A provider is mounted at a single root - no more support for mounting it
at different path at the same time; and a provider always owns the root.
So if a provider does not return a resource for a given path, no other
provider is asked. This allows for improved implementations and resource
resolving. If we decided that we need this for compatibility we can
solve it differently.

Instead of using marker interface, we define the ResourceProvider as an
abstract class. This allows us to add new methods without breaking
existing providers.

Each method gets a ResolveContext, containing the resource resolver,
the previously mentioned state data object and other things, e.g. the
parameter support recently added to the resource resolving. In the
future we can pass in additional data without breaking the interface.

Apart from that the resource provider is similar to the aggregation of
the already existing marker interfaces. There are two exceptions,
observation and query which I'll handle in different emails.

[1]
https://svn.apache.org/repos/asf/sling/whiteboard/cziegeler/api-v3/src/main/java/org/apache/sling/api/resource/provider/",", "
"   Rename Method,","New Resource Provider API Mail thread from the mailing list:
http://mail-archives.apache.org/mod_mbox/sling-dev/201505.mbox/%3C555983ED.1080800%40apache.org%3E

Starting mail:
The resource provider API has grown a lot over time and when we started
with it we didn't really think about potential extensions of the api.
Today, each time we add a new feature, we come up with a new marker
interface. There is also the distinction between a resource provider
(singleton/stateless) and the factory (creating stateful providers).
Although the api is not intended to be used by the average resource api
user (it's an extension), we put it in the same package. And there are
more minor things.

Therefore I think it's time to start a new API that is more future proof
and solves the known problems. I've created a draft prototype at [1].

During the performance analysis by Joel he found out that getParent
calls to a resource a pretty expensive as in the end these are string
based. Therefore, e.g. the JCR implementation can't simply call
getParent on a node and wrap it in a resource. Therefore I think we
should add a getParent(Resource) method to the resource resolver and
have a better way to handle this in a resource provider.

Instead of having a resource provider and a resource provider factory,
we define a single ResourceProvider which is a singleton. If this
provider needs authentication and/or needs to keep state per user, the
PROPERTY_AUTHENTICATE needs to be set to true and in this case the
authenticate method is called. This one returns a data object which is
passed in to each and every method. If auth is not required, the method
is not called and null is passed in as the data object.
For authentication, providers do not support login administrative
anymore, just users and service users.

A provider is mounted at a single root - no more support for mounting it
at different path at the same time; and a provider always owns the root.
So if a provider does not return a resource for a given path, no other
provider is asked. This allows for improved implementations and resource
resolving. If we decided that we need this for compatibility we can
solve it differently.

Instead of using marker interface, we define the ResourceProvider as an
abstract class. This allows us to add new methods without breaking
existing providers.

Each method gets a ResolveContext, containing the resource resolver,
the previously mentioned state data object and other things, e.g. the
parameter support recently added to the resource resolving. In the
future we can pass in additional data without breaking the interface.

Apart from that the resource provider is similar to the aggregation of
the already existing marker interfaces. There are two exceptions,
observation and query which I'll handle in different emails.

[1]
https://svn.apache.org/repos/asf/sling/whiteboard/cziegeler/api-v3/src/main/java/org/apache/sling/api/resource/provider/",", "
"   Move Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Decouple the model provider from the actual validation service As being mentioned in SLING-4027, it would be good to completely decouple the model providing capabilities from the actual validation, as that would
a) make the codebase cleaner (and would ease testing)
b) allow to plug-in other services which provide models (e.g. based on annotations on some files)","Duplicated Code, Long Method, , , , , "
"   Move Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Decouple the model provider from the actual validation service As being mentioned in SLING-4027, it would be good to completely decouple the model providing capabilities from the actual validation, as that would
a) make the codebase cleaner (and would ease testing)
b) allow to plug-in other services which provide models (e.g. based on annotations on some files)","Duplicated Code, Long Method, , , , , "
"   Move Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Decouple the model provider from the actual validation service As being mentioned in SLING-4027, it would be good to completely decouple the model providing capabilities from the actual validation, as that would
a) make the codebase cleaner (and would ease testing)
b) allow to plug-in other services which provide models (e.g. based on annotations on some files)","Duplicated Code, Long Method, , , , , "
"   Rename Method,Move Method,","Support resource type inheritance for validator models Currently there must be a direct match of the resource type given in the validation model and the resource type of the content resource. 
But resource type inheritance should also be taken into account, so the validation model of the given resource type and the one for the super resource types should all be taken into account and the results should be merged!",", , "
"   Rename Class,Move Method,","Slingstart Maven Plugin: Allow to read variables from POM by default provisioning variables can only be resolved by a variables section defined inside the provisioning file.

if processed by the slingstart maven plugin it should be optionally possible to reference variables defined within the pom from which the plugin is executed. additionally it should be possible to attach an resolved (effective model) with those variables replaced when storing it as artifact.

this is useful if the same property is required within the POM and the provisioning file and avoids haven to define and maintain it in two locations.",", , "
"   Rename Class,Move Method,","Slingstart Maven Plugin: Allow to read variables from POM by default provisioning variables can only be resolved by a variables section defined inside the provisioning file.

if processed by the slingstart maven plugin it should be optionally possible to reference variables defined within the pom from which the plugin is executed. additionally it should be possible to attach an resolved (effective model) with those variables replaced when storing it as artifact.

this is useful if the same property is required within the POM and the provisioning file and avoids haven to define and maintain it in two locations.",", , "
"   Rename Class,Rename Method,Extract Method,","Slingstart Maven Plugin: Allow to get artifact versions from POM Currently it is possible to define an exact artifact version in a provisioning file, or define now version, ""LATEST"" is used in this case automatically.

When no explicit version is defined in the provisioning file it should be optionally possible to use the version defined in the dependency or dependencyManagement section of the maven projects POM file.

This is especially useful if the dependency is there defined anyway and should not have to be set and maintained on two locations.","Duplicated Code, Long Method, , "
"   Move Method,Move Attribute,","Allow to register a Servlet for a ResourceType and a Path I have the following use case:
I'd like to register a Servlet to handle the modification of a given ResourceType
And in order to have a fallback I'd like to register the same servlet at a fixed path.
Such a fallback case could be that the first Resource of the given Type has to be created.

The declaration of this use case would look like this

sling.servlet.paths = [""/my/fallback/path""]
sling.servlet.resourcetypes = [""my.example.ResourceType""]

On servlet resolution only the paths property is respected and any other property is ignored.
I would suggest to change resolution such that a combination of paths, resourcetype and method is evaluated.",", , , "
"   Move Method,Move Attribute,","Allow to register a Servlet for a ResourceType and a Path I have the following use case:
I'd like to register a Servlet to handle the modification of a given ResourceType
And in order to have a fallback I'd like to register the same servlet at a fixed path.
Such a fallback case could be that the first Resource of the given Type has to be created.

The declaration of this use case would look like this

sling.servlet.paths = [""/my/fallback/path""]
sling.servlet.resourcetypes = [""my.example.ResourceType""]

On servlet resolution only the paths property is respected and any other property is ignored.
I would suggest to change resolution such that a combination of paths, resourcetype and method is evaluated.",", , , "
"   Move Method,Move Attribute,","Sightly RenderContextImpl contains utility methods that don't belong there The current implementation of Sightly's {{RenderContext}} contains a lot of of utility methods ([example|https://github.com/apache/sling/blob/90d2ed9e42deb144a7f6e1610871e72726cd810a/bundles/scripting/sightly/engine/src/main/java/org/apache/sling/scripting/sightly/impl/engine/runtime/RenderContextImpl.java#L142]).

These are not related to the actual context and belong to an utility class. They are also unrelated to a specific instance/state and should be made static.

Refactoring these out of {{RenderContextImpl}} will allow us to avoid unnecessarily passing an object of this class to other parts of the code just to use these utility methods ([example|https://github.com/apache/sling/blob/90d2ed9e42deb144a7f6e1610871e72726cd810a/bundles/scripting/sightly/engine/src/main/java/org/apache/sling/scripting/sightly/impl/compiler/expression/node/BinaryOperator.java#L31]).",", , , "
"   Rename Method,","Deprecate the asynchronous JavaScript API provided by the Sightly JS Use Provider With the improvement of the API provided by the {{org.apache.sling.scripting.javascript}} bundle, the asynchronous promise-based API provided by the Sightly JS Use Provider bundle is redundant.

Deprecating the API together with conditionally loading the support for it only when detecting that a Use object actually needs the API namespaces should help in optimising the execution speed for JavaScript Use objects.",", "
"   Rename Method,Move Method,","Deprecate the asynchronous JavaScript API provided by the Sightly JS Use Provider With the improvement of the API provided by the {{org.apache.sling.scripting.javascript}} bundle, the asynchronous promise-based API provided by the Sightly JS Use Provider bundle is redundant.

Deprecating the API together with conditionally loading the support for it only when detecting that a Use object actually needs the API namespaces should help in optimising the execution speed for JavaScript Use objects.",", , "
"   Rename Method,",Optimise the SightlyJavaCompilerService to provide objects faster The {{SightlyJavaCompilerService}} can be optimised to provide objects faster in its {{getInstance}} method by delaying repository search for POJO objects.,", "
"   Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,",Optimise the SightlyJavaCompilerService to provide objects faster The {{SightlyJavaCompilerService}} can be optimised to provide objects faster in its {{getInstance}} method by delaying repository search for POJO objects.,"Duplicated Code, Long Method, , , , , "
"   Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,",Optimise the SightlyJavaCompilerService to provide objects faster The {{SightlyJavaCompilerService}} can be optimised to provide objects faster in its {{getInstance}} method by delaying repository search for POJO objects.,"Duplicated Code, Long Method, , , , , "
"   Rename Method,","Allow to enable the usage of regular JCR users for service resolvers With SLING-3854 a {{ServiceUserValidator}} interface was introduced. Basically all OSGi services implementing that interface may decide whether certain users can be used as backing user for a call to {{ResourceResolverFactory.getServiceResolver(...)}}. The only implementation of that in Sling is {{JcrSystemUserValidator}} which only allows to use JCR system users.

The list of all those services is bound in the {{ServiceUserMapperImpl}} dynamically.
If you for example want to use that service to relax the policy being introduced with SLING-3854 (to e.g. allow all users as service users) you may register your own service just returning {{true}} for all users in the only method {{isValid}}. Unfortunately you don't know when your {{ServiceUserValidator}} service is bound (due to the dynamic restart behaviour of services). Therefore other services cannot rely on the fact that your own {{ServiceUserValidator}} is being available at a certain point in time and therefore their call to {{ResourceResolverFactory.getServiceResolver(...)}} may fail, if they rely on a non-System JCR user. Therefore this mechanism is not suitable to disable the enforcing of JCR system users.

Instead I would propose the following:
# allow to configure the {{JcrSystemUserValidator}} via an OSGi property named {{allowOnlySystemUsers}} which by default should be {{true}}.
# within the method {{JcrSystemUserValidator.isValidUser}} you either allow all users or leave the current logic in place (in case {{allowOnlySystemUsers}} is {{true}}).

Only that way it would be possible to reliably allow all users as service users which is especially helpful during development of a certain feature (although this is probably not a config you would set on a production instance).
",", "
"   Rename Method,","Allow to enable the usage of regular JCR users for service resolvers With SLING-3854 a {{ServiceUserValidator}} interface was introduced. Basically all OSGi services implementing that interface may decide whether certain users can be used as backing user for a call to {{ResourceResolverFactory.getServiceResolver(...)}}. The only implementation of that in Sling is {{JcrSystemUserValidator}} which only allows to use JCR system users.

The list of all those services is bound in the {{ServiceUserMapperImpl}} dynamically.
If you for example want to use that service to relax the policy being introduced with SLING-3854 (to e.g. allow all users as service users) you may register your own service just returning {{true}} for all users in the only method {{isValid}}. Unfortunately you don't know when your {{ServiceUserValidator}} service is bound (due to the dynamic restart behaviour of services). Therefore other services cannot rely on the fact that your own {{ServiceUserValidator}} is being available at a certain point in time and therefore their call to {{ResourceResolverFactory.getServiceResolver(...)}} may fail, if they rely on a non-System JCR user. Therefore this mechanism is not suitable to disable the enforcing of JCR system users.

Instead I would propose the following:
# allow to configure the {{JcrSystemUserValidator}} via an OSGi property named {{allowOnlySystemUsers}} which by default should be {{true}}.
# within the method {{JcrSystemUserValidator.isValidUser}} you either allow all users or leave the current logic in place (in case {{allowOnlySystemUsers}} is {{true}}).

Only that way it would be possible to reliably allow all users as service users which is especially helpful during development of a certain feature (although this is probably not a config you would set on a production instance).
",", "
"   Move Class,Move Method,Extract Method,Move Attribute,","Allow to set multiple validation error messages in one Validator Currently each {{Validator}} can only set one error message through the String return value (SLING-4010). It should be possible for one {{Validator.validate}} call to set multiple validation messages at once.
Maybe the solution [~radu.cotescu] proposed in https://issues.apache.org/jira/browse/SLING-4027?focusedCommentId=14219170&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14219170 can be used here to just pass the ValidationResult around.","Duplicated Code, Long Method, , , , "
"   Extract Method,Move Attribute,","Allow to set multiple validation error messages in one Validator Currently each {{Validator}} can only set one error message through the String return value (SLING-4010). It should be possible for one {{Validator.validate}} call to set multiple validation messages at once.
Maybe the solution [~radu.cotescu] proposed in https://issues.apache.org/jira/browse/SLING-4027?focusedCommentId=14219170&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14219170 can be used here to just pass the ValidationResult around.","Duplicated Code, Long Method, , , "
"   Move Class,Move Method,Extract Method,Move Attribute,","Allow to set multiple validation error messages in one Validator Currently each {{Validator}} can only set one error message through the String return value (SLING-4010). It should be possible for one {{Validator.validate}} call to set multiple validation messages at once.
Maybe the solution [~radu.cotescu] proposed in https://issues.apache.org/jira/browse/SLING-4027?focusedCommentId=14219170&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14219170 can be used here to just pass the ValidationResult around.","Duplicated Code, Long Method, , , , "
"   Rename Method,","Simplified server-side tests with TeleporterRule I've been working on a prototype at [1] that makes it much easier to create server-side tests, using a JUnit Rule to ""teleport"" them to the server.

_edit: removed obsolete description of {{ServerSideTestRule}}_

This can be useful along with something like https://github.com/fizzed/maven-plugins to quickly update the bundle under development when it's modified.

[1] https://svn.apache.org/repos/asf/sling/whiteboard/bdelacretaz/test-rules",", "
"   Rename Method,","Simplified server-side tests with TeleporterRule I've been working on a prototype at [1] that makes it much easier to create server-side tests, using a JUnit Rule to ""teleport"" them to the server.

_edit: removed obsolete description of {{ServerSideTestRule}}_

This can be useful along with something like https://github.com/fizzed/maven-plugins to quickly update the bundle under development when it's modified.

[1] https://svn.apache.org/repos/asf/sling/whiteboard/bdelacretaz/test-rules",", "
"   Rename Method,Extract Method,","Clarify the order of the iterator given by RankedServices Currently the iterator in {{RankedServices}} returns the services in the ascending order from lowest to highest ranking. This is a different order then being used by OSGi internally [1]. Usually you are only interested in an iterator which gives you the service with the highest ranking first (because that would also be the one being returned by {{BundleContext.getServiceReference(...)}}).

If we don't change the iterator logic we should at least make it clearer in the Javadoc that the iterator really gives the services in ascending order of their service ranking property and we should probably add a more useful iterator which gives back the services with the highest ranking first.

The issue came up in the context of SLING-5035.

[1] - https://osgi.org/javadoc/r4v42/org/osgi/framework/BundleContext.html#getServiceReference(java.lang.String)","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Clarify the order of the iterator given by RankedServices Currently the iterator in {{RankedServices}} returns the services in the ascending order from lowest to highest ranking. This is a different order then being used by OSGi internally [1]. Usually you are only interested in an iterator which gives you the service with the highest ranking first (because that would also be the one being returned by {{BundleContext.getServiceReference(...)}}).

If we don't change the iterator logic we should at least make it clearer in the Javadoc that the iterator really gives the services in ascending order of their service ranking property and we should probably add a more useful iterator which gives back the services with the highest ranking first.

The issue came up in the context of SLING-5035.

[1] - https://osgi.org/javadoc/r4v42/org/osgi/framework/BundleContext.html#getServiceReference(java.lang.String)","Duplicated Code, Long Method, , "
"   Move Class,Move And Rename Class,","RepositoryPinger makes log unusable in debug mode If debug logging is used, every two seconds a lot of debug messages are added by the repository pinger. This makes the log nearly unusable as it grows way too fast.",", "
"   Move Class,Move And Rename Class,","RepositoryPinger makes log unusable in debug mode If debug logging is used, every two seconds a lot of debug messages are added by the repository pinger. This makes the log nearly unusable as it grows way too fast.",", "
"   Move Class,Rename Method,Move Method,Move Attribute,","sling-mock: Register JCR node types for OSGi bundles in class path for the resource resolver types ""JCR_JACKRABBIT"" and ""JCR_OAK"" node types are required to store data with certain nodetypes or query for it.

if in the classpath OSGi bundles are present which define node type definitions in their {{Sling-Nodetypes}} bundle header they should be picked up and registered automatically in the repository.

because the correct order of multiple bundles registering node types is not known it should try to re-register it multiple times (up to 5 times) to get the right order ""by chance"".",", , , "
"   Move Class,Rename Method,Move Method,Move Attribute,","sling-mock: Register JCR node types for OSGi bundles in class path for the resource resolver types ""JCR_JACKRABBIT"" and ""JCR_OAK"" node types are required to store data with certain nodetypes or query for it.

if in the classpath OSGi bundles are present which define node type definitions in their {{Sling-Nodetypes}} bundle header they should be picked up and registered automatically in the repository.

because the correct order of multiple bundles registering node types is not known it should try to re-register it multiple times (up to 5 times) to get the right order ""by chance"".",", , , "
"   Move Class,Move Method,Move Attribute,","Introduce ConsistencyService to discovery.commons As described [in SLING-4627|https://issues.apache.org/jira/browse/SLING-4627?focusedCommentId=14532334&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14532334] doing any kind of discovery implementation on top of an eventually consistent repository requires synchronization with this very repository in order to ensure all events of an old incarnation of a view are processed before a new incarnation of a view is announced to everybody. Thus a cluster-wide synchronization is required.

Such a synchronization can be achieved fairly straight-forward by storing a well-defined 'sync token' into a location which is known to everybody else in the cluster - and having everybody wait for seeing this token before continuing.",", , , "
"   Move Class,Move Method,Move Attribute,","Introduce ConsistencyService to discovery.commons As described [in SLING-4627|https://issues.apache.org/jira/browse/SLING-4627?focusedCommentId=14532334&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14532334] doing any kind of discovery implementation on top of an eventually consistent repository requires synchronization with this very repository in order to ensure all events of an old incarnation of a view are processed before a new incarnation of a view is announced to everybody. Thus a cluster-wide synchronization is required.

Such a synchronization can be achieved fairly straight-forward by storing a well-defined 'sync token' into a location which is known to everybody else in the cluster - and having everybody wait for seeing this token before continuing.",", , , "
"   Rename Method,",[HApi] Add java HApi microdata client The hapi tools extension could make use of a java HTML and microdata client to consume html markup annotated using HApi.,", "
"   Rename Class,Rename Method,",[HApi] Add java HApi microdata client The hapi tools extension could make use of a java HTML and microdata client to consume html markup annotated using HApi.,", "
"   Rename Method,",[HApi] Add java HApi microdata client The hapi tools extension could make use of a java HTML and microdata client to consume html markup annotated using HApi.,", "
"   Move Class,Move Method,Extract Method,","Make dependency handling code more reusable If you want to build other maven plugins dealing with the provisioning model, it's currently a little bit hard to reuse the dependency lifecycle handler and extend it's functionality.","Duplicated Code, Long Method, , , "
"   Move Class,Move Method,Extract Method,","Make dependency handling code more reusable If you want to build other maven plugins dealing with the provisioning model, it's currently a little bit hard to reuse the dependency lifecycle handler and extend it's functionality.","Duplicated Code, Long Method, , , "
"   Rename Method,",Optimise last modified information retrieval in UnitChangeMonitor The {{UnitChangeMonitor}}'s mechanism for storing the last modified date for Sightly compiled classes can be optimised to provide a better cache creation.,", "
"   Rename Method,",Optimise last modified information retrieval in UnitChangeMonitor The {{UnitChangeMonitor}}'s mechanism for storing the last modified date for Sightly compiled classes can be optimised to provide a better cache creation.,", "
"   Move Method,Extract Method,","Refactor merging of models into separate utility class and add merge options We should move the merge methods from the ModelUtility to a MergeUtility and provide a MergeOptions class which allows to specify:
- if the remove run mode should be handled
- if the latest artifact (default) or the artifact with the highest version wins","Duplicated Code, Long Method, , , "
"   Move Method,Extract Method,","Refactor merging of models into separate utility class and add merge options We should move the merge methods from the ModelUtility to a MergeUtility and provide a MergeOptions class which allows to specify:
- if the remove run mode should be handled
- if the latest artifact (default) or the artifact with the highest version wins","Duplicated Code, Long Method, , , "
"   Rename Method,","Add Path and PathSet There is often a need to check whether a resource path is equal or a sub path of a given path. We have a lot of different solutions throughout our code base.
Therefore I think we should provide some basic functionality directly in the API for reuse.",", "
"   Rename Method,Inline Method,","Allow more fine grained logging configuration Currently Sling logging only allows for a global log file (file or console) with a global log level setting. For application development and issue tracking it is required to be able to have more fine grained control over the configuration:

* multiple log files
* log levels per category",", , "
"   Rename Method,Extract Method,","maven-sling-plugin: Allow installing bundles through SlingPostServlet mechanisms Add an additional route of installing bundles in maven-sling-plugin through the SlingPostServlet mechanisms.

Currently, there are only two methodologies supported for installing (and uninstall) bundles:
# Through POSTing to the Felix Web Console
# Through PUTing to WebDAV

By adding the ability to install through SlingPostServlet, bundles can be deployed into the resource tree (JCR) when WebDAV is not installed.

For maintaining full-backwards compatibility, the proposition here is the addition of another boolean flag to the Mojo named {{useSlingPost}} that defaults to {{false}}.
Altering this new flag to be the value {{true}} will trigger the use of the new methodology.
","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","maven-sling-plugin: Allow installing bundles through SlingPostServlet mechanisms Add an additional route of installing bundles in maven-sling-plugin through the SlingPostServlet mechanisms.

Currently, there are only two methodologies supported for installing (and uninstall) bundles:
# Through POSTing to the Felix Web Console
# Through PUTing to WebDAV

By adding the ability to install through SlingPostServlet, bundles can be deployed into the resource tree (JCR) when WebDAV is not installed.

For maintaining full-backwards compatibility, the proposition here is the addition of another boolean flag to the Mojo named {{useSlingPost}} that defaults to {{false}}.
Altering this new flag to be the value {{true}} will trigger the use of the new methodology.
","Duplicated Code, Long Method, , "
"   Rename Method,","sling-mock: Support remote add, remote host and remote port in MockSlingHttpSevletRequest 0",", "
"   Rename Method,","Meaningful thread names As discussed on the mailing list\[0], it would be good to assign meaningful names to threads managed by Sling's thread pools.

\[0] http://markmail.org/thread/ltq4fdyo5himfwo3",", "
"   Rename Method,","Meaningful thread names As discussed on the mailing list\[0], it would be good to assign meaningful names to threads managed by Sling's thread pools.

\[0] http://markmail.org/thread/ltq4fdyo5himfwo3",", "
"   Rename Method,","Meaningful thread names As discussed on the mailing list\[0], it would be good to assign meaningful names to threads managed by Sling's thread pools.

\[0] http://markmail.org/thread/ltq4fdyo5himfwo3",", "
"   Rename Method,","Meaningful thread names As discussed on the mailing list\[0], it would be good to assign meaningful names to threads managed by Sling's thread pools.

\[0] http://markmail.org/thread/ltq4fdyo5himfwo3",", "
"   Rename Method,","Create service users and ACLs from the provisioning model As discussed in the ""Removing loginAdministrative, how to test that, and service username conventions"" thread on our dev list [1] we need to be able to create service users and set the corresponding ACLs from our provisioning model.

This should be implemented using distinct utility classes, one for the users and one for the ACLs, that take simple mini-languages as input. This will allow for reusing these utilities in test code for example.

[1] http://markmail.org/message/kcvuhwfdald2dyuz

*Edit: high-level requirements*

As discussed in the ""SLING-5355 - configs vs. content for ACLs and service users"" thread - http://markmail.org/message/tzno2via2wjckhuc

* HR1 - Create service users and set their ACLs as defined in the Sling instance's provisioning model.
* HR2 - Create initial paths like /var/discovery, so that ACLs can be set on them.
* HR3 - Make the full text of the ACL definitions available at runtime for auditing purposes (see Michael Marth's Dec.17 comment in SLING-5355). Also useful for upgrades where merging with conflict detection is needed.

",", "
"   Rename Method,","Create service users and ACLs from the provisioning model As discussed in the ""Removing loginAdministrative, how to test that, and service username conventions"" thread on our dev list [1] we need to be able to create service users and set the corresponding ACLs from our provisioning model.

This should be implemented using distinct utility classes, one for the users and one for the ACLs, that take simple mini-languages as input. This will allow for reusing these utilities in test code for example.

[1] http://markmail.org/message/kcvuhwfdald2dyuz

*Edit: high-level requirements*

As discussed in the ""SLING-5355 - configs vs. content for ACLs and service users"" thread - http://markmail.org/message/tzno2via2wjckhuc

* HR1 - Create service users and set their ACLs as defined in the Sling instance's provisioning model.
* HR2 - Create initial paths like /var/discovery, so that ACLs can be set on them.
* HR3 - Make the full text of the ACL definitions available at runtime for auditing purposes (see Michael Marth's Dec.17 comment in SLING-5355). Also useful for upgrades where merging with conflict detection is needed.

",", "
"   Rename Class,Rename Method,Extract Method,","Create service users and ACLs from the provisioning model As discussed in the ""Removing loginAdministrative, how to test that, and service username conventions"" thread on our dev list [1] we need to be able to create service users and set the corresponding ACLs from our provisioning model.

This should be implemented using distinct utility classes, one for the users and one for the ACLs, that take simple mini-languages as input. This will allow for reusing these utilities in test code for example.

[1] http://markmail.org/message/kcvuhwfdald2dyuz

*Edit: high-level requirements*

As discussed in the ""SLING-5355 - configs vs. content for ACLs and service users"" thread - http://markmail.org/message/tzno2via2wjckhuc

* HR1 - Create service users and set their ACLs as defined in the Sling instance's provisioning model.
* HR2 - Create initial paths like /var/discovery, so that ACLs can be set on them.
* HR3 - Make the full text of the ACL definitions available at runtime for auditing purposes (see Michael Marth's Dec.17 comment in SLING-5355). Also useful for upgrades where merging with conflict detection is needed.

","Duplicated Code, Long Method, , "
"   Rename Method,","Create service users and ACLs from the provisioning model As discussed in the ""Removing loginAdministrative, how to test that, and service username conventions"" thread on our dev list [1] we need to be able to create service users and set the corresponding ACLs from our provisioning model.

This should be implemented using distinct utility classes, one for the users and one for the ACLs, that take simple mini-languages as input. This will allow for reusing these utilities in test code for example.

[1] http://markmail.org/message/kcvuhwfdald2dyuz

*Edit: high-level requirements*

As discussed in the ""SLING-5355 - configs vs. content for ACLs and service users"" thread - http://markmail.org/message/tzno2via2wjckhuc

* HR1 - Create service users and set their ACLs as defined in the Sling instance's provisioning model.
* HR2 - Create initial paths like /var/discovery, so that ACLs can be set on them.
* HR3 - Make the full text of the ACL definitions available at runtime for auditing purposes (see Michael Marth's Dec.17 comment in SLING-5355). Also useful for upgrades where merging with conflict detection is needed.

",", "
"   Rename Class,Move Method,","Create service users and ACLs from the provisioning model As discussed in the ""Removing loginAdministrative, how to test that, and service username conventions"" thread on our dev list [1] we need to be able to create service users and set the corresponding ACLs from our provisioning model.

This should be implemented using distinct utility classes, one for the users and one for the ACLs, that take simple mini-languages as input. This will allow for reusing these utilities in test code for example.

[1] http://markmail.org/message/kcvuhwfdald2dyuz

*Edit: high-level requirements*

As discussed in the ""SLING-5355 - configs vs. content for ACLs and service users"" thread - http://markmail.org/message/tzno2via2wjckhuc

* HR1 - Create service users and set their ACLs as defined in the Sling instance's provisioning model.
* HR2 - Create initial paths like /var/discovery, so that ACLs can be set on them.
* HR3 - Make the full text of the ACL definitions available at runtime for auditing purposes (see Michael Marth's Dec.17 comment in SLING-5355). Also useful for upgrades where merging with conflict detection is needed.

",", , "
"   Rename Method,","Create service users and ACLs from the provisioning model As discussed in the ""Removing loginAdministrative, how to test that, and service username conventions"" thread on our dev list [1] we need to be able to create service users and set the corresponding ACLs from our provisioning model.

This should be implemented using distinct utility classes, one for the users and one for the ACLs, that take simple mini-languages as input. This will allow for reusing these utilities in test code for example.

[1] http://markmail.org/message/kcvuhwfdald2dyuz

*Edit: high-level requirements*

As discussed in the ""SLING-5355 - configs vs. content for ACLs and service users"" thread - http://markmail.org/message/tzno2via2wjckhuc

* HR1 - Create service users and set their ACLs as defined in the Sling instance's provisioning model.
* HR2 - Create initial paths like /var/discovery, so that ACLs can be set on them.
* HR3 - Make the full text of the ACL definitions available at runtime for auditing purposes (see Michael Marth's Dec.17 comment in SLING-5355). Also useful for upgrades where merging with conflict detection is needed.

",", "
"   Rename Class,Pull Up Method,Pull Up Attribute,","Create service users and ACLs from the provisioning model As discussed in the ""Removing loginAdministrative, how to test that, and service username conventions"" thread on our dev list [1] we need to be able to create service users and set the corresponding ACLs from our provisioning model.

This should be implemented using distinct utility classes, one for the users and one for the ACLs, that take simple mini-languages as input. This will allow for reusing these utilities in test code for example.

[1] http://markmail.org/message/kcvuhwfdald2dyuz

*Edit: high-level requirements*

As discussed in the ""SLING-5355 - configs vs. content for ACLs and service users"" thread - http://markmail.org/message/tzno2via2wjckhuc

* HR1 - Create service users and set their ACLs as defined in the Sling instance's provisioning model.
* HR2 - Create initial paths like /var/discovery, so that ACLs can be set on them.
* HR3 - Make the full text of the ACL definitions available at runtime for auditing purposes (see Michael Marth's Dec.17 comment in SLING-5355). Also useful for upgrades where merging with conflict detection is needed.

",", Duplicated Code, Duplicated Code, "
"   Rename Class,Rename Method,Extract Method,","Create service users and ACLs from the provisioning model As discussed in the ""Removing loginAdministrative, how to test that, and service username conventions"" thread on our dev list [1] we need to be able to create service users and set the corresponding ACLs from our provisioning model.

This should be implemented using distinct utility classes, one for the users and one for the ACLs, that take simple mini-languages as input. This will allow for reusing these utilities in test code for example.

[1] http://markmail.org/message/kcvuhwfdald2dyuz

*Edit: high-level requirements*

As discussed in the ""SLING-5355 - configs vs. content for ACLs and service users"" thread - http://markmail.org/message/tzno2via2wjckhuc

* HR1 - Create service users and set their ACLs as defined in the Sling instance's provisioning model.
* HR2 - Create initial paths like /var/discovery, so that ACLs can be set on them.
* HR3 - Make the full text of the ACL definitions available at runtime for auditing purposes (see Michael Marth's Dec.17 comment in SLING-5355). Also useful for upgrades where merging with conflict detection is needed.

","Duplicated Code, Long Method, , "
"   Rename Method,","New ResourceBuilder module - fluent API to create content structures As discussed recently on our dev list the {{ContentBuilder}} currently provided by the Sling Mocks library [1] can be very useful in testing. _(edit: while implementing I have diverged from that idea and created a new, more powerful {{ResourceBuilder}} API, for now that {{ContentBuilder}} stays unchanged)_.

In order to make it usable for both client-side and server-side testing (as well as in server-side code) I'm planning to

* Extract it into its own module
* Define an API that allows for creating nodes and properties, importing JSON and other files via the Sling ContentLoader and providing a simple a way to cleanup the test content
* Implement (first) a server-side version of that API
* Implement (as a second priority) a client-side version that can be used in test run via HTTP

This shouldn't affect the existing Sling Mocks library users, except maybe forcing them to rebuild their tests to use the new API.

[1] https://sling.apache.org/documentation/development/sling-mock.html#building-content",", "
"   Extract Method,Move Attribute,","New ResourceBuilder module - fluent API to create content structures As discussed recently on our dev list the {{ContentBuilder}} currently provided by the Sling Mocks library [1] can be very useful in testing. _(edit: while implementing I have diverged from that idea and created a new, more powerful {{ResourceBuilder}} API, for now that {{ContentBuilder}} stays unchanged)_.

In order to make it usable for both client-side and server-side testing (as well as in server-side code) I'm planning to

* Extract it into its own module
* Define an API that allows for creating nodes and properties, importing JSON and other files via the Sling ContentLoader and providing a simple a way to cleanup the test content
* Implement (first) a server-side version of that API
* Implement (as a second priority) a client-side version that can be used in test run via HTTP

This shouldn't affect the existing Sling Mocks library users, except maybe forcing them to rebuild their tests to use the new API.

[1] https://sling.apache.org/documentation/development/sling-mock.html#building-content","Duplicated Code, Long Method, , , "
"   Extract Method,Move Attribute,","New ResourceBuilder module - fluent API to create content structures As discussed recently on our dev list the {{ContentBuilder}} currently provided by the Sling Mocks library [1] can be very useful in testing. _(edit: while implementing I have diverged from that idea and created a new, more powerful {{ResourceBuilder}} API, for now that {{ContentBuilder}} stays unchanged)_.

In order to make it usable for both client-side and server-side testing (as well as in server-side code) I'm planning to

* Extract it into its own module
* Define an API that allows for creating nodes and properties, importing JSON and other files via the Sling ContentLoader and providing a simple a way to cleanup the test content
* Implement (first) a server-side version of that API
* Implement (as a second priority) a client-side version that can be used in test run via HTTP

This shouldn't affect the existing Sling Mocks library users, except maybe forcing them to rebuild their tests to use the new API.

[1] https://sling.apache.org/documentation/development/sling-mock.html#building-content","Duplicated Code, Long Method, , , "
"   Rename Method,","New ResourceBuilder module - fluent API to create content structures As discussed recently on our dev list the {{ContentBuilder}} currently provided by the Sling Mocks library [1] can be very useful in testing. _(edit: while implementing I have diverged from that idea and created a new, more powerful {{ResourceBuilder}} API, for now that {{ContentBuilder}} stays unchanged)_.

In order to make it usable for both client-side and server-side testing (as well as in server-side code) I'm planning to

* Extract it into its own module
* Define an API that allows for creating nodes and properties, importing JSON and other files via the Sling ContentLoader and providing a simple a way to cleanup the test content
* Implement (first) a server-side version of that API
* Implement (as a second priority) a client-side version that can be used in test run via HTTP

This shouldn't affect the existing Sling Mocks library users, except maybe forcing them to rebuild their tests to use the new API.

[1] https://sling.apache.org/documentation/development/sling-mock.html#building-content",", "
"   Rename Method,","New ResourceBuilder module - fluent API to create content structures As discussed recently on our dev list the {{ContentBuilder}} currently provided by the Sling Mocks library [1] can be very useful in testing. _(edit: while implementing I have diverged from that idea and created a new, more powerful {{ResourceBuilder}} API, for now that {{ContentBuilder}} stays unchanged)_.

In order to make it usable for both client-side and server-side testing (as well as in server-side code) I'm planning to

* Extract it into its own module
* Define an API that allows for creating nodes and properties, importing JSON and other files via the Sling ContentLoader and providing a simple a way to cleanup the test content
* Implement (first) a server-side version of that API
* Implement (as a second priority) a client-side version that can be used in test run via HTTP

This shouldn't affect the existing Sling Mocks library users, except maybe forcing them to rebuild their tests to use the new API.

[1] https://sling.apache.org/documentation/development/sling-mock.html#building-content",", "
"   Rename Method,","Support renaming of bundles via Sling Provisioning Model Because the Sling OSGi Installer only allows a single OSGi bundle with a given BSN, it is sometimes necessary to rename a bundle's BSN to enable it to be installed more than once. 

To make this renaming simple and do it on the fly, we can extend the slingstart-maven-plugin to do this renaming automatically, with a configuration like this:
{code}org.foo.bar/blah/1.2.3 [rename-bsn=com.adobe.foo.bar.blah]{code}

One note, in case there are multiple model files that all reference the same artifact. For example, with a base model as above. 

Model A inherits from Base model and has:
{code}org.foo.bar/blah/1.2.3{code}
without the rename.

In this case the rename still happens as the attributes are inherited.",", "
"   Move Method,Move Attribute,","Support renaming of bundles via Sling Provisioning Model Because the Sling OSGi Installer only allows a single OSGi bundle with a given BSN, it is sometimes necessary to rename a bundle's BSN to enable it to be installed more than once. 

To make this renaming simple and do it on the fly, we can extend the slingstart-maven-plugin to do this renaming automatically, with a configuration like this:
{code}org.foo.bar/blah/1.2.3 [rename-bsn=com.adobe.foo.bar.blah]{code}

One note, in case there are multiple model files that all reference the same artifact. For example, with a base model as above. 

Model A inherits from Base model and has:
{code}org.foo.bar/blah/1.2.3{code}
without the rename.

In this case the rename still happens as the attributes are inherited.",", , , "
"   Move Method,Move Attribute,","Support renaming of bundles via Sling Provisioning Model Because the Sling OSGi Installer only allows a single OSGi bundle with a given BSN, it is sometimes necessary to rename a bundle's BSN to enable it to be installed more than once. 

To make this renaming simple and do it on the fly, we can extend the slingstart-maven-plugin to do this renaming automatically, with a configuration like this:
{code}org.foo.bar/blah/1.2.3 [rename-bsn=com.adobe.foo.bar.blah]{code}

One note, in case there are multiple model files that all reference the same artifact. For example, with a base model as above. 

Model A inherits from Base model and has:
{code}org.foo.bar/blah/1.2.3{code}
without the rename.

In this case the rename still happens as the attributes are inherited.",", , , "
"   Rename Method,","Support renaming of bundles via Sling Provisioning Model Because the Sling OSGi Installer only allows a single OSGi bundle with a given BSN, it is sometimes necessary to rename a bundle's BSN to enable it to be installed more than once. 

To make this renaming simple and do it on the fly, we can extend the slingstart-maven-plugin to do this renaming automatically, with a configuration like this:
{code}org.foo.bar/blah/1.2.3 [rename-bsn=com.adobe.foo.bar.blah]{code}

One note, in case there are multiple model files that all reference the same artifact. For example, with a base model as above. 

Model A inherits from Base model and has:
{code}org.foo.bar/blah/1.2.3{code}
without the rename.

In this case the rename still happens as the attributes are inherited.",", "
"   Rename Class,Rename Method,","Support renaming of bundles via Sling Provisioning Model Because the Sling OSGi Installer only allows a single OSGi bundle with a given BSN, it is sometimes necessary to rename a bundle's BSN to enable it to be installed more than once. 

To make this renaming simple and do it on the fly, we can extend the slingstart-maven-plugin to do this renaming automatically, with a configuration like this:
{code}org.foo.bar/blah/1.2.3 [rename-bsn=com.adobe.foo.bar.blah]{code}

One note, in case there are multiple model files that all reference the same artifact. For example, with a base model as above. 

Model A inherits from Base model and has:
{code}org.foo.bar/blah/1.2.3{code}
without the rename.

In this case the rename still happens as the attributes are inherited.",", "
"   Rename Method,","Provide support for running singleton jobs on non leader cluster nodes also With SLING-2979 support for running singleton jobs on specific instance was provided. In most cases we want to run a job as singleton and not want to ""pin"" it to specific nodes. For this {{scheduler.runOn}} needs to be set to {{SINGLE}}.

However per [current implementation|https://github.com/apache/sling/blob/org.apache.sling.commons.scheduler-2.4.14/src/main/java/org/apache/sling/commons/scheduler/impl/QuartzJobExecutor.java#L64] {{SINGLE}} is treated as {{LEADER}}. This effectively causes *all singleton* jobs to get executed on leader only thus putting extra load.

For better utilization of cluster resources it should be possible to distribute such singleton jobs on other cluster nodes and still ensure that singleton contract is honoured!

We would like to make use of this feature to ensure Oak AsyncIndexTask to run on different cluster nodes (OAK-2749)",", "
"   Rename Method,","Provide support for running singleton jobs on non leader cluster nodes also With SLING-2979 support for running singleton jobs on specific instance was provided. In most cases we want to run a job as singleton and not want to ""pin"" it to specific nodes. For this {{scheduler.runOn}} needs to be set to {{SINGLE}}.

However per [current implementation|https://github.com/apache/sling/blob/org.apache.sling.commons.scheduler-2.4.14/src/main/java/org/apache/sling/commons/scheduler/impl/QuartzJobExecutor.java#L64] {{SINGLE}} is treated as {{LEADER}}. This effectively causes *all singleton* jobs to get executed on leader only thus putting extra load.

For better utilization of cluster resources it should be possible to distribute such singleton jobs on other cluster nodes and still ensure that singleton contract is honoured!

We would like to make use of this feature to ensure Oak AsyncIndexTask to run on different cluster nodes (OAK-2749)",", "
"   Rename Class,Inline Method,",Enable FindBugs for the Sightly Scripting Engine FindBugs should run as part of the normal build process for the Sightly Scripting Engine.,", , "
"   Move Method,Move Attribute,","Add helper class to construct valid paths When working with paths it's easy to combine two valid paths into an invalid one. For instance

* '/parent/' + '/child' -> '/parent//child'
* '/parent' + 'child' -> '/parentchild'

We should add a simple helper class which allows to build paths, with a fluent syntax.",", , , "
"   Move And Rename Class,","Implement SlingRepositoryInitializer plug-ins to setup the repository I'd like to implement a {{SlingRepositoryInitializer}} extension point for use cases like setting up service users and ACLs and creating the ""base tree"" of content as described in SLING-5449. 

This can also be very useful to handle content migrations or other cleanup operations in upgrades.

The scenario is that before registering the {{SlingRepository}} service, all active {{SlingRepositoryInitializer}} services are called in order of their service ranking, passing them the upcoming {{SlingRepository}} service so that they can act on it. Any exception thrown in this processing causes the {{SlingRepository}} service registration to be canceled.

The {{SlingRepositoryInitializer}} javadocs must stress that those services need to take clustered scenarios into account, and if necessary implement locking mechanisms to avoid stepping on each other's toes.",", "
"   Rename Method,Extract Method,","Add possibility to extract archives into the repository Currently the content from a bundle is put as-is into the repository.
We could add a new feature where the user can specify an archive (zip or jar as a start) together with a path in the manifest. The archive will then be extracted into the repository at the given path.","Duplicated Code, Long Method, , "
"   Move Method,Extract Method,Inline Method,Move Attribute,","Add possibility to extract archives into the repository Currently the content from a bundle is put as-is into the repository.
We could add a new feature where the user can specify an archive (zip or jar as a start) together with a path in the manifest. The archive will then be extracted into the repository at the given path.","Duplicated Code, Long Method, , , , , "
"   Move Method,Extract Method,Inline Method,Move Attribute,","Add possibility to extract archives into the repository Currently the content from a bundle is put as-is into the repository.
We could add a new feature where the user can specify an archive (zip or jar as a start) together with a path in the manifest. The archive will then be extracted into the repository at the given path.","Duplicated Code, Long Method, , , , , "
"   Rename Method,Pull Up Method,Pull Up Attribute,","Create new Sightly file wizards With SLING-4076 fixed, it would be very convenient to also have some new file wizards:

- new Sightly (HTML) file
- new Sightly JS Use-Script
- new Sightly Java Use-Script",", Duplicated Code, Duplicated Code, "
"   Rename Method,Pull Up Method,Pull Up Attribute,","Create new Sightly file wizards With SLING-4076 fixed, it would be very convenient to also have some new file wizards:

- new Sightly (HTML) file
- new Sightly JS Use-Script
- new Sightly Java Use-Script",", Duplicated Code, Duplicated Code, "
"   Rename Method,","Update to Sling API 2.11 and dependencies Sling API 2.11 and the related new implementations of resourceresolver and jcr.resource introduce deep changes to the resource provider interface and the internal registration processes of the JCR resource provider, making it incompatible with sling-mock 1.x.

thus we introduce a new version sling-mock 2.x which is compatible with the latest Sling API and the relevant dependencies.

the old sling-mock 1.x version is still available at https://svn.apache.org/repos/asf/sling/branches/testing/mocks/sling-mock-1.x",", "
"   Rename Method,","Update to Sling API 2.11 and dependencies Sling API 2.11 and the related new implementations of resourceresolver and jcr.resource introduce deep changes to the resource provider interface and the internal registration processes of the JCR resource provider, making it incompatible with sling-mock 1.x.

thus we introduce a new version sling-mock 2.x which is compatible with the latest Sling API and the relevant dependencies.

the old sling-mock 1.x version is still available at https://svn.apache.org/repos/asf/sling/branches/testing/mocks/sling-mock-1.x",", "
"   Rename Class,Rename Method,Extract Method,","Improve WebDAV support for installing a bundle The WebDAV support for installing a bundle ({{usePut=true}}) should be improved in several regards:
# only WebDAV methods should be used (even for creating intermediate folders and for checking if a folder is already there)
# it should be able to deal with the redirect being issued by the Sling WebDAV Plugin (see SLING-5557)

Compare also with http://apache-sling.73963.n3.nabble.com/maven-sling-plugin-and-usePut-with-intermediate-folders-td4059733.html.","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,Extract Method,","Improve WebDAV support for installing a bundle The WebDAV support for installing a bundle ({{usePut=true}}) should be improved in several regards:
# only WebDAV methods should be used (even for creating intermediate folders and for checking if a folder is already there)
# it should be able to deal with the redirect being issued by the Sling WebDAV Plugin (see SLING-5557)

Compare also with http://apache-sling.73963.n3.nabble.com/maven-sling-plugin-and-usePut-with-intermediate-folders-td4059733.html.","Duplicated Code, Long Method, , "
"   Move Class,Move And Rename Class,","Move Health Checks core integration tests to the core module A number of integration tests found in the ""it"" module depend on the core bundle only. Moving them to the core module will make it easier to measure the aggregate coverage of the unit and integration tests.",", "
"   Rename Class,Rename Method,Move Method,Move Attribute,","Distribution package implementation should be independent of serialization Currently we have different implementations of {{DistributionPackage}} interface in _org.apache.sling.distribution.core_ and _org.apache.sling.distribution.extensions_, some are based on plain files, some on FileVault packages (backed by files or JCR content) however it'd be good that packages would be agnostic to serialization type, simply based on {{Resources}}.",", , , "
"   Rename Method,",Use plural for method ValidationModelProvider#getModel(...):Collection<ValidationModel> 0,", "
"   Rename Method,",Use plural for method ValidationModelProvider#getModel(...):Collection<ValidationModel> 0,", "
"   Move Method,Move Attribute,","Provide a Jobs API and implementation suitable for widely distributed job processing. This issue is to track work on a proof of concept to create a Jobs API and implementation that will work in a distributed environment where the job submitters and job consumers are not necessarily in the same JVM or in the same Sling cluster. 

Work is being done in a branch at https://github.com/ieb/sling/tree/jobs_28/contrib/extensions/jobs


Since the implementation needs supporting APIs/Capabilities not already present in Sling. There are some sub-tasks associated with this issue to address those.",", , , "
"   Move Class,Move Method,Move Attribute,",Make ScriptEngine factory configurable 0,", , , "
"   Rename Method,","Allow to configure resource resolver mapping Currently the {{ResourceResolverFactoryActivator}} being registered in https://github.com/apache/sling/blob/trunk/testing/mocks/sling-mock/src/main/java/org/apache/sling/testing/mock/sling/ResourceResolverFactoryInitializer.java#L123 is not configured at all.
Since resource resolver mapping is a topic which is often implemented in the wrong way, it should be possible to setup some mapping for SlingContext to allow to test in a more realistic way.",", "
"   Rename Method,Move Method,Inline Method,","Allow distribution config to be stored in content Using the distribution endpoint {{/libs/sling/distribution/settings/agents/agentName}} one should be able to also edit replication properties.
{noformat}
replication.triggerOnModification=true
{noformat}",", , , "
"   Rename Method,Move Method,Inline Method,","Allow distribution config to be stored in content Using the distribution endpoint {{/libs/sling/distribution/settings/agents/agentName}} one should be able to also edit replication properties.
{noformat}
replication.triggerOnModification=true
{noformat}",", , , "
"   Move And Rename Class,Move Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Modularise the Sightly script engine The Sightly script engine should be broken into three modules:

# a Sightly frontend compiler that interprets Sightly scripts and produces an Abstract Syntax Tree (AST)
# a Sightly Java backend compiler that interprets the AST and produces Java class files
# a Compilable Script Engine that reuses the previous two modules","Duplicated Code, Long Method, , , , , "
"   Rename Method,Move Method,Extract Method,",Expose DistributionContentSerializer Expose {{DistributionContentSerializer}} API from _org.apache.sling.distribution.core_ in order to allow implementation of custom serialization formats (e.g. Avro and Kryo defined in _org.apache.sling.distribution.extensions_).,"Duplicated Code, Long Method, , , "
"   Move Class,Move Method,","Support different thread pools for scheduled tasks Right now the scheduler uses a single thread pool. While this thread pool can be configured, it means that all scheduled tasks share this pool. In order to prioratize different tasks over others and avoid blocking important jobs through unimportant once, we could maybe add a configuration property to select a thread pool name.
If a pool with that name exists, it's used - if not the default is used.",", , "
"   Extract Method,Inline Method,","Support different thread pools for scheduled tasks Right now the scheduler uses a single thread pool. While this thread pool can be configured, it means that all scheduled tasks share this pool. In order to prioratize different tasks over others and avoid blocking important jobs through unimportant once, we could maybe add a configuration property to select a thread pool name.
If a pool with that name exists, it's used - if not the default is used.","Duplicated Code, Long Method, , , "
"   Extract Method,Inline Method,","Support different thread pools for scheduled tasks Right now the scheduler uses a single thread pool. While this thread pool can be configured, it means that all scheduled tasks share this pool. In order to prioratize different tasks over others and avoid blocking important jobs through unimportant once, we could maybe add a configuration property to select a thread pool name.
If a pool with that name exists, it's used - if not the default is used.","Duplicated Code, Long Method, , , "
"   Rename Class,Extract Superclass,","Register JCR nodetypes from the provisioning model Now that we can create repository paths from the provisioning model [1] we need to be able to register nodetypes before that, in case nodes need to be created with non-default nodetypes.

This should use a mechanism similar to the {{org.apache.sling.repoinit.jcr.RepositoryInitializer}} to read the provisioning model, using an URL and scheme that can be configured to point to another data source, in case Sling is not started via a provisioning model.

This needs to be documented at http://sling.apache.org/documentation/bundles/repository-initialization.html

[1] http://sling.apache.org/documentation/bundles/repository-initialization.html",", Duplicated Code, Large Class, "
"   Move Class,Move Method,Extract Method,Move Attribute,","DefaultGetServlet obtains input stream for binary even if request is a HEAD As per current implementation any HEAD request will be handled by defaultHeadServlet which majorly does two changes

1.) coverts response output stream to be null so that there should be no message body in response
2.) coverts HEAD request to GET request. 
Now this request is dispatched and served by defaultGetServlet.

With this approach, we get the desired output but response is delayed as it reads the complete binary data of a resource. and also it increases data transfer which is not needed.

So IMO this approach should be improved.

thanks,
","Duplicated Code, Long Method, , , , "
"   Move Class,Move Method,Extract Method,Move Attribute,","DefaultGetServlet obtains input stream for binary even if request is a HEAD As per current implementation any HEAD request will be handled by defaultHeadServlet which majorly does two changes

1.) coverts response output stream to be null so that there should be no message body in response
2.) coverts HEAD request to GET request. 
Now this request is dispatched and served by defaultGetServlet.

With this approach, we get the desired output but response is delayed as it reads the complete binary data of a resource. and also it increases data transfer which is not needed.

So IMO this approach should be improved.

thanks,
","Duplicated Code, Long Method, , , , "
"   Rename Method,","Sling Context-Aware Configuration - Initial Contribution as discussed in the mailing list (see [my post from april 2016|http://apache-sling.73963.n3.nabble.com/RT-Use-cases-for-content-specific-configurations-in-Sling-amp-Contribution-td4060813.html]) i want to contribute the wcm.io Configuration parts that are not AEM-specific.

the current features of wcm.io Configuration are described here: http://wcm.io/config/

the main goal is to support ""context-specific"" configuration, that means configuration that is different for different content paths (e.g. sites, tenants).

during the contribution some changes and refactorings are required/planned, e.g.:
* remove some dependencies to wcm.io build environment, Guava and others
* remove the ""application"" distinction currently part of wcm.io Configuration in favor or a more path-based distinction
* refactor the user-faced configuration API to further simplify it and support OSGi R6-style annotation classed for typed configuration access

_Update: as discussed at http://sling.markmail.org/thread/ka3ewlswfgjy7rpu the name of this new module is Context-Aware Configuration_
",", "
"   Rename Method,","Sling Context-Aware Configuration - Initial Contribution as discussed in the mailing list (see [my post from april 2016|http://apache-sling.73963.n3.nabble.com/RT-Use-cases-for-content-specific-configurations-in-Sling-amp-Contribution-td4060813.html]) i want to contribute the wcm.io Configuration parts that are not AEM-specific.

the current features of wcm.io Configuration are described here: http://wcm.io/config/

the main goal is to support ""context-specific"" configuration, that means configuration that is different for different content paths (e.g. sites, tenants).

during the contribution some changes and refactorings are required/planned, e.g.:
* remove some dependencies to wcm.io build environment, Guava and others
* remove the ""application"" distinction currently part of wcm.io Configuration in favor or a more path-based distinction
* refactor the user-faced configuration API to further simplify it and support OSGi R6-style annotation classed for typed configuration access

_Update: as discussed at http://sling.markmail.org/thread/ka3ewlswfgjy7rpu the name of this new module is Context-Aware Configuration_
",", "
"   Rename Method,","Sling Context-Aware Configuration - Initial Contribution as discussed in the mailing list (see [my post from april 2016|http://apache-sling.73963.n3.nabble.com/RT-Use-cases-for-content-specific-configurations-in-Sling-amp-Contribution-td4060813.html]) i want to contribute the wcm.io Configuration parts that are not AEM-specific.

the current features of wcm.io Configuration are described here: http://wcm.io/config/

the main goal is to support ""context-specific"" configuration, that means configuration that is different for different content paths (e.g. sites, tenants).

during the contribution some changes and refactorings are required/planned, e.g.:
* remove some dependencies to wcm.io build environment, Guava and others
* remove the ""application"" distinction currently part of wcm.io Configuration in favor or a more path-based distinction
* refactor the user-faced configuration API to further simplify it and support OSGi R6-style annotation classed for typed configuration access

_Update: as discussed at http://sling.markmail.org/thread/ka3ewlswfgjy7rpu the name of this new module is Context-Aware Configuration_
",", "
"   Rename Method,","Sling Context-Aware Configuration - Initial Contribution as discussed in the mailing list (see [my post from april 2016|http://apache-sling.73963.n3.nabble.com/RT-Use-cases-for-content-specific-configurations-in-Sling-amp-Contribution-td4060813.html]) i want to contribute the wcm.io Configuration parts that are not AEM-specific.

the current features of wcm.io Configuration are described here: http://wcm.io/config/

the main goal is to support ""context-specific"" configuration, that means configuration that is different for different content paths (e.g. sites, tenants).

during the contribution some changes and refactorings are required/planned, e.g.:
* remove some dependencies to wcm.io build environment, Guava and others
* remove the ""application"" distinction currently part of wcm.io Configuration in favor or a more path-based distinction
* refactor the user-faced configuration API to further simplify it and support OSGi R6-style annotation classed for typed configuration access

_Update: as discussed at http://sling.markmail.org/thread/ka3ewlswfgjy7rpu the name of this new module is Context-Aware Configuration_
",", "
"   Rename Method,","Provide a default Launchpad Oak Tar configuration as Option for Pax Exam * {{org.apache.felix.http}}: {{org.osgi.service.http.port}}
* {{org.apache.jackrabbit.oak.plugins.segment.SegmentNodeStoreService}}: {{repository.home}}, {{name}}
* {{org.apache.jackrabbit.oak.plugins.index.lucene.LuceneIndexProviderService}}: {{localIndexDir}}",", "
"   Move Class,Rename Method,Move Method,Extract Method,Pull Up Attribute,Move Attribute,","Move Web Console Plugin to separate bundle The log implementation currently contains a web console plugin which uses a dynamic package import. Once the log impl is wired to a bundle providing the servlet api and that bundle is updated/removed, it causes the log implementation to refresh, which usually means a complete restart of the system (as every bunde uses logging).
We should reduce this coupling and remove any dependency to the servlet api from the logging implementation","Duplicated Code, Long Method, , , , Duplicated Code, "
"   Rename Method,Extract Method,Pull Up Attribute,Move Attribute,","Move Web Console Plugin to separate bundle The log implementation currently contains a web console plugin which uses a dynamic package import. Once the log impl is wired to a bundle providing the servlet api and that bundle is updated/removed, it causes the log implementation to refresh, which usually means a complete restart of the system (as every bunde uses logging).
We should reduce this coupling and remove any dependency to the servlet api from the logging implementation","Duplicated Code, Long Method, , , Duplicated Code, "
"   Rename Method,","Add pre and post processing hooks to the sling post servlet Currently during a post, a SlingPostOperation is selected and then executed. The post operation does ""everything"", performing the changes and saving them.

I think we could reintroduce a Changes interface with several implementations like Add, Remove, Copy etc. This interface has all knowledge of what to change.

The SlingPostOperation would then return a list of changes:

interface SlingPostOperation {
List <Change> prepare(SlingHttpServletRequest);
}

So the first step when a post comes in, is still to select the post operation, but then this operation just generates a list of changes without changing anything in the repository.

We then introduce pre and post processor interfaces (these are no final names yet:)
interface PreProcessor {
void process(SlingHttpServletRequest, List<Change>);
}

interface PostProcessor {
void process(SlingHttpServletRequest, List<Change>);
}

There can be several pre and post processor registered, a property of a processor is used to order them and guarantee an ordered execution.
A pre processor can alter the list of changes.

When all pre processors are run, the changes are applied to the repository by Sling, then all post processors are executed.
Finally all changes are saved.
",", "
"   Move And Rename Class,",Refactor out SimpleDistributionAgent inner classes {{SimpleDistributionAgent}} has 2 inner classes that were created time ago and that grew up in importance and complexity over time; it'd be good to refactor those classes out for reading and testing code more comfortably.,", "
"   Move Method,Move Attribute,",Refactor out SimpleDistributionAgent inner classes {{SimpleDistributionAgent}} has 2 inner classes that were created time ago and that grew up in importance and complexity over time; it'd be good to refactor those classes out for reading and testing code more comfortably.,", , , "
"   Rename Method,",[SCD] support deep property filters 0,", "
"   Rename Method,",[SCD] support deep property filters 0,", "
"   Move Method,Move Attribute,","Improve auth requirement whiteboard implementation The current auth requirement whiteboard implementation could be improved: if a service registration is modified currently, all existing registrations are removed and then all new registrations are added - which can result in a lot of churn going on. The implementation can be improved to make a diff between the old and the new array and only apply the diff.",", , , "
"   Rename Class,Rename Method,Extract Method,","Context-Aware Config: Support adapting configuration resources carsten introduced in rev. 1758332 a new feature:
the ConfigurationBuilder#as method should also support adapting to any other object for which a sling adapter manager exits, not only to configuration annotation classes.

this is currently not reflected in the API, and unit tests are missing.","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,Extract Method,","Context-Aware Config: Support adapting configuration resources carsten introduced in rev. 1758332 a new feature:
the ConfigurationBuilder#as method should also support adapting to any other object for which a sling adapter manager exits, not only to configuration annotation classes.

this is currently not reflected in the API, and unit tests are missing.","Duplicated Code, Long Method, , "
"   Move Class,Rename Class,Move And Rename Class,Move Method,Extract Method,Move Attribute,","Context-Aware Config: Add pluggable context paths strategies by default the context paths for which configurations can be linked are defined by the existence of a {{sling:config}} property on that node.

we want to keep this as default behavior, but this is getting cumbersome if a ""massive multi-tenant scenario"" is used with hundreds of sites grouped by regions etc. that follow up a consistent repository scheme. in this case it would be easier to not to be forced to create a sling:config property on each site root and link it to the correct configuration (and move it when the site is moved), but to provide an own strategy implementation how these context paths are detected.

new strategies can be registered as OSGi services, service ranking controls which is asked first. via a service property it should be possible to register a special strategy only for a subpath e.g. /content/tenant1 it should apply to.","Duplicated Code, Long Method, , , , "
"   Move Class,Rename Class,Move And Rename Class,Move Method,Extract Method,Move Attribute,","Context-Aware Config: Add pluggable context paths strategies by default the context paths for which configurations can be linked are defined by the existence of a {{sling:config}} property on that node.

we want to keep this as default behavior, but this is getting cumbersome if a ""massive multi-tenant scenario"" is used with hundreds of sites grouped by regions etc. that follow up a consistent repository scheme. in this case it would be easier to not to be forced to create a sling:config property on each site root and link it to the correct configuration (and move it when the site is moved), but to provide an own strategy implementation how these context paths are detected.

new strategies can be registered as OSGi services, service ranking controls which is asked first. via a service property it should be possible to register a special strategy only for a subpath e.g. /content/tenant1 it should apply to.","Duplicated Code, Long Method, , , , "
"   Rename Method,","Context-Aware Config: Introduce ""bucket name"" parameter in ConfigurationResourceResolver follow-up from discussion: http://apache-sling.73963.n3.nabble.com/context-aware-config-why-sling-configs-node-tt4064393.html

we want to introduce a ""bucket-name"" parameter in ConfigurationResourceResolver interface to make it explicit each high-level configuration-like resolver should define one.",", "
"   Move Method,Extract Method,Move Attribute,","Context-Aware Config: Provide configuration parameter metadata in order to support configuration editors GUIs we need to provide metadata which configurations with parameter metadata are defined by the applications.

this means:
* list of all configurations registered (singleton, collections, nested) with
** their respective configuration names
** label (optional)
** description (optional)
* list of all parameters for each configuration
* parameter metadata:
** name
** type (only supported: String,int,long,double,boolean and arrays of them)
** label (optional)
** description (optional)
** default value
** further custom properties that may customized the configuration editor (e.g. widget type to use, optional)

the applications needs a possibility to provide such configuration+parameter metadata. by default the annotation interface classes are used for this. they have to be detected on the runtime in the classpath when a new bundle is deployed using an osgi extender pattern (quite similar to sling models). to the annotation classes further annotations can be applied an class and property level to provide the additional metadata (label, description etc.).

currently we can only support automatic detection of parameter metadata for configurations which are defined and accessed with annotation classes, not when the application used direct valuemap access or the low-level ConfigurationResourceResolver.

by making the configuration metadata provider pluggable via an SPI we can ship the default configuration providing metadata detected from the deployed annotation classes, but leave a door open to add other sources as well.","Duplicated Code, Long Method, , , , "
"   Rename Method,","Context-Aware Config: Adapt defaulting strategy as discusesd in mail thread http://apache-sling.73963.n3.nabble.com/contextaware-config-Default-configuration-and-naming-td4064399.html we should change:
* default folder name to /conf
* rename the {{sling:config}} property name to {{sling:config-ref}}
* and change the way default configuration is detected by following the example below

for a content structure like this
{noformat}
/content
/tenant1 - context path /content/tenant1
/region1 - context path /content/tenant1/region1
/site1 - context path /content/tenant1/region1/site1
/page1
{noformat}

the configuration is looked up at this paths (in this order)
{noformat}
/conf/tenant1/region1/site1
/conf/tenant1/region1
/conf/tenant1
/conf/global
/apps/conf
/libs/conf
{noformat}",", "
"   Rename Method,","Context-Aware Config: Adapt defaulting strategy as discusesd in mail thread http://apache-sling.73963.n3.nabble.com/contextaware-config-Default-configuration-and-naming-td4064399.html we should change:
* default folder name to /conf
* rename the {{sling:config}} property name to {{sling:config-ref}}
* and change the way default configuration is detected by following the example below

for a content structure like this
{noformat}
/content
/tenant1 - context path /content/tenant1
/region1 - context path /content/tenant1/region1
/site1 - context path /content/tenant1/region1/site1
/page1
{noformat}

the configuration is looked up at this paths (in this order)
{noformat}
/conf/tenant1/region1/site1
/conf/tenant1/region1
/conf/tenant1
/conf/global
/apps/conf
/libs/conf
{noformat}",", "
"   Move Method,Move Attribute,","ResourceBuilder: Split up ResourceBuilder and ResourceBuilderFactory i do not understand why we combine the use cases
* resource builder creation
* resource builder usage

in one single ResourceBuilder interface, with two implementations each only supporting the one part or another, throwing exception when the other methods are used.

it think it would be quite cleaner and easier to understand to create a separate ResourceBuilderFactory interface.",", , , "
"   Move Method,Move Attribute,","ResourceBuilder: Split up ResourceBuilder and ResourceBuilderFactory i do not understand why we combine the use cases
* resource builder creation
* resource builder usage

in one single ResourceBuilder interface, with two implementations each only supporting the one part or another, throwing exception when the other methods are used.

it think it would be quite cleaner and easier to understand to create a separate ResourceBuilderFactory interface.",", , , "
"   Rename Method,","sling-mock ContentBuilder: Support creating resources with object vararg parameter similar to ResourceBuilder sling-mock's ContentBuilder should support creating resources with an object vararg array specifying the properrties additionally to supporting a Map<String,Object>.",", "
"   Rename Method,","sling-mock ContentBuilder: Support creating resources with object vararg parameter similar to ResourceBuilder sling-mock's ContentBuilder should support creating resources with an object vararg array specifying the properrties additionally to supporting a Map<String,Object>.",", "
"   Rename Method,Move Method,",osgi-mock: Support passing map/dictionary properties with object vararg parameter on various osgi mock methods map or dictionary objects can be passed in e.g. as service properties. we should also offer a variant with passing in key/value paris as object vararg parameter.,", , "
"   Rename Method,","Context-Aware Config: Property Inheritance/Merging currently the context-aware config implementation supports resource inheritance, but not property inheritance, that means no properties gets merged in the resource inheritance chain.

there was a long discussion on the mailing list about this topics with arguments to support this, and other not to support this
http://apache-sling.73963.n3.nabble.com/Context-Aware-Configs-Merging-tt4063382.html

the goal of this ticket is to support it, but make it configurable so it can be switched on and off.",", "
"   Rename Method,","Context-Aware Config: Make resource inheritance for configuration collections configurable currently and automatic merging/combining of configuration resource items takes place. example:
{noformat}
/conf/site1/feature/a
/conf/site1/feature/c
/conf/global/feature/b
/libs/conf/feature/c
{noformat}

this returns a,b,c when config resource collection for ""feature"" is requested. c is from /conf/site1/feature/c and not from /libs/conf/feature/c.

this is inconsistent to the support for properties (where currently no such ""merging"" is supported), and can have undesired effects.

furthermore it is problematic when saving configuration collections via the ConfigurationManager interface (SLING-6026). when storing a set of config resources for /conf/site1 they are stored as children of /conf/site1/feature. but when reading them again they are automatically merged with the others from the fallback paths, and the user has no possibility to prevent this.

so we should either disable this merging on paths, or implement it correctly with giving the user control when merging should take place or not (confmgr has a special property for this).",", "
"   Rename Method,","Context-Aware Config: Make resource inheritance for configuration collections configurable currently and automatic merging/combining of configuration resource items takes place. example:
{noformat}
/conf/site1/feature/a
/conf/site1/feature/c
/conf/global/feature/b
/libs/conf/feature/c
{noformat}

this returns a,b,c when config resource collection for ""feature"" is requested. c is from /conf/site1/feature/c and not from /libs/conf/feature/c.

this is inconsistent to the support for properties (where currently no such ""merging"" is supported), and can have undesired effects.

furthermore it is problematic when saving configuration collections via the ConfigurationManager interface (SLING-6026). when storing a set of config resources for /conf/site1 they are stored as children of /conf/site1/feature. but when reading them again they are automatically merged with the others from the fallback paths, and the user has no possibility to prevent this.

so we should either disable this merging on paths, or implement it correctly with giving the user control when merging should take place or not (confmgr has a special property for this).",", "
"   Rename Method,","Context-Aware Config: Make resource inheritance for configuration collections configurable currently and automatic merging/combining of configuration resource items takes place. example:
{noformat}
/conf/site1/feature/a
/conf/site1/feature/c
/conf/global/feature/b
/libs/conf/feature/c
{noformat}

this returns a,b,c when config resource collection for ""feature"" is requested. c is from /conf/site1/feature/c and not from /libs/conf/feature/c.

this is inconsistent to the support for properties (where currently no such ""merging"" is supported), and can have undesired effects.

furthermore it is problematic when saving configuration collections via the ConfigurationManager interface (SLING-6026). when storing a set of config resources for /conf/site1 they are stored as children of /conf/site1/feature. but when reading them again they are automatically merged with the others from the fallback paths, and the user has no possibility to prevent this.

so we should either disable this merging on paths, or implement it correctly with giving the user control when merging should take place or not (confmgr has a special property for this).",", "
"   Rename Method,","Context-Aware Config: Make resource inheritance for configuration collections configurable currently and automatic merging/combining of configuration resource items takes place. example:
{noformat}
/conf/site1/feature/a
/conf/site1/feature/c
/conf/global/feature/b
/libs/conf/feature/c
{noformat}

this returns a,b,c when config resource collection for ""feature"" is requested. c is from /conf/site1/feature/c and not from /libs/conf/feature/c.

this is inconsistent to the support for properties (where currently no such ""merging"" is supported), and can have undesired effects.

furthermore it is problematic when saving configuration collections via the ConfigurationManager interface (SLING-6026). when storing a set of config resources for /conf/site1 they are stored as children of /conf/site1/feature. but when reading them again they are automatically merged with the others from the fallback paths, and the user has no possibility to prevent this.

so we should either disable this merging on paths, or implement it correctly with giving the user control when merging should take place or not (confmgr has a special property for this).",", "
"   Move Method,Move Attribute,","Context-Aware Config: Configuration property override providers add support similar to http://wcm.io/config/core/override-providers.html

this is esp. useful for test/QA systems where a bunch of content and configuration packages are imported from the production system, and only a few of them need to be overwritten e.g. ip addresses, host names.

these override providers should not be active by default. perhaps they should go into a separate package - but we need the SPI for them in the core implementation.

this is somewhat related to SLING-6058

information about overriding that is in place should be provided in the Configuration Management API (ConfigurationManager) as well.",", , , "
"   Rename Class,Rename Method,Pull Up Method,Pull Up Attribute,","Make Sling IDE independent of m2e-tycho Currently Sling IDE requires the installation of m2e-tycho. This was being added in https://issues.apache.org/jira/browse/SLING-3608. Now that the maven-bundle-plugin ships with m2e support OOTB (FELIX-4009) we should get rid of that dependency.

This is also important since newer versions of the maven-bundle-plugin conflict with that extension (https://issues.apache.org/jira/browse/FELIX-4009?focusedCommentId=15192263&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15192263).

See also the discussion at http://www.mail-archive.com/dev@sling.apache.org/msg60112.html.",", Duplicated Code, Duplicated Code, "
"   Rename Method,","Hamcrest: Simplify ResourceMatchers method signatures the resource matching methods of ResourceMatchers should be simplified (before doing the 1.0.0 release):
* eliminate ""resource"" prefix from method names, because resource is already in the ""ResourceMatchers"" class names, and the child methods do not contain this prefix as well
* allow to specify property maps either as map, or as object vararg array (similar to resource builder and sling mocks)

existing sling projects using this snapshot release will be updated as well.",", "
"   Rename Class,Rename Method,","Hamcrest: Simplify ResourceMatchers method signatures the resource matching methods of ResourceMatchers should be simplified (before doing the 1.0.0 release):
* eliminate ""resource"" prefix from method names, because resource is already in the ""ResourceMatchers"" class names, and the child methods do not contain this prefix as well
* allow to specify property maps either as map, or as object vararg array (similar to resource builder and sling mocks)

existing sling projects using this snapshot release will be updated as well.",", "
"   Rename Method,","Hamcrest: Simplify ResourceMatchers method signatures the resource matching methods of ResourceMatchers should be simplified (before doing the 1.0.0 release):
* eliminate ""resource"" prefix from method names, because resource is already in the ""ResourceMatchers"" class names, and the child methods do not contain this prefix as well
* allow to specify property maps either as map, or as object vararg array (similar to resource builder and sling mocks)

existing sling projects using this snapshot release will be updated as well.",", "
"   Rename Method,","Hamcrest: Simplify ResourceMatchers method signatures the resource matching methods of ResourceMatchers should be simplified (before doing the 1.0.0 release):
* eliminate ""resource"" prefix from method names, because resource is already in the ""ResourceMatchers"" class names, and the child methods do not contain this prefix as well
* allow to specify property maps either as map, or as object vararg array (similar to resource builder and sling mocks)

existing sling projects using this snapshot release will be updated as well.",", "
"   Rename Class,Rename Method,","Hamcrest: Simplify ResourceMatchers method signatures the resource matching methods of ResourceMatchers should be simplified (before doing the 1.0.0 release):
* eliminate ""resource"" prefix from method names, because resource is already in the ""ResourceMatchers"" class names, and the child methods do not contain this prefix as well
* allow to specify property maps either as map, or as object vararg array (similar to resource builder and sling mocks)

existing sling projects using this snapshot release will be updated as well.",", "
"   Rename Method,","Ability to specify TTL for separate health check Currently there is no ability to specify TTL for separate health check.

Ex.: in my case ""hc"" validating against repository about 3-5 minutes therefore I couldn't specify TTL globally to not impact on other ""hc"" results. This ""hc"" I couldn't execute by scheduler to prevent CPU from high loading, also results for this check remains relevant for an 1-3 hours.

Therefore if it make sense not only for me I've added ""[pull request|https://github.com/apache/sling/pull/180]"" for this functionality:
* If property ""hc.ttl"" specified within HC - it will be used as TTL for result, otherwise cache will use default TTL value
",", "
"   Rename Method,","Context-Aware Config: Config reference should be detected in ContextPathStrategy currently the DefaultContextPathStrategy checks for the availability of the sling:config-ref prop, but it does not return it's value, this is done in the DefaultConfigurationResourceResolvingStrategy.

this is as bit inconsistent, leading to configure the lookup of SLING-6149 in multiple places, and makes it more difficult to define scenarios where the config references is build on a conventions-based pattern (e.g. derived from content path) instead of an explicit sling:config-ref property.

the logic itself and other implementation details of both strategies remain untouched.",", "
"   Rename Method,","Context-Aware Config: Config reference should be detected in ContextPathStrategy currently the DefaultContextPathStrategy checks for the availability of the sling:config-ref prop, but it does not return it's value, this is done in the DefaultConfigurationResourceResolvingStrategy.

this is as bit inconsistent, leading to configure the lookup of SLING-6149 in multiple places, and makes it more difficult to define scenarios where the config references is build on a conventions-based pattern (e.g. derived from content path) instead of an explicit sling:config-ref property.

the logic itself and other implementation details of both strategies remain untouched.",", "
"   Rename Method,","Improve MapEntries implementation Looking at the MapEntries implementation it does the same thing in some cases several times during handling change events.
We should optimize this and also verify that everything is handled by tests. The current unit tests check more single methods but not the whole functionality",", "
"   Extract Interface,Extract Method,","Improve MapEntries implementation Looking at the MapEntries implementation it does the same thing in some cases several times during handling change events.
We should optimize this and also verify that everything is handled by tests. The current unit tests check more single methods but not the whole functionality","Duplicated Code, Long Method, , Large Class, "
"   Extract Interface,Extract Method,","Improve MapEntries implementation Looking at the MapEntries implementation it does the same thing in some cases several times during handling change events.
We should optimize this and also verify that everything is handled by tests. The current unit tests check more single methods but not the whole functionality","Duplicated Code, Long Method, , Large Class, "
"   Move Class,Move Method,Move Attribute,","Context-Aware Config: Change java package name to o.a.s.caconfig change the java package names from {{org.apache.sling.contextaware.config}} to {{org.apache.sling.caconfig}}.

see discussion https://lists.apache.org/thread.html/844a6e56c5b5020106d145edc7fd9faa721642b2c905987c81a1b548@%3Cdev.sling.apache.org%3E",", , , "
"   Move Class,Rename Method,","Expose a service for Sling Scripting that provides request-scoped Resource Resolvers for scripting dependencies A new Sling Scripting service ({{ScriptingResourceResolverProvider}}) should be implemented in order to provide access to request-based {{ResourceResolvers}} for solving script dependencies.

The following method should be available:

{noformat}
/**
* Provides a request-scoped {@link ResourceResolver} with only read access to the search paths. This resolver should be used for script 
* resolution in the context of the same request rendering process. The {@code ResourceResolver} should not be closed by consumers (calling
* {@link ResourceResolver#close} doesn't do anything), since this service will handle the closing operation automatically. The 
* {@code ResourceResolver} will be shared between scripting dependencies that render parts of the response for the same request.
*/
ResourceResolver getRequestScopedResourceResolver()
{noformat}

[sling-dev email thread|https://lists.apache.org/thread.html/db2a78249baf2d6234a4549a5aff8b5474256add9829f86ac78d1c56@%3Cdev.sling.apache.org%3E]",", "
"   Rename Method,","Expose a service for Sling Scripting that provides request-scoped Resource Resolvers for scripting dependencies A new Sling Scripting service ({{ScriptingResourceResolverProvider}}) should be implemented in order to provide access to request-based {{ResourceResolvers}} for solving script dependencies.

The following method should be available:

{noformat}
/**
* Provides a request-scoped {@link ResourceResolver} with only read access to the search paths. This resolver should be used for script 
* resolution in the context of the same request rendering process. The {@code ResourceResolver} should not be closed by consumers (calling
* {@link ResourceResolver#close} doesn't do anything), since this service will handle the closing operation automatically. The 
* {@code ResourceResolver} will be shared between scripting dependencies that render parts of the response for the same request.
*/
ResourceResolver getRequestScopedResourceResolver()
{noformat}

[sling-dev email thread|https://lists.apache.org/thread.html/db2a78249baf2d6234a4549a5aff8b5474256add9829f86ac78d1c56@%3Cdev.sling.apache.org%3E]",", "
"   Rename Method,","Support relative references in DefaultConfigurationResourceResolvingStrategy If the value from the sling:confRef property is relative, the parent
hierarchy of the context resource is traversed up to find a context
resource with an absolute property. Once found, these are concatenated.
For example
/content/a
+ sling:confRef=/conf/a
/content/a/b
+ sling:confRef=b

If you try to find configurations for /content/a/b it searches in
/conf/a/b first, then /conf/a",", "
"   Move Method,Move Attribute,","Implement LoginAdminWhitelist in JCR Base JCR Base should provide a default implementation {{AbstractSlingRepositoryManager#allowLoginAdministrativeForBundle(Bundle)}} with a configurable {{LoginAdminWhitelist}}. Implementation that want a different logic can then still choose to overwrite the method.

cc [~cziegeler], [~bdelacretaz]",", , , "
"   Move Class,Extract Method,","CAConfig: Separate SPI for Property Inheritance currently, the property merging support is part of the DefaultConfigurationResourceResolvingStrategy, together with the support for resource collection merging.

this conflicts with custom ConfigurationPersistenceStrategy which redirect the root resource e.g. to a {{jcr:content}} child node - property merging is broken then. by definition ConfigurationPersistenceStrategy does not has any knowledge of ConfigurationPersistenceStrategy's (they are on a ""higher level"" dedicated only to configurations).

so the solution is to create a separate SPI for property merging, and apply it only to ConfigurationResolver and ConfigurationManager, not for the generic ConfigurationResourceResolver.","Duplicated Code, Long Method, , "
"   Move Class,Extract Method,","CAConfig: Separate SPI for Property Inheritance currently, the property merging support is part of the DefaultConfigurationResourceResolvingStrategy, together with the support for resource collection merging.

this conflicts with custom ConfigurationPersistenceStrategy which redirect the root resource e.g. to a {{jcr:content}} child node - property merging is broken then. by definition ConfigurationPersistenceStrategy does not has any knowledge of ConfigurationPersistenceStrategy's (they are on a ""higher level"" dedicated only to configurations).

so the solution is to create a separate SPI for property merging, and apply it only to ConfigurationResolver and ConfigurationManager, not for the generic ConfigurationResourceResolver.","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,","Context-Aware Config: Web Console Configuration Printer alongside with SLING-6115 it would be nice to also have an inventory plugin, which lists all registered strategies/providers and their ranking, all defined override strings etc.",", "
"   Rename Class,Rename Method,","Context-Aware Config: Web Console Configuration Printer alongside with SLING-6115 it would be nice to also have an inventory plugin, which lists all registered strategies/providers and their ranking, all defined override strings etc.",", "
"   Rename Method,","JcrInstaller.counters should be accessed in a thread-safe manner The JcrInstaller.counter thread is a final long array. However, that does not ensure that changes performed are safely published. Since they are potentially accessed from multiple threads, including unit tests, this can lead to random-looking, hard-to-debug issues.",", "
"   Rename Method,","JcrInstaller.counters should be accessed in a thread-safe manner The JcrInstaller.counter thread is a final long array. However, that does not ensure that changes performed are safely published. Since they are potentially accessed from multiple threads, including unit tests, this can lead to random-looking, hard-to-debug issues.",", "
"   Move Method,Move Attribute,","Implement a ResourceResolverWrapper A {{ResourceResolverWrapper}} would help consumers who implement the {{ResourceResolver}} interface to not define restrictive import ranges for the {{org.apache.sling.api.resource}} API package.

The implementation should delegate all calls to the wrapped resource resolver, similar to the {{ResourceWrapper}}.",", , , "
"   Move Method,Move Attribute,","Implement a ResourceResolverWrapper A {{ResourceResolverWrapper}} would help consumers who implement the {{ResourceResolver}} interface to not define restrictive import ranges for the {{org.apache.sling.api.resource}} API package.

The implementation should delegate all calls to the wrapped resource resolver, similar to the {{ResourceWrapper}}.",", , , "
"   Rename Method,Extract Method,","Context-Aware Config: Properly support nested configuration classes in SPI and Mangement API currently nested configuration classes are supported in the configuration resolver, but are not properly supported in the AnnotationClassConfigurationMetadataProvider SPI implementation and the Management API implementation.","Duplicated Code, Long Method, , "
"   Rename Method,","Context-Aware Config: Properly support nested configuration classes in SPI and Mangement API currently nested configuration classes are supported in the configuration resolver, but are not properly supported in the AnnotationClassConfigurationMetadataProvider SPI implementation and the Management API implementation.",", "
"   Rename Method,Extract Method,","Context-Aware Config: Properly support nested configuration classes in SPI and Mangement API currently nested configuration classes are supported in the configuration resolver, but are not properly supported in the AnnotationClassConfigurationMetadataProvider SPI implementation and the Management API implementation.","Duplicated Code, Long Method, , "
"   Rename Method,","Context-Aware Config: Access to Inheritance Properties in Management API, SPI the configuration management API (and the related configuration persistence SPI) should provide access (read/write) to inheritance-related properties e.g. for controlling resource collection inheritance and resource property inheritance.

because both are depending on the resource resolving strategy the APIs should only route through these control properties without interpreting them themselves.",", "
"   Move Method,Move Attribute,","Context-Aware Config: Access to Inheritance Properties in Management API, SPI the configuration management API (and the related configuration persistence SPI) should provide access (read/write) to inheritance-related properties e.g. for controlling resource collection inheritance and resource property inheritance.

because both are depending on the resource resolving strategy the APIs should only route through these control properties without interpreting them themselves.",", , , "
"   Move Method,Move Attribute,","Context-Aware Config: Access to Inheritance Properties in Management API, SPI the configuration management API (and the related configuration persistence SPI) should provide access (read/write) to inheritance-related properties e.g. for controlling resource collection inheritance and resource property inheritance.

because both are depending on the resource resolving strategy the APIs should only route through these control properties without interpreting them themselves.",", , , "
"   Rename Method,Move Method,","Context-Aware Config: Access to Inheritance Properties in Management API, SPI the configuration management API (and the related configuration persistence SPI) should provide access (read/write) to inheritance-related properties e.g. for controlling resource collection inheritance and resource property inheritance.

because both are depending on the resource resolving strategy the APIs should only route through these control properties without interpreting them themselves.",", , "
"   Rename Class,Rename Method,","Context-Aware Config: Access to Inheritance Properties in Management API, SPI the configuration management API (and the related configuration persistence SPI) should provide access (read/write) to inheritance-related properties e.g. for controlling resource collection inheritance and resource property inheritance.

because both are depending on the resource resolving strategy the APIs should only route through these control properties without interpreting them themselves.",", "
"   Move And Rename Class,","Context-Aware Config: Access to Inheritance Properties in Management API, SPI the configuration management API (and the related configuration persistence SPI) should provide access (read/write) to inheritance-related properties e.g. for controlling resource collection inheritance and resource property inheritance.

because both are depending on the resource resolving strategy the APIs should only route through these control properties without interpreting them themselves.",", "
"   Rename Method,","osgi-mock, sling-mock: Add support for Context Plugins * osgi-mock should provide a OsgiContextBuilder as well
* the context builders should accept a list of context callbacks for each lifecycle phase
* introduce a generics ContextCallback which can be used with all mock variants

a better solution for all this is add support for ""context plugins"".",", "
"   Rename Method,","osgi-mock, sling-mock: Add support for Context Plugins * osgi-mock should provide a OsgiContextBuilder as well
* the context builders should accept a list of context callbacks for each lifecycle phase
* introduce a generics ContextCallback which can be used with all mock variants

a better solution for all this is add support for ""context plugins"".",", "
"   Rename Method,","osgi-mock, sling-mock: Add support for Context Plugins * osgi-mock should provide a OsgiContextBuilder as well
* the context builders should accept a list of context callbacks for each lifecycle phase
* introduce a generics ContextCallback which can be used with all mock variants

a better solution for all this is add support for ""context plugins"".",", "
"   Rename Method,","osgi-mock, sling-mock: Add support for Context Plugins * osgi-mock should provide a OsgiContextBuilder as well
* the context builders should accept a list of context callbacks for each lifecycle phase
* introduce a generics ContextCallback which can be used with all mock variants

a better solution for all this is add support for ""context plugins"".",", "
"   Rename Method,",Context-Aware Config: Sling Mock Context Plugin using the new mock context plugin feature from SLING-6359 we should provide a plugin for setting up context-aware configuration in the unit test environment.,", "
"   Rename Class,Move Class,Rename Method,Push Down Method,Extract Method,","sling-mock: Automatically register Sling Models from Classpath up to this version sling models in unit tests only worked when registered beforehand with the addModelsForPackage(...) method.

when running unit test with sling mocks existing bundle headers in MANIFEST files in the class path should be scanned for {{Sling-Model-Packages}} and {{Sling-Model-Classes}} entries, and those should be registered automatically.","Duplicated Code, Long Method, , , "
"   Rename Class,Move Class,Rename Method,Push Down Method,Extract Method,","sling-mock: Automatically register Sling Models from Classpath up to this version sling models in unit tests only worked when registered beforehand with the addModelsForPackage(...) method.

when running unit test with sling mocks existing bundle headers in MANIFEST files in the class path should be scanned for {{Sling-Model-Packages}} and {{Sling-Model-Classes}} entries, and those should be registered automatically.","Duplicated Code, Long Method, , , "
"   Rename Method,","Context-Aware Config: Delete Configurations via ConfigurationManager currently it is not possible to delete singleton configurations - only to clear all properties.
we should add a delete method to ConfigurationManager API, and to the related ConfigurationPersistenceStrategy SPI.",", "
"   Move And Rename Class,","Context-Aware Config: Delete Configurations via ConfigurationManager currently it is not possible to delete singleton configurations - only to clear all properties.
we should add a delete method to ConfigurationManager API, and to the related ConfigurationPersistenceStrategy SPI.",", "
"   Rename Method,","Context-Aware Config: Support default values in ConfigurationResolver for ValueMaps currently, default values as defined in the configuration annotation classes are only supported in ConfigurationResolver when accessing them via this Annotation class. when using ValueMap only the real data stored in the repository is returned.

in ConfigurationManager the default values are also supported in the ValueMaps, so we should support this in ConfigurationResolver as well for consistency, and to make them available e.g. in Sightly templates via SLING-6384.",", "
"   Rename Method,","Context-Aware Config: Support default values in ConfigurationResolver for ValueMaps currently, default values as defined in the configuration annotation classes are only supported in ConfigurationResolver when accessing them via this Annotation class. when using ValueMap only the real data stored in the repository is returned.

in ConfigurationManager the default values are also supported in the ValueMaps, so we should support this in ConfigurationResolver as well for consistency, and to make them available e.g. in Sightly templates via SLING-6384.",", "
"   Rename Method,",Implement support for date and number formatting for HTL Version 1.3 of the HTL Specification extends the functionality of the {{format}} option by providing support for also formatting dates and numbers - https://github.com/Adobe-Marketing-Cloud/htl-spec/blob/1.3/SPECIFICATION.md#122-format.,", "
"   Rename Method,",Implement support for date and number formatting for HTL Version 1.3 of the HTL Specification extends the functionality of the {{format}} option by providing support for also formatting dates and numbers - https://github.com/Adobe-Marketing-Cloud/htl-spec/blob/1.3/SPECIFICATION.md#122-format.,", "
"   Rename Method,",Implement support for date and number formatting for HTL Version 1.3 of the HTL Specification extends the functionality of the {{format}} option by providing support for also formatting dates and numbers - https://github.com/Adobe-Marketing-Cloud/htl-spec/blob/1.3/SPECIFICATION.md#122-format.,", "
"   Move And Rename Class,Rename Method,Move Method,","Enhance Conversion Rules for ValueMapDecorator background in this discussion in the [mailing list|https://lists.apache.org/thread.html/696deaf2f6b67d99ee969aad19e5b8b00d9c53c78e9ad75633e9e809@%3Cdev.sling.apache.org%3E] the conversion rules of the ValueMapDecorator should be unified with the conversion rules of the JCR ValueMap implement (as long as they are not too JCR specific).

the conversion rules that are currently supported are outlined here: https://cwiki.apache.org/confluence/x/OQkIB

with this ticket we want to add support for
* Convert any number type of (Long, Byte, Short, Integer, Double, Float, BigDecimal) -> to each of them or String and vice versa
* Convert any date type of (Calendar, Date) -> to each of them or String and vice versa
* and make the existing implementation a bit more efficient (avoid string parsing when possible)",", , "
"   Move And Rename Class,Move Method,Move Attribute,","Filesystem Resource Provider: Support ""mounting"" content resources from JSON files it would be nice if the filesystem resource provider does not only support serving binary files and folders, but also arbitrary resources stored in .json files in the folder hierarchy which are normally extracted when installing the bundle with sling-initial-content.

is is necessary to explicitly switch this feature on per configuration, because it may be desired to directly serve the JSON files as binary files in some cases.",", , , "
"   Rename Class,Rename Method,Move Method,Move Attribute,","Filesystem Resource Provider: Support ""mounting"" content resources from JSON files it would be nice if the filesystem resource provider does not only support serving binary files and folders, but also arbitrary resources stored in .json files in the folder hierarchy which are normally extracted when installing the bundle with sling-initial-content.

is is necessary to explicitly switch this feature on per configuration, because it may be desired to directly serve the JSON files as binary files in some cases.",", , , "
"   Rename Class,Rename Method,Move Method,Move Attribute,","Filesystem Resource Provider: Support ""mounting"" content resources from JSON files it would be nice if the filesystem resource provider does not only support serving binary files and folders, but also arbitrary resources stored in .json files in the folder hierarchy which are normally extracted when installing the bundle with sling-initial-content.

is is necessary to explicitly switch this feature on per configuration, because it may be desired to directly serve the JSON files as binary files in some cases.",", , , "
"   Rename Class,Extract Interface,","All context aware configuration multiplexers should be exposed outside of the bundle While caconfig's ""Service Provider Interfaces (SPI) [...] allows you to overlay, enhance or replace the default implementation and adapt it to your needs"", custom implementations (outside of caconfig bundles) need to access the multiplexers' API to reuse other pieces of the framework.
",", Large Class, "
"   Rename Class,Extract Interface,","All context aware configuration multiplexers should be exposed outside of the bundle While caconfig's ""Service Provider Interfaces (SPI) [...] allows you to overlay, enhance or replace the default implementation and adapt it to your needs"", custom implementations (outside of caconfig bundles) need to access the multiplexers' API to reuse other pieces of the framework.
",", Large Class, "
"   Move Class,Rename Method,Move Method,Extract Method,","Filesystem Resource Provider: Support ""mounting"" content resources from FileVault JCR XML files it would be nice if the filesystem resource provider does not only support serving binary files and folders, but also arbitrary resources stored in FileVault JCR XML files in the folder hierarchy which are normally packaged and uploaded.

is is necessary to explicitly switch this feature on per configuration, because it may be desired to directly serve the JSON and XML files as binary files in some cases.
","Duplicated Code, Long Method, , , "
"   Move And Rename Class,Rename Method,Move Method,Extract Method,","Filesystem Resource Provider: Support ""mounting"" content resources from FileVault JCR XML files it would be nice if the filesystem resource provider does not only support serving binary files and folders, but also arbitrary resources stored in FileVault JCR XML files in the folder hierarchy which are normally packaged and uploaded.

is is necessary to explicitly switch this feature on per configuration, because it may be desired to directly serve the JSON and XML files as binary files in some cases.
","Duplicated Code, Long Method, , , "
"   Rename Method,Pull Up Attribute,","slingstart-maven-plugin: keepLaunchpadRunning should be replaced by an option which can be used with start and/or stop Right now the parameter {{keepLaunchpadRunning}} blocks the start mojo until one of the servers is terminated. It is not that easy to explicitly terminate the server though because Maven does not propagate the {{Ctrl+C}} to the server. 

Usually it is much more useful to instead block stopping of the server (i.e. the stop mojo).
I would propose to deprecate {{keepLaunchpadRunning}} and instead introduce the following parameter (which may be used together with start and/or stop):
{{blockUntilKeyIsPressed}} which should leverage the {{Prompter}} component (http://stackoverflow.com/a/21977269/5155923) which forces the Mojo to wait until the user entered some value.",", Duplicated Code, "
"   Rename Method,Pull Up Attribute,","slingstart-maven-plugin: keepLaunchpadRunning should be replaced by an option which can be used with start and/or stop Right now the parameter {{keepLaunchpadRunning}} blocks the start mojo until one of the servers is terminated. It is not that easy to explicitly terminate the server though because Maven does not propagate the {{Ctrl+C}} to the server. 

Usually it is much more useful to instead block stopping of the server (i.e. the stop mojo).
I would propose to deprecate {{keepLaunchpadRunning}} and instead introduce the following parameter (which may be used together with start and/or stop):
{{blockUntilKeyIsPressed}} which should leverage the {{Prompter}} component (http://stackoverflow.com/a/21977269/5155923) which forces the Mojo to wait until the user entered some value.",", Duplicated Code, "
"   Move And Rename Class,",Merge it-http module into core module 0,", "
"   Move And Rename Class,",Merge it-http module into core module 0,", "
"   Move Class,Extract Method,","More granularly invalidate the cached ValidationModels Currently {{ValidationModelProviders}} are asked once for validation models for a specific resource type. The result is cached, which may be invalidated as a whole by the {{ValidationModelProvider}} as well. 
While this works, it always requires a full cache invalidation whenever a new model come into play. Consider the following use case:
# Only one model for resource type {{a}} is available with {{applicablePath}} = {{/content}}
# The model is retrieved from the {{ValidationModelProvider}} for a validation of a resource below {{/content/test/c}}
# A new model for resource type {{a}} becomes available with {{applicablePath}} = {{/content/test}}
That new model should take precedence because its applicable path is more specific. This would only work if between 2. and 3. the cache is fully invalidated.

The new model is actually never leveraged unless the full cache is invalidated in between.","Duplicated Code, Long Method, , "
"   Rename Method,","JCR Content Parser for different usecases around file system resource provider and sling mocks (see related tickets) we need to parse content structures from files in the file system, e.g. in JSON format or JCR XML format.

we should put this code in a new commons library so it can be reused from the different projects.",", "
"   Move Class,Move And Rename Class,","JCR Content Parser for different usecases around file system resource provider and sling mocks (see related tickets) we need to parse content structures from files in the file system, e.g. in JSON format or JCR XML format.

we should put this code in a new commons library so it can be reused from the different projects.",", "
"   Move Method,Move Attribute,","sling-mock: Use File System Content Parser for parsing JSON files the parsing of JSON files for content loading should be switched to the new ""File System Content Parser"".
we this we get also rid of the {{org.apache.sling.commons.json}} dependency which has license problems.",", , , "
"   Move Method,Move Attribute,","sling-mock: Use File System Content Parser for parsing JSON files the parsing of JSON files for content loading should be switched to the new ""File System Content Parser"".
we this we get also rid of the {{org.apache.sling.commons.json}} dependency which has license problems.",", , , "
"   Rename Method,Pull Up Attribute,","maven-sling-plugin: Add ""fsmount"" and ""fsunmount"" goals the feature to add osgi configurations for the file system resource provider is quite hidden with the optional ""mountByFs"" property from the install/uninstall goals.

we should additionally add two explicit goals ""fsmount"" and ""fsunmount"" allowing to add or remove these configs without installing/uninstalling the bundle at the same time.",", Duplicated Code, "
"   Pull Up Method,Move Method,Extract Method,Pull Up Attribute,Move Attribute,","maven-sling-plugin: Add ""fsmount"" and ""fsunmount"" goals the feature to add osgi configurations for the file system resource provider is quite hidden with the optional ""mountByFs"" property from the install/uninstall goals.

we should additionally add two explicit goals ""fsmount"" and ""fsunmount"" allowing to add or remove these configs without installing/uninstalling the bundle at the same time.","Duplicated Code, Long Method, , , , Duplicated Code, Duplicated Code, "
"   Pull Up Method,Move Method,Extract Method,Pull Up Attribute,Move Attribute,","maven-sling-plugin: Add ""fsmount"" and ""fsunmount"" goals the feature to add osgi configurations for the file system resource provider is quite hidden with the optional ""mountByFs"" property from the install/uninstall goals.

we should additionally add two explicit goals ""fsmount"" and ""fsunmount"" allowing to add or remove these configs without installing/uninstalling the bundle at the same time.","Duplicated Code, Long Method, , , , Duplicated Code, Duplicated Code, "
"   Rename Method,Extract Method,Pull Up Attribute,Move Attribute,","Add the possibility to execute sling pipes in a background thread For now the only way to execute a pipe through HTTP is to wait for servlet's response to end.
For very long pipes this is an issue (as well as for the fact all pending changes might impact memory), as well as it is error prone in case the same pipe is executed several times.

this ticket is for adding a new execution mode, that write the running status of the pipe configuration, and makes the pipe executed in a separate thread than the servlet thread.","Duplicated Code, Long Method, , , Duplicated Code, "
"   Rename Method,Extract Method,Pull Up Attribute,Move Attribute,","Add the possibility to execute sling pipes in a background thread For now the only way to execute a pipe through HTTP is to wait for servlet's response to end.
For very long pipes this is an issue (as well as for the fact all pending changes might impact memory), as well as it is error prone in case the same pipe is executed several times.

this ticket is for adding a new execution mode, that write the running status of the pipe configuration, and makes the pipe executed in a separate thread than the servlet thread.","Duplicated Code, Long Method, , , Duplicated Code, "
"   Move Method,Move Attribute,","maven-sling-plugin: Support Mount FileVault XML for File System Resource Provider with SLING-6537 the file system resource provider can also be used to mount filevault xml file system layouts.

we should enhance the maven-sling-plugin to support this layout as well for the fsmount and fsunmount goals (SLING-6622).",", , , "
"   Move Method,Move Attribute,","maven-sling-plugin: Support Mount FileVault XML for File System Resource Provider with SLING-6537 the file system resource provider can also be used to mount filevault xml file system layouts.

we should enhance the maven-sling-plugin to support this layout as well for the fsmount and fsunmount goals (SLING-6622).",", , , "
"   Rename Class,Rename Method,Move Method,","Context-Aware Config: More control about resource paths in Configuration Persistence Strategy currently the SPI interface ConfigurationPersistenceStrategy offers two methods (getResource, getResourcePath) to rewrite resource paths when persisting resources - e.g. insert an additional {{jcr:content}} hierarchy.

but this is a bit limited because it is not known if the resource paths belongs to a singleton resource or resource collection item, or just a config name. and it is not possible to change the parent resource of a configuration collection.",", , "
"   Rename Method,",Context-Aware Config: Separate exception when persist failes due to missing access rights when persisting configuration failes due to missing access rights (e.g. read-only access) the implementation should throw a dedicated exception to be handled separately in upper layers (e.g. configuration editor GUI).,", "
"   Rename Method,","Replace usage of org.apache.sling.commons.json.* and org.json in launchpad relevant bundles Following the deprecation of org.apache.sling.commons.json (SLING-6536) we need to replace its usage everywhere else (at least if we want to be able to release other modules that depend on it). 

This is the umbrella issue for getting this done. The idea is to create sub-issues with patches for individual components, review the patches, and when all are done: close this issue. 

General discussions and problems should go to this issue and specific ones on the sub-issue in question.
",", "
"   Rename Method,Extract Method,","LoginAdminWhitelist.fragment metatype descriptor not as intended The metatype description for {{LoginAdminWhitelist.fragment}} factory configurations is not as intended:

- {{whitelist.regexp}} has no description
- {{whitelist.bundles.default}} is deprecated and should not be in the metatype (but still work if configured)
- {{whitelist.bundles.additional}} is deprecated and should not be in the metatype (but still work if configured)
","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","LoginAdminWhitelist.fragment metatype descriptor not as intended The metatype description for {{LoginAdminWhitelist.fragment}} factory configurations is not as intended:

- {{whitelist.regexp}} has no description
- {{whitelist.bundles.default}} is deprecated and should not be in the metatype (but still work if configured)
- {{whitelist.bundles.additional}} is deprecated and should not be in the metatype (but still work if configured)
","Duplicated Code, Long Method, , "
"   Rename Method,","Align accessors in API We currently have {{getValidationModel(...)}} and {{getModel(...)}}/{{getModels(...)}}, but should use one or the other.

Also we should remove _Validated_ from {{ValidationModel#getValidatedResourceType()}}.",", "
"   Rename Method,","Align accessors in API We currently have {{getValidationModel(...)}} and {{getModel(...)}}/{{getModels(...)}}, but should use one or the other.

Also we should remove _Validated_ from {{ValidationModel#getValidatedResourceType()}}.",", "
"   Rename Method,","Align accessors in API We currently have {{getValidationModel(...)}} and {{getModel(...)}}/{{getModels(...)}}, but should use one or the other.

Also we should remove _Validated_ from {{ValidationModel#getValidatedResourceType()}}.",", "
"   Rename Method,","Align accessors in API We currently have {{getValidationModel(...)}} and {{getModel(...)}}/{{getModels(...)}}, but should use one or the other.

Also we should remove _Validated_ from {{ValidationModel#getValidatedResourceType()}}.",", "
"   Rename Method,","Align accessors in API We currently have {{getValidationModel(...)}} and {{getModel(...)}}/{{getModels(...)}}, but should use one or the other.

Also we should remove _Validated_ from {{ValidationModel#getValidatedResourceType()}}.",", "
"   Extract Superclass,Rename Method,Pull Up Method,Extract Method,Pull Up Attribute,","Provide a custom navigator for provisioning model feature files When working with slingstart projects it's often tedious to always drill down to {{src/main/provisioning}} to view/edit provisioning model files.

We can provide a custom contribution which simply pulls the {{src/main/provisioning}} folder to the top level.","Duplicated Code, Long Method, , Duplicated Code, Large Class, Duplicated Code, Duplicated Code, "
"   Rename Method,",Bundle resource provider: support mounting of JSON files I think similar to SLING-6440 we should support mounting of JSON files through the bundle resource provider (we don't need to support other file formats as xml or vault),", "
"   Move Method,Inline Method,",Bundle resource provider: support mounting of JSON files I think similar to SLING-6440 we should support mounting of JSON files through the bundle resource provider (we don't need to support other file formats as xml or vault),", , , "
"   Rename Method,Extract Method,","Maven Sling Plugin: Deploy fsresource bundles on fsmount goal if required currently only OSGi configuration is created on the ""fsmount"" goal. if the fsresource bundle is not installed this has no effect. on freshly-installed instances this bundles is often not present.

* the goal ""fsmount"" should automatically deploy the bundles if it is not deployed already (configurable, activated by default).
* the minimum version of fsresource bundle can be configured via plugin property - if the bundle is not deployed or too old this version is deployed to the instance","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Maven Sling Plugin: Deploy fsresource bundles on fsmount goal if required currently only OSGi configuration is created on the ""fsmount"" goal. if the fsresource bundle is not installed this has no effect. on freshly-installed instances this bundles is often not present.

* the goal ""fsmount"" should automatically deploy the bundles if it is not deployed already (configurable, activated by default).
* the minimum version of fsresource bundle can be configured via plugin property - if the bundle is not deployed or too old this version is deployed to the instance","Duplicated Code, Long Method, , "
"   Move Class,Move Method,Extract Method,","Maven Sling Plugin: Deploy fsresource bundles on fsmount goal if required currently only OSGi configuration is created on the ""fsmount"" goal. if the fsresource bundle is not installed this has no effect. on freshly-installed instances this bundles is often not present.

* the goal ""fsmount"" should automatically deploy the bundles if it is not deployed already (configurable, activated by default).
* the minimum version of fsresource bundle can be configured via plugin property - if the bundle is not deployed or too old this version is deployed to the instance","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Referrer Filter: Allow Regex User Agent Exclusions For some cases it would be desirable to skip the referrer check altogether for certain resource paths, instead of simply setting ""Allow Empty Referrer"", thus weakening the security overall instead of only for a well known set of paths for which it would be desirable.

For this reason i'd like to propose adding a path whitelist to the referrer filter configuration. Patch attached.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Support OAuth 2.0 server to server authentication in Distribution transport The SCD transport should support OAuth 2.0 Authorization Grants flow [0].
With this flow access token are passed via the {{Authorization}} header to every requests.
The {{SimpleHttpDistributionTransport}} should be extended to support an {{Authorization}} header provided via the {{DistributionTransportSecret}}.
A custom {{DistributionTransportSecretProvider}} implementation would provide the access token in the {{DistributionTransportSecret}}'s credentials map.

[0] https://tools.ietf.org/html/rfc7523","Duplicated Code, Long Method, , "
"   Rename Method,Move Method,Extract Method,","PipesBuilder API is too rigid Some things are missing from Pipes API and there is no workaround:
# additional bindings,
# writers,
# async
# complicated structures (for write, filters, ...) 
# any other parameter i forgot
after trying it groovy + pipebuilder is rather cool UI to generate & test pipes, so would be cool to have everything pipes can do
as i don't want any of the above enhancement to trigger a major bump in the version, i'd like to do that change for 1.0.x

","Duplicated Code, Long Method, , , "
"   Rename Method,Move Method,Extract Method,","PipesBuilder API is too rigid Some things are missing from Pipes API and there is no workaround:
# additional bindings,
# writers,
# async
# complicated structures (for write, filters, ...) 
# any other parameter i forgot
after trying it groovy + pipebuilder is rather cool UI to generate & test pipes, so would be cool to have everything pipes can do
as i don't want any of the above enhancement to trigger a major bump in the version, i'd like to do that change for 1.0.x

","Duplicated Code, Long Method, , , "
"   Move Method,Extract Method,","Add Utility Method to ModelFactory to help proper model creation from child resources in a request context At present, it is somewhat complex to generate Sling Model objects (or really, any adapter class) from an included resource. This is especially true when the adaptable is the SlingHttpServletRequest object.

There are a few different cases which are problematic:

1. When you want to inject an object adapted from a child resource where the adaptable is a request object (i.e. you want to override the request.getResource() to the child resource)
2. When you want to inject an object adapted from a child resource (regardless of the adaptable) and the model class depends upon the Script Bindings.

In the first case, this currently requires creating a wrapper request. The second case is more complex to solve as (to do it correctly) requires re-invoking all of the BindingsValuesProviders (which entails creating a fake ScriptEngine).

To solve both issues, I would like to add a new method named {{getModelFromWrappedRequest}}. Parameters would be {{SlingHttpServletRequest}}, {{Resource}}, {{Class}}.

This method would create a wrapped request with the passed resource, set the bindings to a new object and reinvoke the BVPs.

Patch to follow, but I wanted to file this early to get any feedback.","Duplicated Code, Long Method, , , "
"   Move Method,Extract Method,","Add Utility Method to ModelFactory to help proper model creation from child resources in a request context At present, it is somewhat complex to generate Sling Model objects (or really, any adapter class) from an included resource. This is especially true when the adaptable is the SlingHttpServletRequest object.

There are a few different cases which are problematic:

1. When you want to inject an object adapted from a child resource where the adaptable is a request object (i.e. you want to override the request.getResource() to the child resource)
2. When you want to inject an object adapted from a child resource (regardless of the adaptable) and the model class depends upon the Script Bindings.

In the first case, this currently requires creating a wrapper request. The second case is more complex to solve as (to do it correctly) requires re-invoking all of the BindingsValuesProviders (which entails creating a fake ScriptEngine).

To solve both issues, I would like to add a new method named {{getModelFromWrappedRequest}}. Parameters would be {{SlingHttpServletRequest}}, {{Resource}}, {{Class}}.

This method would create a wrapped request with the passed resource, set the bindings to a new object and reinvoke the BVPs.

Patch to follow, but I wanted to file this early to get any feedback.","Duplicated Code, Long Method, , , "
"   Rename Method,",Add SlingScriptHelper.forward method SLING-692 implements the RequestDispatcher.forward method. The SlingScriptHelper interface should now be enhanced with a forward method.,", "
"   Rename Method,",Add SlingScriptHelper.forward method SLING-692 implements the RequestDispatcher.forward method. The SlingScriptHelper interface should now be enhanced with a forward method.,", "
"   Pull Up Method,Extract Method,Pull Up Attribute,","add csv pipe would be cool to have a pipe able of ingesting csv à la jsonpipe, repeating input to the output but outputing the line values in the bindings.
they should share a same parent class. We could also add the possibility to consume an input stream from the request if any.","Duplicated Code, Long Method, , Duplicated Code, Duplicated Code, "
"   Pull Up Method,Extract Method,Pull Up Attribute,","add csv pipe would be cool to have a pipe able of ingesting csv à la jsonpipe, repeating input to the output but outputing the line values in the bindings.
they should share a same parent class. We could also add the possibility to consume an input stream from the request if any.","Duplicated Code, Long Method, , Duplicated Code, Duplicated Code, "
"   Rename Method,Extract Method,","few pipe builder improvements needed there are a few improvements that would be cool after a few usages of pipe builder:
- add #build(String) method that allows to build a pipe *not* in a random path, in case the location does matter (for now, searching & moving the executed pipe is a bit tedious),
- allow .with() to work before first subpipe, this would affect the container pipe, allowing properties there,
- add runWith(String...) that is a shorter version of run(Map)","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","few pipe builder improvements needed there are a few improvements that would be cool after a few usages of pipe builder:
- add #build(String) method that allows to build a pipe *not* in a random path, in case the location does matter (for now, searching & moving the executed pipe is a bit tedious),
- allow .with() to work before first subpipe, this would affect the container pipe, allowing properties there,
- add runWith(String...) that is a shorter version of run(Map)","Duplicated Code, Long Method, , "
"   Move Class,Move And Rename Class,Rename Method,Extract Method,","Allow the Framework to restart Certain operations may cause the OSGi framework to restart, such as calling Bundle.update on the system bundle or calling Bundle.stop on the system bundle (this should maybe even cause the application to terminate) or removing a framework extension bundle.

The Sling launcher is in control of the Felix framework launch and therefore can easily cope with framework restart requests. In addition OSGi will bring more information on why the framework is terminating as to decide whether the launcher should relaunch the framework or simply terminate.","Duplicated Code, Long Method, , "
"   Rename Method,","Sling Query support for Java 8 Sling Query, while written in Java 7, uses a number of concepts introduces in Java 8, eg. Predicate and Function. Now we can replace these interfaces with the native java.lang.* types.

Also, it should be possible to transform the SlingQuery to the Java 8 stream.",", "
"   Rename Method,","Sling Query support for Java 8 Sling Query, while written in Java 7, uses a number of concepts introduces in Java 8, eg. Predicate and Function. Now we can replace these interfaces with the native java.lang.* types.

Also, it should be possible to transform the SlingQuery to the Java 8 stream.",", "
"   Extract Method,Inline Method,","htl-maven-plugin: Add option to precompile the HTL scripts It would be cool if the HTL Maven Plugin could precompile the HTL scripts, similar to the JSPC plugin.

This would have the following advantages:

* the project needs to specify all the (compile time) dependencies the scripts needs; this makes the project more robust and changes in dependencies are easier detectable;
* the compiled classes could be used as source to generate the import-package statements for the {{filevault-package-maven-plugin}}.","Duplicated Code, Long Method, , , "
"   Rename Method,Move Method,Extract Method,Move Attribute,","Node-based configs for jcrinstall Besides the existing .cfg file format, having node-based configurations would make them more granular and easier to patch.

I'll implement an additional config source for jcrinstall, that uses nodes and properties (including multi-value, and taking property types into account).","Duplicated Code, Long Method, , , , "
"   Rename Method,","Process resource detection and installation in sequence instead of asynchronously Currently the RepositoryObserver's resource detection loop and the OsgiController's installation/update/delete loop run in their own separate threads.

Running the detection and installation operations in sequence would make debugging and testing easier, and there are no downsides except maybe slightly slower processing of installed bundles and configs - I think the tradeoff is worth it, and it's simple to implement: I'll add a executeScheduledOperations() method to the OsgiController interface, which will be called by the RepositoryObserver at the end of its repository observation loop.",", "
"   Extract Method,Inline Method,","Support Start Level assignment for bundles contained in the Launchpad package Currently the Sling launcher has no support to assign start levels to the bundles installed from the resources/corebundles and resources/bundles locations. To support assigning specific start levels, the folders in the resources folder should be supported to denote requested start levels.

This way, we can assign different bundles to different start levels. The bundles in corebundles would default to using start level 1, while the bundles in the bundles would default to the default start level defined by the StartLevel service. Likewise bundles in folder 0 (zero) would be assigned that default start level. Folders whose name cannot be parsed as a positive integer will be ignored.","Duplicated Code, Long Method, , , "
"   Extract Method,Inline Method,","Support Start Level assignment for bundles contained in the Launchpad package Currently the Sling launcher has no support to assign start levels to the bundles installed from the resources/corebundles and resources/bundles locations. To support assigning specific start levels, the folders in the resources folder should be supported to denote requested start levels.

This way, we can assign different bundles to different start levels. The bundles in corebundles would default to using start level 1, while the bundles in the bundles would default to the default start level defined by the StartLevel service. Likewise bundles in folder 0 (zero) would be assigned that default start level. Folders whose name cannot be parsed as a positive integer will be ignored.","Duplicated Code, Long Method, , , "
"   Rename Method,Move Method,Move Attribute,","Web Console Plugin for the OSGi Installer (changed title as discussion moved to creating a console plugin)

Making a servlet out of the jcrinstall RepositoryObserver class introduces dependencies on the sling api and servlet bundles, which are not desired for jcrinstall - its dependencies should be kept to a minimum.

I'll move the servlet to its own bundle - it is currently only needed for integration testing, but might be useful for other monitoring purposes.",", , , "
"   Extract Method,Inline Method,","Web Console Plugin for the OSGi Installer (changed title as discussion moved to creating a console plugin)

Making a servlet out of the jcrinstall RepositoryObserver class introduces dependencies on the sling api and servlet bundles, which are not desired for jcrinstall - its dependencies should be kept to a minimum.

I'll move the servlet to its own bundle - it is currently only needed for integration testing, but might be useful for other monitoring purposes.","Duplicated Code, Long Method, , , "
"   Move Method,Move Attribute,","Provide better configurability and integration for WebDAV The current implementation of (simple) WebDAV support only provide little support for configuration of the inner operations. In addition MIME type resolution is not integrated with Sling support MIME type resolution.

To fix this, Sling WebDAV should add its own configuration by means of extending the Jackrabbit ResourceConfig class and overwriting all methods along these lines:

* MIME Type resolution is delegated to Sling's MIME type resolution by providing a SlingMimeResolver
* IOManager and PropertyManager are hard coded to use the default implementations
* Only DirListingExportHandler and DefaultHandler are registered and used
* Configuration through OSGi Configuration Admin is defined.

The following configuration is possible and supported:
- node types defining non-collection resources
- item filter setting by namespace uri and prefix and node type
- node types for collection, non-collection and content nodes
- (as before) Realm String and URL root for the separate WebDAV servlet",", , , "
"   Move Method,Move Attribute,","Provide better configurability and integration for WebDAV The current implementation of (simple) WebDAV support only provide little support for configuration of the inner operations. In addition MIME type resolution is not integrated with Sling support MIME type resolution.

To fix this, Sling WebDAV should add its own configuration by means of extending the Jackrabbit ResourceConfig class and overwriting all methods along these lines:

* MIME Type resolution is delegated to Sling's MIME type resolution by providing a SlingMimeResolver
* IOManager and PropertyManager are hard coded to use the default implementations
* Only DirListingExportHandler and DefaultHandler are registered and used
* Configuration through OSGi Configuration Admin is defined.

The following configuration is possible and supported:
- node types defining non-collection resources
- item filter setting by namespace uri and prefix and node type
- node types for collection, non-collection and content nodes
- (as before) Realm String and URL root for the separate WebDAV servlet",", , , "
"   Move Method,Move Attribute,","Add possibility to force a reprocess of an queued job entry A FIFO job event queue processes the jobs in proper sequence. when a job cannot be processed it can be scheduled for retry after a certain period.
when the queue waits for that retry delay to pass, new job can be added to the queue. sometime it might be useful to bypass the wait time ""manually"" and let the queue re-process the job immediately. for example after a config change or per user-interaction via a GUI.

add something like: 
JobStatusProvider.forceReProcess(String queueName)



",", , , "
"   Move Method,Move Attribute,","Add possibility to force a reprocess of an queued job entry A FIFO job event queue processes the jobs in proper sequence. when a job cannot be processed it can be scheduled for retry after a certain period.
when the queue waits for that retry delay to pass, new job can be added to the queue. sometime it might be useful to bypass the wait time ""manually"" and let the queue re-process the job immediately. for example after a config change or per user-interaction via a GUI.

add something like: 
JobStatusProvider.forceReProcess(String queueName)



",", , , "
"   Move Class,Move And Rename Class,Rename Method,Move Method,Move Attribute,","Improve our Maven modules layout to make Sling easier to understand As discussed on sling-dev [1], we'd like to restructure our source code, to split Sling in smaller pieces and make it easier to understand what is what.

The suggested Maven modules are as follows. That's the general idea, details will have to be filled in.

sling-api (used by both microsling and sling)
microsling
- microsling-webapp (uses sling-api)
- microsling-cargo-testing (I'm planning to write some integration tests)
sling
- sling-osgi (osgi-related modules, console, ...)
- sling-jcr (JCR utility modules, wrappers, ...)
- sling-jsp (jasper compiler, JSP script engine, ...)
- sling-core
- sling-commons (was not on the mailing list proposal, but probably useful)
- ...
sling-maven-plugins
- ... existing plugins

[1] http://thread.gmane.org/gmane.comp.apache.sling.devel/555/focus=57",", , , "
"   Rename Class,Move And Rename Class,Rename Method,Move Method,Move Attribute,","Improve our Maven modules layout to make Sling easier to understand As discussed on sling-dev [1], we'd like to restructure our source code, to split Sling in smaller pieces and make it easier to understand what is what.

The suggested Maven modules are as follows. That's the general idea, details will have to be filled in.

sling-api (used by both microsling and sling)
microsling
- microsling-webapp (uses sling-api)
- microsling-cargo-testing (I'm planning to write some integration tests)
sling
- sling-osgi (osgi-related modules, console, ...)
- sling-jcr (JCR utility modules, wrappers, ...)
- sling-jsp (jasper compiler, JSP script engine, ...)
- sling-core
- sling-commons (was not on the mailing list proposal, but probably useful)
- ...
sling-maven-plugins
- ... existing plugins

[1] http://thread.gmane.org/gmane.comp.apache.sling.devel/555/focus=57",", , , "
"   Move Class,Move Method,","Improve our Maven modules layout to make Sling easier to understand As discussed on sling-dev [1], we'd like to restructure our source code, to split Sling in smaller pieces and make it easier to understand what is what.

The suggested Maven modules are as follows. That's the general idea, details will have to be filled in.

sling-api (used by both microsling and sling)
microsling
- microsling-webapp (uses sling-api)
- microsling-cargo-testing (I'm planning to write some integration tests)
sling
- sling-osgi (osgi-related modules, console, ...)
- sling-jcr (JCR utility modules, wrappers, ...)
- sling-jsp (jasper compiler, JSP script engine, ...)
- sling-core
- sling-commons (was not on the mailing list proposal, but probably useful)
- ...
sling-maven-plugins
- ... existing plugins

[1] http://thread.gmane.org/gmane.comp.apache.sling.devel/555/focus=57",", , "
"   Move Class,Move And Rename Class,Rename Method,Move Method,Move Attribute,","Improve our Maven modules layout to make Sling easier to understand As discussed on sling-dev [1], we'd like to restructure our source code, to split Sling in smaller pieces and make it easier to understand what is what.

The suggested Maven modules are as follows. That's the general idea, details will have to be filled in.

sling-api (used by both microsling and sling)
microsling
- microsling-webapp (uses sling-api)
- microsling-cargo-testing (I'm planning to write some integration tests)
sling
- sling-osgi (osgi-related modules, console, ...)
- sling-jcr (JCR utility modules, wrappers, ...)
- sling-jsp (jasper compiler, JSP script engine, ...)
- sling-core
- sling-commons (was not on the mailing list proposal, but probably useful)
- ...
sling-maven-plugins
- ... existing plugins

[1] http://thread.gmane.org/gmane.comp.apache.sling.devel/555/focus=57",", , , "
"   Rename Class,Rename Method,","Improve our Maven modules layout to make Sling easier to understand As discussed on sling-dev [1], we'd like to restructure our source code, to split Sling in smaller pieces and make it easier to understand what is what.

The suggested Maven modules are as follows. That's the general idea, details will have to be filled in.

sling-api (used by both microsling and sling)
microsling
- microsling-webapp (uses sling-api)
- microsling-cargo-testing (I'm planning to write some integration tests)
sling
- sling-osgi (osgi-related modules, console, ...)
- sling-jcr (JCR utility modules, wrappers, ...)
- sling-jsp (jasper compiler, JSP script engine, ...)
- sling-core
- sling-commons (was not on the mailing list proposal, but probably useful)
- ...
sling-maven-plugins
- ... existing plugins

[1] http://thread.gmane.org/gmane.comp.apache.sling.devel/555/focus=57",", "
"   Rename Method,Extract Method,","Remove/reduce 90 seconds wait time the current event processor waits 90 seconds (since service activation) until it start processing jobs. this might be too long for small/fast systems, or even too low for very big/slow systems.

especially job processing should start as soon the respective queue listeners are registered.
","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Remove/reduce 90 seconds wait time the current event processor waits 90 seconds (since service activation) until it start processing jobs. this might be too long for small/fast systems, or even too low for very big/slow systems.

especially job processing should start as soon the respective queue listeners are registered.
","Duplicated Code, Long Method, , "
"   Rename Method,",OpenID AuthenticationHandler An implementation of AuthenticationHandler for authenticating users against OpenID providers. Includes basic UI for login & logout.,", "
"   Move Class,Move Method,Move Attribute,","PluggableLoginModule to provide DefaultLoginModule extensions via bundle services Enhance the Jackrabbit Server module by adding a PluggableDefaultLoginModule extending DefualtLoginModule and a LoginModulePlugin interface. The PluggableDefaultLoginModule uses the LoginModulePlugin instances to verify the credentials presented by the user. 

The PluggableDefaultLoginModule falls back to the DefaultLoginModule implementation if there is not support for the Credentials instance presented.

This approach allows custom AuthenticationHandlers to provide custom login behavior for the Credentials they pass to the SlingAuthenticator - particularly useful when no password is made available to the handler by the authentication process.",", , , "
"   Move Class,Move Method,Move Attribute,","PluggableLoginModule to provide DefaultLoginModule extensions via bundle services Enhance the Jackrabbit Server module by adding a PluggableDefaultLoginModule extending DefualtLoginModule and a LoginModulePlugin interface. The PluggableDefaultLoginModule uses the LoginModulePlugin instances to verify the credentials presented by the user. 

The PluggableDefaultLoginModule falls back to the DefaultLoginModule implementation if there is not support for the Credentials instance presented.

This approach allows custom AuthenticationHandlers to provide custom login behavior for the Credentials they pass to the SlingAuthenticator - particularly useful when no password is made available to the handler by the authentication process.",", , , "
"   Move Class,Move Method,Extract Method,Inline Method,Move Attribute,",Use new commons compiler for java servlet scripting Currently the java servlet scripting bundle embeds its own version of the eclipse compiler. This can be changed to use the new commons compiler.,"Duplicated Code, Long Method, , , , , "
"   Rename Class,Move Class,Rename Method,Push Down Method,Push Down Attribute,","New Bundle for a ResourceProvider and Sling Post Operations for interacting with the jackrabbit UserManager This is a new bundle that provides a custom ResourceProvider and SlingPostOperations for interacting with the Jackrabbit UserManager.

The ResourceProvider exposes users at this path /system/userManager/user/[userName] and groups at this path /system/userManager/group/[groupName]

In order for the custom SlingPostOperations to work, the patch for SLING-651 must be applied first.

These custom SlingPostOperations are provided:
1. createUser
2. createGroup
3. changePassword
4. updateAuthorizable
5. deleteAuthorizable

Sample usage of the operations is in the samples.html.esp attachment.



",", , , "
"   Rename Class,Move Class,Rename Method,Push Down Method,Push Down Attribute,","New Bundle for a ResourceProvider and Sling Post Operations for interacting with the jackrabbit UserManager This is a new bundle that provides a custom ResourceProvider and SlingPostOperations for interacting with the Jackrabbit UserManager.

The ResourceProvider exposes users at this path /system/userManager/user/[userName] and groups at this path /system/userManager/group/[groupName]

In order for the custom SlingPostOperations to work, the patch for SLING-651 must be applied first.

These custom SlingPostOperations are provided:
1. createUser
2. createGroup
3. changePassword
4. updateAuthorizable
5. deleteAuthorizable

Sample usage of the operations is in the samples.html.esp attachment.



",", , , "
"   Rename Method,","Use the sling-api from SLING-28 in microsling Once we agree on SLING-28, microsling can be ""ported"" to this API, so that both microsling and Sling use it.

Make sure to tag http://svn.apache.org/repos/asf/incubator/sling/whiteboard/microsling/ before doing this",", "
"   Rename Method,Extract Method,","Allow servlets to be registered with extensions for all request methods As SLING-754 introduced/fixed registering servlets for non-GET methods with selectors (eg. /apps/myapp/selector.POST.servlet), the same should be possible for extensions: /apps/myapp/extension.POST.servlet

Use case: symmetric URLs when you import and export stuff at a given resource path:

Export = GET @ /some/path/foo.ext
Import/Update = POST @ /some/path/foo.ext

Currently you are forced to either use a selector for the POST case (/some/path/foo.ext.ext) or to have a sling resource type set on the /some/path/foo, but then you could only have a single POST servlet for that resource type, regardless of the extension (which is unpractical if you for example have a resource type like ""calendar"", but want to import various calendar formats, separated by their file extension).","Duplicated Code, Long Method, , "
"   Pull Up Method,Move Method,","microjax - lightweight Ajax client and servlet for microsling As discussed on the mailing list [1], I'll contribute to microsling the experimental ""r-jax"" stuff [2] that we wrote together with David Nuescheler.

The main changes are:
1) The default microsling servlet needs to handle POSTs in a more clever way, to provide useful services to the javascript client

2) For GETs, the default microsling servlet needs to provide a JSON representation, when the json extension is used. This will be extensible to XML and other output formats.

[1] http://thread.gmane.org/gmane.comp.apache.sling.devel/721
[2] http://www.day.com/maven/rjax/

",", , Duplicated Code, "
"   Move Class,Move Method,","microjax - lightweight Ajax client and servlet for microsling As discussed on the mailing list [1], I'll contribute to microsling the experimental ""r-jax"" stuff [2] that we wrote together with David Nuescheler.

The main changes are:
1) The default microsling servlet needs to handle POSTs in a more clever way, to provide useful services to the javascript client

2) For GETs, the default microsling servlet needs to provide a JSON representation, when the json extension is used. This will be extensible to XML and other output formats.

[1] http://thread.gmane.org/gmane.comp.apache.sling.devel/721
[2] http://www.day.com/maven/rjax/

",", , "
"   Rename Method,Extract Method,","JSON Item renderer for microsling For SLING-92 we need a JSON renderer, I'll attach some JSON output here, from our r-jax prototype, as examples.","Duplicated Code, Long Method, , "
"   Rename Method,","Refine initiaition of the authentication process Currently the authentication process can only be initiated by explicitly calling a login page provided by some AuthenticationHandler implementation bundle. There is no way to initiate the authentication process from within a servlet or script (e.g. to have the user log in a 404/NOT FOUND error handler).

To support this kind of functionality the existing SlingAuthenticator.requestAuthentcation method should be publicly accessible through Service interface. Servlets or scripts which want to request authentication from the client for the current request may then call this service method.

This method applies the same authentication handler selection algorithm for the given HttpServletRequest object as it does for finding the authentication handler in the authenticate process. This ensures, that for a given request, the appropriate authentication handler is called which is then able to initiate authentication appropriately, for example by drawing a form.

For full details refer to http://cwiki.apache.org/SLING/authentication-initiation.html",", "
"   Rename Method,","Add suppport for TCP/IP based control connection for Sling standalone app Currently the Sling standalone application can only be stopped by stopping the system bundle or by killing the Sling process. To better control Sling control connection support based on TCP/IP would be nice. This way the same sling standalone application may be used to start sling, to check a running sling instance and to stop a running sling instance.

The following command line arguments are added:

""start"" - Sling opens a TCP/IP server socket for the control conneciton
""status"" - Checks whether a Sling instance is listening on a TCP/IP socket
""stop"" - Stops a Sling instance listening on a TCP/IP socket
none of the above - starts Sling without listening on a TCP/IP socket

The socket to listen on (start option) or to send the command to (status, stop) is configured with the ""-j"" command line option. This option takes an argument of the following form:

host:port -- host name or ip address and port number of the socket
port -- port number on localhost (InetAddress.getLocalHost()) of the socket
none or -j not specified -- defaults to port 63000 on localhost

Note, that setting any host name or IP address, which is reachable from remote systems may be considered as security issue, since the connection is not secured by password or such. For this reason the default interface to listen on is local host and the server socket is only created if explicitly asked for with the start parameter.",", "
"   Rename Method,","Add suppport for TCP/IP based control connection for Sling standalone app Currently the Sling standalone application can only be stopped by stopping the system bundle or by killing the Sling process. To better control Sling control connection support based on TCP/IP would be nice. This way the same sling standalone application may be used to start sling, to check a running sling instance and to stop a running sling instance.

The following command line arguments are added:

""start"" - Sling opens a TCP/IP server socket for the control conneciton
""status"" - Checks whether a Sling instance is listening on a TCP/IP socket
""stop"" - Stops a Sling instance listening on a TCP/IP socket
none of the above - starts Sling without listening on a TCP/IP socket

The socket to listen on (start option) or to send the command to (status, stop) is configured with the ""-j"" command line option. This option takes an argument of the following form:

host:port -- host name or ip address and port number of the socket
port -- port number on localhost (InetAddress.getLocalHost()) of the socket
none or -j not specified -- defaults to port 63000 on localhost

Note, that setting any host name or IP address, which is reachable from remote systems may be considered as security issue, since the connection is not secured by password or such. For this reason the default interface to listen on is local host and the server socket is only created if explicitly asked for with the start parameter.",", "
"   Rename Method,Extract Method,","Ensure official mime types cannot be overwritten by bundles and/or configuration Currently the Mimetype service is implemented, such that any mime type registration overwrites any registration already existing. This may cause third-party bundles or mime type providers to overwrite official mappings from the core MIME type file.

The service should be modified such, that :

* the core MIME type file is read first
* later mappings cannot overwrite existing mappings
","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Ensure official mime types cannot be overwritten by bundles and/or configuration Currently the Mimetype service is implemented, such that any mime type registration overwrites any registration already existing. This may cause third-party bundles or mime type providers to overwrite official mappings from the core MIME type file.

The service should be modified such, that :

* the core MIME type file is read first
* later mappings cannot overwrite existing mappings
","Duplicated Code, Long Method, , "
"   Rename Method,Move Method,Extract Method,Pull Up Attribute,","Allow Sling logs to be configured to rotate on time instead of file size Log4j has the ability to rotate logs daily/based on a time parameter rather than a file size. This makes system administration easier since the ""right"" log file can be easily identified based on when a problem occurred. 

A similar feature inside Sling logging would be useful.","Duplicated Code, Long Method, , , Duplicated Code, "
"   Rename Method,Move Method,Extract Method,Pull Up Attribute,","Allow Sling logs to be configured to rotate on time instead of file size Log4j has the ability to rotate logs daily/based on a time parameter rather than a file size. This makes system administration easier since the ""right"" log file can be easily identified based on when a problem occurred. 

A similar feature inside Sling logging would be useful.","Duplicated Code, Long Method, , , Duplicated Code, "
"   Move Class,Move And Rename Class,Rename Method,","[storm-elasticsearch] Expose TransportClient configuration Map to EsConfig storm-elasticsearch has hardcoded configuration for TransportClient, client.transport.sniff = true.

Users may want to change this value, and may also want to add another configuration.
(Please refer https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/transport-client.html)

If we want to pass configurations to TransportClient, I think it would be better to expose Map from EsConfig, and put all things inside Map to ImmutableSettings.settingsBuilder().",", "
"   Rename Method,",Storm Cassandra connector 0,", "
"   Rename Method,",Storm Cassandra connector 0,", "
"   Rename Method,","Support AND, OR and NOT operators in StormSQL This jira proposes to compile AND, OR, and NOT operators to Java source code.",", "
"   Rename Method,","Have the IConnection push batches instead of buffering them The messaging layer currently buffers tuples and waits for one or more threads to take the tuples and route them where they need to go.

This adds an extra thread that a tuple has to go through before it can be processed, and places the tuple in a LinkedBlockingQueue waiting for one of the threads to be ready to process the data.",", "
"   Move Class,Extract Method,",Support string operations in StormSQL This jira tracks the effort of implementing string operations in StormSQL.,"Duplicated Code, Long Method, , "
"   Rename Method,","Reduce Thread Usage of Netty Transport When users start to create large topologies the storm netty messaging layer
uses lots of threads. This has resulted in OOMs because the default ulimit on most linux distros is around 4000 processes. It looks like the messaging layer wants to have one thread per server it is connected to, so that means the total number of other workers in the System.

For one particular case we saw.

1 (Curator delay thread)
1 (Curator Event Processor)
1 (Finalizer)
1 (GC???)
1 (Storm messaging recv thread asking netty for messages)
1 (Thread pool polling on a Synchronous queue???)
1 (ZK Connection)
1 (ZK epoll)
2 (???)
2 (Netty epoll)
6 (Timer Thread)
15 (Disruptor consume batches)
104 (Netty Thread pool taking messages to be sent)

and this process was dieing with OOMs because it could not create any more netty threads.

Looking at the code it appears that come from two different things. First The Client code is using it's own thread pool for each Client instead of sharing a thread pool, but also the protocol itself blocks the thread in takeMessages() if there are no messages to send.

So we need to make the thread pool shared between all of the clients and modify the protocol so that takeMessages does not block. But with it not blocking we also need a way to have Client.send write directly to the Channel in some situations so that the messages still are sent.",", "
"   Move Class,Move Method,Move Attribute,",Support collations of primary keys This jira proposes to add support of specifying collations of primary keys. Collations provide information on uniqueness and monotonicity of columns. The information is essential to implement aggregation functions over streaming data.,", , , "
"   Rename Method,Extract Method,",port backtype.storm.security.auth.AuthUtils-test to java Just a test moving to junit,"Duplicated Code, Long Method, , "
"   Rename Method,",port backtype.storm.utils-test to java junit test migration,", "
"   Rename Method,",port backtype.storm.utils-test to java junit test migration,", "
"   Rename Method,",port backtype.storm.utils-test to java junit test migration,", "
"   Move Method,Extract Method,","port backtype.storm.daemon.nimbus to java https://github.com/apache/storm/tree/jstorm-import/jstorm-core/src/main/java/com/alibaba/jstorm/daemon/nimbus

as a possible example","Duplicated Code, Long Method, , , "
"   Rename Method,",port backtype.storm.daemon.worker to java https://github.com/apache/storm/tree/jstorm-import/jstorm-core/src/main/java/com/alibaba/jstorm/daemon/worker as an example,", "
"   Move Class,Extract Method,","port backtype.storm.daemon.logviewer to java This is providing a UI for accessing and searching logs. hiccup will need to be replaced, possibly with just hard coded HTML + escaping.","Duplicated Code, Long Method, , "
"   Move Class,Move Method,Move Attribute,","port backtype.storm.daemon.logviewer to java This is providing a UI for accessing and searching logs. hiccup will need to be replaced, possibly with just hard coded HTML + escaping.",", , , "
"   Rename Class,Rename Method,","port backtype.storm.daemon.logviewer to java This is providing a UI for accessing and searching logs. hiccup will need to be replaced, possibly with just hard coded HTML + escaping.",", "
"   Move Method,Move Attribute,","port backtype.storm.daemon.logviewer to java This is providing a UI for accessing and searching logs. hiccup will need to be replaced, possibly with just hard coded HTML + escaping.",", , , "
"   Move Class,Move Method,Extract Method,Move Attribute,",port backtype.storm.ui.core to java User Interface + REST -> java,"Duplicated Code, Long Method, , , , "
"   Rename Method,","Trident should support writing to multiple Kafka clusters Current it is impossible to instantiate two instances of the {{TridentKafkaState}} class that write to different Kafka cluster. This is because that {{TridentKafkaState}} obtains the the location of the Kafka producer from configuration. Multiple instances can only get the same configuration in the {{prepare()}} method.

This jira proposes to introduce a configuration class like {{TridentKafkaConfig}} to allow multiple instances of {{TridentKafkaState}} to write to different Kafka clusters.",", "
"   Rename Method,Move Method,",Support the GROUP BY clause in StormSQL This jira tracks the effort of implement the support `GROUP BY` clause in StormSQL.,", , "
"   Move Class,Move Method,Extract Method,","Compile the Calcite logical plan to Storm Trident logical plan As suggested in https://issues.apache.org/jira/browse/STORM-1040?focusedCommentId=15036651&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15036651, compiling the logical plan from Calcite down to Storm physical plan will clarify the implementation of StormSQL.

> Motive behind this big change and benefits
This is started from [Julian's comment|https://issues.apache.org/jira/browse/STORM-1040?focusedCommentId=15034472&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15034472] and also [Milinda's comment|https://issues.apache.org/jira/browse/STORM-1040?focusedCommentId=15035182&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15035182].

For me having own relational algebras (rel) has several advantages,

* We can push operator handling logic to rel itself. Before that we should traverse Calcite logical rel tree with PostOrderRelNodeVisitor, and visitor needs to handle Calcite's rel directly. Now the logic how to configure Trident topology is all handled from separate rels.

* We sometimes want to have more derived rels compared to Calcite logical operators. One of example is Join. There's only one logical rel regarding join in Calcite - LogicalJoin - but we're now converting LogicalJoin to EquiJoin if conditions are met. If we have various types of join it will make the difference. We're not prepared yet, but streaming scan vs table scan, and streaming insert vs table insert are the other cases.

{code}
TridentStormAggregateRel(group=[{0}], EXPR$1=[COUNT()])
TridentStormCalcRel(expr#0..4=[{inputs}], expr#5=[0], expr#6=[>($t0, $t5)], DEPTID=[$t3], EMPID=[$t0], $condition=[$t6])
TridentStormEquiJoinRel(condition=[=($2, $3)], joinType=[inner])
TridentStormStreamScanRel(table=[[EMP]])
TridentStormStreamScanRel(table=[[DEPT]])
{code}

* We can even override the methods how to represent the rel in explain string if we think Calcite's explain is less informational. For example, showing initial parallelism (when we support) for Scan.

* We can apply query optimizations: Defining derived rels helps further query optimizations, like filter pushdown. Calcite rels is not aware of data source characteristic, and we can include it to our own rels.","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Fix bugs and refactor code in ResourceAwareScheduler Code refactored:

1. Refactor RAS_Nodes. Pushed some of the functionality in to RAS_Nodes. Each RAS_Node will now be initialized with a map of all its assignments. Each RAS_Node will also figure out resources used and available. Removed unnecessary functions.

2. Made WorkerSlot immutable so that a scheduling strategy won't mistakenly modify it

3. Added a wrapping layer for RAS_Node to feed into scheduling strategies so that the semantics of what a scheduling strategy should do will be more clear. Each scheduling strategy shouldn't be actually assigning anything. The strategy should only calculate a scheduling.

Bug fixes:

1. Minor bug in displaying the assigned resources for a supervisor on the UI. The function updateSupervisorResources was placed in the wrong place

2. Minor bug fix in freeing memory in RAS_Node there was some wrong math that was done.



","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,Move Attribute,","storm-hdfs should support writing to multiple files Examples of when this is needed include:
- One avro bolt writing multiple schemas, each of which require a different file. Schema evolution is a common use of avro and the avro bolt should support that seamlessly.
- Partitioning output to different directories based on the tuple contents. For example, if the tuple contains a ""USER"" field, it should be possible to partition based on that value.","Duplicated Code, Long Method, , , "
"   Rename Method,","Load Balancing Shuffle Grouping https://github.com/nathanmarz/storm/issues/571

Hey @nathanmarz,

I think that the current shuffle grouping is creating very obvious hot-spots in load on hosts here at Twitter. The reason is that randomized message distribution to the workers is susceptible to the balls and bins problem:

http://pages.cs.wisc.edu/~shuchi/courses/787-F07/scribe-notes/lecture07.pdf

the odds that some particular queue gets bogged down when you're assigning tasks randomly is high. You can solve this problem with a load-aware shuffle grouping -- when shuffling, prefer tasks with lower load.

What would it take to implement this feature?

----------
sritchie: Looks like Rap Genius was heavily affected when Heroku started running a ""shuffle grouping"" on tasks to its dynos:

http://rapgenius.com/James-somers-herokus-ugly-secret-lyrics

50x performance degradation over a more intelligent load-balancing scheme that only sent tasks to non-busy dynos. Seems very relevant to Storm.

----------
nathanmarz: It's doing randomized round robin, not fully random distribution. So every downstream task gets the same number of messages. But yes, I agree that this would be a great feature. Basically what this requires is making stats of downstream tasks available to the stream grouping code. The best way to implement this would be:

Implement a broadcast message type in the networking code, so that one can efficiently send a large object to all tasks in a worker (rather than having to send N copies of that large message)
Have a single executor in every topology that polls nimbus for accumulated stats once per minute and then broadcasts that information to all tasks in all workers
Wire up the task code to pass that information along from the task to the outgoing stream groupings for that task (and adding appropriate methods to the CustomStreamGrouping interface to receive the stats info)

----------
sorenmacbeth: @nathanmarz @sritchie Did any progress ever get made on this? Is the description above still relevant to Storm 0.9.0. We are getting bitten by this problem and would love to see something like this implemented.
",", "
"   Rename Method,Extract Method,Move Attribute,","Minor Refactoring of Resource Aware Scheduler Refactored the following:

1. The API interface to define custom scheduling strategies. The API exposed from RAS to a per topology scheduling strategy is messy and needs to be cleaned up.
2. In RAS, the state of the scheduler is held by several member variables. Its cleaner if the scheduler state is represented by a single object. This will help we reduce the amount of code need when checkpointing the state of the scheduler and potentially restoring the state in the future when a bad scheduling happens
","Duplicated Code, Long Method, , , "
"   Rename Method,",DRPCSpout should attempt reconnect if on fail it cannot reach client 0,", "
"   Extract Method,Move Attribute,","abstract batch processing to common api `BatchHelper` Some external projects(`storm-hbase`, `storm-hive`, `storm-mongodb`) have the same batch processing logic.
Abstracting for that could make code simpler and cleaner.","Duplicated Code, Long Method, , , "
"   Rename Method,","Cap on number of retries for a failed message in kafka spout The kafka-spout module based on newer APIs has a cap on the number of times, a message is to be retried. It will be a good feature add in the older kafka spout code as well.",", "
"   Rename Method,Move Method,Extract Method,","A better algorithm server rack selection for RAS Currently the getBestClustering algorithm for RAS finds the ""Best"" cluster/rack based on which rack has the most available resources this may be insufficient and may cause topologies not to be able to be scheduled successfully even though there are enough resources to schedule it in the cluster. We attempt to find the rack with the most resources by find the rack with the biggest sum of available memory + available cpu. This method is not effective since it does not consider the number of slots available. This method also fails in identifying racks that are not schedulable due to the exhaustion of one of the resources either memory, cpu, or slots. The current implementation also tries the initial scheduling on one rack and not try to schedule on all the racks before giving up which may cause topologies to be failed to be scheduled due to the above mentioned shortcomings in the current method. Also the current method does not consider failures of workers. When executors of a topology gets unassigned and needs to be scheduled again, the current logic in getBestClustering may be inadequate if not complete wrong. When executors needs to rescheduled due to a fault, getBestClustering will likely return a cluster that is different from where the majority of executors from the topology is originally scheduling in.

Thus, I propose a different strategy/algorithm to find the ""best"" cluster. I have come up with a ordering strategy I dub subordinate resource availability ordering (inspired by Dominant Resource Fairness) that sorts racks by the subordinate (not dominant) resource availability.

For example given 4 racks with the following resource availabilities
{code}
//generate some that has alot of memory but little of cpu
rack-3 Avail [ CPU 100.0 MEM 200000.0 Slots 40 ] Total [ CPU 100.0 MEM 200000.0 Slots 40 ]
//generate some supervisors that are depleted of one resource
rack-2 Avail [ CPU 0.0 MEM 80000.0 Slots 40 ] Total [ CPU 0.0 MEM 80000.0 Slots 40 ]
//generate some that has a lot of cpu but little of memory
rack-4 Avail [ CPU 6100.0 MEM 10000.0 Slots 40 ] Total [ CPU 6100.0 MEM 10000.0 Slots 40 ]
//generate another rack of supervisors with less resources than rack-0
rack-1 Avail [ CPU 2000.0 MEM 40000.0 Slots 40 ] Total [ CPU 2000.0 MEM 40000.0 Slots 40 ]
rack-0 Avail [ CPU 4000.0 MEM 80000.0 Slots 40( ] Total [ CPU 4000.0 MEM 80000.0 Slots 40 ]
Cluster Overall Avail [ CPU 12200.0 MEM 410000.0 Slots 200 ] Total [ CPU 12200.0 MEM 410000.0 Slots 200 ]
{code}

It is clear that rack-0 is the best cluster since its the most balanced and can potentially schedule the most executors, while rack-2 is the worst rack since rack-2 is depleted of cpu resource thus rendering it unschedulable even though there are other resources available.

We first calculate the resource availability percentage of all the racks for each resource by computing:
{code}
(resource available on rack) / (resource available in cluster)
{code}

We do this calculation to normalize the values otherwise the resource values would not be comparable.

So for our example:
{code}
rack-3 Avail [ CPU 0.819672131147541% MEM 48.78048780487805% Slots 20.0% ] effective resources: 0.00819672131147541
rack-2 Avail [ 0.0% MEM 19.51219512195122% Slots 20.0% ] effective resources: 0.0
rack-4 Avail [ CPU 50.0% MEM 2.4390243902439024% Slots 20.0% ] effective resources: 0.024390243902439025
rack-1 Avail [ CPU 16.39344262295082% MEM 9.75609756097561% Slots 20.0% ] effective resources: 0.0975609756097561
rack-0 Avail [ CPU 32.78688524590164% MEM 19.51219512195122% Slots 20.0% ] effective resources: 0.1951219512195122
{code}

The effective resource of a rack, which is also the subordinate resource, is computed by: 
{code}
MIN(resource availability percentage of {CPU, Memory, # of free Slots}).
{code}
Then we order the racks by the effective resource.

Thus for our example:
{code}
Sorted rack: [rack-0, rack-1, rack-4, rack-3, rack-2]
{code}
Also to deal with the presence of failures, if a topology is partially scheduled, we find the rack with the most scheduled executors for the topology and we try to schedule on that rack first.

Thus for the sorting for racks. We first sort by the number of executors already scheduled on the rack and then by the subordinate resource availability.","Duplicated Code, Long Method, , , "
"   Move Method,Inline Method,Move Attribute,",Kinesis Spout As Storm is increasingly used in Cloud environments. It will great to have a Kinesis Spout integration in Apache Storm.,", , , , "
"   Rename Method,Extract Method,","One topology can't use hdfs spout to read from two locations The hdfs uri is passed using config:
{code}
conf.put(Configs.HDFS_URI, hdfsUri);
{code}
I see two problems with this approach:
1. If someone wants to used two hdfsUri in same or different spouts - then that does not seem feasible.
https://github.com/apache/storm/blob/d17b3b9c3cbc89d854bfb436d213d11cfd4545ec/examples/storm-starter/src/jvm/storm/starter/HdfsSpoutTopology.java#L117-L117
https://github.com/apache/storm/blob/d17b3b9c3cbc89d854bfb436d213d11cfd4545ec/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/HdfsSpout.java#L331-L331
{code}
if ( !conf.containsKey(Configs.SOURCE_DIR) ) {
LOG.error(Configs.SOURCE_DIR + "" setting is required"");
throw new RuntimeException(Configs.SOURCE_DIR + "" setting is required"");
}
this.sourceDirPath = new Path( conf.get(Configs.SOURCE_DIR).toString() );
{code}
2. It does not fail fast i.e. at the time of topology submissing. We can fail fast if the hdfs path is invalid or credentials/permissions are not ok. Such errors at this time can only be detected at runtime by looking at the worker logs.
https://github.com/apache/storm/blob/d17b3b9c3cbc89d854bfb436d213d11cfd4545ec/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/HdfsSpout.java#L297-L297","Duplicated Code, Long Method, , "
"   Rename Class,Move Class,Push Down Method,Extract Method,Push Down Attribute,",Kafka New Client API - Support for Topic Wildcards 0,"Duplicated Code, Long Method, , , , "
"   Rename Method,Extract Method,","Topology submission improvement: support adding local jars and maven artifacts on submission This JIRA tracks actual work on below proposal / design document.

https://cwiki.apache.org/confluence/display/STORM/A.+Design+doc%3A+adding+jars+and+maven+artifacts+at+submission

Proposal discussion thread is here: http://mail-archives.apache.org/mod_mbox/storm-dev/201608.mbox/%3CCAF5108i9+tJaNZ0LgRkTMkVQEL7F+53k9uyzxcT6zhSU6OHx9Q@mail.gmail.com%3E

Let's post on discussion thread if we have any opinions / ideas on this instead of leaving comments on this issue.
","Duplicated Code, Long Method, , "
"   Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Simplify Threading Model of the Supervisor We have been trying to roll out CGROUP enforcement and right now are running into a number of race conditions in the supervisor. When using CGROUPS the timing of some operations are different and are exposing issues that we would not see without this.

In order to make progress with testing/deploying CGROUP and RAS we are going to try and refactor the supervisor to have a simpler threading model, but likely with more threads. We will base the code off of the java code currently in master, and may replace that in the 2.0 release, but plan on having it be a part of 1.x too, if it truly is more stable.

I will try to keep this JIRA up to date with what we are doing and the architecture to keep the community informed. We need to move quickly to meet some of our company goals but will not just shove this in. We welcome any feedback on the design and code before it goes into the community.","Duplicated Code, Long Method, , , , , "
"   Move Class,Extract Method,","Support JOIN statement in Storm SQL It would be great to support JOIN statement in Storm SQL.

http://storm.apache.org/releases/1.0.1/Trident-API-Overview.html

According to this page, Trident supports 'join' across multiple spouts which is done by synchronizing spouts. 
This might be good start for Storm SQL join feature. This restricts the boundary of join to batch, but we're OK for now since aggregation is implemented via same restriction.","Duplicated Code, Long Method, , "
"   Rename Method,","Blacklist Scheduler My company has gone through a fault in production, in which a critical switch causes unstable network for a set of machines with package loss rate of 30%-50%. In such fault, the supervisors and workers on the machines are not definitely dead, which is easy to handle. Instead they are still alive but very unstable. They lost heartbeat to the nimbus occasionally. The nimbus, in such circumstance, will still assign jobs to these machines, but will soon find them invalid again, result in a very slow convergence to stable status.
To deal with such unstable cases, we intend to implement a blacklist scheduler, which will add the unstable nodes (supervisors, slots) to the blacklist temporarily, and resume them later.",", "
"   Move Class,Rename Method,Move Method,Extract Method,Move Attribute,","after supervisor v2 merge async localizer and localizer Once we mere in STORM-2018 
https://github.com/apache/storm/pull/1642 

we should look into merging the two localizers into a single class.","Duplicated Code, Long Method, , , , "
"   Move Class,Move Method,Move Attribute,","Replace Consumer of ISqlTridentDataSource with StateFactory and StateUpdater Currently ISqlTridentDataSource exposes Function as Consumer which provides only single row update. To maximize the performance, it should be changed to StateFactory (or StateSpec) and StateUpdater.

This also includes change of storm-sql-kafka.",", , , "
"   Rename Method,Extract Method,",Improve logging in trident core and examples Improve logging to make code easier to debug and extend.,"Duplicated Code, Long Method, , "
"   Move Class,Extract Method,",Introduce new sql external module: storm-sql-mongodb This issue tracks adding storm-sql-mongodb as a new sql external module.,"Duplicated Code, Long Method, , "
"   Rename Method,Move Method,","Use Calcite's implementation of Rex Compiler Looking into Calcite more, I found that Calcite has its own interpreter, and also has Rex Compiler.
In other words, we don't need to handle Rex operation by ourselves. We can just borrow this and it will provide all of what Calcite supports Rex handling.

After pulling this, we just need to make tests to check that Storm SQL provides all the functionalities on SQL reference on Calcite page: http://calcite.apache.org/docs/reference.html.
",", , "
"   Rename Class,Extract Method,",improving the current scheduling strategy for RAS 0,"Duplicated Code, Long Method, , "
"   Rename Method,Move Method,Move Attribute,","[Storm SQL] Trident mode: back to code generate and compile Trident topology Now Storm SQL just converts Rex to code block and pass to Evaluation~ class so that each class can evaluate the code block in runtime.

This change made the code greatly simplified, but I expect that it is not same as performant as compiling and execute natively.

Linq4j in Calcite provides utility methods to make the code nicely. It's going to really really more verbose, but really better than having string concatenated code block since it doesn't have any guards.

So let's convert it back to code generation, but more elegant.",", , , "
"   Move Class,Move Method,Move Attribute,",Refactor Storm Kafka Examples Into Own Modules Refactor storm-kafka-client and storm-kafka examples similarly to what was done in STORM-1970,", , , "
"   Move Method,Extract Method,Move Attribute,",Finish porting drpc to java drpc.clj is all but gone. All that remains is main and the REST API. We should finish translating it all to java.,"Duplicated Code, Long Method, , , , "
"   Move Class,Extract Method,","storm kafka client should support manual partition management. Currently storm kafka client relies on kafka to assign partition to each spout. This may cause unnecessary rebalance in cases where storm itself, e.g. worker restart, slow processing of tuples.","Duplicated Code, Long Method, , "
"   Move Class,Move And Rename Class,Rename Method,Extract Method,","Kafka Spout Refactoring to Increase Modularity and Testability Per the discussion here https://github.com/apache/storm/pull/1826 the KafkaSpout class should be split up a bit, and the unit tests should be improved to use time simulation and not break encapsulation on the spout to test.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","Abstract class ConfigurableTopology Classes which run topologies often repeat the same code and pattern to:
* populate the configuration from a file instead of ~/.storm
* determine whether to run locally or remotely
* set a TTL for a topology

Flux provides an elegant way of dealing with these but sometimes it is simpler to define a topology in Java code. 

In [StormCrawler|http://stormcrawler.net], we implemented an abstract class named ConfigurableTopology which can be extended and saves users the hassle of having to write code for the things above. I will open a PR containing this class so that we can discuss and comment whether it is of any use at all.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","CGroup Metrics Now that we have isolation through cgroups it would be really nice if we could also do some monitoring of various usage metrics from the cgroups.

For the most part this looks like reading some special files if they exist.

We should be able to read /proc/self/cgroup to see which cgroup we are a part of (or if we are in a CGroup). And then read in the config about where the CGroup is mounted storm.cgroup.hierarchy.dir

After that we can read

https://lwn.net/Articles/529927/
memory.usage_in_bytes - current usage in bytes
memory.limit_in_bytes - current limit in bytes

https://kernel.googlesource.com/pub/scm/linux/kernel/git/glommer/memcg/+/cpu_stat/Documentation/cgroups/cpu.txt

cpuacct.usage - The aggregate CPU time, in nanoseconds, consumed by all tasks in this group.
OR cpuacct.stat - aggregate user and system time consumed by tasks in this group.
The format is
user: x
system: y

NOTE: cpuacct does not need to be mounted for cgroup support to work, so if those files don't exist don't register the metric (or just don't report anything).","Duplicated Code, Long Method, , "
"   Move And Rename Class,Pull Up Method,Extract Method,Inline Method,Pull Up Attribute,","[storm-redis] Use binary type for State management Currently storm-redis provides RedisKeyValueState which uses string type of methods with additional Base64 encoding/decoding. We can change this to use binary type of methods and get rid of Base64 usages.
(Maybe need to provide a tool to convert Base64 encoded State to origin...)

There's another thing to correct: it doesn't support Redis Cluster.","Duplicated Code, Long Method, , , Duplicated Code, Duplicated Code, "
"   Rename Method,Move Method,Extract Method,","[storm-elasticsearch] switch ES client to Java REST API following documentation:
https://storm.apache.org/releases/1.0.1/storm-elasticsearch.html



https://github.com/apache/storm/blob/master/external/storm-elasticsearch/pom.xml#L40

this causes errors while writing to elastic 5.x

{code:language=java}
java.lang.NoClassDefFoundError: org/elasticsearch/common/base/Preconditions
at org.apache.storm.elasticsearch.common.EsConfig.<init>(EsConfig.java:62) ~[storm-elasticsearch-1.0.2.jar:1.0.2]
at org.apache.storm.elasticsearch.common.EsConfig.<init>(EsConfig.java:49) ~[storm-elasticsearch-1.0.2.jar:1.0.2]

Caused by: java.lang.ClassNotFoundException: org.elasticsearch.common.base.Preconditions
at java.net.URLClassLoader.findClass(URLClassLoader.java:381) ~[?:1.8.0_112]
at java.lang.ClassLoader.loadClass(ClassLoader.java:424) ~[?:1.8.0_112]
at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) ~[?:1.8.0_112]
at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ~[?:1.8.0_112]


261538 [elasticsearch[Ringleader][generic][T#2]] INFO o.e.c.transport - [Ringleader] failed to get node info for [#transport#-1][svaddi][inet[localhost/127.0.0.1:9200]], disconnecting...
org.elasticsearch.transport.ReceiveTimeoutTransportException: [][inet[localhost/127.0.0.1:9200]][cluster:monitor/nodes/info] request_id [26] timed out after [5005ms]
at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:529) ~[elasticsearch-1.6.0.jar:?]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_112]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_112]
at java.lang.Thread.run(Thread.java:745) [?:1.8.0_112]
{code}


elastic logs:
{code:language=java}
[2017-02-23T15:47:04,487][WARN ][o.e.t.n.Netty4Transport ] [Qt9qlNV] exception caught on transport layer [[id: 0x8f15e875, L:/127.0.0.1:9300 - R:/127.0.0.1:52031]], closing connection
java.lang.IllegalStateException: Received message from unsupported version: [1.0.0] minimal compatible version is: [5.0.0]
at org.elasticsearch.transport.TcpTransport.messageReceived(TcpTransport.java:1199) ~[elasticsearch-5.0.0.jar:5.0.0]
at org.elasticsearch.transport.netty4.Netty4MessageChannelHandler.channelRead(Netty4MessageChannelHandler.java:74) ~[transport-netty4-5.0.0.jar:5.0.0]
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:372) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:358) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:350) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:293) [netty-codec-4.1.5.Final.jar:4.1.5.Final]
at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:280) [netty-codec-4.1.5.Final.jar:4.1.5.Final]
at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:396) [netty-codec-4.1.5.Final.jar:4.1.5.Final]
at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:248) [netty-codec-4.1.5.Final.jar:4.1.5.Final]
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:372) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:358) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:350) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:372) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:358) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:350) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1334) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:372) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:358) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:926) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:129) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:610) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:513) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:467) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:437) [netty-transport-4.1.5.Final.jar:4.1.5.Final]
at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:873) [netty-common-4.1.5.Final.jar:4.1.5.Final]
at java.lang.Thread.run(Thread.java:745) [?:1.8.0_121]
{code}","Duplicated Code, Long Method, , , "
"   Rename Method,Move Method,Extract Method,","on-demand resource requirement scaling As a first step towards true elasticity in a storm topology we propose allowing rebalance to also modify the resource requirements for each bolt/spout in the topology. It will not be automatic, but it will let users scale up and down the CPU/memory needed for a component.","Duplicated Code, Long Method, , , "
"   Move Class,Move And Rename Class,Extract Interface,Move Method,Move Attribute,","Break down 'storm-core' to extract client (worker) artifacts There were some interests for having client side artifacts or modularize Storm.

- STORM-30
- STORM-117

I'd like to give credit to [~revans2] since idea of this issue is based on the discussion with Bobby, especially below link:
http://mail-archives.apache.org/mod_mbox/storm-dev/201703.mbox/%3C986037879.3649881.1490623807299%40mail.yahoo.com%3E

This issue will only focus to extract client from storm-core, not intended to address hiding local mode or changing base classpath for worker. So we may need to file new issues for them if it's great to address.

We have been having one huge artifact 'storm-core' for supporting all the things, and recently we extracted 'storm-drpc-server' due to dependency issue.

We also have been struggling to handle shade & relocation for lots artifacts, but in fact the artifacts which we need to relocate are just Worker related thing. So extracting worker and related classes into separate artifact and assigning different classpath in Storm cluster should help remedying.

We may still need to struggle with relocation, but the count of target artifacts should be much smaller.

Another thing we need to consider is LocalCluster, which pulls most of daemons but user topology still need to pull it. So we need to have separate artifact for LocalCluster (maybe also daemons without webapp).

Last, we need to consider about UI and Logviewer porting. Artifact 'storm-drpc-server' is not generic and I think we can group UI, Logviewer, DRPC (only http server) into this artifact. Artifact name should be changed then.",", , , Large Class, "
"   Move Method,Inline Method,Move Attribute,","Make local cluster transparent As part of the work for STORM-2441. We would like to split the storm classpath down so the client jar only has what it needs. Everything else is in a separate classpath(s) for daemon processes. One of the issues with this is that local mode is built into almost all examples because it uses a separate API from the normal storm client API.

To work around this we really should add in a new option to {{storm jar}} that will include everything on the classpath, set a SystemProperty and call into a special Main Method. The new main method will 

1) start up the LocalCluster
2) configure SotrmSubmitter, NimbusClinet and DRPCClient to talk to the LocalCluster instead.
3) run the regular main method
4) optionally sleep for a configurable amount of time
5) shut down the local cluster.",", , , , "
"   Rename Class,Rename Method,Extract Method,","Support running workers using older JVMs/storm versions As a part of STORM-2441 we are separating out the classpaths for the client+worker process from everything else in storm. This is great but it really will make some of our users upset, because it is not a rolling upgrade, and because they will need to recompile their topologies (again).

We have done a really good job in maintaining compatibility with older versions of storm because we use thrift for all communication and state storage. This means that a new supervisor and or nimbus should be able to talk to just about any existing client/worker out there. So we should explicitly support this.

We should add in config options to supervisors.

{{storm.supported.jvms}} which is a map of the version of the JVM to the JAVA_HOME path for it.

and

{{storm.legacy.worker.classpath}} which is a map of the version of storm to a CLASSPATH that can be used to launch a legacy worker process.

They should be set for all supervisors and nimbus.

Then we also add in some metadata that the client submits to nimbus along with a topology. Namely the version of the storm client they are running on and the version of the JVM they are running on.

Nimbus can then decide (possibly through another config, but probably just through convention with some config overrides) if the version of the client + JVM is compatible with the version of storm + JVM currently on the cluster. If so it should just let it through. If not it should pick a version of the JVM + storm that is compatible. If there are none available it will reject the request. We should also allow end users to set these configs.

For this to work well we need some good version matching/sorting code that is lenient, even during rolling upgrades.

For example if a user submits a topology with a 1.0.3 client to a 2.0.0 cluster. Nimbus sees that it has 1.0.3 installed great it will start running with that, but then we do a rolling upgrade of the cluster and upgrade move to 1.0.4. The supervisors should be able to launch the workers for that topology with a 1.0.4 classpath.

As such part of the worker heartbeat should also include the version of storm + JVM that the worker is running with.

We should display that on the UI and display the version it was submitted with on the UI too.

","Duplicated Code, Long Method, , "
"   Move Class,Push Down Method,Move Method,Push Down Attribute,","Remove Clojure from storm-client It would be great to remove clojure as a dependency of storm-client. We should start looking at moving as much out of storm-client as possible, as the initial separation left some things in there that should not be part of the client, but are not easy to separate just yet.",", , , , "
"   Rename Method,Extract Method,","Configs should have generics Config since the beginning has not really had generics it has just been a Map. We should really have it be consistent everywhere a {{Map<String, Object>}}

This will reduce the number of warnings in the code base by a lot.","Duplicated Code, Long Method, , "
"   Rename Method,","Configs should have generics Config since the beginning has not really had generics it has just been a Map. We should really have it be consistent everywhere a {{Map<String, Object>}}

This will reduce the number of warnings in the code base by a lot.",", "
"   Move Class,Move Method,Move Attribute,","Refactor the Storm auto credential plugins to be more usable Currently, the auto credential plugins are part of the respective external modules like storm-hdfs, storm-hbase etc. If users want to use it, they need to place the jars (storm-hdfs, storm-hbase) and its dependencies into ext lib. Currently these plugins does not accept any hadoop configuration programatically. These are set by placing config files like hdfs-site.xml in the class path and this does not scale well nor does it allow users to connect and fetch tokens from different clusters (say two different name nodes) with a single topology.

To make the auto cred plugins more usable,

1. Refactor the AutoHdfs, AutoHbase etc into a separate storm external module (say storm-autocreds). This jars along with its dependencies can be packaged and extracted to a folder like lib-autocreds which can be loaded into the class path when storm runs in secure mode (e.g. by setting STORM_EXT_CLASSPATH). The required plugins would be loaded by nimubs/workers based on the user configuration in storm.yaml.

2. Modify the plugins to accept ""configKeys"" via topology config. ""configKeys"" would be a list of string ""keys"" that the user would pass in the topology config.

{noformat}
// for hdfs
topoConf.set(""hdfsCredentialsConfigKeys"", Arrays.asList(new String[] {""cluster1Key"", ""cluster2Key""}));
// put respective config map for the config keys,
topoConf.set(""cluster1Key"", configMap1);
topoConf.set(""cluster2Key"", configMap2);

{noformat}

This way we can support credentials from multiple clusters.

3. During topology submission, nimbus invokes ""populateCredentials"". If ""configKeys"" are specified, the plugins will login to hadoop for each config key and fetch the credentials (delegation tokens) and store it with respective keys in the storm cluster state. Cluster state already stores the credentials as a Map<String, String> so no changes are needed there.

The workers will download the credentials and invoke ""populateSubject"". The plugin would populate all the credentials for all the configured ""configKeys"" into the subject. Similar steps would be performed during ""updateSubject""

4. Nimbus periodically invokes ""renew"" credentials. At this time the plugin will fetch the credentials for the configured ""configKeys"" (i.e. for the users from different clusters) and renew the respective credentials.

5. The user could specify different principal and keytab within the config key map so that the plugin will use appropriate user for logging into the respective cluster.

We also need to enhance the auto cred by adding more plugins. E.g for hbase and kafka delegation tokens which are missing currently (this could be a separate JIRAs).",", , , "
"   Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Support Shared Memory Scheduling in RAS In some cases bolt and or spouts can share memory, but the scheduler has not good way to express that. We should be able to support this.","Duplicated Code, Long Method, , , , , "
"   Rename Class,Rename Method,",Resolve checkstyle violations in storm-webapp module There are a few checkstyle violations. This issue is to clean up checkstyle violations in the module 'storm-webapp',", "
"   Extract Method,Inline Method,","Simplify KafkaSpoutConfig Some suggestions for simplifying KafkaSpoutConfig off the mailing list:

* We should not duplicate properties that users would normally set in the KafkaConsumer properties map. We should just have a setter (setProp) for setting properties in that map. For instance, setGroupId is just duplicating a setting that the user should be able to set directly in the consumer properties.

* We should get rid of the key/value deserializer setters. Setting the deserializers as classes is something the user can just as well do by using setProp. The SerializableDeserializer class should be removed. It is only offering extra type safety in the case where the user is defining their own deserializer type, and has the opportunity to subclass SerializableDeserializer. The setters don't work with the built in Kafka deserializers.","Duplicated Code, Long Method, , , "
"   Move Method,Move Attribute,","Break down Auto* classes into Nimbus plugin and Worker / Submitter plugin Current implementation of Auto* classes implements both Nimbus plugin interface and Worker/Submitter plugin interface, which makes the implementation complicated and hard to debug.

We could break down implementation into two classes for each Auto* class so that complexity would be lower.

Since it breaks backward compatibility, I'll also initiate discussion on dev@.",", , , "
"   Rename Method,","Spout throtteling metrics are unusable When helping someone debug an issue with backpressure I realized that the metrics we are collecting in the spout are mistakenly being multiplied by the rate, even though we are not sub-sampling them. This results in the values being, by default, 20 times higher then they should be. Thinking about how I would use the metrics to debug an issue also showed that some of them. skipped-max-spout and skipped-throttle correspond to about 1 ms of sleep, but skipped-inactive corresponds to about 100 ms of sleep. And the 1 ms sleep is configurable so it could be different from one topology to another, and even the code around it is pluggable, so it could be doing anything from not sleeping to sleeping a random amount of time.

I think we just need to scrap what we have been doing and record how long we sleep for and use that as the metric instead.

These metrics also don't appear to be documented anywhere so I am going to change what they mean and document them to actually be useful, and correct.",", "
"   Rename Method,",Apply new code style to storm-sql-kafka 0,", "
"   Move Method,Extract Method,Inline Method,","Kafka spout can't show acks/fails and complete latency when auto commit is enabled The storm-kafka-client spout currently emits tuples with no message ids if auto commit is enabled. This causes the ack/fail/complete latency counters in Storm UI to be 0. In some cases this is desirable because the user may not care, and doesn't want the overhead of Storm tracking tuples. [~avermeerbergen] expressed a desire to be able to use auto commit without these counters being disabled, presumably to monitor topology performance.

We should add a toggle that allows users to enable/disable tuple anchoring in the auto commit case.","Duplicated Code, Long Method, , , , "
"   Rename Method,","Update config validation check to give better information As part of submitting a topology we need to serialize the config as JSON. The check right now is rather bad. it just calls Utils.isValidConf and throws some generic Exception about problems.

https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/utils/Utils.java#L978-L980

It would be much better to have a new function/method that would check if they are equal and if not it would walk through them looking for which configs are different when it finds one it includes which configs and how they are different in the exception.",", "
"   Rename Method,","Update config validation check to give better information As part of submitting a topology we need to serialize the config as JSON. The check right now is rather bad. it just calls Utils.isValidConf and throws some generic Exception about problems.

https://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/utils/Utils.java#L978-L980

It would be much better to have a new function/method that would check if they are equal and if not it would walk through them looking for which configs are different when it finds one it includes which configs and how they are different in the exception.",", "
"   Move Method,Move Attribute,","Provide storm-kafka-client spout examples There are a few example topologies in storm-kafka-client, but trying them out as a new user requires you to modify the storm-kafka-client pom to add shading, then rebuild storm-kafka-client and copy the jar-with-dependencies into Storm's extlib. After that you can take the test jar and run the topology. I think this is needlessly complicated.

We should move the example topologies to examples/storm-kafka-client, and make the storm-kafka-client pom produce a jar with all dependencies. Since we are only including the example source with Storm distributions, I don't see a reason to try to minimize jar size at the cost of adding more steps for the user to try out the examples. storm-starter is a good example of a user friendly example test topology, since it contains all its dependencies. If we want to make the user aware that extlib can be used to reduce jar size, we can add notes and commented out provided scopes to the example pom.",", , , "
"   Rename Method,","For debugging allow users to tell the scheduler which nodes they would prefer In some cases with debugging it would be nice to let the user tell the scheduler that it wants to run on host X and not run on host Y.

This is mostly for the case where we saw an odd issue with a topology and it was running on a specific host. So to unblock the user giving them the ability to avoid a specific host is helpful, at the same time we may want to reproduce the issue and causing the topology to be scheduled on that bad node is helpful.",", "
"   Move Class,Rename Class,Inline Method,","storm-kafka-examples and storm-kafka-client-examples are difficult for new users to run The storm-kafka-examples and storm-kafka-client-examples projects configure their dependencies in a way that makes them difficult to run for a new user. The other example projects set up a provided dependency on storm-client, and otherwise include all their dependencies in a shaded jar. 

storm-kafka(-client) by default produce jars without several necessary dependencies, e.g. the Kafka client libraries. The provided.scope Maven parameter was intended to be used to allow users to produce a shaded jar with all dependencies, but if provided scope is set to compile, the resulting jar will also contain storm-client. This prevents the jar from running on a real cluster.

While users can work around this by producing the slim jar and using --artifacts when submitting the topology, this is unnecessarily tedious. We should just produce a fat jar by default, then mention in the example documentation that --artifacts is there for users that want to make slimmer jars.

Edit:
This issue now includes simplifying storm-kafka-examples and storm-kafka-client-examples in general. The examples demonstrate use of State and DRPC when the focus should be on how to use storm-kafka(-client). It also causes the modules to have some undesirable dependencies, e.g. they both depend on storm-starter.",", , "
"   Rename Method,","Better load generation testing tools We have some tools that test load generation, but it would be nice to standardize it a bit and clean things up.",", "
"   Rename Method,Inline Method,","Better load generation testing tools We have some tools that test load generation, but it would be nice to standardize it a bit and clean things up.",", , "
"   Move Class,Move Method,Move Attribute,","Better load generation testing tools We have some tools that test load generation, but it would be nice to standardize it a bit and clean things up.",", , , "
"   Rename Method,Push Down Method,Move Method,Extract Method,Inline Method,Move Attribute,","Let users indicate if a worker should restart on blob download Some blobs (like jar files) really should be tied to the life cycle of a worker. If a new blob is ready the worker should be restarted. Otherwise there is no way to pick up the contents of the newly downloaded blob.

STORM-2438 already sets the ground work for this.","Duplicated Code, Long Method, , , , , , "
"   Rename Method,",Refactor storm-kafka-client KafkaSpout Processing Guarantees 0,", "
"   Move Method,Extract Method,",Refactor partial key grouping for greater flexibility My organization needed behavior similar to the PartialKeyGrouping. This is a port of our refactoring which makes it possible to use the PartialkeyGrouping in new ways. The simplest such usecase would be to send tuples to one out of N downstream instances instead of always one out of two.,"Duplicated Code, Long Method, , , "
"   Move Class,Rename Method,","Clean up RAS and remove possible loops The code for RAS is rather complex, and we have found that if there is a mismatch between the priority strategy and the eviction strategy that it can result in infinite loops. To make this simpler there really should just be one strategy to prioritize all topologies. Scheduling happens for the highest priority topologies and eviction happens for the lowest priority ones.",", "
"   Rename Method,","Integration test should shut down topologies immediately after the test The integration test kills topologies with the default 30 second timeout. This is unnecessary and delays the following tests, because the killed topology is still occupying worker slots. 

When the integration test kills topologies, it tries sending the kill message to Nimbus once, and may fail quietly. This breaks following tests, because the default Storm install has only 4 worker slots, and the test topologies each take up 3. When a topology is not shut down, it prevents the following topologies from being assigned.",", "
"   Move Method,Extract Method,Move Attribute,","Clean up RAS resource Map. Under the new Generic RAS code we use a Map<String, Number> or Map<String, Double> for the resources. This is really inefficient and we should look at normalizing the Maps into an array, which will be faster, and hopefully will make the code cleaner.","Duplicated Code, Long Method, , , , "
"   Rename Method,","Add Kerberos support to Solr bolt Update Solr bolt to work with Kerberized Solr clusters. 
Instructions for the SolrJ clients are here: 
https://lucene.apache.org/solr/guide/6_6/kerberos-authentication-plugin.html#KerberosAuthenticationPlugin-UsingSolrJwithaKerberizedSolr",", "
"   Move Method,Extract Method,Inline Method,","Some improvements for storm-rocketmq module * Upgraded RocketMQ version to 4.2.0 which brings improvements and new features like batch sending 
* Imporved retry policy for RocketMQ consumer push mode to avoid data loss in some scenes 
* Batch sending supported for bolt and trident state 
* Allow running several consumer instances in one process, that is to say, different topics in one worker is possible 

 ","Duplicated Code, Long Method, , , , "
"   Pull Up Method,Move Method,Move Attribute,","New Metrics Reporting API - for 2.0.0 This is a proposal to provide a new metrics reporting API based on [Coda Hale's metrics library | http://metrics.dropwizard.io/3.1.0/] (AKA Dropwizard/Yammer metrics). 

h2. Background 
In a [discussion on the dev@ mailing list | http://mail-archives.apache.org/mod_mbox/storm-dev/201610.mbox/%3cCAGX0URh85NfH0Pbph11PMc1oof6HTycjCXSxgwP2nnofuKq0pQ@mail.gmail.com%3e] a number of community and PMC members recommended replacing Storm’s metrics system with a new API as opposed to enhancing the existing metrics system. Some of the objections to the existing metrics API include: 

# Metrics are reported as an untyped Java object, making it very difficult to reason about how to report it (e.g. is it a gauge, a counter, etc.?) 
# It is difficult to determine if metrics coming into the consumer are pre-aggregated or not. 
# Storm’s metrics collection occurs through a specialized bolt, which in addition to potentially affecting system performance, complicates certain types of aggregation when the parallelism of that bolt is greater than one. 


In the discussion on the developer mailing list, there is growing consensus for replacing Storm’s metrics API with a new API based on Coda Hale’s metrics library. This approach has the following benefits: 

# Coda Hale’s metrics library is very stable, performant, well thought out, and widely adopted among open source projects (e.g. Kafka). 
# The metrics library provides many existing metric types: Meters, Gauges, Counters, Histograms, and more. 
# The library has a pluggable “reporter” API for publishing metrics to various systems, with existing implementations for: JMX, console, CSV, SLF4J, Graphite, Ganglia. 
# Reporters are straightforward to implement, and can be reused by any project that uses the metrics library (i.e. would have broader application outside of Storm) 

As noted earlier, the metrics library supports pluggable reporters for sending metrics data to other systems, and implementing a reporter is fairly straightforward (an example reporter implementation can be found here). For example if someone develops a reporter based on Coda Hale’s metrics, it could not only be used for pushing Storm metrics, but also for any system that used the metrics library, such as Kafka. 

h2. Scope of Effort 
The effort to implement a new metrics API for Storm can be broken down into the following development areas: 

# Implement API for Storms internal worker metrics: latencies, queue sizes, capacity, etc. 
# Implement API for user defined, topology-specific metrics (exposed via the {{org.apache.storm.task.TopologyContext}} class) 
# Implement API for storm daemons: nimbus, supervisor, etc. 

h2. Relationship to Existing Metrics 
This would be a new API that would not affect the existing metrics API. Upon completion, the old metrics API would presumably be deprecated, but kept in place for backward compatibility. 

Internally the current metrics API uses Storm bolts for the reporting mechanism. The proposed metrics API would not depend on any of Storm's messaging capabilities and instead use the [metrics library's built-in reporter mechanism | http://metrics.dropwizard.io/3.1.0/manual/core/#man-core-reporters]. This would allow users to use existing {{Reporter}} implementations which are not Storm-specific, and would simplify the process of collecting metrics. Compared to Storm's {{IMetricCollector}} interface, implementing a reporter for the metrics library is much more straightforward (an example can be found [here | https://github.com/dropwizard/metrics/blob/3.2-development/metrics-core/src/main/java/com/codahale/metrics/ConsoleReporter.java]. 

The new metrics capability would not use or affect the ZooKeeper-based metrics used by Storm UI. 

h2. Relationship to JStorm Metrics 
[TBD] 

h2. Target Branches 
[TBD] 

h2. Performance Implications 
[TBD] 

h2. Metrics Namespaces 
[TBD] 

h2. Metrics Collected 
*Worker* 
|| Namespace || Metric Type || Description || 

*Nimbus* 
|| Namespace || Metric Type || Description || 

*Supervisor* 
|| Namespace || Metric Type || Description || 

h2. User-Defined Metrics 
[TBD] 
",", , , Duplicated Code, "
"   Rename Method,Inline Method,","Remove enable.auto.commit support from storm-kafka-client The enable.auto.commit option causes the KafkaConsumer to periodically commit the latest offsets it has returned from poll(). It is convenient for use cases where messages are polled from Kafka and processed synchronously, in a loop. 

Due to https://issues.apache.org/jira/browse/STORM-2913 we'd really like to store some metadata in Kafka when the spout commits. This is not possible with enable.auto.commit. I took at look at what that setting actually does, and it just causes the KafkaConsumer to call commitAsync during poll (and during a few other operations, e.g. close and assign) with some interval. 

Ideally I'd like to get rid of ProcessingGuarantee.NONE, since I think ProcessingGuarantee.AT_MOST_ONCE covers the same use cases, and is likely almost as fast. The primary difference between them is that AT_MOST_ONCE commits synchronously. 

If we really want to keep ProcessingGuarantee.NONE, I think we should make our ProcessingGuarantee.NONE setting cause the spout to call commitAsync after poll, and never use the enable.auto.commit option. This allows us to include metadata in the commit.",", , "
"   Rename Method,Extract Method,","Upgrade ZK instance for security It would be great to have the ability to move an existing cluster with it's ZK from insecure to secure without wiping everything clean.  This does not allow for a rolling upgrade because the running topologies will not have the credentials that they need, but you don't need to do the manual step of deleting the root ZK node.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,",RAS scheduling performance improvements Even after fixing a lot of the loops in RAS scheduling there is still more we can do to make the performance even better.  This is especially true for the generic resource aware strategy.,"Duplicated Code, Long Method, , "
"   Push Down Method,Push Down Attribute,Move Attribute,","Extend metrics on supervisor, workers, and DRPC This patch serves to extend metrics on supervisor and worker. Currently the following metrics are being implemented, including but not limited to: 

Worker: 
# Kill Count by Category - Assignment Change/HB too old/Heap Space 
# Time spent in each state 
# Time to Actually Kill worker (from identifying need by supervisor and actual change in the state of the worker) - per worker? 
# Time to start worker for topology from reading assignment for the first time. 
# Worker cleanup Time/Worker cleanup Retries 
# Worker Suicide Count - category: internal error or Assignment Change 

Supervisor: 
# Supervisor restart Count 
# Blobstore (Request to download time) 
- # Download time individual blob (inside localizer) localizer gettting requst to actually download hdfs request to finish 
- # Download rate individual blob (inside localizer) 
- # Supervisor localizer thread blob download - how long (outside localizer) 
# Blobstore Update due to Version change Cnts 
# Blobstore Storage by users 

DRPC: 
# Avg/Max Time to respond to Http Request 

There might be more metrics added later. 

This patch will also refactor code in relevant files. Bugs found during the process will be reported in other issues and handled separately.",", , , , "
"   Rename Method,","Refactoring methods in components for Supervisor and DRPC This is a supplement issue page to STORM-3099, separating out refactoring work from metrics addition. 

A few misc bug discovered during refactoring have been incorporate in this issue as well. See links for more information.",", "
"   Move Method,Move Attribute,","Add support for JUnit 5 tests I think it would be nice if we could use the new JUnit 5 APIs for testing. Since JUnit 5 can run JUnit 4 tests, it shouldn't be too much work to add support.",", , , "
"   Rename Method,Inline Method,",Improve Gauge Registration in StormMetricsRegistry Make #registerGauge and #registerProvidedGauge generic and clean up other code.,", , "
"   Rename Method,Extract Method,","Fix cascading Storm failure by improving reconnection strategy and buffering messages _Note: The original title of this ticket was: ""Add Option to Config Message handling strategy when connection timeout""._


This is to address a [concern brought up|https://github.com/apache/incubator-storm/pull/103#issuecomment-43632986] during the work at STORM-297:

{quote}
[~revans2] wrote: Your logic makes since to me on why these calls are blocking. My biggest concern around the blocking is in the case of a worker crashing. If a single worker crashes this can block the entire topology from executing until that worker comes back up. In some cases I can see that being something that you would want. In other cases I can see speed being the primary concern and some users would like to get partial data fast, rather then accurate data later.

Could we make it configurable on a follow up JIRA where we can have a max limit to the buffering that is allowed, before we block, or throw data away (which is what zeromq does)?
{quote}

If some worker crash suddenly, how to handle the message which was supposed to be delivered to the worker?

1. Should we buffer all message infinitely?
2. Should we block the message sending until the connection is resumed?
3. Should we config a buffer limit, try to buffer the message first, if the limit is met, then block?
4. Should we neither block, nor buffer too much, but choose to drop the messages, and use the built-in storm failover mechanism? 

","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,",Add kafka trident state so messages can be sent to kafka topics Currently storm has a bolt for writing to kafka but we have no implementation of trident state. We need a trident state implementation that allows wrting tuples directly to kafka topics as part of trident topology.,", "
"   Rename Method,Extract Method,Push Down Attribute,",add storm-jdbc to list of external connectors. There have been several questions in the apache mailing list around how to use storm to write tuples to a relational database. Storm should add a jdbc connector to its list of external connectors that has a general implementation to insert data into relational dbs as part of a topology.,"Duplicated Code, Long Method, , , "
"   Rename Method,",add storm-jdbc to list of external connectors. There have been several questions in the apache mailing list around how to use storm to write tuples to a relational database. Storm should add a jdbc connector to its list of external connectors that has a general implementation to insert data into relational dbs as part of a topology.,", "
"   Rename Method,",Storm Trident support for sliding/tumbling windows 0,", "
"   Move Class,Rename Class,Rename Method,","[storm-redis] Add basic lookup / persist bolts Currently storm-redis provides AbstractRedisBolt for normal (not Trident) Bolt.

Jedis is easy to use so it may be enough, but we can also provide implementations of AbstractRedisBolt for simple usage.
eg. store (key, value) pair, get key's value

Since Redis has various data types and commands, we can't cover whole things, but seems like below things could be considered.

|| Type || Read || Write ||
| STRING | GET (key) | SET (key, value) |
| HASH | HGET (key, field) | HSET (key, field, value) |
| LIST | LPOP (key) | RPUSH (key, value) |
| SET | SCARD (key) | SADD (key, member) |
| SORTED SET | ZSCORE (key, member) | ZADD (key, score, member) |
| HLL (HyperLogLog) | PFCOUNT (key) | PFADD (key, element) |

Btw, since we will normally get key & value from tuple (as most external module did), HASH, SET, SORTED SET needs additional key to process.",", "
"   Rename Method,","Storm ElasticSearch connector It would be nice to provide storm driver for elasticsearch, just like it does for hive, redis and so on.",", "
"   Rename Method,Extract Method,Move Attribute,",Storm Solr connector Storm solr connector should provide bolt and trident implementation to allow users to index data coming through the topology into solr.,"Duplicated Code, Long Method, , , "
"   Move Method,Extract Method,","Add priorities and per user resource guarantees to Resource Aware Scheduler In a multi-tenant environment we would like to be able to give individual users a guarantee of how much CPU/Memory/Network they will be able to use in a cluster. We would also like to know which topologies a user feels are the most important to keep running if there are not enough resources to run all of their topologies.

Each user should be able to specify if their topology is production, staging, or development. Within each of those categories a user should be able to give a topology a priority, 0 to 10 with 10 being the highest priority (or something like this).

If there are not enough resources on a cluster to run a topology assume this topology is running using resources and find the user that is most over their guaranteed resources. Shoot the lowest priority topology for that user, and repeat until, this topology is able to run, or this topology would be the one shot. Ideally we don't actually shoot anything until we know that we would have made enough room.

If the cluster is over-subscribed and everyone is under their guarantee, and this topology would not put the user over their guarantee. Shoot the lowest priority topology in this workers resource pool until there is enough room to run the topology or this topology is the one that would be shot. We might also want to think about what to do if we are going to shoot a production topology in an oversubscribed case, and perhaps we can shoot a non-production topology instead even if the other user is not over their guarantee.","Duplicated Code, Long Method, , , "
"   Move Method,Extract Method,","Add priorities and per user resource guarantees to Resource Aware Scheduler In a multi-tenant environment we would like to be able to give individual users a guarantee of how much CPU/Memory/Network they will be able to use in a cluster. We would also like to know which topologies a user feels are the most important to keep running if there are not enough resources to run all of their topologies.

Each user should be able to specify if their topology is production, staging, or development. Within each of those categories a user should be able to give a topology a priority, 0 to 10 with 10 being the highest priority (or something like this).

If there are not enough resources on a cluster to run a topology assume this topology is running using resources and find the user that is most over their guaranteed resources. Shoot the lowest priority topology for that user, and repeat until, this topology is able to run, or this topology would be the one shot. Ideally we don't actually shoot anything until we know that we would have made enough room.

If the cluster is over-subscribed and everyone is under their guarantee, and this topology would not put the user over their guarantee. Shoot the lowest priority topology in this workers resource pool until there is enough room to run the topology or this topology is the one that would be shot. We might also want to think about what to do if we are going to shoot a production topology in an oversubscribed case, and perhaps we can shoot a non-production topology instead even if the other user is not over their guarantee.","Duplicated Code, Long Method, , , "
"   Rename Method,",provide a feature to declare TestCases as 'notYetImplemented' and thus expected to fail provide notYetImplemented() as in HtmlUnit and use it right away for the Groovy build itself,", "
"   Rename Class,Rename Method,",provide a feature to declare TestCases as 'notYetImplemented' and thus expected to fail provide notYetImplemented() as in HtmlUnit and use it right away for the Groovy build itself,", "
"   Rename Method,",provide a feature to declare TestCases as 'notYetImplemented' and thus expected to fail provide notYetImplemented() as in HtmlUnit and use it right away for the Groovy build itself,", "
"   Rename Method,",provide a feature to declare TestCases as 'notYetImplemented' and thus expected to fail provide notYetImplemented() as in HtmlUnit and use it right away for the Groovy build itself,", "
"   Rename Method,",provide a feature to declare TestCases as 'notYetImplemented' and thus expected to fail provide notYetImplemented() as in HtmlUnit and use it right away for the Groovy build itself,", "
"   Rename Method,Extract Method,",provide a feature to declare TestCases as 'notYetImplemented' and thus expected to fail provide notYetImplemented() as in HtmlUnit and use it right away for the Groovy build itself,"Duplicated Code, Long Method, , "
"   Rename Method,","Implement groupBy() for Maps as well as for Lists The method groupBy() works only for lists. It should work for Maps as well.

The code for this functionality and the respective test is commented out in DefaultGroovyMethods and GroovyMethodsTest.

public static Map groupBy(Map self, Closure closure) {
final Map answer = new HashMap();
for (final Iterator iter = self.entrySet().iterator(); iter.hasNext();) {
groupCurrentElement(closure, answer, iter);
}
return answer;
}
",", "
"   Rename Method,","SQL pipeline improvement groovy.sql.Sql is very usefull for SQL pipeline as 
{code}
/* Fetch Person */
sql.eachRow('select * from person') { person -> 
/* Fetch unit */
...process person data ...
if ( person.status == 'active' ) {
sql.eachRow('select * from unit where unit_id = ? and seq = ?', [ person.unit , ] ) { unit ->
... process unit data ...
/* For each Job, fetchs job description */

[ person.job0, person.job1, person.job2].each { jobDes -> 
sql.eachRow('select * from job where job_id = ? , [ jobDes ] ) {
... process job description ...
}
} else {
....
}
}
{code}

But in groovy.sql.Sql#eachRow(String, List, Closure) preparedStatement is *ever evaluated* at each _iteration_. It would be very easy to add a PreparedStatement Cache feature to improve speed and avoid useless evaluation.
Maybe could we create a new top wrapping statement as 'persistent' for example :
{code}
sql.persistent() {
sql.eachRow('select * from person') { person -> 
....
}
}
{code}
as 
{code}
public void persistent(Closure clos) {
setPersistent(true)
clos.call();
setPersistent(false);
closeResources();
}
{code}

Therefore we could have 
{code}
public void eachRow(String sql, List params, Closure closure) throws SQLException {
Connection connection = createConnection();
PreparedStatement statement = null;
ResultSet results = null;
try {
log.fine(sql);
if ( isPersistent() ) {
statement = (PreparedStatement) cache.get(sql);
if ( statement == null ) {
statement = connection.prepareStatement(sql);
cache.put(sql, statement);
}
} else {
statement = connection.prepareStatement(sql);
}
...
{code}
",", "
"   Rename Method,Extract Method,",provide security sandbox for executing scripts It would be great to have a secure sandbox in which scripts could be evaluated; restricting both the kinds of scripts available and the packages that can be used,"Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,","Expose unary plus and unary minus operators as interceptable method calls: positive() and negative() I'm writing a Groovy builder for Compass (Lucene) queries, and it would be nice to use + and - in the closure, since it mirrors Lucene's own query string syntax, eg:

{code}
def query = builder.build {
+term('keywords', 'book')
-term('keywords', 'audio')
queryString('color theory')
} 
{code}

would create a query that in Lucene query string syntax looks like *+keywords:book -keywords:audio all:color all:theory*.

(Incidentally this query means: 'keywords' MUST have the value 'book' and MUST NOT have the value 'audio' and 'all' SHOULD (may or may not) have 'color' and 'theory'.)

Ok they don't mirror each other exactly, but for those familar with Lucene query string syntax I think this will just feel ""right"".

So can these + and - operators be made interceptable method calls?

I think the opinion of the list was to use positive() and negative() as the method names: http://www.nabble.com/use-of-%2B-and---in-DSL--tf3623677.html
",", "
"   Rename Method,","force SwingBulder to execute builder methods in EDT to avoid problems with GUIs and to elt the user have a simple way of creating GUIs wihtout thinking about the EDT, SwingBuilder should invoke all builder methods (frame, dialog, ...) in the EDT.",", "
"   Extract Method,Inline Method,","SimpleTemplateEngine (and poentially other TemplateEngines) should allow caller to specify classloader SimpleTemplateEngine doesn't allow the caller to specify a parent classloader. When it creates a GroovyShell, it just uses the loader for that class. 

{code}
Index: src/main/groovy/text/SimpleTemplateEngine.java
===================================================================
--- src/main/groovy/text/SimpleTemplateEngine.java (revision 6601)
+++ src/main/groovy/text/SimpleTemplateEngine.java (working copy)
@@ -69,8 +69,14 @@
}
public Template createTemplate(Reader reader) throws CompilationFailedException, IOException {
+ return createTemplate(GroovyShell.class.getClassLoader(), reader);
+ }
+
+ public Template createTemplate(ClassLoader parentLoader, Reader reader)
+ throws CompilationFailedException, IOException
+ {
SimpleTemplate template = new SimpleTemplate();
- GroovyShell shell = new GroovyShell();
+ GroovyShell shell = new GroovyShell(parentLoader);
String script = template.parse(reader);
if (verbose) {
System.out.println(""\n-- script source --"");
{code}

Other template engines should have consistent features with this enhancement.
","Duplicated Code, Long Method, , , "
"   Move Method,Extract Method,",enum support allow the definition of enums in groovy,"Duplicated Code, Long Method, , , "
"   Move Method,Extract Method,","Groovy should have a transpose() method Groovy should have a transpose method like:

Ruby's transpose() method on arrays
Python's zip() method

expected behaviour:

assert [['a', 'b'], [1, 2, 3]].transpose() == [['a', 1], ['b', 2]]
assert transpose(['a', 'b'], [1, 2, 3]) == [['a', 1], ['b', 2]]
assert transpose([1, 2, 3], [4, 5, 6]) == [[1, 4], [2, 5], [3, 6]]
assert transpose([1, 2, 3], [4, 5], [9], [6, 7, 8]) == [[1, 4, 9, 6]]
assert transpose([1, 2, 3]) == [[1], [2], [3]]
assert transpose([]) == []
","Duplicated Code, Long Method, , , "
"   Rename Class,Extract Method,","DGM methods on object for min, max, sum, and perhaps unique and sort As discussed here:

http://www.nabble.com/groovy-equivalent-of-python-zip-function--tf4209158.html
","Duplicated Code, Long Method, , "
"   Rename Method,",rename map methods to collect to be more consistent map() could be reserved for a method which creates a Map. Using 'collect' means we're consistent with Ruby & Smalltalk,", "
"   Rename Method,","Allow JUnit 4 tests to be run by the groovy command Groovy allows JUnit 3.8.x tests and GroovyTestCases to be invoked from the command line. It would be good to also support JUnit 4 tests in this way. The attached patch does this.

Pros and Cons:
(-) It 'simply' saves the '{{JUnitCore.main(MyTest.name)}}' added to the bottom of a script which manually invokes the runner. So we are adding code to the Groovy codebase for only a small saving
(+) It does allow the script to be left as a test class that IDEs would run using their runners and doesn't require them to be polluted with the {{JUnitCore}} manual runner code which makes no sense to a IDE runner
(+) Given that Groovy 1.5 is offering Java 1.5 support, it does round out nicely JUnit 4 support which is often used in the jump to Java 1.5 and hence improves the case for using Groovy as your testing language for Java 1.5 projects (e.g. making TDD and BDD easier out of the box)
(-) Are we showing favouritism vs other alternatives, e.g. TestNG
(+) We already have a testng runner but no direct JUnit runner

I am happy to apply it but wanted to attach it here so that discussion can occur around whether it should be a 1.5.2 or 1.6 feature (or not included).

Paul.
",", "
"   Extract Method,Inline Method,","Numeric literals: default BigDecimal for decimals, support suffixes This issue is from the recent groovy-dev list discussion concering the use of BigDecimal as the default numeric type for literals with a decimal point. Also, numeric literal suffixes should be supported to specify the type.

In the discussion, James Strachan said:
-----------------------------------------------------------------
* BigDecimal is the default type for any floating point number 
literals. For non-decimals we can choose the best size (Integer, Long, 
BigInteger based on the length of the number)

* folks can use B (big decimal) postfix to be explict or use D (double) 
or F (float) for more explicit control.

* to call typesafe methods of lower type (double, float) then you have 
to either use the doubleValue() or floatValue() methods or cast.

foo(float p) { ... }

x = 1.234
foo( (float) x )
foo( x.floatValue() )

unless

x = 1.234F
foo(x)
------------------------------------------------------------------

Concerning:
>For non-decimals we can choose the best size (Integer, Long, 
>BigInteger based on the length of the number)

Should we automatically choose the size or use suffixes (or both)?
B BigInteger (also means BigDecimal for literals with a dot)
I Integer
L Long","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,",DataSet's should allowing sorting 0,"Duplicated Code, Long Method, , "
"   Rename Method,Move Method,Extract Method,Pull Up Attribute,Move Attribute,","Create @Identity annotation to autogenerate equals() / hashCode() / toString() The new @Bindable annotation is great. It (further) reduces the amount of boilerplate code required in Groovy beans. The next logical step is an @Identity annotation that can be applied to one or more attributes, telling Groovy that those properties define the identity / business key / whatever-you-want-to-call-it of the bean. The object's equals(), hashCode() and toString() would behave accordingly.

May be related to GROOVY-27, but I'm not sure (was marked Won't Fix).","Duplicated Code, Long Method, , , , Duplicated Code, "
"   Rename Method,",Groovy should provide an @Immutable annotation to make creating immutable objects easy 0,", "
"   Rename Method,",Groovy should provide an @Immutable annotation to make creating immutable objects easy 0,", "
"   Rename Method,Extract Method,","Ability to create static groovy methods Currently, it's only possible to create instance groovy methods.
Such as 
file = new File(""text.txt"")
file.eachLine{ line | println it }
But we cannot currently add static methods, such as (GROOVY-260):
thread = Thread.start { 
process = ""some long running process"".execute(); 
process.waitFor(); 
}","Duplicated Code, Long Method, , "
"   Rename Method,","Add stripMargin() to multi-line strings As explained in the following thread http://www.nabble.com/GString.stripMargin---td21679146.html Scals provides a stripMargin() method to its multi-line strings which is very helpful in overall situations. For something fancier not covered by the default behavior the user will have to roll its own impl.

Link to Scala api docs
http://www.scala-lang.org/docu/files/api/scala/runtime/RichString.html#stripMargin",", "
"   Rename Method,Extract Method,","Groovy Support for annotations on package declarations and imports It would be good to support annotations for package definitions and import statements.
It would allow scripts like this:
{code}
@Grab(group='commons-lang', module='commons-lang', version='2.4')
package foo

@Grab(group='com.google.collections', module='google-collections', version='1.0-rc2')
import com.google.common.collect.HashBiMap
import static org.apache.commons.lang.WordUtils.*

def fruit = [grape:'purple', lemon:'yellow', orange:'orange'] as HashBiMap
assert capitalize(fruit.inverse().yellow) == 'Lemon'
{code}
","Duplicated Code, Long Method, , "
"   Rename Method,","add Map.synchronized(), List.synchronized() methods We can do map.immutable() we should also be able to do map.synchronized() and list.synchronized().

This is just a trivial helper method for map = CollectionUtils.synchronizedMap(map) and ditto for List - it just saves mucho typing & importing.",", "
"   Pull Up Method,Pull Up Attribute,","gapi document generation ignores access modifiers set on groovydoc org.codehaus.groovy.ant.Groovydoc supports attributes like 'private'/'protected'/'public' but it seems like they are ignored in the gapi documentation generation. The documentation generated contains package-private details too, which it should not.

For more related information, see GROOVY-4135.",", Duplicated Code, Duplicated Code, "
"   Rename Method,Extract Method,","Statically imported properties Describe a class
{code}
class Foo {
static def getBar(){}
static def setBar(def bar){}
}
{code}

and a script
{code}
import static Foo.bar

print bar
print getBar()
{code}

Now 'print getBar()' throws MissingMethodException. 
IMHO 'getBar()' should be resolved to Foo.getBar() without explicit import

The same thing is with setters and aliased imports.
{code}
import static Foo.bar

setBar(2)
{code}

{code} 
import static Foo.bar as baz

setBaz(2)
print getBaz()
{code}

{code}
import static Foo.getBar

print bar
{code}

{code}
import static Foo.setBar

bar = 2
{code}","Duplicated Code, Long Method, , "
"   Rename Method,","Groovy should have a mechanism to create class-level fields in Scripts In scripts, all variables are locally defined variables within the run method of the scripts and hence aren't available in for instance method calls.

The proposed patch adds a {{@ClassScope}} annotation which ""promotes"" a variable declaration into a field declaration.",", "
"   Rename Method,Extract Method,","Power asserts should use DefaultGroovyMethods.toString() instead of 'plain' toString() DefaultGroovyMethods.toString() would lead to Groovyier output; e.g. maps would be printed as ""[a:1, b:2]"" instead of ""{a=1, b=2}"".","Duplicated Code, Long Method, , "
"   Rename Method,","Assert a closure call should look like As per the following conversation:

http://groovy.markmail.org/search/?q=Power%20assert%20question%20when%20asserting%20closure%20calls#query:Power%20assert%20question%20when%20asserting%20closure%20calls+page:1+mid:bzbavvzceem3dd2z+state:results

We should make closure calls look like method calls.

So for:
{code}
def closure = { it }
assert closure(false)
{code}

instead of:
{code}
assert closure(false)
|
ConsoleScript1$_run_closure1@575fa5
{code}

we should get:
{code}
assert closure(false)
|
false
{code}

If one really wans to see the toString() of the closure, he could still do:
{code}
assert closure.call(false)
| |
| false
ConsoleScript2$_run_closure1@f1584a
{code}

",", "
"   Rename Method,Extract Method,","To support PreparedStatement.addBatch() Create a new version of Sql.withBatch with support to prepared statements (PreparedStatement.addBatch()). For the vast majority of cases this should perform better than Sql.withBatch using statements (Statement.addBatch(String)).


I've added some patches to perform the suggested change.
The patch files were generated using ""diff -u base_file modified_file"". Groovy 1.7.5 was used as base.


Additional information here: http://groovy.329449.n5.nabble.com/Groovy-SQL-and-batch-updates-tp4339851p4339851.html

Thanks.","Duplicated Code, Long Method, , "
"   Move Method,Extract Method,","To support PreparedStatement.addBatch() Create a new version of Sql.withBatch with support to prepared statements (PreparedStatement.addBatch()). For the vast majority of cases this should perform better than Sql.withBatch using statements (Statement.addBatch(String)).


I've added some patches to perform the suggested change.
The patch files were generated using ""diff -u base_file modified_file"". Groovy 1.7.5 was used as base.


Additional information here: http://groovy.329449.n5.nabble.com/Groovy-SQL-and-batch-updates-tp4339851p4339851.html

Thanks.","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","add new URL.eachLine() method for working with URLs like File objects similarly we may want a URL.asWritable() like File too as well as

URL.getText()
URL.eachByte()
URL.withReader()

etc","Duplicated Code, Long Method, , "
"   Rename Method,","@Memoized AST Transformation for Methods The whole idea is similar to existing great @Lazy annotation, but it differs in concept: instead of being applied to fields it is applied to methods, thus providing a wider field of use. When applied to getters it serves as an alternative to @Lazy, but applied to other methods it provides what @Lazy can't. Thus it eliminates the need for heavy refactoring in certain situations, by simply letting the user add the annotation to the method.

Here is a suggestion of how it could work:
{code}@Cached
T createX() {
new T(1, 2, 3)
} 
{code}gets transformed into:
{code}
private T $createX$result
T $createX() {
new T(1, 2, 3)
}
T createX() {
T $result_local = $createX$result
if ($result_local != null)
return $result_local
else {
synchronized (this) {
if ($createX$result == null) {
$createX$result = $createX()
}
return $createX$result
}
}
}
{code}

This whole thing could be extended to cache different results of a method depending on its arguments, but it's a topic for a discussion.",", "
"   Rename Method,Extract Method,","@ToString optionally exclude fields with null values Particularly on objects with a high number of fields, having the ability to exclude fields which are null from the output of the generated toString would be a nice to have.","Duplicated Code, Long Method, , "
"   Rename Method,","Implement a ""lowest upper bound"" algorithm For type inference, it is important to determine the lowest upper bound of two types. The current implementation is called _findCommonSuperClass_ which is fine but limited. For example, if two types do not have a common superclass but implement the same interface, _findCommonSuperClass_ would return _Object_ where it could return the interface.

The idea is to replace the current implementation with a smarter one which computes the lowest upper bound (LUB) of two classes.",", "
"   Rename Method,Extract Method,","Groovy could benefit from DGM takeWhile and dropWhile methods Currently, there is a take method on collections that returns the first N items in a collection. Other languages such as scala and haskell also provide a takeWhile method, that is similar. Instead of taking a fixed number of items, it takes items until a condition, in the form of a closure, is met.

For example:

{code}
def items = [1,2,3,4,5,4,3,2,1]
def sublist = items.takeWhile { it < 5 }
assert sublist == [1,2,3,4]
{code}","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,",expression x = 5 should return 5 to outer expressions so we can do x = y = 5 0,"Duplicated Code, Long Method, , "
"   Move Method,Extract Method,","Support for DSL type checking Improve support for DSL type checking:
* allow the user to customize the behaviour of the type checker
* provide means to support static compilation
* provide higher level API for ""simple"" customizations","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Support for DSL type checking Improve support for DSL type checking:
* allow the user to customize the behaviour of the type checker
* provide means to support static compilation
* provide higher level API for ""simple"" customizations","Duplicated Code, Long Method, , "
"   Move Class,Move Attribute,","Support for DSL type checking Improve support for DSL type checking:
* allow the user to customize the behaviour of the type checker
* provide means to support static compilation
* provide higher level API for ""simple"" customizations",", , "
"   Rename Method,","Provide a mechanism to allow the type checker to know what are the expected argument types of a closure For methods like {{each}}, {{collect}}, {{times}}, ... that take a closure as an argument, the type checker doesn't have enough type information to infer what is the expected argument types of the closure. For example, in the following case:

{code}
List<String> strings = ['abc','def']
strings.collect { it.toUpperCase() }
{code}

there's not enough type information in the signature of {{collect}} for the type checker to know that the type of {{it}} is a {{String}}.

With the upcoming lambda support in JDK8, it becomes critical for us to provide such information.",", "
"   Rename Method,Extract Method,","Provide a mechanism to allow the type checker to know what are the expected argument types of a closure For methods like {{each}}, {{collect}}, {{times}}, ... that take a closure as an argument, the type checker doesn't have enough type information to infer what is the expected argument types of the closure. For example, in the following case:

{code}
List<String> strings = ['abc','def']
strings.collect { it.toUpperCase() }
{code}

there's not enough type information in the signature of {{collect}} for the type checker to know that the type of {{it}} is a {{String}}.

With the upcoming lambda support in JDK8, it becomes critical for us to provide such information.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,",The @Singleton transform should complain if other constructors are found The @Singleton transform creates a private no-arg constructor (and in fact provides an implementation which throws an exception) but it doesn't complain if other constructors are already present. Other constructors would allow the Singleton pattern to be violated and so should result in a compile-time error.,"Duplicated Code, Long Method, , "
"   Rename Method,","New Groovy JDK methods - to improve consistency From my August 2004 email:

I've been having a quick look at the Groovy JDK, i.e. DefaultGroovyMethods
and DefaultGroovyStaticMethods

As there are so many methods in these classes, it can be a little hard to
take in all at once. So I've knocked up a quick script (in groovy),
that collates
the methods against the objects to which they become attached.

hacky script -> http://javanicus.com/groovy/MungGroovySourceCode.groovy

The result is a pretty table, with the methods down the left hand side,
and the Classes to which they are attached along the top. Where a method
has been implemented, at the crossover, I have placed a small graphic,
which if you hover your cursor over, will give you a bit more detail
about the method.

http://javanicus.com/groovy/GroovyJDKCrossReference.html

I've had a quick look for methods that I thought would be defined and
from my quick inspection of abs() thru to leftShift() so far, I believe
that the following candidates are available
(this is by no means an exhaustive list...)

* Some possible missing (non static) methods

-Collection.asImmutable()-
Object.asImmutable()
Object.asSynchronized()
-Set.count()-
-Byte[].eachByte()-
-File.filterLine()-
-InputStream.filterLine()-
-List.findIndexOf()-
-Collection.flatten()-
-Date.getAt()- // to get the year etc... (In conjunction with a Calendar?)
-Reader.getText()-
-Collection.intersect()-
-BufferedWriter <<-
Object[] <<
CharSequence <<
-File <<-
-Map << // another Map-
-Process <<- // to the process.out
-Socket <<- // to the socket.out
-Object[].max()-
-Object[].min()-
Collection.minus()
Map.minus() // could compare RHS with key || value
-Object[].minus()-

-URL.newInputStream()-
-URL.newReader()-
CharSequence.padLeft()
CharSequence.padRight()
Object[].pop()

CharSequence.putAt()
Collection.putAt() // !always ordered, but we have
-Collection.getAt()-
-Date.putAt()- // e.g. easy access to components of Date
Matcher.putAt()
InputStream.readBytes()

URL.readBytes()
-URL.readLines()-
CharSequence.reverse()
-Object[].reverse()-
SortedMap.reverse()

SortedSet.reverse()
Collection.reverseEach() // like Collection.each() this could be indeterminate...
-Map.reverseEach()- // we have Map.each()...
Matcher.reverseEach()
Object.reverseEach()

Object[].reverseEach() // perhaps foo.reverseEach() should be foo.reverse().each()
Object.rightShift() // so you can do things like... foo >> log
-Object[].sort()-
-InputStream.splitEachLine()-

-URL.splitEachLine()-
OutputStream.withWriterAppend()
-BufferedWriter.write()-
-OutputStream.write()-
File.writeLine()
OutputStream.writeLine()

* and I'm not quite sure what the difference between append() and << is
supposed to be, perhaps File.append() will become File <<

* As we have {{eachByte()}}, would it not be a good idea to also have {{eachCharacter()}} for the Readers...

thanks

Jeremy.

P.S. This is based entirely on existing methods and existing Owner Objects,
would be nice to think of what other Objects and methods we could include,
(current owner objects come from java.lang.*, java.util.*,
java.util.regex.*, java.io.* and java.net.*)",", "
"   Move Method,Extract Method,Move Attribute,","indy DTT#castToType usage the indy mode has not yet a path for groovy casts. At least the checks for null and object.class==type could be done by indy to simply return object and avoid the call to castToType. castToType is poisson for the JIT, and avoiding it where possible is important.","Duplicated Code, Long Method, , , , "
"   Rename Class,Extract Method,","Improve parser speeds of Groovy JSON parser while maintaining backward compatability This was discussed on the mailing list and in private conversations.
The basic plan is to take some of the ideas in the Boon JSON parser and add them to Groovy.

Groovy will not depend on Boon. The 2 to five classes will be moved over to Groovy. 

A later effort, if approved, will move over the Java Object to JSON serialization work done in Boon (@JsonProperty, @JsonView, etc.). 

This is only a small part of the Boon JSON parsing because a large part of the Boon effort is to map JSON to Java objects, and Groovy already allows easy conversion from Maps to Java objects.

Boon style JSON parser may suite Groovy well because it has a intermediate state of Object Tree that looks like a Map but is actually a hierarchy of Value objects which are index overlay objects. 

More about Boon JSON parse can be found here:

https://github.com/RichardHightower/boon/wiki/Boon-JSON-in-five-minutes#processing-model-data-binding

Early benchmarks appear that the new Boon style JSON parser for in-memory parse would be around 2x to 5x faster than common, popular JSON parsers and 20x or so faster than existing Groovy parser. 

Some effort was made to create a large file parser bc Boon was initially just for REST/Websocket JSON consumption (streaming mode if you will). Boon created a windowing buffer parser so that it could better support this migration. Although created in the Boon project, the intended target of the windowing buffer was Groovy.

Under 2MB files will use the in-memory parser by default, about 2MB will use the windowing buffer by default. 

More details in mailing list just search for JSON Groovy Benchmark.
","Duplicated Code, Long Method, , "
"   Move Method,Extract Method,Move Attribute,","Enhance GroovyAssert for JUnit 4 Support Right now, in Groovy 2.2.1, the {{GroovyAssert}} class just offers a way to embed {{shouldFail}} in JUnit 4 tests. These methods are also used internally by {{GroovyTestCase}}. 

This issue is about making {{GroovyAssert}} an ""official"" API class for supporting JUnit 4 integration by providing a bunch of static methods that can be imported, as this is the case with {{org.junit.Assert}}. 

For that purpose, the class needs JavaDoc documentation and to be mentioned in the new Groovy language documentation. 

Plus, we would like to move more methods from {{GroovyTestCase}} into {{GroovyAssert}}. The same way that it was done with {{shouldFail}}.

So this issue consists of writing documentation for {{GroovyAssert}} and applying some refactoring on {{GroovyTestCase}}.","Duplicated Code, Long Method, , , , "
"   Rename Method,","Reflection API for Traits There needs to be some kind of reflection API for traits, so that programs can reason about traits at runtime. For example, one thing I need to know for Spock is whether a method that, according to Java reflection, is declared in class X, is actually declared in that Groovy class, or inherited from a trait.",", "
"   Move Method,Extract Method,","Align the parameters of @DelegatesTo and @ClosureParams Currently @DelegateTo allows to use the generic type of another argument of the call, but I actually need to delegate to one of the generic parameters of the enclosing class. 

I.e. my signature is: 

{code}
PipelineBuilder<T> map(
boolean passThroughNulls = true, 
@ClosureParams(value=FromString.class, options=[""T""]) Closure<T> mapper
) 
{code}

And I want the mapper closure to be able to refer to the payload through the delegate as well as the 'it' parameter. 

Unfortunately @DelegatesTo won't let me do this as it was written prior 
to @ClosureParams and it does not support things like {{FirstParam}} or {{FromString}}... 

Aligning the config params of @DeleatesTo and @ClosureParams, will make them more powerful and easier to learn.","Duplicated Code, Long Method, , , "
"   Rename Method,Pull Up Method,","@InheritConstructors should replicate annotations on super constructors I want to use Guice to inject some dependencies into a hierarchy of TestNG tests using {{@Inject}} constructor injection. Since many of these tests use exactly the same objects, {{@InheritConstructors}} would be perfect for them, but {{@InheritConstructors}} does not replicate the {{@Inject}} annotation on the superclass, causing Guice to complain about not having an injectable constructor.

Constructors created with {{@InheritConstructors}} should replicate the annotations on the constructor methods they're copying.",", Duplicated Code, "
"   Rename Method,Extract Method,","Extend @Builder InitializerStrategy to support adding the annotation to Constructors Added an AST transformation @PojoBuilder that creates for given provided Pojo builder class ""with"" methods that allow to set values on a Pojo together with the ""build"" method that allows to instantiate it.

Example:

POJO - Person

{code} 
class Person {
String firstName
String surName
}
{code} 
PersonBuilder

{code}
@PojoBuilder(forClass = Person)
class PersonBuilder {

}
{code}

Building

{code}
def person = personBuilder.withFirstName(""Robert"").withSurName(""Lewandowski"").build()
{code}

Verification

{code}
assert person.firstName == ""Robert""
assert person.surName == ""Lewandowski""
{code}


Also added building with validation closure:

{code}
try {
def person = personBuilder.withFirstName(""Robert"").build {
if (it.surName == null ){
throw new IllegalStateException()
}
}
fail(""should fail due to validation closure"")
} catch(Exception exception ) {

}
{code}

Possible extensions:

- Not only for POJOs - for other classes too (right now the object is constructed by picking the default constructor - it can be changed by picking the largest constructor and basing on the types, names etc. create proper ""with"" methods
- A ""withCustomLogic"" method that takes a closure as argument and performs custom logic on object to be built
- ImmutableBuilder that at each ""with"" method creates a new builder instead of setting values on itself

Pull request: https://github.com/groovy/groovy-core/pull/341","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,Inline Method,","Extend @Builder InitializerStrategy to support adding the annotation to Constructors Added an AST transformation @PojoBuilder that creates for given provided Pojo builder class ""with"" methods that allow to set values on a Pojo together with the ""build"" method that allows to instantiate it.

Example:

POJO - Person

{code} 
class Person {
String firstName
String surName
}
{code} 
PersonBuilder

{code}
@PojoBuilder(forClass = Person)
class PersonBuilder {

}
{code}

Building

{code}
def person = personBuilder.withFirstName(""Robert"").withSurName(""Lewandowski"").build()
{code}

Verification

{code}
assert person.firstName == ""Robert""
assert person.surName == ""Lewandowski""
{code}


Also added building with validation closure:

{code}
try {
def person = personBuilder.withFirstName(""Robert"").build {
if (it.surName == null ){
throw new IllegalStateException()
}
}
fail(""should fail due to validation closure"")
} catch(Exception exception ) {

}
{code}

Possible extensions:

- Not only for POJOs - for other classes too (right now the object is constructed by picking the default constructor - it can be changed by picking the largest constructor and basing on the types, names etc. create proper ""with"" methods
- A ""withCustomLogic"" method that takes a closure as argument and performs custom logic on object to be built
- ImmutableBuilder that at each ""with"" method creates a new builder instead of setting values on itself

Pull request: https://github.com/groovy/groovy-core/pull/341","Duplicated Code, Long Method, , , "
"   Rename Method,","@AnnotationCollector could provide more control over how annotations are added in the presence of existing explicit annotations When expanding a meta annotation alias into its annotation collection, it is sometimes useful to be able to control how annotations are added in particular in the presence of existing explicit annotations. This issue proposes adding an annotation parameter to {{@AnnotationCollection}} which lets the addition of collected annotations be controlled in more flexible ways. The following modes are proposed for @AC:

{code}
DUPLICATE: Annotations from the annotation collection will always be inserted. After all transforms have been run, it will be an error if multiple annotations (excluding those with SOURCE retention) exist.

PREFER_COLLECTOR: Annotations from the collector will be added and any existing annotations with the same name will be removed.

PREFER_EXPLICIT: Annotations from the collector will be ignored if any existing annotations with the same name are found.

PREFER_COLLECTOR_MERGED: Annotations from the collector will be added and any existing annotations with the same name will be removed but any new parameters found within existing annotations will be merged into the added annotation.

PREFER_EXPLICIT_MERGED: Annotations from the collector will be ignored if any existing annotations with the same name are found but any new parameters on the collector annotation will be added to existing annotations.
{code}
",", "
"   Rename Method,","Not public constructors for groovy.transform.Immutable anotated class groovy.transform.Immutable annotation generates only public constructors for annotated class.

I want to be able to create class with constructors with another modifiers e. g. private constructors and define static method create/build/from which will be the only way to create my immutable class.

The pull request will be provided.",", "
"   Rename Method,","Groovy TemplateServlet NPE fixed and totally revamped Hello Groovy dev team,

here is my revamped version of the TemplateServlet class. I didn't produce a diff, because I use another code formatter than the Groovy project team. You must sit in front of wide screens... :) 

Fixes, changes and improvements:

* Fixed NPE in log()-calls by calling super() in init().
* Rewrote template source file finding according to the JspServlet
found in Apache/Tomcat.
* Added ""setVariables(ServletBinding)"" method allowing extension 
to bind their own variables.
* Implemented a simple template cache using WeakHashMap.
* Javadoc'ed a lot.
* Removed 1.5 syntax... ;-)",", "
"   Extract Interface,Extract Superclass,Extract Method,","Make JUnit3/4 GroovyRunners Current {{GroovyShell}} currently has reflective methods to detect and run JUnit3 and JUnit4 classes. These methods might work god as {{GroovyRunner}} classes and they could be added to the RUNNER_REGISTRY in {{GroovySystem}}.

Runners should be able to be registered during class loading (like most other things) rather than during Grape processing.","Duplicated Code, Long Method, , Duplicated Code, Large Class, Large Class, "
"   Extract Method,Inline Method,",@Immutable annotation should be re-vamped to be a meta-annotation 0,"Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,",@Immutable annotation should be re-vamped to be a meta-annotation 0,"Duplicated Code, Long Method, , "
"   Extract Method,Move Attribute,",@Immutable annotation should be re-vamped to be a meta-annotation 0,"Duplicated Code, Long Method, , , "
"   Move Method,Extract Method,Move Attribute,","@Immutable-related transformations should be more configurable This would allow other immutability libraries to be leveraged, e.g. Guava's immutable collections. The proposal is to provide a property handler class which controls the generated code for getting, setting and initializing properties. See the preliminary documentation for @PropertyOptions for more details.","Duplicated Code, Long Method, , , , "
"   Move Class,Rename Method,Move Method,Inline Method,Move Attribute,","@Immutable-related transformations should be more configurable This would allow other immutability libraries to be leveraged, e.g. Guava's immutable collections. The proposal is to provide a property handler class which controls the generated code for getting, setting and initializing properties. See the preliminary documentation for @PropertyOptions for more details.",", , , , "
"   Move Class,Extract Method,","SqlGroovyMethods could be moved to avoid split packages - part (1) Intended to be a two step process. 
(1) move it to the new package but leave behind a @Deprecated delegating skeleton file in the old location 
(2) remove the deprecated file in a future release","Duplicated Code, Long Method, , "
"   Rename Method,Inline Method,","remove PropertyValue from Object.allProperties() and eachProperty{}, refactor methods to use a Map > Object.allProperties() and eachProperty{pv ->}
> deal with PropertyValue objects that merely encapsulate a name/value
> pair.
> This is not very consistent with Maps and the general handling of
> key/value pairs throughout the GDK.
> 
> a) name/value in PV vs. key/value in Maps
> b) eachProperty doesn't allow {key,value -> } like Maps do
> 
> Is there a reason for calling it ""all""Properties instead of ""get"" ?
> 
> Can we let it return a Map instead of a list of PropertyValue objects?
> We can even remove eachProperty and eachPropertyName then because
> obj.properties.each{}
> does the job.
> 
> any opinions?

Sounds like a wise and consistent change.
You can go for it.

--
Guillaume Laforge
",", , "
"   Move Class,Extract Superclass,",Provide size() on StringBuilder GROOVY-920 resulted in size() being implemented for StringBuffer. size() should also be implemented for StringBuilder on JDK1.5+ platforms.,", Duplicated Code, Large Class, "
"   Move Method,Move Attribute,","EntityLinking should sort Suggestions with the same score based on entity rankings Vocabularies the EntityLinkingEngine links against can define EntityRanking. An entity ranking defines the popularity (similar to page-rank) of an Entity within the knowledge base. For Wikipedia this is e.g. generated by the number of incoming links from other wikipedia pages.

While the EntityRanking is defined in the internal API of the EntityLinkingEngine it is currently not used for the score of linked entities. This is mainly because the score represent how well the label of an Entity does match the section in the processed text.

This will introduce a new option that allows for Suggestions that would have the same score (typically 1.0 - for exact matches) to get their score slightly (less than 0.1) changed so that they are ranked based on their entity ranking. For linking against wikipedia this will ensure that Paris (France) will have a slightly higher ranking as Paris (Texas, US).

Those changes MUST NOT change the order of suggestions for entities that do not have the same score.

Users will be able to enable/disable this feature via a setting of for the entity linking configuration. By default this feature will be activated.",", , , "
"   Rename Method,Extract Method,","Improvements to EntityLinking This Issue covers several improvements to the EntityLinking Engine.

The general goals of those improvements are:

* reduce the number of lookups to the Vocabulary as those consume up to 90% of the processing time
* reduce the number of results that need to be processed without loosing recall by skipping possible matches (e.g. by using better ranking)
* fix some issues that require","Duplicated Code, Long Method, , "
"   Rename Class,Extract Method,","Implement a Lucene FST based Entity Linking Engine (based on OpenSextant / SolrTextTagger) This will implement an in-memory EntityLinking EnhancementEngine based on Lucenes FST (Finite State Transducer) technology.

This engine could make direct use of the Classes contained in the OpenSextant / SolrTextTagger code [1]. This uses a two layered FST 

(1) to represent words and 
(2) to map those words to phrases

With this it is possible to efficiently hold big vocabularies in memory (> 300MByte for geonames.org). See the presentation at [2] for more details.

While the license is fully compatible (ASL 2.0) the library is currently not available on Maven Central. We need to contact the author regarding this.


[1] https://github.com/OpenSextant/SolrTextTagger
[2] http://www.lucenerevolution.org/2013/Text-Tagging-with-Finite-State-Transducers","Duplicated Code, Long Method, , "
"   Extract Method,Inline Method,","Add co-reference resolution and dependency tree support in the Stanbol NLP processing API Extend the Stanbol NLP Processing API with annotations for co-reference resolution and dependency trees.
Also, add support for JSON Serialisation/Parsing for the co-reference and dependency tree annotations so that the RESTful NLP Analysis Service can provide co-reference information.
","Duplicated Code, Long Method, , , "
"   Move And Rename Class,","Have each KReS component manage stores individually KReS has its own adapters for managing interaction with ontology stores (e.g. Clerezza TcManager, Jena TDB, in-memory, IKS persistence store). 

- eu.iksproject.kres.storage and eu.iksproject.kres.storage.provider in the old setting
- org.apache.stanbol.ontologymanager.store in Stanbol

This component should be removed as Clerezza storage (TcManager and WeightedTcProvider) should be directly interacted with by each component (ontologymanager, reengineer, rules) independently.",", "
"   Rename Method,","Add Lookup Cache to EntityLinking Engine The EntityLinkingEngine should cache results of lookups on the EntitySearchers.

Entities are often reoccurring in analyzed Documents. Because of that caching results for look upped tokens should provide considerable performance improvements as tatistics shows that ~90% of the processing time for the EntityLinking engine is contributed by the entity look-up. 

So if 20% of all Entity mentions are about reoccurring Entities the processing time should be reduced by about 18%.

The cache will use the list of search string as key and a list of returned Entities as value. The cache will only collect look-up results for the currently analyzed document. 

EntityLinking statistics will be updated to include the cache hit percentage.

This issue affects both the trunk (1.0.0-SNAPSHOT) as well as the stable 0.12 releasing branch.",", "
"   Move Class,Move And Rename Class,Move Method,Move Attribute,","Make Entityhub Serializer extendable The Serialization of the Entityhub models is implemented as JAX-RS MessageBodyWriter. Model classes to be serialized include Representation, Entity and QueryResultList. Where an Entity is constructed of two Representations (data and metadata) and QueryResultLists can include Strings (ids only), Representations or Entities.

Currently there are two implementations of model serializer: (1) an Implementation for the Entityhub specific JSON format and (2) an implementation based on Clerezza that supports the various RDF formats. Those implementations are registered to JAX-RS via the supported interfaces and the supported media types.

WIth STANBOL-1165 support for Sesame was added to the Stanbol Entityhub, but serialization of Sesame backed Representation to RDF based media formats is still done by the Clerezza serializers what is extremely inefficient as (1) the Sesame Model needs to be converted to a Clerezza backed model requiring to copy all the data field by field (2) Clerezza internally will use the Jena serializer via a Clerzza/Jena adapter.

To avoid this the Entityhub requires an extendable serializer framework that allows to register ModelWriter for specific Entityhub Model implementation. For Sesame this will be done within STANBOL-1234.

The MessageBodyWirter extension point provided by JAX-RS can not be used for this as there are no native implementations for Entity and QueryResultList. Also the use of generic types is not an option as the Entity interface does not use the same.

Because of that this issue will introduce an new extension point called ""ModelWriter"" that can be implemented by modules that need to provide native serialization support for Representation implementations. ModelWriter will be registered as OSGI services. A ModelWriterRegistry will be responsible for tracking those. Three MessageBodyWriter implementation for Representation, Entity and QueryResultList will use this registry to forward the calls of JAX-RS to the best fitting ModelWriter service.

In addition this will provide two ModelWriter implementations (1) for 'application/json' with no specific native type and (2) for the various RDF formats with native Support for the Clerezza Model implementation.

STANBOL-1234 will provide a third implementation of a ModelWriter for the Sesame model implementation.",", , , "
"   Move Method,Extract Method,","FST Linking Engine / Linkable Token Filter should consider Chunks The LinkableTokenFilter a Solr TokenFilter is used by the FST linking engine to add the TaggingAttribute (supported by the Solr Text Tagger library) to tokens that should be looked up in the FST - the vocabulary.

This implementation can be improved by taking chunks into consideration that are

* chunks representing named entities
* processable (typically Noun phrases but no Verb phrases ...) AND
* have a linkable token in the chunk OR
* have two or more matchable tokens in the chunk

All tokens in such chunks should be classified as tagable by setting the TaggingAttribute to true.","Duplicated Code, Long Method, , , "
"   Rename Method,",Upgrade to OpenNLP 1.5 The OpenNLP 1.5 release makes it easier to train new custom models.,", "
"   Move Method,Extract Method,","Add NIF 2.0 support for the nlp2rdf engine The nlp2rdf engine is based on NIF 1.0 (String Ontology). This will add a 2nd engine to this module that will output NLP results in the AnalysedText content part by following the NIF 2.0 specification [1] and especially the nif-core ontology [2]


[1] http://persistence.uni-leipzig.org/nlp2rdf/
[2] http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core/nif-core.html","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Improve FST Corpus handling and updates The Lucene FST linking engine manages FST corpora that need to be rebuild when the Solr Core has updates. Because of the every FST corpora knows the Solr version it was built for.

Currently it is checked on every call the the Lucene FST linking engine if the version of the FST corpora is still in sync with the version of the the SolrCore. If not the re-creation of the FST corpora is enqueued. However until the conpletion of this task enhancement requests are processed using an older version of the FST corpora.

The initial idea of this was to prevent long waits for huge indexes (e.g. for DBPedia the creation of the FST corpara takes more as a minute). But in reality FST corpora are typically built in seconds.

That means that in most cases it would be better to wait for the re-creation of the corpus rather than using an outdated version.

This issue will change the FST corpora management to use Futures so that components can wait for corpora to be created. The code will also use a reasonable wait time for corpora to be built.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","ReferencedSites (Entityhub) need to provide metadata about there configuration (RESTful API) Currently only the URL of the Service is available for ReferencedSites. 
This need to be extended by the following parameters:
- id
- name
- description (optional)
- offline/online mode (if this side uses a local cache or depends on an remote service
- license (the license used for the provided data)
- attribution

For some of the parameters also the Java API need to be extended.
","Duplicated Code, Long Method, , "
"   Rename Class,Move Method,Inline Method,","ReferencedSites (Entityhub) need to provide metadata about there configuration (RESTful API) Currently only the URL of the Service is available for ReferencedSites. 
This need to be extended by the following parameters:
- id
- name
- description (optional)
- offline/online mode (if this side uses a local cache or depends on an remote service
- license (the license used for the provided data)
- attribution

For some of the parameters also the Java API need to be extended.
",", , , "
"   Move Class,Rename Method,Extract Method,","Rule management must support an offline mode The ontologymanager/ontonet and rules components should provide heuristics for loading recipes and networked ontologies from an offline resource, possibly hijacking dependencies so that they resolve locally recursively.","Duplicated Code, Long Method, , "
"   Rename Class,Move Method,","Rule management must support an offline mode The ontologymanager/ontonet and rules components should provide heuristics for loading recipes and networked ontologies from an offline resource, possibly hijacking dependencies so that they resolve locally recursively.",", , "
"   Rename Method,Move Method,Move Attribute,","Rule management must support an offline mode The ontologymanager/ontonet and rules components should provide heuristics for loading recipes and networked ontologies from an offline resource, possibly hijacking dependencies so that they resolve locally recursively.",", , , "
"   Rename Method,Inline Method,Pull Up Attribute,","Extendable indexing infrastructure for the Entityhub Currently the Entityhub includes some utilities to create Indexes for dbPedia, geonames and dblp. There exists also an generic RDF indexer that is used by the dbPedia and dblp however also this implementation is not extendable and not really suitable to add features requested by issues like STANBOL-92, STANBOL-93 and STANBOL-163.

The goal is to create an infrastructure that provides an implementation of
- the indexing workflow
- configuration and initialization
and defines Interfaces that allows to plug in
- different Data Sources
- entity ranking implementations
- entity data mapper (e.g. filtering some fields, schema translations ...)
- indexing targets (the Yard that stores the indexed entities)

The existing Indexing utilities need to be moved to use the new Infrastructure",", , Duplicated Code, "
"   Move Class,Move Attribute,","Migration of CMSAdapter component CMSAdapter component previously known as Ontology Generator will be migrated to Stanbol. It mainly aims to generate an ontology using bridge definitions between content management systems and the ontology to be generated e.g create individuals using this node hierarchy of my CMS.

For the time being, it only supports CMSs implementing JCR interface. Graphical user interface of it will not be migrated because of license issues. There will be two simple RESTful resources. One of them provides submission of bridge definitions and the other one provides submission of updated items in the CMS to reflect the changes to ontology. The component will be located under /trunk/cmsadapter as decided in telco in April 20.

Next tasks will be to enrich RESTful services and then adapt the CMIS bundle previously implemented.",", , "
"   Rename Method,","Integration tests framework Create a framework for running tests at build time, by starting our runnable jars and testing them via http.",", "
"   Rename Method,",Processors should become OSGI Components Bridge processors should become OSGI components and implement Processor interface. In this way it will be possible to plug any other Processor implementation based on a specific need.,", "
"   Rename Method,",Processors should become OSGI Components Bridge processors should become OSGI components and implement Processor interface. In this way it will be possible to plug any other Processor implementation based on a specific need.,", "
"   Rename Method,",Processors should become OSGI Components Bridge processors should become OSGI components and implement Processor interface. In this way it will be possible to plug any other Processor implementation based on a specific need.,", "
"   Move Method,Extract Method,Move Attribute,","Add support for ""Named Solr configuration"" The SolrYard already supports to initialise Solr Cores based on the default configuration.
This default configuration is currently loaded from ""solr/core"" in the SolrYard bundle.

The Idea is to extend this by allowing to load ""named"" Solr Configurations.

The goal is to use the Stanbol datafileprovider service similar to the
opennlp.ner engine that also supports to load language specific configurations via the
classpath if the required files are not present in the datafile directory.

The used path to load configuration via the classpath will be
""solr/core/{name}"".

The current default config will be located at ""config/solr/core/default"" within the SolrYard bundle

Other generally useful configurations such as

* index only (no storage of values)
* entity-tagging (a configuration similar to the index currently used by the autotagger engine)

should be also included within a Stanbol distribution.","Duplicated Code, Long Method, , , , "
"   Rename Method,Extract Method,","Update JSON-LD implementation to Specification version 20110911 The current JSON-LD implementation is based on version 20110201. The implementation should be updated to support the newer specification version 20110911.

The 20110911 version is online at
http://json-ld.org/spec/ED/20110911/


","Duplicated Code, Long Method, , "
"   Move Method,Move Attribute,","Implement FactStore Implement a FactStore according to the specification at http://wiki.iks-project.eu/index.php/FactStore_Specification

This would help to make Stanbol easy to use for storing relations between entities.",", , , "
"   Move And Rename Class,","Refactor the Solr specific utilits out of the SolrYard Bundle Currently all Solr specific utilities such as

* Managing an internal EmbeddedSolrServer
* loading a SolrIndex via the DataFileProvider infrastructure
* Initializing a SolrCore based on a configuration managed by the sling-installer infrastructure

This functionality is also relevant to other (future) components such as the Stanbol Contenthub.

The mentioned functionality should become the core of a new bundle (org.apache.stanbol.commons.solr).
This Bundle will also export the SolrJ packages and therefore ease the use of Solr in other Stanbol packages.

Further developments (extensions) of this new bundle will be covered by additional Jira issues.",", "
"   Rename Method,","Add support for selective ontology library (lazy) loading When supplied an ontology registry, OntoNet currently loads all the ontologies from all the libraries referenced by this registry. This is an overkill if a user only wants to manage a single library and load it into a scope.

Registry management should be configurable to support laziness, in that the actual ontology resources are not loaded until the corresponding model is ""touched"" (e.g. a request is made for the OWL ontologies contained in a library).

Add the possibility to provide the IDs of the libraries to load and avoid loading other libraries, both in the Java and REST API.",", "
"   Move Class,Move Attribute,","Add support for selective ontology library (lazy) loading When supplied an ontology registry, OntoNet currently loads all the ontologies from all the libraries referenced by this registry. This is an overkill if a user only wants to manage a single library and load it into a scope.

Registry management should be configurable to support laziness, in that the actual ontology resources are not loaded until the corresponding model is ""touched"" (e.g. a request is made for the OWL ontologies contained in a library).

Add the possibility to provide the IDs of the libraries to load and avoid loading other libraries, both in the Java and REST API.",", , "
"   Rename Class,Rename Method,Extract Method,","Add support for selective ontology library (lazy) loading When supplied an ontology registry, OntoNet currently loads all the ontologies from all the libraries referenced by this registry. This is an overkill if a user only wants to manage a single library and load it into a scope.

Registry management should be configurable to support laziness, in that the actual ontology resources are not loaded until the corresponding model is ""touched"" (e.g. a request is made for the OWL ontologies contained in a library).

Add the possibility to provide the IDs of the libraries to load and avoid loading other libraries, both in the Java and REST API.","Duplicated Code, Long Method, , "
"   Move Class,Rename Class,Move Method,","Add support for selective ontology library (lazy) loading When supplied an ontology registry, OntoNet currently loads all the ontologies from all the libraries referenced by this registry. This is an overkill if a user only wants to manage a single library and load it into a scope.

Registry management should be configurable to support laziness, in that the actual ontology resources are not loaded until the corresponding model is ""touched"" (e.g. a request is made for the OWL ontologies contained in a library).

Add the possibility to provide the IDs of the libraries to load and avoid loading other libraries, both in the Java and REST API.",", , "
"   Rename Method,","Add support for selective ontology library (lazy) loading When supplied an ontology registry, OntoNet currently loads all the ontologies from all the libraries referenced by this registry. This is an overkill if a user only wants to manage a single library and load it into a scope.

Registry management should be configurable to support laziness, in that the actual ontology resources are not loaded until the corresponding model is ""touched"" (e.g. a request is made for the OWL ontologies contained in a library).

Add the possibility to provide the IDs of the libraries to load and avoid loading other libraries, both in the Java and REST API.",", "
"   Rename Method,Extract Method,","Make root ontology management implicit in ontology spaces The root ontology of each ontology space (e.g. {scopeid}/core/root.owl and {scopeid}/custom/root.owl) is managed as an actual OWLOntology object just like actual ontologies loaded in the scope.

However this does not bring any benefit and creates clutter when managing import statements and serialization of ontology scopes and spaces for RESTful services.

Root ontologies can be generated on-the-fly for GET services on scopes and spaces, so that import statements can be swiftly moved across spaces without worrying about reloading the whole imports closure. Also, since they do not map to actual files, we can remove .owl extensions (which should be reserved for included ontologies).

As a consequence, OntologySpace#setTopOntology() methods would no longer make sense and should be deprecated in the API, then removed altogether. On the contrary, getTopOntology() could still be used to dynamically generate root ontologies from the existing resources.","Duplicated Code, Long Method, , "
"   Extract Interface,Move Method,","Have OntoNet manage graphs in Clerezza natively. OntoNet has been managing scopes, spaces and sessions so far, by creating OWL API objects that live throughout the whole Stanbol lifecycle, and are serialized for Web Services.

The current implementation creates OWL ontology objects that persist in memory even when they are not being used. This has the advantage to provide an axiom-oriented view on RDF graphs, which is very useful for applications and users that dig OWL(2). 

However, it is also inefficient for a number of reasons:

- memory occupation with data that essentially replicate the content of the persistence layer
- no slick reaction to changes in low-level graphs. If an imported ontology is updated, or a new import is added, pre-existing axioms should be re-interpreted (e.g. for classification and other reasoning tasks). This generally doesn't happen, unless the ontologies are serialized and re-loaded in the OWL API ontology manager.
- non OWL-aware applications that use the OntoNet Java API are forced to handle OWL if they need to access RDF graphs.

An ideal solution would be to avoid loading ontologies in OWLOntologyManager objects when ontology scopes and sessions are set up. Ontology network information could be stored in the Clerezza TcManager just like every other RDF graph, and only when the content of a scope is requested as an OWL ontology it is brought up that way, to be then garbage-collected.",", , Large Class, "
"   Move Class,Rename Method,","Have OntoNet manage graphs in Clerezza natively. OntoNet has been managing scopes, spaces and sessions so far, by creating OWL API objects that live throughout the whole Stanbol lifecycle, and are serialized for Web Services.

The current implementation creates OWL ontology objects that persist in memory even when they are not being used. This has the advantage to provide an axiom-oriented view on RDF graphs, which is very useful for applications and users that dig OWL(2). 

However, it is also inefficient for a number of reasons:

- memory occupation with data that essentially replicate the content of the persistence layer
- no slick reaction to changes in low-level graphs. If an imported ontology is updated, or a new import is added, pre-existing axioms should be re-interpreted (e.g. for classification and other reasoning tasks). This generally doesn't happen, unless the ontologies are serialized and re-loaded in the OWL API ontology manager.
- non OWL-aware applications that use the OntoNet Java API are forced to handle OWL if they need to access RDF graphs.

An ideal solution would be to avoid loading ontologies in OWLOntologyManager objects when ontology scopes and sessions are set up. Ontology network information could be stored in the Clerezza TcManager just like every other RDF graph, and only when the content of a scope is requested as an OWL ontology it is brought up that way, to be then garbage-collected.",", "
"   Rename Method,","Clerezza converters to handle TripleCollection instead of MGraph OWL transformation APIs such as OWLAPIToClerezzaConverter transform to and from MGraph objects. This is unnecessary as there is no need to modify graphs internally and developers might have to obtain MGraph objects just for the sake of passing them to this API.

Replacing MGraph with TripleCollection to accommodate Graph and MGraph alike will make the transformation API more versatile.",", "
"   Move Method,Extract Method,Move Attribute,","Long term operations (Jobs) for reasoning services The current implementation of the reasoning services execute the operation in real time. 
Reasoning tasks are often time consuming, and in some cases cannot be completed in the lifetime of a http request.
For this reason, there is the need of a way of running reasoning operations in the background, and a web service to ping the running operation and retrieve the result, when ready.","Duplicated Code, Long Method, , , , "
"   Move Class,Move Method,Extract Method,Move Attribute,","Make SolrCore(s) and CoreContainer available as OSGI services Provide SolrCores and CoreContainer also as OSGI services so that other components that depend on such services can be implemented more efficiently.

The main advantage of this approach would be the ability to react on changes to SolrCores (e.g. if an index was updated with new data).
In addition this feature will be also required for the implementation of STANBOL-353","Duplicated Code, Long Method, , , , "
"   Move Class,Move Method,","Removing enhancement listener from Contenthub Contenthub was keeping a graph of all enhancements by registering a listener to the corresponding graph of TCManager. Instead of a listener, enhancements should be retrieved upon submission of the content item and global enhancements graph should be managed without a listener.

This improvement also brings additional features. Removal of any content item should also remove the enhancements from the global enhancement graph. Any change (edit functionality) on the content item should be reflected to the Solr backend and global enhancements graph.

In addition to removal of the enhancementlistener, presentation of recently submitted documents should be processed through Solr instead of the global enhancements graph. This would decrease the dependency of the system to the enhancements graph. If a document is submitted with no enhancements, it should also be presented as a ""recently submitted"" document. Solr provides efficient page&offset mechanism to support this feature.",", , "
"   Move Class,Rename Method,Move Method,Move Attribute,","Adding new feature to CMS Adapter so that it will be able to submit content repository objects to Contenthub With this feature it will be possible to submit content repository objects to Contenthub based on their content repository IDs or paths in the first step. Based on the requirements, selection of objects may be improved. It will also be possible to delete content items from Contenthub.

Properties of objects (e.g jcr:title, cmis:createdBy) will be indexed as metadata of the content items in Contenthub. Thanks to external metadata of the content items, faceted search will be possible over the documents submitted to the Contenthub.",", , , "
"   Move Class,Move And Rename Class,Extract Interface,Rename Method,Move Method,Move Attribute,",Rename the rick java package New name as discussed on the mailing list: org.apache.stanbol.entityhub,", , , Large Class, "
"   Rename Class,Move And Rename Class,","Rename the KReS java package New name: org.apache.stanbol.reasoning or org.apache.stanbol.reasoner (I don't remember what was decided, I cannot find the email, please comment below).",", "
"   Move Class,Move And Rename Class,Rename Method,Move Method,Move Attribute,","Rename the fise java package The fise package should be splitted into new top level packages (below the org.apache.stanbol namespace):

- org.apache.stanbol.common # for all common utilities
- org.apache.stanbol.store # for the store API that blends the clerezza triple stores, the content items, and there binary attachements
- org.apache.stanbol.analysis # for the engines and related compenent
- org.apache.stanbol.web # for the default web interface and REST API (need to make it pluggable)
- org.apache.stanbol.ontology # common utilities from kres and the initial ""persistencestore"" to handle the registered ontology definitions to be managed through the web interface and used by the org.apache.stanbol.reasoning packaged utilities

The sling based launchers should be extracted in a toplevel maven artifacts too.",", , , "
"   Pull Up Method,Move Method,Extract Method,Pull Up Attribute,Move Attribute,","Refactor EnhancementEngine to support contents with multiple related parts Some notes from a discussion with Rupert and Olivier

ContentItem
- getBlob() returns a blob of type multipart/mime iff the ContentItem was created from a multipart/mime content
- ci.getBlob() eq ci.getPart(0, Blob.class)
- ci.getUri()+""_main"" eq ci.getPartUri(0)

Rest
/enhancer
/enhancer/engine/<engineId>
/enhancer/chains/<chaiId>
- query params: 
Optional inputWithMetadata -> expects multipart/mime with 2 sections of which the first is rdf
Optional outputWithContentParts[=<section-ordinal>] -> the result is multipart (instead of rdf) containing rdf as the first section and the parts in the second section, if there is more than one part this second section is itself multipart, this argument might be repated to have different sections
Optional omitMetada -> no metadate in the result, makes only sense with outputContentParts argument, the result will correspond to the second section of the malipart returned without this argument","Duplicated Code, Long Method, , , , Duplicated Code, Duplicated Code, "
"   Move Class,Extract Superclass,Pull Up Method,Move Method,Extract Method,Pull Up Attribute,","Improve Enhancer/ REST endpoint From STANBOL-414, Reto :

Rest
/enhancer
/enhancer/engine/<engineId>
/enhancer/chains/<chainId>

- query params:
Optional inputWithMetadata -> expects multipart/mime with 2 sections of which the first is rdf
Optional outputWithContentParts[=<section-ordinal>] -> the result is multipart (instead of rdf) containing rdf as the first section and the parts in the second section, if there is more than one part this second section is itself multipart, this argument might be repated to have different sections
Optional omitMetada -> no metadate in the result, makes only sense with outputContentParts argument, the result will correspond to the second section of the malipart returned without this argument 
","Duplicated Code, Long Method, , , Duplicated Code, Large Class, Duplicated Code, Duplicated Code, "
"   Rename Method,Move Method,Extract Method,Move Attribute,","Add session management for not using a new session in each operation Currently, for each operation in CMS Adapter, a new session is obtained with the provided connection parameters. Instead of doing this, newly created sessions would be cached and they can be used later on. Furthermore, already available sessions can be used. This would make OSGI based integration easier as in sling-stanbol case.

When using REST services, first a session would be created and cached and this same session is used in subsequent operations by a session key corresponding to the cached session.","Duplicated Code, Long Method, , , , "
"   Rename Method,",Enabling the SCR factory for the configuration of multiple Refactor engine components via Felix console In order to have multiple Refactor engine components active during the enhancement (each one with its mapping recipe) it would be appropriate to enable the SCR factory in the Refactor engine for the configuration of this component via Felix console.,", "
"   Rename Method,Push Down Method,Push Down Attribute,",Enabling the SCR factory for the configuration of multiple Refactor engine components via Felix console In order to have multiple Refactor engine components active during the enhancement (each one with its mapping recipe) it would be appropriate to enable the SCR factory in the Refactor engine for the configuration of this component via Felix console.,", , , "
"   Rename Class,Extract Interface,Move Method,Extract Method,Move Attribute,","LDPath integration to Contenthub LDPath should be integrated so that all modules of Contenthub can operate through LDPath programs
","Duplicated Code, Long Method, , , , Large Class, "
"   Move Method,Extract Method,Inline Method,","Change Metaxa Engine to create PlainText version as ContentPart and change other Engines to retrieve PlainText version from ContentPart Instead of adding/reading the ""text/plain"" version of an ContentItem to/from the metadata of the ContentItem the new ContentPart API should be used for that.

This will require the Metaxa Engine to store literal values of all Triples with the ContentItem.getUri() as subject and

http://www.semanticdesktop.org/ontologies/2007/01/19/nie#plainTextContent

as property to a Blob and add this as ContentPart to the ContentItem.

Other EnhancementEngines need than to search for a Blob with the MimeType ""text/plain"" instead of retrieving the plain text from the metadata.","Duplicated Code, Long Method, , , , "
"   Rename Class,Extract Method,","Web UI for LDPath integration in Contenthub Web user interface to submit/delete LDPath programs is needed.
Contenthub Store and Search services (and HTML pages) should be aligned with the LDPath program name selection.","Duplicated Code, Long Method, , "
"   Extract Interface,Rename Method,Extract Method,","Store the ontologyIRI -> graphName mapping persistently The OntoNet ClerezzaOntologyProvider includes a HashMap that maps ontology IRIs (both physical and logical, e.g. the id of the owl:Ontology resource) to Clerezza Graph names (UriRef).

Because it is not backed up by storage, the mapping is lost when OntoNet or Stanbol are re-activated.

See to it that the mapping is preserved across Stanbol running instances.","Duplicated Code, Long Method, , Large Class, "
"   Rename Class,Move Class,Rename Method,Move Method,Extract Method,","Support OWL 2 versionIRI The ontologyIRI+versionIRI combination of OWL ontologies can be a powerful tool to determine which ontologies are being shared across multiple scopes or sessions, and which version is being used from time to time.

As a matter of fact, setting a versionIRI can be a way to ""claim ownership"" of a particular instance of an ontology, if for example an ontology space sets its own ID as the version IRI of an ontology. versionIRI can also be used to determine if an ontology should outlive its owner or not.","Duplicated Code, Long Method, , , "
"   Rename Method,Move Method,Extract Method,","Allow usage of SimilarityConstraints via the RESTful API of the Entityhub Currently SimilarityConstraints as introduced by STANBOL-202 are not supported in the RESTful API.

To allow this a parser/writer for those constraints must be added to the FieldQuery reader/writer part of the Entityhub jersey module","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Add Stress Test utility to Integration Tests This test is intended to test the Stanbol Enhancer and Enhancement engines under multiple concurrent requests. It will be used during normal integration testing, but will also be useable for Stanbol users to test their servers with specific data.","Duplicated Code, Long Method, , "
"   Move Class,Move And Rename Class,Move Method,Move Attribute,","Get rid of usage of com.sun.jersey.api.view.Viewable We should no longer use com.sun.jersey.api.view.Viewable mainly for two reasons:
- com.sun.jersey.api.view.Viewable is provided by the jersey server package, it is not part of the jax-rs standard. Tha main advantage of adhering to a standard is portability this is lost if implementation specific classes are used.
- It is not the business of the jax-rs resource class to care about the presenation. The resource method should just return objects than can be rendered to many formats using MessageBodyWriters.",", , , "
"   Rename Method,","Sentiment Summarization EnhancementEngine An EnhancementEngine that consumes word level Sentiment annotations and sums them up to

* Noun Phrases
* Sentences
* the whole Document

Notes: 

* As this EnhancementEngine is expected to create Enhancement we will need to define how SentimentAnnotation should be represented by using the Stanbol enhancement Structure.
* This Engine should also support the detection of negations (e.g. this was not a nice trip, the weather was not as bad as the forecast suggested)
",", "
"   Rename Class,Move Attribute,",Extract jersey specific parts from web.core to web.core.jersey project The jersey specific implementation should be moved out of web.core so that WebFragment can be used without jersey dependency and alternative implementations become possible.,", , "
"   Move Class,Move And Rename Class,Move Method,Move Attribute,","Porting of the geonames.org indexer to the Entityhub Indexing Tool The geonames.org indexer was not ported to the Entityhub Indexing Tool as part of STANBOL-187. This issue covers exactly this task.

Features of the current version will be changed as follows:

* Reading the main Geonames data -> GeonamesIndexingSource implementing the EntityDataIterable interface
* Providing Hierarchy information -> Will be converted to an EntityProcessor
* Providing alternate labels -> Will be converted to an EntityProcessor
* Storage to the SolrCore -> Removed as this is covered by the SolrYard indexing destination
* Mapping of Geonames.org properties to other ontologies -> Removed as this is covered by the FieldMapping processor and/or the LDPath processor.

The o.a.s.entityhub.indexing.geonames module will provide a standalone jar that comes with a default configuration of the EntityhubIndexingTool suitable for indexing geonames.org",", , , "
"   Move Class,Move Method,Extract Method,Move Attribute,",Implement Lucene Tokenizer based LabelTokenizer Lucene supports Tokenizers for a lot of languages. While the OpenNLP or Whitespace character based Tokenizers are fine for most of the languages this allows users to use special one (e.g. for Chinese the smartcn analyzer package),"Duplicated Code, Long Method, , , , "
"   Rename Method,","Make it possible to define expansion of RdfViewable by resource method According to https://svn.apache.org/viewvc/stanbol/trunk/commons/web/rdfviewable-writer/src/main/java/org/apache/stanbol/commons/web/viewable/ldpath/writer/impl/RdfSerializingWriter.java?view=markup&pathrev=1447265:

""the expansion can be widened by using the query parameters xPropObj and xProSubj""

It would be nice if I could specify that a bit more fine grained on the server side. From what I get templatePath is used for the HTML export to have more influence on how the result values looks like. I would like to have a similar functionality for RDF representations.

I do something similar in a framework for interfacing RDF based data. In there I use so called Recipes to define on which rdf:Properties I'm interested in. The current vocab can be found here: http://vocab.netlabs.org/recipe. 

",", "
"   Rename Method,","Update Synapse to Axis2 - 1.3 Final release Hi Folks,

Please see enclosed a patch to update Synapse to latest Axis2 1.3 final release. There is one test failure. So i added an exclude in modules/core/pom.xml.

thanks,
dims",", "
"   Move Method,Extract Method,","Allow XPath expressions to be specified relative to envelope or body via an attribute This would make XPath expressions simpler without consideration for SOAP 1.1 or 1.2 or REST etc

Default could be envelope (i.e. what we have now - for backward compatibility), and an optional attribute could specify if it should be relative to the body","Duplicated Code, Long Method, , , "
"   Rename Method,","Allow XPath expressions to be specified relative to envelope or body via an attribute This would make XPath expressions simpler without consideration for SOAP 1.1 or 1.2 or REST etc

Default could be envelope (i.e. what we have now - for backward compatibility), and an optional attribute could specify if it should be relative to the body",", "
"   Rename Method,","Allow XPath expressions to be specified relative to envelope or body via an attribute This would make XPath expressions simpler without consideration for SOAP 1.1 or 1.2 or REST etc

Default could be envelope (i.e. what we have now - for backward compatibility), and an optional attribute could specify if it should be relative to the body",", "
"   Rename Method,","Dynamic load balancing There are some limitations in the current load balancer implementation. e.g. if we have 2 identical services in 2 different worker nodes, which are fronted by a synapse load balancer instance. In such a case, we need to provide 4 endpoints in the synapse.xml file. As can be seen, this is not a very scalable solution. Hence, I have implemented an dynamic load balancing mechanism where the application members are discovered at runtime, and the endpoint do not need to be statically specified in the synapse.xml file.

Currently, the application endpoints are calculated by replacing the IP and port of the incoming request with that of the member to which this request will be forwarded to. I have only tested with HTTP/S for the moment. More details about the concept & design can be found here: http://afkham.org/2008/06/fault-resilient-dynamic-load-balancing.html


",", "
"   Move Method,Extract Method,Inline Method,Move Attribute,","Securing password in the datasource definition Currently ,passwords in the datasource definition are in clear text format. (In synapse.properties). Those have to be encrypted. 
","Duplicated Code, Long Method, , , , , "
"   Rename Method,Extract Method,","Securing password in the datasource definition Currently ,passwords in the datasource definition are in clear text format. (In synapse.properties). Those have to be encrypted. 
","Duplicated Code, Long Method, , "
"   Move Method,Move Attribute,","Improve Synapse Memory Footprint under HTTP 1.0 Synapse serializes the SOAP envelope in order to calculate the content length of the message under HTTP 1.0. This serialization is done inside the class Axis2HttpRequest which is inside the transports module. The serialized bytes are then stored in the message context for future use by the streamMessageContents() method inside the same class.

Thus, the entire content of the SOAP envelope is stored inside the memory leading to a possible out of memory situation when the XML data is large. A solution to this would be to write some of the data to a permanent storage (like hard disk) based on a threshold value. The TemporaryData class inside the core module is a good solution to this kind of work. It would, however, incur a cyclic dependency if used inside the Axis2HttpRequest class. 

If we can resolve this issue (probably by moving the TemporaryData class to a utility module and then making the core too depend on that) it will of great use in improving the memory footprint of Synapse.",", , , "
"   Move Method,Extract Method,Move Attribute,","Improvements to the DataSource Connection Pool Improvements to the DataSource Connection Pool

1) If the DB connection pools are defined in the synapse.properties, then , those need to be accessed just providing data source name. Simply, we have to differentiates the accessing datasources that we have created and datasources that we are looking up.
Then , For access datasources we have defined (Example dblookup mediator)
<pool>
<dsName>lookupds</dsName> 
</pool>

and for any external datasources as before 
<pool>
<dsName>lookupds</dsName>
<icClass>com.sun.jndi.rmi.registry.RegistryContextFactory</icClass>
<url>rmi://localhost:2199</url>
<user>esb</user>
<password>esb</password>
</pool>
11) If the datasources that we have defined is not intended to use by any other application. Then, we never want to register those in the JNDI tree. Those can be in a local pool. Currently, all are registered in the JNDI. This need to be configurable so that if someone want to avoid JNDI registration , he can tell it by configuration option.","Duplicated Code, Long Method, , , , "
"   Rename Method,","Adding the tracing capability to the mediator hi all

Adding a new feature for enable tracing capability to each of the mediator,endpoint and proxy service, Then it is flexible to trace specific information which local to a particular mediator. 

thanks

Regards 
indika",", "
"   Rename Method,","Enhanced JMX-support for Synapse Hi,

currently it is only possible to use Java's ""out-of-the-Box JMX solution"" configured via system properties and/or property files which is sufficient for many cases.
Anyway there are the following advantages of using the according Remote API to create, configure and deploy the management agent (server connector) programatically:
1) easier configuration for average users
2) exporting RMI server's remote objects over a certain port to allow passage through a firewall (very important for enterprise deployments), possibility to configure a specific network interface (also sometimes important for enterprise deployments in multi-homed systems 
3) possibility to use a custom JMXAuthenticator to handle own credential configuration (including use of Secret-API to encrypt passwords in plain text files if required/prefered over setting os permissions accordingly.",", "
"   Rename Method,Inline Method,","Possibility to put server in maintenance mode and reload synapse configuration via JMX Unfortunately I did not find the old issue we had for this, so I created a new one. The provided patch extends the existing server states with a maintenance state. While entering the maintenance mode all transports supporting a pause operation are paused. So no new requests will be accepted. Once all requests in progress are finished the server can be safely restarted to load a changed configuration. If running in a clustered environment behind a loadbalancer this allows dynamic configuration updates. 
All server state changes can be triggered via JMX using an extended ServerManagerMBean which can also be used to query the current server state.

While working on this patch I thought about moving some of the classes in the synapse top level package to a new server subpackage. I actually did not perform this change to ease the review, but I still think it would be a good idea.

So what about moving the following classes:
Axis2SynapseController
JmxAdapter
ServerConfigurationInformation
ServerConfigurationInformationFactory
ServerContextInformation
ServerManager
ServerState
ServerStateDetectionStrategy
SynapseController
SynapseControllerFactory
SynapseServer
to 
synapse.server
and
ServerManagerView
ServerManagerViewMBean
to
synapse.server.mbean (maybe also renaming them to ServerManagerControl and ServerManagerControlMBean.
",", , "
"   Rename Method,Extract Method,","Synapse should be able to act as a native proxy service to REST services Synapse should be able to act as a native proxy to REST services. Thus HTTP support for methods GET, PUT, HEAD, DELETE, POST and OPTIONS should be available as per RFC 2616

Related to SYNAPSE-477, SYNAPSE-186, SYNAPSE-386

e.g. implementation

<proxy name=""synapse-resteasy"" startOnLoad=""true"" >
<target endpoint=""rest-ep"">
<inSequence>
<!--log level=""full"">
<property name=""Method"" expression=""get-property('axis2', 'HTTP_METHOD')""/>
<property name=""REST URL postfix"" expression=""get-property('axis2', 'REST_URL_POSTFIX')""/>
<property name=""Service prefix"" expression=""get-property('axis2', 'SERVICE_PREFIX')""/>
</log-->
</inSequence>
<outSequence>
<send/>
</outSequence>
</target>
</proxy>

<endpoint name=""rest-ep"">
<address uri=""http://localhost:8080/simple-jetty/rest-services"" format=""rest"">
</address>
</endpoint>
","Duplicated Code, Long Method, , "
"   Rename Method,Move Method,Extract Method,","AbstractDBMediatorFactory should not create/lookup data sources The process of building the in-memory object representation of the Synapse configuration should be separated from the initialisation of mediators.

The attached patch moves the creation/lookup of data sources to the AbstractDBMediator. The factory only reads the XML confguration and transforms it to an object model.
This allows to create a Synapse Configuration including data sources from an XML file without having to have a fully initialized Synapse Environment available.

","Duplicated Code, Long Method, , , "
"   Rename Method,",Implement Ability to Intercept Admin Level Messages in the FIX Transport Currently FIX transport can only intercept application level messages. It would be most useful to have the ability to intercept admin level messages as well. Basically we should be able to add custom fields to the outgoing admin level messages.,", "
"   Rename Method,Move Method,Extract Method,Move Attribute,","Combine Script mediators for inline and external script support See post http://marc.theaimsgroup.com/?l=synapse-dev&m=117212280327926&w=2

The script and inline script mediators could be combined into a single script mediator","Duplicated Code, Long Method, , , , "
"   Rename Method,Extract Method,","Improvements the DynamicLBEndpoint to support Session Affinity based Load Balancing. As per the discussion we had in synapse-dev list(subject:Improving the DynamicLBEndpoint to support Session Affinity based Load Balancing), I've improved the DynamicLBEndpoint.","Duplicated Code, Long Method, , "
"   Rename Method,Pull Up Method,Extract Method,Pull Up Attribute,",Extend the Evaluator Framework to Support Evaluation of SOAP Envelope Content and Message Properties This will allow one to evaluate message context properties and SOAP payload content using the evaluators like MatchEvaluator and EqualEvaluator. This will particularly come in handy with the URLRewriteMediator.,"Duplicated Code, Long Method, , Duplicated Code, Duplicated Code, "
"   Rename Method,",Upgrading quartz version used in synapse-task Current synapse-task implementation has used quartz 1.6.0 version which is something old. The quartz latest stable version 2.1.1 seem to have many functional improvements and new features over 1.6.0 version. So it will be a good idea to upgrade quartz version to 2.1.1 to have future enhancements in synapse-task.,", "
"   Rename Method,","OCSP/CRL Certificate Validation Feature for Synapse. Please find the implementation of the feature along with unit tests and a working sample in the attached ""certificate_validation_feature.zip"" file. This feature can be plugged into both NHTTP and Passthru transports. For more information please read the README.

Thanks,
Jeewantha",", "
"   Rename Method,Extract Method,Move Attribute,","New Blocking Sender implementation Currently we have following two main places we use a blocking client to invoke services.
1. Callout Mediator
2. Forwarding Message Processor

Both these components use the axis2 service client to invoke services in a blocking manner.
Since the implementation is very simple, provided functionality is very limited.

Currently we have following major limitations.

* Supports only SOAP
* Message format conversion is not supported
* QOS functionality (WS-Security, WS-A) are not supported
* Cannot specify an endpoint as the service endpoint reference 
(In message processors we can specify an Address endpoint. But none of the other endpoint functionality is supported)

I have implemented a new blocking client which can be used as a common blocking sender for both above components.
This new implementation will provide the following functionality.

* Support all leaf endpoint types
* Support for REST
* Support Endpoint functionality.
- Endpoint format conversions (soap11,soap12,pox, etc.)
- WS-Security, WS-A
- Endpoint Timeout (nhttp transport specific functionality is not supported)
* message format conversions (ability to use messageType/ContentType properties)

","Duplicated Code, Long Method, , , "
"   Rename Method,","Allow bookie garbage collection to be triggered manually from tests The current gc tests rely on waiting on a timeout for gc to run. It's
never certain whether it has run or not or if it's still running. 

This patch allows tests to trigger a gc run and gives the client
a future to know when it has completed. The gc algorithm is unchangedI but now it runs in a scheduled executor rather than as a
Thread.

This work was originally done by Ivan Kelly and I am just pushing it back to open source",", "
"   Rename Method,Extract Method,Move Attribute,","Move to netty4 As part of the Yahoo push back and in general we would like to move to netty 4, preferably netty 4.1.x for the client and server communication.

This lays the ground work for zero copy, or very nearly zero copy handling on the server side.","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","Add Fencing to Bookkeeper BookKeeper is designed for use as a Write ahead log. In systems with a primary/backup architecture, the primary will write state updates to the WAL. If the primary dies the backup comes online, reads the WAL to get the latest state and starts serving requests. However, if the primary was only partitioned from the network, or stuck in a long GC, a split brain occurs. Both primary and backup can service client requests. 

Fencing(http://en.wikipedia.org/wiki/Fencing_%28computing%29) ensures that this cannot happen. With fencing, the backup can close the WAL of the primary, and cause any subsequent attempt by the primary to write to the WAL to give an error. 

We fence a ledger whenever it is opened by another client using BookKeeper#openLedger. BookKeeper#openLedgerNoRecovery will not fence.
The opening client marks the ledger as fenced in zookeeper, and then sends a readEntry message to a all of bookies with the DO_FENCING flag set. Once at least 1 bookie in each possible quorum of bookies have responded, we can proceed with opening the ledger. Any subsequent attempt to write to the ledger will fail as it will not be able to write to a quorum without one of the bookie in the quorum responding with a ledger fenced error. The client will also be unable to change the quorum without seeing that the ledger has been marked as fenced in zookeeper.
","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,","When all disks are full, start Bookie in RO mode if RO mode is enabled - When all disks are full start Bookie in RO mode if RO mode is enabled
- This will work only if ""isForceGCAllowWhenNoSpace"" is allowed, since LedgerDirsManager.getWritableLedgerDirsForNewLog will be able to find new writableLedgerDir even when all disks are full.
- If bookie has died abruptly then it may have missed flushing EntryMemtable and
IndexInMemoryPageManager. So next time when it starts when disc is full
it fails to create index file and it shuts down. So Bookie should be able to create index file though it has reached the diskusagethreshold, while starting the Bookie in Readonly Mode. But ofcourse there should be some config to safeguard when disk usable space is so low.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,Inline Method,","Removed PacketHeader serialization/deserialization allocation When parsing the request packet header, use static methods to avoid creating a {{PacketHeader}} instance.","Duplicated Code, Long Method, , , "
"   Rename Method,Move Method,Inline Method,Move Attribute,","bookie server needs to do compaction over entry log files to reclaim disk space bookie server aggregates entries into entry log file. suppose there is lots of ledgers, each ledger has little messages. so a entry log file would contains messages from lots of different ledgers. if there is only one ledger not be deleted, the entry log file would not be removed, whose occupied disk space could not be reclaimed.",", , , , "
"   Extract Method,Move Attribute,",Add versioning support for journal files Add versioning in the journal so that we can add new features to the journal without breaking backward compatibility.,"Duplicated Code, Long Method, , , "
"   Move Method,Inline Method,Push Down Attribute,","Bookie code is very coupled Bookie owns EntryLogger, LedgerCache, LedgerDescriptors which all depend on each other in strange ways. Sometimes we access the ledgerCache directly, sometimes through LedgerDescriptors. etc, etc. It's messy and there's no hierarchy.

I propose that we refactor Bookie to only contain the EntryLogger and journalling code (this should be factored at some stage also). The EntryLogger can then own the ledgerCache and the LedgerDescriptors, and then we would how have to have the entanglement as observed on BOOKKEEPER-160.",", , , , "
"   Move Method,Extract Method,","Hub server should change ledger to write, so consumed messages have chance to be garbage collected. currently, hub server write entries to only one ledger, if a topic doesn't change ownership, all entries will be added to this ledger. so those consumed messages don't have chance to be garbage collected.","Duplicated Code, Long Method, , , "
"   Move Method,Move Attribute,","Define interface between bookie and ledger storage EntryLogger and LedgerCache are both part of a very interdependent storage mechanism where entries are interleaved in a single log(EntryLogger) and index files are maintained (LedgerCache). I'd like to experiment with some other schemes (Im not convinced the interleaving is required for high performance). ZOOKEEPER-507 brought in these changes, but it also brought in a lot of other stuff, and I think its the other stuff (specifically taking the writing to separate files out of the critical path) which gave us the performance boost. To do this cleanly, we need a well defined storage interface for this. This JIRA is to provide this. Future work can move the interleaved implementation into another package as org.apache.bookkeeper.bookie is getting a little crowded now.",", , , "
"   Move Class,Move Method,Extract Method,Move Attribute,","Provide journal manager to manage journal related operations Currently we put all journal related operations in Bookie class. It would be better to provide a journal manager to provide journal related operations. It would make Bookie logic more clearly. 

Besides that, some admin tools like BOOKKEEPER-183 needs to provide could use JournalManager to read/check journal files directly.","Duplicated Code, Long Method, , , , "
"   Rename Method,","CompositeException message is not useful Exceptions logged via slf4j don't actually have their toString method called, so the current behaviour of overriding toString for CompositeException is rarely/never triggered in client code.

Composing a better `message` field for CompositeException would make it loggable.",", "
"   Rename Method,Extract Method,","Simplify AbstractSubscriptionManager It's difficult to maintain a duplicated/cached count of local subscribers, and we've experienced a few issues due to it getting out of sync with the actual set of subscribers. Since a count of local subscribers can be calculated from the top2sub2seq map, let's do that instead.","Duplicated Code, Long Method, , "
"   Move Class,Move Method,Move Attribute,","Provide separate read and write threads in the bookkeeper server The current bookkeeper server is single threaded. The same thread handles reads and writes. When reads are slow (possibly because of excessive seeks), add entry operations suffer in terms of latencies. Providing separate read and write threads helps in reducing add entry latencies and increasing throughput even when we're facing slow reads. Having a single read thread also results in low disk utilization because seeks can't be ordered efficiently by the OS. Multiple read threads would help in improving the read throughput. 

Discussion on this can be found at http://mail-archives.apache.org/mod_mbox/zookeeper-bookkeeper-dev/201209.mbox/%3cCAOLhyDQpzn-v10zyNFwUd_h0qzRxtmJgTTx7a9eoFoHYytyJbA@mail.gmail.com%3e

Reviewboard : https://reviews.apache.org/r/7560/",", , , "
"   Move Class,Move Method,Extract Method,Move Attribute,","multiple threads for delivery manager similar as BOOKKEEPER-461. there is only one thread running processing delivery, which would be bottleneck when sub/closesub or throttle became frequently.","Duplicated Code, Long Method, , , , "
"   Rename Method,Move Method,Extract Method,Inline Method,","Better checkpoint mechanism Currently, SyncThread made a checkpoint too frequently, which affects performance. data is writing to entry logger file might be blocked by syncing same entry logger file, which affect bookie to achieve higher throughput. We could schedule checkpoint only when rotating an entry log file. so new incoming entries would be written to newer entry log file and old entry log file could be synced.","Duplicated Code, Long Method, , , , "
"   Rename Method,","allow application to recommend ledger data locality For application like hbase WAL, it will be useful if application like hbase can give a hint to bk about application's preferred ledger location. In that way, application can fail over to specific machines where one of the ledger replica is located; the recovery time will be faster. Another scenario is hbase's support for hot standby region server where read request can be served from a different machine other than the active region server. That requires the hot standby region server to read from ledger. If the ledger is on the same machine as standby region server, the performance will be better.",", "
"   Rename Method,Extract Method,","Speed up bookkeeper tests Some tests use addEntry when they should be using asyncAddEntry, leading to the tests taking much to long. Others have timeouts set too high for cases which are expected to fail.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,",Turn readonly back to writable if spaces are reclaimed. should be able to turn a bookie from readonly back to writable if the spaces are reclaimed.,"Duplicated Code, Long Method, , "
"   Move Class,Extract Superclass,","Major GC should kick in immediately if remaining space reaches a warning threshold in a high throughput case, Major GC should kick in immediately if remaining spaces reaches a warning threshold.",", Duplicated Code, Large Class, "
"   Rename Method,Extract Method,","bookkeeper: delay ensemble change if it doesn't break ack quorum requirement flag to allow delay ensemble change. if that is set to change, will not do ensemble change until it breaks ack quorum requirement","Duplicated Code, Long Method, , "
"   Move Method,Move Attribute,","Bookie should calculate ledgers map writing a new entry log file Bookie should calculate ledgers map when writing a new entry log file. so the bookie doesn't need to scan that entry log file again, which it would improve garbage collection efficiency",", , , "
"   Rename Method,Extract Method,","Collect stats with sub-milliseconds precision Most operations in Bookie are taking <1ms. Stats should be collected in nanos , expecially since latencies are already measured in nanos.","Duplicated Code, Long Method, , "
"   Rename Method,Extract Method,",Configurable LedgerStorageImplementation Allow to configure a different implementation for the LedgerStorage interface in the Bookie configuration.,"Duplicated Code, Long Method, , "
"   Rename Method,","Dispatch individual callbacks from journal in different threads Currently the journal is sending all the responses from a single thread, after the entries in a batch are synced. Since a thread pool has been configured, it is better to spread the send-response tasks to all the available threads.",", "
"   Move Class,Rename Method,Move Method,Move Attribute,","Bookkeeper API changes for initial Bookkeeper release Changes are as follows.

BookKeeper#createLedger, parameter is named passwd, ""Key"" used in LedgerHandle api
BookKeeper#getBookieClient shouldn't be public
BookKeeper#createComplete shouldn't be public
BookKeeper#openComplete shouldn't be public
BookKeeper#deleteComplete shouldn't be public
BookKeeper#halt could be changed to close(), should throw a BKException

LedgerHandle#getLedgerKey passwd is used in BookKeeper, should possibly be private
LedgerHandle#getLedgerMetadata shouldn't be public
LedgerHandle#getDigestManager shouldn't be public
LedgerHandle#getDistributionSchedule shouldn't be public
LedgerHandle#writeLedgerConfig shouldn't be public
LedgerHandle#addEntry should return void, errors should go in an Exception
LedgerHandle#readComplete should not be public
LedgerHandle#addComplete should not be public
LedgerHandle#readLastConfirmedCompelte should not be public
LedgerHandle#closeComplete should not be public

ASyncCallback#RecoverCallback shouldn't be public
",", , , "
"   Move Class,Extract Method,","bookies should not retain ledgers which no longer belong to them The bookies do not clean up ledgers on their disk which exist in zookeeper but are not assigned to them by the ensemble definition. This happens if a bookie has a ledger, went offline, it was replicated elsewhere, and then the bookie comes back up. Then we have an extra copy of the same ledger.

Solution:
Bookie should handle this case in the garbage collector. Since we will have to read the ledger metadata and go through its ensemble set to determine if the bookie exists in the ensemble, this is an expensive operation. Thus, we will only run this task once every day.","Duplicated Code, Long Method, , "
"   Rename Class,Rename Method,","Hedwig API changes for initial Bookkeeper release HedwigClient#getSslFactory shouldn't be public
HedwigClient#getConsumeCallback shouldn't be public
HedwigClient#doConnect shouldn't be public
HedwigClient#getHostFromChannel shouldn't be public
HedwigClient#getResponseHandlerFromChannel shouldn't be public
HedwigClient#getHostForTopic shouldn't be public
HedwigClient#clearAllTopicsForHost shouldn't be public
HedwigClient#getClientTimer shoulnd't be public
HedwigClient#stop should throw some sort of Exception in the case of errors

HedwigPublisher#publish shouldn't use protobuf ByteString, as it requires the user to import protobufs
HedwigPublisher#getChannelForHost shouldn't be public

HedwigSubscriber#HedwigSubscriber shouldn't be public
HedwigSubscriber#doConsume shouldn't be public
HedwigSubscriber#hasSubscription probably shouldn't be public
HedwigSubscriber#getSubscriptionList shoulnd't exist
HedwigSubscriber#getChannelForTopic shouldn't be public
HedwigSubscriber#setChannelforTopic shouldn't be public
HedwigSubscriber#removeChannelForTopic shound't be public

MessageHandler#consume should be called 'deliver'

The hedwig client is under a netty package. There's nothing netty specific about the api, so it should be in the org.apache.hedwig.client package. 
",", "
"   Rename Method,","Bookkeeper and hedwig clients should not use log4j directly Using log4j directly requires that any application using bookkeeper or hedwig clients have to configure log4j. 

We should use something like commons logging[1] or slf4j[2].

[1] http://commons.apache.org/logging/index.html
[2] http://www.slf4j.org/",", "
"   Move Class,Move Method,Move Attribute,","Publish sources and javadocs to Maven Central We should push sources and javadocs to Maven Central during the release process. 
It is very useful to have such artifacts in IDEs",", , , "
"   Rename Method,","Upgrade protobuf to 2.6 I had to update protobuf definition for some internal experiments and found that working with protobuf 2.4 is rather inconvenient. It cannot be installed with brew on mac and building it on mac always result is build errors hence leaves an option of switching to linux to run protoc.

I decided to upgrade to 2.6 instead. It is compatible with 2.4 on wire and shaded so should not create any problems. All tests passed.

Please ignore changes in java files in attached patch during review; these are auto-generated.",", "
"   Move Method,Move Attribute,","Multiple issues and improvements to BK Compaction. We have identified multiple issues with BK compaction.
This issue is to list all of them in one Jira ticket.

1.
MajorCompaction and MinorCompaction are very basic. Either they do it or won’t do it. Proposal is to add Low Water Mark(LWM) and High Water Mark(HWM) to the disk space. Have different compaction frequency and re-claim %s when the disk space is < low water mark , > LWM < HWM, > HWM.

2.
MajorCompaction and Minor Compactions are strictly frequency based. They should at least time of the day based, and also run during low system load, and if the system load raises, reduce the compaction depending on the disk availability 

3.
Current code disables compaction when disk space grows beyond configured threshold. There is no exit from this point. Have an option to keep reserved space for compaction, at least 2 entryLog file sizes when isForceGCAllowWhenNoSpace enabled.

4.
Current code toggles READONLY status of the bookie as soon as it falls below the disk storage threshold. Imagine if we keep 95% as the threshold, Bookie becomes RW as soon as it falls below 95 % and few more writes pushes it above 95 and it turns back to RONLY. Use a set of defines (another set of LWM/HWM?) where Bookie turns RO on high end and won't become RW until it hits low end.

5.
Current code never checks if the compaction is enabled or disabled once the major/minor compaction is started. If the bookie goes > disk threshold (95%) and at that compaction is going on, it never checks until it finishes but there may not be disk available for compaction to take place. So check if compaction is enabled after processing every EntryLog.

6.
Current code changes the Bookie Cookie value even when new storage is added. When the cookie changes Bookie becomes a new one, and BK cluster treats it as new bookie. If we have mechanism to keep valid cookie even after adding additional disk space, we may have a chance to bring the bookie back to healthy mode and have compaction going.

7. Bug
CheckPoint was never attempted to complete after once sync failure. There is a TODO in the code for this area.

8.
When the disk is above threshold, Bookie goes to RO. If we have to restart the bookie, on the way back, bookie tries to create new entrylog and other files, which will fail because disk usage is above threshold, hence bookie refuses to come up.",", , , "
"   Rename Method,","Assing read/write request for same ledger to a single thread When entries for the same ledger are processed by the bookie we should avoid
the reordering of the request. Currently, if multiple read/write threads are
configured, the requests will be passed to the executor and writes for same
ledger will be spread across multiple threads.

This poses 2 issues:
# Mutex contention to access the LedgerDescriptor
# If the client receives add-entry acks out of order it has anyway to wait
for the acks of previous entries before acknowledging the whole sequence
to the application. In practice, the reordering is increasing the latency
experienced by the application.
",", "
"   Rename Method,","Package datagen scripts with standalone SystemML distribution The random data generation scripts in scripts/datagen are a useful way for people to become familiar with SystemML, assuming they generate data files in a manner that can easily be consumed by the algorithms being executed.

Right now the scripts/datagen directory is not packaged into the standalone distribution. We should probably include this directory.",", "
"   Extract Method,Inline Method,","Package datagen scripts with standalone SystemML distribution The random data generation scripts in scripts/datagen are a useful way for people to become familiar with SystemML, assuming they generate data files in a manner that can easily be consumed by the algorithms being executed.

Right now the scripts/datagen directory is not packaged into the standalone distribution. We should probably include this directory.","Duplicated Code, Long Method, , , "
"   Rename Method,Extract Method,","New rev() builtin function In various scripts, permutation matrices are used in order to reverse a matrix via matrix multiplication. This is not very intuitive and requires the materialization of large ultra-sparse permutation matrices. Accordingly, this task aims to add a new builtin function rev() for reversing a matrix. This builtin function has similar semantics as R's rev() function but is more general as in supporting matrices instead of vectors.","Duplicated Code, Long Method, , "
"   Extract Method,Inline Method,","MLContext Redesign This JIRA proposes a redesign of the Java MLContext API with several goals:
• Simplify the user experience
• Encapsulate primary entities using object-oriented concepts
• Make API extensible for external users
• Make API extensible for SystemML developers
• Locate all user-interaction classes, interfaces, etc under a single API package
• Extensive Javadocs for all classes in the API
• Potentially fold JMLC API into MLContext so as to have a single programmatic API
","Duplicated Code, Long Method, , , "
"   Rename Method,","Compare Performance of LeNet Scripts With & Without Using SystemML-NN This JIRA issue tracks the comparison of the performance of the LeNet scripts with & without using SystemML-NN. The goal is that they should have equal performance in terms of both accuracy and time. Any difference will be indicate areas of engine improvement.

Scripts:

* [mnist_lenet-train.dml | https://github.com/apache/incubator-systemml/blob/master/scripts/staging/SystemML-NN/examples/mnist_lenet-train.dml] - LeNet script that *does* use the SystemML-NN library.
* [lenet-train.dml | https://github.com/apache/incubator-systemml/blob/master/scripts/staging/lenet-train.dml] - LeNet script that *does not* use the SystemML-NN library.

*Current Status - Forced Singlenode:*

Equal performance when running the scripts in standalone mode with the {{-exec singlenode}} flag, 20GB of memory, and using data inputs in the SystemML binary format -- see {{run.sh}} and {{perf.sh}} for information.

Results:
- Run #1:
|| Script | Time (s) | Accuracy ||
| mnist_lenet-train.dml | 2987.400704441 | 99.32% |
| lenet-train.dml | 2816.369435579 | 99.28% |

- Run #2:
|| Script | Time (s) | Accuracy ||
| mnist_lenet-train.dml | 2847.790531812 | 99.16% |
| lenet-train.dml | 2950.520494210 | 99.18% |

So, same accuracy, and same runtime in singlenode mode!

*Current Status - Spark Local:*

The two scripts now have the same performance in Spark local mode (non-singlenode), equivalent to the performance in forced singlenode mode due to the creation of only CP jobs!

---

To fully reproduce, I basically created a directory, placed the two attached bash scripts in it, grabbed a copy of the NN library and placed it into the directory, ran the examples/get_mnist_data.sh script from the library to get the data (placed into examples/data), then used the attached convert.dml to create binary copies of the data for both scripts, then ran run.sh. Also, I copied examples/data to the base directory as well. Adjust the {{EXEC}} and related variables in {{perf.sh}} to switch between standalone, Spark, memory sizes, explain, stats, etc.",", "
"   Rename Method,","Update MLContext Matrix 'as' methods to 'to' The MLContext Matrix class allows for easy conversion to different formats (DataFrames, RDDs, etc) using several 'as' methods. Updating these to be 'to' methods would more closely follow Spark's 'to' conversion method conventions.

The documentation should also be updated.
",", "
"   Move Class,Rename Method,Move Method,Inline Method,","Add additional classes for MLContext Frame support Create and implement Frame classes to enhance MLContext API Frame support. Classes/enums to create include: Frame, FrameFormat, FrameMetadata, FrameSchema, and BinaryBlockFrame. Add support for these classes in MLContextUtil, MLContextConvertUtil, Script, MLResults, and any other related classes. Most code should be quite similar to the code for Matrix support.
",", , , "
"   Rename Method,Extract Method,Move Attribute,","Performance: Improve Vector DataFrame Conversions Currently, the performance of vector DataFrame conversions leaves much to be desired, with regards to frequent OOM errors, and overall slow performance. 

Scenario:
* Spark DataFrame:
** 3,745,888 rows
** Each row contains one {{vector}} column, where the vector is dense and of length 65,539. Note: This is a 256x256 pixel image stretched out and appended with a 3-column one-hot encoded label. I.e. this is a forced workaround to get both the labels and features into SystemML as efficiently as possible.
* SystemML script + MLContext invocation (Python): Simply accept the DataFrame as input and save as a SystemML matrix in binary form. Note: I'm not grabbing the output here, so the matrix will literally be written by SystemML.
** {code}
script = """"""
write(train, ""train"", format=""binary"")
""""""
script = dml(script).input(train=train_YX)
ml.execute(script)
{code}

I'm seeing large amounts of memory being used in conversion during the {{mapPartitionsToPair at RDDConverterUtils.java:311}} stage. For example, I have a scenario where it read in 1493.2 GB as ""Input"", and performed a ""Shuffle Write"" of 2.5 TB. A subsequent stage of {{saveAsHadoopFile at WriteSPInstruction.java:261}} then did a ""Shuffle Read"" of 2.5TB, and ""Output"" 1829.1 GB. This was for a simple script that took in DataFrames with a vector column and wrote to disk in binary format. It kept running out of heap space memory, so I kept increasing the executor memory 3x until it finally ran. Additionally, the latter stage had a very skewed execution time across the partitions, with ~1hour for the first 1000 paritions (out of 20,000), ~20 minutes for the next 18,000 partitions, and ~1 hour for the final 1000 partitions. The passed in DataFrame had an average of 180 rows per partition with a max of 215, and a min of 155.

cc [~mboehm7]","Duplicated Code, Long Method, , , "
"   Rename Method,","Performance: Improve Vector DataFrame Conversions Currently, the performance of vector DataFrame conversions leaves much to be desired, with regards to frequent OOM errors, and overall slow performance. 

Scenario:
* Spark DataFrame:
** 3,745,888 rows
** Each row contains one {{vector}} column, where the vector is dense and of length 65,539. Note: This is a 256x256 pixel image stretched out and appended with a 3-column one-hot encoded label. I.e. this is a forced workaround to get both the labels and features into SystemML as efficiently as possible.
* SystemML script + MLContext invocation (Python): Simply accept the DataFrame as input and save as a SystemML matrix in binary form. Note: I'm not grabbing the output here, so the matrix will literally be written by SystemML.
** {code}
script = """"""
write(train, ""train"", format=""binary"")
""""""
script = dml(script).input(train=train_YX)
ml.execute(script)
{code}

I'm seeing large amounts of memory being used in conversion during the {{mapPartitionsToPair at RDDConverterUtils.java:311}} stage. For example, I have a scenario where it read in 1493.2 GB as ""Input"", and performed a ""Shuffle Write"" of 2.5 TB. A subsequent stage of {{saveAsHadoopFile at WriteSPInstruction.java:261}} then did a ""Shuffle Read"" of 2.5TB, and ""Output"" 1829.1 GB. This was for a simple script that took in DataFrames with a vector column and wrote to disk in binary format. It kept running out of heap space memory, so I kept increasing the executor memory 3x until it finally ran. Additionally, the latter stage had a very skewed execution time across the partitions, with ~1hour for the first 1000 paritions (out of 20,000), ~20 minutes for the next 18,000 partitions, and ~1 hour for the final 1000 partitions. The passed in DataFrame had an average of 180 rows per partition with a max of 215, and a min of 155.

cc [~mboehm7]",", "
"Rename Method,","Views: min and max version Add support for including optional min, max or min+max Ambari version in view.xml. This can include exact versions or variables. A few examples:

< min-ambari-version>1.7.0</min-ambari-version>
< min-ambari-version>1.7.*</min-ambari-version>
< max-ambari-version>1.*</max-ambari-version>

On deploy, ambari should validate the view meets the min/max specified and if not, not deploy and produce an error in the log with the details (for example: ""view requires minimum ambari version 1.7"")",", "
"Rename Class,Move And Rename Class,Move Method,Move Attribute,","Refactor rolling upgrades prerequisite checks to expose and repackage utility classes The logic in the org.apache.ambari.server.state.UpgradeCheckHelper should be exposed so that resources may use it. 

The following steps will be taken:
* Move the static classes out of {{org.apache.ambari.server.state.UpgradeCheckHelper}} and into a package named {{org.apache.ambari.server.checks}}
* Rename {{UpgradeCheckDescriptor}} to {{AbstractCheckDescriptor}}
* Create {{org.apache.ambari.server.checks.CheckHelper}} and move {{org.apache.ambari.server.state.UpgradeCheckHelper#performPreUpgradeChecks}} into it
* Adjust {{org.apache.ambari.server.controller.internal.PreUpgradeCheckResourceProvider}} accordingly
* {{UpgradeCheckRequest}}, {{UpgradeCheck}} and other similar resources which encapsulate check data will be generalized to {{PrereqCheckRequest}} , {{PrerequisiteCheck}} and so on",", , , "
"Move Class,Rename Class,Pull Up Method,Move Method,Inline Method,Pull Up Attribute,Move Attribute,","Enable Flume metrics sink to AMS Ambari Metrics needs to be able to collect metrics from Flume (https://github.com/apache/flume)
-Create a TimelineMetricsSink class that implements MonitorService.
Look at org.apache.flume.instrumentation.GangliaServer
- Configure Flume if Ambari Metrics is deployed to use TimelineMetricsSink in its classpath
http://flume.apache.org/FlumeUserGuide.html#custom-reporting
- Look into how to support GET queries for FLUME agent, stacks/HDP/2.0.6/services/FLUME/metrics.json
User is unaware of group name and Channel name for a given agent.

*Key points*:
- Store the appId = 'flume""
- Allow for regex queries in GET API, example if flume metrics looks like: ""agentName.groupName.channelName.MetricName"", user can query AMS API as (\\w)\+.(\\w)\+.(\\w)\+.metricName
This transforms to a LIKE SQL clause which needs to be added to the timeline service API
http://phoenix.apache.org/language/index.html#expression",", , , , Duplicated Code, Duplicated Code, "
"Pull Up Method,Move Method,Extract Method,","Add support for ""add hosts"" specifying host name, blueprint name and host group name Provide a higher level api for host provisioning.
This api will accept a host name, blueprint name and host group.
The result of this api call will be fully operational hosts added to the existing cluster with configuration and components which are defined in the specified blueprint/host_group. All components on the added hosts will be installed and started.

All hosts must be reachable via ambari server and have the ambari agent running and registered with the ambari server prior to using this api. 

This is an asynchronous api so it will return the standard asynchronous response.
{code}
202 - Accepted
{
""href"" : ""http://AMBARI_HOST:8080/api/v1/clusters/c1/requests/2"",
""Requests"" : {
""id"" : 2,
""status"" : ""InProgress""
}
}
{code}

This api will support adding a single host or multiple hosts.

h4. Single Host:
{code}
POST http://AMBARI_HOST:8080/api/v1/clusters/c1/hosts/newHostName.domain

{
""blueprint"" : ""my_blueprint"",
""host_group"" : ""slave""
}
{code}

h4. Multiple Hosts
{code}
POST http://AMBARI_HOST:8080/api/v1/clusters/c1/hosts

[
{
""blueprint"" : ""my_blueprint""
""host_group"" : ""slave"",
""host_name"" : ""host2.domain""
},
{
""blueprint"" : ""my_blueprint"",
""host_group"" : ""slave"",
""host_name"" : ""host5.domain""
}
]
{code}","Duplicated Code, Long Method, , , Duplicated Code, "
"Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","View: Pig ui updates + flow changes Popup tabs should be name <script name> for script
< script-name> - Log
< script-name> - Completed
< script-name> - Results
Do not use all capital letters in tabs
Add link on History Page for Details - this will pop-up the
Make Pig Editor Window 35 lines deep (vs. current 20)
“Copy created” should be “Copy Created” on “Copy created” popup change
“Stay"" -> ""Continue Editing""
""Check out"" -> ""Go to Copy”
“Do you want to check out the copy” -> “<name-of-copy> created successfully.” (note spelling of successfully).
Move “+Add” button next to text input field
On Scripts Page:
--Change “Last run” -> “Last Executed”
--Change “Last run” -> “Last Results”
Add Feature to backend for Deleting History of Pig Script Execution
If a user deletes a script and, then “Script” tab should be greyed out and and there should be a tool tip available above “Script” tab that says “The script has been deleted and is no longer available.”
History Table order should show most recent run first by default.","Duplicated Code, Long Method, , , , , "
"Move Method,Extract Method,Move Attribute,","Provide stage resource information via REST API Currently, it is possible to query Ambari (via the REST API) for details about _asynchronous_ requests and their related tasks. This useful when trying to obtain progress information. However, some information necessary for the UI to indicate meaningful progress is not available. This information is related to the stages that are generated. 

*NOTE:* Each _asynchronous_ request is broken down into 1 or more stages and each stage contains 1 or more tasks.

If stage information was available via the REST API, it would be possible for the caller (maybe a UI) to track high-level tasks (at the {{stage}} level) rather than each lower-level unit of work (at the {{task}} level). 

To allow for this, a new API resource (and associated handler) needs to be created. The resource should be read-only (like {{requests}} and {{tasks}}), and should provide information stored in the {{stage}} table from the Ambari database. 

The following properties should be returned for each {{stage}}:

* stage_id
* request_id
* cluster_id
* request_context 
** _This should probably be renamed to something more appropriate, like stage_context, stage_name, or etc..._
* start_time
* end_time
* progress_percent
* status

It is expected that the resources would be queried using:

{code}
GET /api/v1/clusters/{clusterid}/requests/{requestid}/stages
{code}

Also, some subset of the stage data should be provided when querying for details about a specific {{request}}, like in:

{code}
GET /api/v1/clusters/{clusterid}/requests/{requestid}
{code}

See {{request}} and {{task}} resource for examples.


","Duplicated Code, Long Method, , , , "
"Move And Rename Class,Rename Method,Move Method,",Implement logic in Ambari to support common services. Implement logic in Ambari to support common services. See design discussed in AMBARI-7201.,", , "
"Rename Method,","Views: CapScheduler service endpoint for operator - Expose endpoint for operator setting
- cleanup unused code
- up'd version to 0.2.0",", "
"Move Class,Rename Class,Move And Rename Class,Rename Method,Move Method,Extract Method,Move Attribute,","Include configuration in exported blueprint Include both host group and cluster scoped configuration in an exported blueprint.

To export a blueprint, use the api:
AMBARI_HOST:8080/api/v1/clusters/:clustername?format=blueprint

The exported blueprint will contain the entire configuration for the associated cluster. The only properties which are not included are those that are marked as ""input required"" in the stack. All passwords are marked as required so they will not be exported. Also, any hostnames in the configuration properties are replaced with a hostgroup token: %HOSTGROUP::group1%. When a cluster is provisioned from the blueprint, the hostgroup tokens are resolved to host names for the target cluster.","Duplicated Code, Long Method, , , , "
"Move Method,Move Attribute,","Customize the Hadoop metrics sink to write to MySQL store The SqlServerSink should support pushing metrics to MySQL store.

This Jira addresses changes needed to support sink to a MySQL store.",", , , "
"Pull Up Method,Pull Up Attribute,","Customize the Hadoop metrics sink to write to MySQL store The SqlServerSink should support pushing metrics to MySQL store.

This Jira addresses changes needed to support sink to a MySQL store.",", Duplicated Code, Duplicated Code, "
"Pull Up Method,Move Method,Pull Up Attribute,Move Attribute,","Provide topology validation when creating blueprint Provide topology validation when a blueprint is being created.
Topology is validated based on dependency and cardinality information provided in the associated Ambari stack definition. I addition to validation, some components, mostly clients, are auto-deployed if not present in the topology. Again, this behavior is specified in the corresponding stack.

If topology validation fails, a 400 response is returned with a descriptive message:
{code}
{
status: 400
message: ""Cluster Topology validation failed. Invalid service component count: [MYSQL_SERVER(actual=0, required=1), HIVE_METASTORE(actual=0, required=1), HIVE_SERVER(actual=0, required=1)]. To disable topology validation and create the blueprint, add the following to the end the url: '?validate_topology=false'""
}
{code}
To create a blueprint that doesn't validate, a user can disable topology validation in the URL via '?validate_topology=false'.

The following cases are not currently supported by blueprint topology validation and will require a user to disable validation when creating a blueprint.
- external reference to MYSQL_SERVER or HIVE_METASTORE
- HA topology with > 1 NN and < 1SNN
- any other topology in which the default cardinality is is conditional on other topology information. HA is one example of this.",", , , Duplicated Code, Duplicated Code, "
"Move Class,Move Method,",Moving Clusters and Nodes container objects into controller Moving Clusters and Nodes container objects into controller. Currently they are part of client/entities.,", , "
"Rename Method,",Moving Clusters and Nodes container objects into controller Moving Clusters and Nodes container objects into controller. Currently they are part of client/entities.,", "
"Rename Method,","Validate required fields including passwords for blueprint cluster creation For blueprint creation, validate all non-password required properties have been set in the blueprint.

For cluster creation via a blueprint, validate that all required password properties have been set in configuration or that a 'default_password' property has been included in the request. Password properties can be set in the blueprint cluster or host group configurations or as part of the cluster create call as either cluster or host group properties.

",", "
"Rename Method,Extract Method,","Provide basic validation of fields for blueprint create api This task is for non topology related validation of blueprint create api calls.
The following validations are included:

- no blueprint name
- no host groups
- no host group name
- no host group components
- component specified with no name
- invalid component name for stack
- no/invalid stack name
- no/invalid stack version

Validation failures result in a 400 response with a descriptive message of the problem.","Duplicated Code, Long Method, , "
"Rename Method,Move Method,","Add dependency related information to stacks in the REST API Added dependency related information to stacks in the REST API. This information describes dependencies between components which are part of a stack. 

A new dependencies resources will be added as a child to the stacksServices/serviceComponent (soon to be renamed to services/component) resource.

For example:
GET http://172.18.192.3:8080/api/v1/stacks/HDP/versions/1.3.3/stackServices/HBASE/serviceComponents/HBASE_MASTER?fields=dependencies/*

{code}
{
""href"" : ""http://172.18.192.3:8080/api/v1/stacks/HDP/versions/1.3.3/stackServices/HBASE/serviceComponents/HBASE_MASTER?fields=dependencies/*"",
""StackServiceComponents"" : {
""component_name"" : ""HBASE_MASTER"",
""service_name"" : ""HBASE"",
""stack_name"" : ""HDP"",
""stack_version"" : ""1.3.3""
},
""dependencies"" : [
{
""href"" : ""http://172.18.192.3:8080/api/v1/stacks/HDP/versions/1.3.3/stackServices/HBASE/serviceComponents/HBASE_MASTER/dependencies/HDFS_CLIENT"",
""Dependencies"" : {
""component_name"" : ""HDFS_CLIENT"",
""dependent_component_name"" : ""HBASE_MASTER"",
""dependent_service_name"" : ""HBASE"",
""scope"" : ""host"",
""service_name"" : ""HDFS"",
""stack_name"" : ""HDP"",
""stack_version"" : ""1.3.3""
},
""auto_deploy"" : {
""enabled"" : true
}
},
{
""href"" : ""http://172.18.192.3:8080/api/v1/stacks/HDP/versions/1.3.3/stackServices/HBASE/serviceComponents/HBASE_MASTER/dependencies/ZOOKEEPER_SERVER"",
""Dependencies"" : {
""component_name"" : ""ZOOKEEPER_SERVER"",
""dependent_component_name"" : ""HBASE_MASTER"",
""dependent_service_name"" : ""HBASE"",
""scope"" : ""cluster"",
""service_name"" : ""ZOOKEEPER"",
""stack_name"" : ""HDP"",
""stack_version"" : ""1.3.3""
},
""auto_deploy"" : {
""enabled"" : true,
""location"" : ""HBASE/HBASE_MASTER""
}
}
]
}
{code}

",", , "
"Extract Interface,Rename Method,Extract Method,","Allow host group scoped configuration to be specified in Blueprint Allow configuration to be specified for host groups. This configuration will provide overrides for all hosts assigned to the host group. This configuration is specified inline within the host group. 

An example of a simple one host group blueprint with both cluster scoped and host group scoped configuration.

{code}
{
""configurations"" : [
{
""core-site"" : {
""fs.trash.interval"" : ""480"",
""ipc.client.idlethreshold"" : ""8500"",
""my.awesome.property"" : ""excellent""
}
},
{
""mapred-site"" : {
""tasktracker.http.threads"" : ""45""
}
}
],
""host_groups"" : [
{
""name"" : ""host_group_1"",
""configurations"" : [
{
""core-site"" : {
""fs.trash.interval"" : ""475""
}
}
], 
""components"" : [
{
""name"" : ""HDFS_CLIENT""
},
{
""name"" : ""GANGLIA_SERVER""
},
{
""name"" : ""AMBARI_SERVER""
},
{
""name"" : ""MAPREDUCE_CLIENT""
},
{
""name"" : ""GANGLIA_MONITOR""
},
{
""name"" : ""DATANODE""
},
{
""name"" : ""NAMENODE""
},
{
""name"" : ""JOBTRACKER""
},
{
""name"" : ""HISTORYSERVER""
},
{
""name"" : ""SECONDARY_NAMENODE""
},
{
""name"" : ""NAGIOS_SERVER""
},
{
""name"" : ""TASKTRACKER""
}
],
""cardinality"" : ""1""
}
],
""Blueprints"" : {
""blueprint_name"" : ""single-node-test"",
""stack_name"" : ""HDP"",
""stack_version"" : ""1.3.3""
}
}
{code} 

*Note:* Allowing service configurations which are specified external to the host group definition to allow for configuration reuse across host groups will be handled in a subsequent Jira.

","Duplicated Code, Long Method, , Large Class, "
"Rename Method,Extract Method,","Provision a cluster from an Ambari Blueprint Allow a cluster to be fully provisioned via the Ambari REST API with a single REST API call by specifying a blueprint name. This means that from a single, simple asynchronous REST API call, a cluster can now be provisioned from INIT->STARTED.

*The process to provision a cluster via the REST API using blueprints is as follows:*
* Create a blueprint resource instance on the Ambari instance which is going to be used to provision the cluster. The blueprint may have been hand created or exported from a running cluster. 

POST myAmbariServer:8080/api/v1/blueprints/single-node-test
{code}
{
""host_groups"" : [
{
""name"" : ""host_group_1"",
""components"" : [
{
""name"" : ""HISTORYSERVER""
},
{
""name"" : ""JOBTRACKER""
},
{
""name"" : ""NAMENODE""
},
{
""name"" : ""OOZIE_SERVER""
},
{
""name"" : ""NAGIOS_SERVER""
},
{
""name"" : ""SECONDARY_NAMENODE""
}, 
{
""name"" : ""AMBARI_SERVER""
},
{
""name"" : ""GANGLIA_SERVER""
},
{
""name"" : ""GANGLIA_MONITOR""
},
{
""name"" : ""DATANODE""
},
{
""name"" : ""TASKTRACKER""
},
{
""name"" : ""HDFS_CLIENT""
},
{
""name"" : ""MAPREDUCE_CLIENT""
},
{
""name"" : ""OOZIE_CLIENT""
},
{
""name"" : ""GANGLIA_MONITOR""
}
],
""cardinality"" : ""1""
}
],
""Blueprints"" : {
""blueprint_name"" : ""single-node-test"",
""stack_name"" : ""HDP"",
""stack_version"" : ""1.3.3""
}
}
{code}
** For more information on exporting a blueprint see: https://issues.apache.org/jira/browse/AMBARI-4786 
** For more information on the blueprint resource see: https://issues.apache.org/jira/browse/AMBARI-4467

* Invoke a POST on the cluster endpoint and specify the blueprint related information. Host information must be provided for each host group being deployed in the cluster. Each host specified will run the components associated with the host group. The host group names specified in the request must match a host group in the specified blueprint.

POST myAmbariServer:8080/api/v1/clusters/c1
{code}
{
""blueprint"" : ""single-node-test"",
""host_groups"" :[
{ 
""name"" : ""host_group_1"", 
""hosts"" : [ 
{ 
""fqdn"" : ""myHost.novalocal"", 
""ip"" : ""172.18.192.3"" 
}
]
}
] 
{code}

This call is asynchronous and returns information that can be used to check the status of the request
{code}
202 Accepted
{
""href"" : ""http://your.ambari.server/api/v1/clusters/c1/requests/1"",
""Requests"" : {
""id"" : 1,
""status"" : ""InProgress""
} 
}
{code}
Some limitations in this change which will be addressed shortly in other patches:
* configuration information included in the blueprint will not be processed meaning that the cluster will be provisioned with default configurations
* Ambari Agent must be running and have registered with the server on all hosts in the cluster prior to invoking the REST API to provision the cluster","Duplicated Code, Long Method, , "
"Extract Interface,Extract Method,","Add ability to export a blueprint from a running cluster Export a blueprint for a running cluster using an alternate rendering for the cluster resource.

api/v1/clusters/c1?format=blueprint

For this change, the blueprint will be very minimal and will only contain node groups and no configuration. Subsequent patches will introduce configuration and other cluster data.

{code}
{
""host_groups"" : [
{
""name"" : ""host_group_1"",
""components"" : [
{
""name"" : ""HISTORYSERVER""
},
{
""name"" : ""OOZIE_CLIENT""
},
{
""name"" : ""JOBTRACKER""
},
{
""name"" : ""NAMENODE""
},
{
""name"" : ""OOZIE_SERVER""
},
{
""name"" : ""TASKTRACKER""
},
{
""name"" : ""NAGIOS_SERVER""
},
{
""name"" : ""SECONDARY_NAMENODE""
},
{
""name"" : ""MAPREDUCE_CLIENT""
},
{
""name"" : ""AMBARI_SERVER""
},
{
""name"" : ""GANGLIA_SERVER""
},
{
""name"" : ""HDFS_CLIENT""
},
{
""name"" : ""DATANODE""
},
{
""name"" : ""GANGLIA_MONITOR""
}
],
""cardinality"" : ""1""
}
],
""Blueprints"" : {
""blueprint_name"" : ""blueprint-c1"",
""stack_name"" : ""HDP"",
""stack_version"" : ""1.3.3""
}
}
{code}","Duplicated Code, Long Method, , Large Class, "
"Rename Method,","Create new stack with Gluster support for 1.3.2 HDP version This feature extends the ability for Ambari to support an Hadoop Compatible File System outside of HDFS. For this stack definition we are introducing the use of GlusterFS, but it provides a good road map for other Hadoop Compatible File Systems to also be able to leverage Ambari. The feature/patch does not remove the HDFS patch, but simply provides an alternative when selecting the services within the installer.",", "
"Move Method,Extract Method,","Extending Stack definition to remove duplication in stack metadata info. The idea is to allow extending a stack definition and overriding some of its values.

Current stack definition structure of files and directories:

{noformat}
|_ stacks
|_ <dist_name>
|_ <version_number>
metainfo.xml
|_ repos
repoinfo.xml
|_ services
|_ <service_name>
metainfo.xml
|_ configuration
configuration files
{noformat}
We introduce extends element at the top level (stack version level) that links to the parent stack definition for this stack version.

*Rules of extension*: 
1. All of the parent stack services are automatically a part of the child stack unless explicitly excluded.
2. Only one parent stack can be extended by the child stack.
3. <stacks>.<dist_name>.<repos>.repoinfo.xml, will not be overridden
4. All service configurations unless explicitly excluded are a part of the child service definition.

*1. Extending stack definition*

File: <stacks>.<dist_name>.<version_number>.metainfo.xml
E.g.: /stacks/HDP/1.3.1/metainfo.xml

{noformat}
< metainfo>
<versions>
<upgrade>1.2.0</upgrade>
</versions>
<active>true</active>
<extends>1.3.0</extends>
< /metainfo>
{noformat}

*2. Extending service definition*

< stacks>.<dist_name>.<version_number>.<service_name>.metainfo.xml
E.g.: /stacks/HDP/1.2.1/services/HDFS/metainfo.xml

{noformat}
< metainfo>
<user>root</user>
<comment>Apache Hadoop Distributed File System</comment>
<version>1.1.2</version>
<deleted>false<deleted>
< /metainfo>
{noformat}

*2.1 Extending component definition*

Components can be added or deleted from the stack.
{noformat}
< component>
<name>YARN_CLIENT</name>
<category>CLIENT</category>
<deleted>true</deleted>
< /component>
{noformat}


*3. Extending service configurations*

File: <stacks>.<dist_name>.<version_number>.<service_name>.<configuration>.configuration_file
E.g.: /stacks/HDP/1.3.1/services/HDFS/configuration/hdfs-site.xml

{noformat}
< configuration>
<property>
<name>dfs.name.dir</name>
<deleted>true</deleted>
</property>
< /configuration>
{noformat}

*Notes*: 
The same property can disappear and reappear in an extension graph.

{noformat}
1.3.0 ----------------> 1.3.1 ----------------> 1.3.2 ----------------> 1.3.5
a = b a = c a = c
{noformat}

","Duplicated Code, Long Method, , , "
"Rename Method,Extract Method,","Allow skipping parts of Add Service request validation Provide ability to disable parts of the validation for the Add Service request, eg. configuration validation and topology validation. Some parts still need to be validated (eg. it probably makes no sense to add unknown services).","Duplicated Code, Long Method, , "
"Move Class,Extract Method,",Improve Add Service request validation Improve request validation for Add Service request (eg. reject request to add existing service).,"Duplicated Code, Long Method, , "
"Move Method,Extract Method,Move Attribute,",Add AMS Metrics publisher to Infra Solr ,"Duplicated Code, Long Method, , , , "
"Move Class,Move Method,","Rename agent rest url to /agent, and public rest api to /rest For separating public API and private API, it may be best to rename /v1 to /agent and /rest respectively.",", , "
"Rename Class,Rename Method,Extract Method,Inline Method,",Infra Manager: define scheduling of archiving Infra Solr Documents ,"Duplicated Code, Long Method, , , "
"Rename Class,Move And Rename Class,Rename Method,Pull Up Method,Move Method,Extract Method,Inline Method,Move Attribute,","Infra Manager: scheduled deleting of Infra Solr documents delete Document from Infra Solr specified by an interval 
also in archiving mode delete Documents which are successfully exported and uploaded","Duplicated Code, Long Method, , , , , Duplicated Code, "
"Rename Class,Rename Method,Push Down Method,Move Method,Push Down Attribute,","Log Search Config should be separated into a Server and Log Feeder interface Log Search Config is getting too complex, it serves too many kind of things. It should be separated into two interfaces, one for the Server and one for the Log Feeders to make it cleaner.",", , , , "
"Move Method,Extract Method,Move Attribute,","Log Feeder properties should be handled by one class Log Feeder properties are scattered all over the code, with their documentation, putting large blocks of codes into several classes. Instead of it they should be handled by one class together with their documentation and default values, having all the rest of the classes asking that one for their value.","Duplicated Code, Long Method, , , , "
"Rename Method,Extract Method,","Log Search Solr output properties should be provided by the Config API The Solr output properties should be persisted into the Config API by the Log Search Server as the output is initialized, and fetched by he Log Feeders from there","Duplicated Code, Long Method, , "
"Move Method,Inline Method,Move Attribute,",Change Storage of Data on Request/Stage/Task To Reduce Redundency (trunk) Move stage.cluster_host_info to request.cluster_host_info,", , , , "
"Extract Superclass,Rename Method,Pull Up Attribute,",Log Search use POJOs for input configuration Instead of parsing the input configuration jsons in logfeeder the configuration api should return POJOs with the input and filter descriptions.,", Duplicated Code, Large Class, Duplicated Code, "
"Move Method,Extract Method,","Allow extensions to auto-link with supported stack versions It would possible to link extensions to supported stack versions while parsing the stacks, extensions and common-services directories.

This would allow extensions to avoid making rest API calls to set up the link.","Duplicated Code, Long Method, , , "
"Rename Class,Move And Rename Class,Move Method,Extract Method,Move Attribute,",Add Log Level Filter to the Log Search config API ,"Duplicated Code, Long Method, , , , "
"Rename Method,Extract Method,",Support removal of pending host requests from a TopologyRequest In Blueprint deployments it is supported to specify a host predicate and host count to control which hosts and how many hosts will automatically join the host group. In case later is decided that less hosts to be used for the host group thus only a few hosts (less than the original host count) will be provisioned currently there is no way to remove pending host requests (created for hosts never joined to the cluster) so actually reducing host count for a host group.,"Duplicated Code, Long Method, , "
"Rename Method,","Hosts tab: clicking on red badge should not toggle ""Alerts"" filter Clicking on the red badge in the Hosts tab should not toggle the ""Alerts"" filter on the Hosts page (clicking anywhere in Hosts tab should go to Hosts page with ""All"" selected).",", "
"Rename Method,Extract Method,","Allow acceptor / seclector configuration for API and agent connectors _Objectives_:
- Allow acceptors for agent and api connectors to be configurable
- The thread pool configuration did not take into account both 2-way and 1-way connectors are configured for agent every time although only one is used and not a mixed-mode. This causes insufficient threads in agent threadpool for a high cpu core environment.

","Duplicated Code, Long Method, , "
"Extract Method,Inline Method,","Ambari Server should work with MySQL and Oracle where the Ambari Server data might be stored Ambari Server should work with MySQL and Oracle where the Ambari Server data might be stored.
We need to create DDL scripts that will setup a already running MySQL server/Oracle Server. Ambari Server then needs to be configured to use the right adaptor for connecting to either MySQL or Oracle.","Duplicated Code, Long Method, , , "
"Move Method,Extract Method,","Provide support for S3 as a first class destination for log events AMBARI-17045 added support for uploading Hadoop service logs from machines to S3. The intended usage there was as a one time trigger where, on-demand, the log files matching certain paths can be uploaded to a given S3 bucket and path.

While useful, there are some use cases where we might need more than this one time activity, particularly when clusters are deployed on ephemeral machines such as cloud instances:

* The machines running the logfeeder could be irrevocably lost and in that case we would not be able to retrieve any logs.
* If we are copying logs at one time, that were generated over a long period of time, the time to copy all the logs at the end could extend cluster up-time and cost.

It would be nice to have an ability to support S3 as another output destination in logsearch just like Kafka, Solr etc. This JIRA is to track work towards this enhancement.","Duplicated Code, Long Method, , , "
"Rename Method,","Controller marks nodes unhealthy upon command execution failures AMBARI-171 handles retries of commands on the agent side. This jira makes the controller aware of nodes where repeated tries of any command execution failed, and marks such nodes unhealthy. The nodes are put back in the healthy state when the agent is restarted.",", "
"Rename Method,","Fix misnamed Zookeeper connect strings in Log Search Variables/properties holding zookeeper connect strings are misnamed as zk_host, or zk_hosts, which may be misleading. Variable / property names fixed.",", "
"Rename Method,",Support loading of logs to S3 Upload logs to S3 so it can be archived and analyzed offline.,", "
"Rename Method,Extract Method,","Implement in ams collector batch insert operations to ams-hbase Check if there is performance boost after replacing insert operations with bulk loading. 
If yes, implement and provide switch property for enabling/disabling this feature.","Duplicated Code, Long Method, , "
"Rename Method,Inline Method,","Implement in ams collector batch insert operations to ams-hbase Check if there is performance boost after replacing insert operations with bulk loading. 
If yes, implement and provide switch property for enabling/disabling this feature.",", , "
"Rename Method,","Ambari Should Suspend Alerts Notifications During Upgrade Ambari reports alerts and triggers notifications during a stack upgrade. In most cases, alert notifications should be suppressed during the upgrade to prevent false positives. 

However, some alerts, such as those which don't related to the cluster, should remain fully operational:

- Host disk space
- Upgrade not finalized
- Ambari Server Performance",", "
"Rename Class,Rename Method,Move Method,Extract Method,Pull Up Attribute,","Incremental changes to LogSearch to bring it up to date in the trunk During the initial LogSearch commit, some changes were not included. This JIRA tracks these addition items.

1. Use standard tokenizer for indexing.
2. Configuring Log level filter for LogFeeder 
3. Tab graph clean up","Duplicated Code, Long Method, , , Duplicated Code, "
"Rename Method,Extract Method,","Ambari LDAP integration cannot handle LDAP directories with multiple entries for the same user *Problem:*
In case LDAP set up with multiple Domains which are joined into a Forrest with trusts between the different Domains users may appear in different locations in LDAP.

Since users who wants to access Ambari can be in any domain Ambari has to search the whole forrest, and as the users appearing in multiple domains are identical Ambari cannot filter out all but one of the user entries.

This leads to the following error message when they try to login to Ambari with one of the users that has multiple entries:
{code}
ServletHandler:563 - /api/v1/users/USERNAME 
org.springframework.dao.IncorrectResultSizeDataAccessException: Incorrect result size: expected 1, actual 2 
at org.springframework.security.ldap.SpringSecurityLdapTemplate.searchForSingleEntryInternal(SpringSecurityLdapTemplate.java:243) 
at org.springframework.security.ldap.SpringSecurityLdapTemplate$3.executeWithContext(SpringSecurityLdapTemplate.java:198) 
at org.springframework.ldap.core.LdapTemplate.executeWithContext(LdapTemplate.java:807) 
at org.springframework.ldap.core.LdapTemplate.executeReadOnly(LdapTemplate.java:793) 
at org.springframework.security.ldap.SpringSecurityLdapTemplate.searchForSingleEntry(SpringSecurityLdapTemplate.java:196) 
at org.springframework.security.ldap.search.FilterBasedLdapUserSearch.searchForUser(FilterBasedLdapUserSearch.java:116) 
at org.springframework.security.ldap.authentication.BindAuthenticator.authenticate(BindAuthenticator.java:90) 
at org.apache.ambari.server.security.authorization.AmbariLdapBindAuthenticator.authenticate(AmbariLdapBindAuthenticator.java:53) 
at org.springframework.security.ldap.authentication.LdapAuthenticationProvider.doAuthentication(LdapAuthenticationProvider.java:178) 
at org.springframework.security.ldap.authentication.AbstractLdapAuthenticationProvider.authenticate(AbstractLdapAuthenticationProvider.java:61) 
at org.apache.ambari.server.security.authorization.AmbariLdapAuthenticationProvider.authenticate(AmbariLdapAuthenticationProvider.java:60) 
at org.springframework.security.authentication.ProviderManager.authenticate(ProviderManager.java:156) 
at org.springframework.security.authentication.ProviderManager.authenticate(ProviderManager.java:174) 
at org.springframework.security.web.authentication.www.BasicAuthenticationFilter.doFilter(BasicAuthenticationFilter.java:168) 
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342) 
at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:87) 
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:342) 
at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:192) 
at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160) 
at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:237) 
at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:167) 
at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467) 
at org.apache.ambari.server.api.MethodOverrideFilter.doFilter(MethodOverrideFilter.java:72) 
at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467) 
at org.apache.ambari.server.api.AmbariPersistFilter.doFilter(AmbariPersistFilter.java:47) 
at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467) 
at org.eclipse.jetty.servlets.UserAgentFilter.doFilter(UserAgentFilter.java:82) 
at org.eclipse.jetty.servlets.GzipFilter.doFilter(GzipFilter.java:294) 
at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467) 
at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:501) 
at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137) 
at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557) 
at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231) 
at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086) 
at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:429) 
at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193) 
at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020) 
at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135) 
at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:209) 
at org.apache.ambari.server.controller.AmbariHandlerList.processHandlers(AmbariHandlerList.java:198) 
at org.apache.ambari.server.controller.AmbariHandlerList.handle(AmbariHandlerList.java:132) 
at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116) 
at org.eclipse.jetty.server.Server.handle(Server.java:370) 
at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494) 
at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971) 
at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033) 
at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644) 
at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235) 
at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82) 
at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:696) 
at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:53) 
at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608) 
at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543) 
at java.lang.Thread.run(Thread.java:745)
{code}

*Solution:*
If the LDAP search upon login to Ambari leads to multiple match user match due to the user appears in multiple domains show an error message to user prompting for providing domain as well to log-in. (e.g. _Login Failed: Please append your domain to your username and try again. Example: username@domain_)

When user provides domain information at login as well Ambari looks up the user in LDAP using different filter which is configurable. If this configuration is not set Ambari defaults to filter by _userPrincipalName_","Duplicated Code, Long Method, , "
"Rename Class,Rename Method,",Introduce a configuration file Introduce a configuration file that allows the user to pick the data store implementation.,", "
"Move Class,Rename Class,Move Method,Move Attribute,",Introduce a configuration file Introduce a configuration file that allows the user to pick the data store implementation.,", , , "
"Rename Method,","simplify states in controller state machine There are some states that are redundant such as the STOPPED state. There are also some state transitions that end up in a dead end.
",", "
"Extract Method,Inline Method,","Provide Metrics discovery API for AMS {quote}/ws/v1/timeline/metrics/metadata{quote}
Endpoint provides JSON data representing Metrics metadata for all metrics that were ever received by AMS groupbed by the appId that sent the metrics.

{quote}/ws/v1/timeline/metrics/hosts{quote}
Endpoint provides JSON data representing hosts along with the apps that reported a metric","Duplicated Code, Long Method, , , "
"Extract Interface,Rename Method,Move Method,Move Attribute,",Investigate and implement Guice in StateMachineInvoker I think some thought should be put in as to how guice can be applied in the StateMachineInvoker to aid testability. I'd also like to remove the static datastructures since they make it difficult to use the class in unit tests.,", , , Large Class, "
"Move Class,Pull Up Method,Pull Up Attribute,",Refactor Collector logging for AMS Current aggregator logs are hard to read since the log prints out the class name which does not sufficiently indicate which aggregator thread is responsible for the message.,", Duplicated Code, Duplicated Code, "
"Rename Method,Extract Method,","Improve Job Diagnostics There are a number of improvements I'd like to make to the job diagnostics, including allowing zooming in the job swimlane / timeline view and overlaying additional metrics on the same timeline.","Duplicated Code, Long Method, , "
"Rename Method,","Allow client to specify a ""context"" value for asynchronous requests This context value will be added to the associated ""request"" resource to give some context as to what the request was doing. The context value will only have meaning for asynchronous requests and will be ignored for synchronous requests. This is a request from the UI team.",", "
"Extract Method,Inline Method,",Make Hosts table update dynamically ,"Duplicated Code, Long Method, , , "
"Rename Method,",Improve error checking for blueprint resource creation Improving error checking to avoid NPE errors on blueprint resource creation when request is missing,", "
"Rename Method,","Ambari API: Add additional query operator 'isEmpty' for categories Add additional query operators including 'isEmpty' to determine if a category contains any properties.

For example: ?category.isEmpty()
Should return true if there are no properties in the category named 'category'; otherwise it should return false.",", "
"Rename Method,Move Method,","Dynamic stack extensions - install and upgrade support for custom services The purpose of this proposal is to facilitate adding custom services to an existing stack. Ideally this would support adding and upgrading custom services separately from the core services defined in the stack. In particular we are looking at custom services that need to support several different stacks (different distributions of Ambari). The release cycle of the custom services may be different from that of the core stack; that is, a custom service may be upgraded at a different rate than the core distribution itself and may be upgraded multiple times within the lifespan of a single release of the core distribution. 

One possible approach to handling this would be dynamically extending a stack (after install time). It would be best to extend the stack in packages where a stack extension package can have one or more custom services.",", , "
"Rename Method,","Improve Agent Registration and Heartbeat json Improve data coming back from agent registration to include rpm queries, more flexible way to add directories",", "
"Rename Method,Extract Method,","CapSched Improvements - Add Automatic ResourceManager URL configuration for http/https
- Validate that Queue capacity adds up to 100%
- Allow spaces in configuration:

{code}
<property>
<name>yarn.scheduler.capacity.root.acl_administer_queue</name>
<value> </value>
</property>

<property>
<name>yarn.scheduler.capacity.root.acl_submit_applications</name>
<value> </value>
</property>
{code}
- Do not allow views to be created with duplicate names
- Do not allow views to be created with empty names
","Duplicated Code, Long Method, , "
"Rename Method,","RU Improvements. Part 1
* Important: Before running HDFS Finalize, run a Server Action, similar to FinalizeUpgradeAction, that will confirm that all host components have been upgraded to the version. This action is to inform the user, so it may fail and the user can skip. [Update] Just modify FinalizeUpgradeAction to do this work.
* /api/v1/clusters/c1/hosts/host_name/host_components/component_name to show the current version field",", "
"Rename Method,","AMQ-975 provide a way of setting the timeToLive from the point in time the message is received by the broker to avoid clock sync issues The JMS default is to use timeToLive relative to the client send; which means things get converted to GMT and can suffer from clock sync issues.

A useful alternative could be to set the timeToLive on a message relative to the broker's clock when it receives it. That way there's no need to rely on a properly sync'd clock.

So we could either 

* use a new header - ActiveMQBrokerTimeToLive or something.
* use a negative time to live value to indicate, its the time to live relative to the broker (rather than relative to the client)

Am wondering if the negative timeToLive value would break any existing software? (Currently we tend to ignore any TTL values less than or equal to zero)",", "
"Rename Method,Extract Method,","AMQ-1109 JDBC based Master/Slave not supported for TransactSQL based databases (SQL Server and Sybase) The main issue is figuring out the exclusive lock SQL syntax. I think the following is valid...

SELECT * FROM TABLE WITH XLOCK","Duplicated Code, Long Method, , "
"Rename Method,","provide a new simple API so end users can view available destinations and query them to find their statistics (such as queue depth etc) From the dev list..

On 3/9/07, Dhawan, Vikram (LNG-DAY) <vikram.dhawan@lexisnexis.com> wrote:
> Hi,
> Is there a direct way using OpenWire Client API to get the number of
> consumers attached to a Queue at a given point of time? I know it can be
> done using JMX API but I was wondering if it can be done using Open Wire
> client api.

4.2 has a command agent...
http://activemq.apache.org/command-agent.html

which can be used for sending simple text commands to the broker and getting replies - such as to enquire about queue depths and the like.

There's also advisory messages...
so you can listen to the operation of the broker, seeing new destinations, consumers, producers etc.

However we've never yet wrapped up a little easy API folks can use on the client to easily view the available destinations and get their stats etc.


Something like the following...

{code}
Collection<ActiveMQQueue> connection.findQueues()
Collection<ActiveMQTopic> connection.findTopics()

// look up stats like queue depth etc
DestinationStats getDestinationStatistics(ActiveMQDestination);
{code}
",", "
"Move Class,Extract Method,","allow a broker to be configured using a properties file So running the broker via

{code}
activemq properties:foo.properties
{code}

Or using the BrokerFactory using Java code","Duplicated Code, Long Method, , "
"Rename Method,",Display the establised Neteowork Connector Bridges via JMX ,", "
"Rename Method,","Allow to view connections and consumers in the WebConsole This patch enables viewing of open connections and active consumers on queues in the activemq web-console.
It also contains a minor fix enabling the use of password protected connections to remote brokers.",", "
"Move Method,Move Attribute,","Logging improvement contribution Hello people,

After our proposal some time ago (view http://www.nabble.com/Logging-improvement-proposal-tf3570794s2354.html#a9976200), we would like to add a little contribution to the ActiveMQ code, and we hope that the ActiveMQ developers and users like it.
We thought it would be good to improve ActiveMQ logging's capabilities, in order to make debugging, tracing, and understanding ActiveMQ easier.
For this, we have added customization of ActiveMQ's Transport level logging, allowing users to write their own logging formats. Also, we have added dynamic control of this logging functions (via JMX). We have also developed a simple tool to analyze the log files produced.
We have split the contribution into 2 patches, one to add customized log formats functionality, and the other to add dynamic management of logging via JMX. 1st patch is against SVN rev. 564978. 2nd patch is against 1st patch (incremental).

*A. New features*
(1) Logging enhancements (1st patch):
It is now possible to customize what will be written to the log file at transport level, by writing a custom class which implements the new LogWriter interface.
Also, the option: logWriterName = name has to be added to a transport URI to choose which class (format) will be used (more details on how to implement your own class / format later).
To activate transport logging, the trace=true flag has to be used.
People who don't want to use the new functionality will not have to change their URI's.

(2) Dynamic control of logging enhancements (2nd patch):
(a) Added ability to reload log4j.properties file to the BrokerView MBean.
(b) If the option dynamicManagement=true is appended to the URI, transport logging can be managed through JMX:
-Logging can be switched on / off for every transport or for all of them at once.
-If the option startLogging=false is appended to the URI, a transport will initially not log, but a TransportLogger instance will be created, and later it can be activated by JMX.
-Another option lets the user change the JMX port used for these functions.

(3) Log parsing and analysis tool:
The tool parses log files (any number of them) of the CustomLogWriter format (our own implementation of the LogWriter interface) and has the following features:
(a) The tool detects incorrect situations at Transport level: sent but not received messages, duplicate messages at transport level.
(b) The tool can show the communication sequence for a message (""travel path"" of a message).
(c) Other features:
(c1) Loading of log files is done by choosing a directory with them.
(c2) Incorrect features can be filtered per type or per connection.
(c3) Long, hard to read connection / client IDs can be replaced by short, easy to compare IDs.
(c4) Summary of connections, producers, consumers, log files.
Some screen shots are included.

(Continues on first comment)",", , , "
"Rename Class,Extract Method,","support mirrored queues to make it easier to monitor message flows via a topic (or queue via virtual topics) for things like BAM or monitoring etc Mirrored queues would be very handy; so that if enabled folks can monitor a virtual topic to see messages being delivered to a topic; while being out of band of the queue message flow. i.e. so that any existing queue based application works as before, but meanwhile other services can consume those same messages via a topic or virtual topic","Duplicated Code, Long Method, , "
"Rename Class,Rename Method,Extract Method,Inline Method,","Stomp frame translator improvements * added Spring context for marshaller
* changed header semantics, e.g jms-xml to jms-object-xml
* added support for map and byte messages
* added ""transformation error"" header
* ignored jms object ""transformation"" header","Duplicated Code, Long Method, , , "
"Rename Method,","Improvements/Bug Fixes for LDAP Discovery Mechanism (LDAP Network Connector) These are much needed improvements and bug fixes for the AMQ-358. The original patch worked for only a subset of cases. I have updated the original network connector I submitted with the following features and fixes.

- detection and handling of multiple LDAP entries pointing to the same upstream broker
- anonymous binding support (not everyone wants to put their login credentials in an XML file!)
- LDAP server failover support
- general logging improvements
- fix bug that only allowed a single discovered network connector from a broker
- persistent search capabilities allowing a broker to stay in sync with the LDAP server entries (only works for LDAP servers who support the extensions defined in [draft-ietf-ldapext-psearch-03.txt|http://www.ietf.org/proceedings/01aug/I-D/draft-ietf-ldapext-psearch-03.txt])
",", "
"Move Method,Extract Method,","""Confirmation on delivery"" and ""Confirmation on arrive"" reporting messages support. It would be nice if ActiveMQ could support ""Confirmation on delivery"" and ""Confirmation on arrive"" report messages such as IBM WebSphereMQ does.
For example, the broker could send a report message to the sending application when it puts the message in the target queue
or when the application gets it off the queue.","Duplicated Code, Long Method, , , "
"Move And Rename Class,Rename Method,",Make activemq-console jar a osgi bundle so it can be re-used from servicemix 4 ,", "
"Move Method,Move Attribute,","Create Simple Java Stomp client This is something we must have mostly for our testing purposes. Writing complex Stomp test cases is tedious and results in unreadable tests. Part from that, a few users asked for one, so it would be good to have it.

I've started work on it, basically adding methods to StompConnection class (which will be moved from 'test' to 'main' tree). We also need an easy way to parse Stomp frames from strings, so we can check frames broker returns and extract headers such as message-id. I'll add a method in StompFrame for that.",", , , "
"Rename Method,","Allow suppression of duplicate queue subscriptions in a cyclic network topology in a cyclic network of brokers (where each broker knows about each other) it is possible to have a cyclic graph and multiple routes across the network. this occurs because some brokers pick up the second order advisories that arise from a broker responding to an advisory from another broker. The result is that a consumer on one broker can manifest itself as multiple consumers on brokers across the network. Network priority gives precedence to the shortest route when it is configured. This enhancement would ensure that there is only one route for a given destination and makes the network more deterministic and a little simpler. With small numbers of brokers in the network, this is often what you want.

When topics are involved, the duplication leads to duplicate messages so duplicates for topics are suppressed by default on trunk and will be in 5.3. For queues, as the message goes to just one consumer, there is no duplicate issue, just some indeterminism in how a message is routed through the network. This indeterminism is a means of fault tolerance and can be a good thing so this feature is enabled via configuration for queues.

see some more background on the topic case @ https://issues.apache.org/activemq/browse/AMQ-2030",", "
"Move Method,Extract Method,Move Attribute,","Enable Tracing of Messages in a network of brokers In environments that have a larger network of brokers set up, it is sometimes of interest which path a message has taken from the producer to the consumer. One could enable tracing in the brokers and kind of review the log files, but this is very time consuming if not impossible. I propose a small BrokerPlugin that can be used to append the brokerName to a given message property. On the consumer side or in the logs we could then evaluate that property and see exactly which broker has been touched by the message.","Duplicated Code, Long Method, , , , "
"Rename Method,Extract Method,","AMQ-2448 Make network duplicate subscription suppression feature priority based when networkConsumerPrioritys are in play. Improvement to duplicate subscription suppression feature for queues http://issues.apache.org/activemq/browse/AMQ-2198
Rather than first subscription wins, allow a higher priority subscription to replace an existing duplicate subscription.

Given brokers A,B,C, a consumer on A can be visible on C as A-C or A-B-C. With suppression, only one would be allowed but it was non deterministic. With this improvement, when decreaseNetworkConsumerPriority is uses, A-C will replace A-B-C as it will have higher priority.","Duplicated Code, Long Method, , "
"Rename Method,Extract Method,","Allow xmpp clients to message each other directly, more tests, updated schema Am working on getting the activemq-xmpp transport to work as a simple XMPP server so I can unit test the camel-xmpp component. While multi-user chat worked great my direct chat tests would fail. So I've implemented this functionality in activemq-xmpp and a bunch of tests for it as well. Also updated the xmpp schema files and added one though there weren't many changes.","Duplicated Code, Long Method, , "
"Rename Method,","AMQ-2904 Failover connection recovery needs a new command to indicate recovery completion that can gate dispatch on a recovered consumer Unconsumed messages at a consumer need to be rolledback on recovery as they can get redispatched in arbitrary order. see - https://issues.apache.org/activemq/browse/AMQ-2573
As operations are in progress, like a send transaction, the rollback cannot happen till the send transaction commit completes so it must be async with the failover interruption. Dispatch needs to be gated on completion of the outstanding operations as it currently is with the resolution to AMQ-2573

However there is the possibility that the broker starts to dispatch to that consumer/connection before recovery is complete and can block the receipt of messages, the response to the send commit for example as the dispatch is waiting for the send to complete so that any unconsumed messages are rolledback in advance of dispatch. With asyncDispatch=false and optimizedDispatch it is possible to simulate this. 

The solution requires two wireformat changes, An indication on a connection that it is recovering (this can be propagated to a consumer) and an indication that recovery is complete such that dispatch on a recovered consumer can complete. An additional AckMode AckRecoveryComplete could do it.
Thus dispatch would be gated such that it cannot interfere with outstanding work that needs to be restored and completed inorder to correctly clear unconsumed and delivered messages.",", "
"Move Method,Extract Method,Move Attribute,",Implement stomp+nio+ssl transport ,"Duplicated Code, Long Method, , , , "
"Rename Method,","Make JDBC store resilient on broker sequence id order Currently if the message is sent in a transaction, there's a chance that messages are added to the cursor out of order (regarding broker seq id). The problem with JDBC store is that it does message recovery based on this seq id, which can lead to all kind of problems (such as orphaned messages in the database).

The solution is to refactor JDBC store to use its own seq generator for recovering purposes and replace broker seq id, with message id for all other operations",", "
"Pull Up Method,Extract Method,","Make JDBC store resilient on broker sequence id order Currently if the message is sent in a transaction, there's a chance that messages are added to the cursor out of order (regarding broker seq id). The problem with JDBC store is that it does message recovery based on this seq id, which can lead to all kind of problems (such as orphaned messages in the database).

The solution is to refactor JDBC store to use its own seq generator for recovering purposes and replace broker seq id, with message id for all other operations","Duplicated Code, Long Method, , Duplicated Code, "
"Rename Method,Extract Method,","Add support for setting the Differentiated Services or Type Of Service on outgoing TCP/IP packets to support Quality Of Service It should be possible to specify the desired Differentiated Services class, as outlined in RFC 2475 (http://tools.ietf.org/html/rfc2475), or Type of Service value, on outgoing TCP/IP packets by specifying a diffServ or typeOfService Tcp Transport Option (http://activemq.apache.org/tcp-transport-reference.html).

e.g. tcp://somehost:61616?trace=false&soTimeout=60000&diffServ=AF21
e.g. tcp://somehost:61616?trace=false&soTimeout=60000&typeOfService=3

I am part of a student group (http://maljub01.svnrepository.com/comp190/trac.cgi/wiki) at Tufts University that is implementing this functionality in ActiveMQ for potential use by MIT Lincoln Labs as part of the NextGen Network Enabled Weather Program (https://wiki.ucar.edu/display/NNEWD/The+NNEW+Wiki)..

We would like to contribute our changes back to the ActiveMQ trunk, if possible. Attached is a initial patch against the 5.3.0 release version, which implements setting the Differentiated Services class via a Tcp Transport Option on the connection URI.

Some notes about this approach:

The basic underlying mechanism for actually setting the bits in the packet headers is the java.net.Socket.setTrafficClass method. This was
the most elegant implementation out of all the possible implementations that we came up with.

In order for setTrafficClass to work in JDK 6, it is necessary to set the System property java.net.preferIPv4Stack to be true. We found that this has precedent in ActiveMQ (http://activemq.apache.org/multicast-watch-out-for-ipv6-vs-ipv4-support-on-your-operating-system-or-distribution-or-network.html), but we are hoping that this issue will be resolved in JDK 7, so that we can use the IPv6 stack when possible.

In addition, the current implementation only sets the specified Differentiated Services bits on the outgoing packets, and has no control over the Acknowledgments sent back for those packets. We have yet to find an elegant and cross-platform way to have the ActiveMQ Broker find out what the Differentiated Services bits on the incoming packets are directly in Java, although we are considering approaches that would involve calling a shell script from TcpTransportServer that would utilize IPtables.

We would like to know if might be interested in accepting this work into the ActiveMQ trunk. The sooner you let us know one way or another, the better, as this is a single-semester project.","Duplicated Code, Long Method, , "
"Rename Method,","Make fileserver app jetty-neutral Currently the app uses some classes from jetty-util. Also, we'd want to trim the size of the war as it doesn't need all these jars in the WEB-INF/lib folder.",", "
"Pull Up Method,Extract Method,","AMQ-2680 Allow setting storeUsage limit per individual queue/topic At the moment it's only possible to configure a storeLimit per broker.

In some cases, you want to set an upper bound for an individual queue, to guarantee that there will be storage available for other queues.
For example, consider a setup with a request queue, and a response queue, where both request and response messages are huge.

In that situation, we want to set an overall store limit, to prevent flooding the filesystem.
At the same time, we don't want to allow the request queue to consume 100% of the broker store.

For example, we'd like to define:
total broker store limit: 20 GB
queue ""request_queue"" store limit: 15 GB
queue ""response_queue"" store limit: 5 GB
","Duplicated Code, Long Method, , Duplicated Code, "
"Move Method,Extract Method,Move Attribute,","Provide visibility onto destination SlowConsumerStrategy via JMX Would be nice to have JMX visibility and possibly some ability to force abort for the SlowConsumerStrategy from https://issues.apache.org/activemq/browse/AMQ-378

Ability to see the current list of slow consumers and a manual abort option.","Duplicated Code, Long Method, , , , "
"Rename Method,Extract Method,",Add Support For Message Priority Add support for delivering messages by priority.,"Duplicated Code, Long Method, , "
"Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Add ability for KahaDB log files to be created on a per-destination basis. KahaDB persistence uses rolling log files to store unconsumed messages. These are named db-1.log, db-2.log ... db3.log. At present these files contain messages for all destinations managed by the broker.

A configurable option could be added that would allow these files to be created on a per-destination basis. For example, if the broker contained two queue destinations, queue_1 and queue_2, the log files would become:

queue_1-1.log, queue_1-2.log....etc
queue_2-1.log, queue_2-2.log....etc

Each set of log files would only contain messages for the relevant destination. 

This would help in the following situation:

queue_1: receives one message every 15 seconds or so. These messages remain unconsumed for several hours.
queue_2: receives thousands of messages per second. These messages are consumed as they arrive.

At present, the scenario above leads to each log files containing a few messages that have yet to be consumed and thousands of messages that have been consumed, but the log file cannot be deleted until all messages logged in it have been consumed (which may be hours later).

Logging on a per-destination basis would allow the log files for queue_2 in the above example to be deleted, meaning the unconsumed messages on queue_1 take up far less disk space. This would also reduce the number of file handles required.","Duplicated Code, Long Method, , , , , "
"Rename Method,Extract Method,Move Attribute,","Add ability for KahaDB log files to be created on a per-destination basis. KahaDB persistence uses rolling log files to store unconsumed messages. These are named db-1.log, db-2.log ... db3.log. At present these files contain messages for all destinations managed by the broker.

A configurable option could be added that would allow these files to be created on a per-destination basis. For example, if the broker contained two queue destinations, queue_1 and queue_2, the log files would become:

queue_1-1.log, queue_1-2.log....etc
queue_2-1.log, queue_2-2.log....etc

Each set of log files would only contain messages for the relevant destination. 

This would help in the following situation:

queue_1: receives one message every 15 seconds or so. These messages remain unconsumed for several hours.
queue_2: receives thousands of messages per second. These messages are consumed as they arrive.

At present, the scenario above leads to each log files containing a few messages that have yet to be consumed and thousands of messages that have been consumed, but the log file cannot be deleted until all messages logged in it have been consumed (which may be hours later).

Logging on a per-destination basis would allow the log files for queue_2 in the above example to be deleted, meaning the unconsumed messages on queue_1 take up far less disk space. This would also reduce the number of file handles required.","Duplicated Code, Long Method, , , "
"Rename Method,","Implement custom brokerId assignment strategy In network of brokers, duplicate route detection is done by checking broker ids. After the restart, the broker ids change, which can cause duplicate routes and messages stuck due to reached ttl.

We need to provide a mechanism for users to configure how their broker ids are assigned and make sure that they stay the same after the restart. Which will ensure correct duplicate route detection even after some of the broker in the mesh is restarted.",", "
"Rename Method,Inline Method,","Outbound bridges should handle remote connectivity problems at startup With a JMS-to-JMS bridge, one can configure JmsQueueConnector or JmsTopicConnector in an outbound bridge mode. In this mode, the ""foreign"" JMS provider is a remote service and therefore subject to disconnections, failures, etc.

While the connectors do support reconnecting to a remote provider, the connectors will not start up correctly if the remote service is not available at start-up time.

The attached patch (with test cases) solves this problem by separating the initialization of the local and remote parts of the bridge. A failure during initialization of the remote broker is not considered fatal, since the bridge attempts to reconnect to the foreign provider when a message is received from the local broker anyway. 

This logic could also be implemented for inbound bridges but slightly more difficult in that it'll require a separate Thread to manage the connectivity state to the remote i.e. if the system fails to connect to the remote at startup time, a separate Thread can be created to periodically continue to try to connect. This would also apply for inbound bridges that lose their connection due to the remote losing connectivity or being recycled. This has not been done yet.",", , "
"Rename Method,Extract Method,","Full table scan for durable subs in jdbc store when priority enabled; very slow with large message backlog Priority support is delegate to the DB, but it currently requires a full table scan to recover a new batch of messages. This can be very slow when the message table is large (a consumer has been offline for some time). With multiple concurrent consumers this gets worse.
The store needs to use more internal state w.r.t priority such that simpler/faster queries are used.","Duplicated Code, Long Method, , "
"Rename Method,","Stomp Frame should mask passcode header in toString output, so it does not pollute the log Logging of stomp CONNECT frame includes the raw passcode. This should be masked with ""****""
so instead of:{code}22011-03-18 09:38:30,634 [38.171.43:40701] WARN TransportConnection - Failed to add Connection ID:xx.xx.38456-99-2:3, reason: java.lang.SecurityException: User name or password is invalid.
2011-03-18 09:38:30,634 [38.171.43:40701] WARN ProtocolConverter - Exception occurred processing:
CONNECT
host:big77
accept-version:1.0,1.1
passcode:bar
login:foo{code} it should be:{code}22011-03-18 09:38:30,634 [38.171.43:40701] WARN TransportConnection - Failed to add Connection ID:xx.xx.38456-99-2:3, reason: java.lang.SecurityException: User name or password is invalid.
2011-03-18 09:38:30,634 [38.171.43:40701] WARN ProtocolConverter - Exception occurred processing:
CONNECT
host:big77
accept-version:1.0,1.1
passcode:*****
login:foo{code}
",", "
"Extract Method,Inline Method,","Support Temporary Destinations in a network without advisories Typically network require advisory message to allow peer broker to know about dynamic destination and consumer creation. However the advisory overhead can be significant as the numbers of peer brokers in a network increase to double digits.
A statically configured network can exist without advisories but using request reply with temporary currently destinations fails: a) because there is no way to configure them as statically included as their generated name is dynamically created from a connectionId and does not contain a wild card separator '.'. b) it is not possible to auto create a temp destination by a replying message producer (AMQ-2571)

Some discussion at: http://mail-archives.apache.org/mod_mbox/activemq-users/201103.mbox/%3CAANLkTi=N3LaQ4AWP8hK48OYAgWpSVodvjw4AN57j3+vz@mail.gmail.com%3E","Duplicated Code, Long Method, , , "
"Rename Method,Extract Method,","Implement ""exactly once"" delivery with kahaDB and XA in the event of a failure post prepare With XA 2PC, a camel route, jms 'to' jdbc should ensure exactly once delivery to jdbc. In the event of a failure after prepare, where commit to jdbc is done, the jms message must remain with a pending ack till the commit outcome is relayed from the transaction manager.
Current versions of geronimo will correctly retry the commit in a timer thread, so activemq eventually gets the commit outcome after recovery. (btw: it looks like howl will not persist a commit outcome per NamedXAResource, so after a failure of the TM it may consier the transaction completed and the message may still be pending, need to check that!)

At the moment, ActiveMQ does a heuristic rollback after recovery which leads to message redelivery in error.
With the fix, an acked message remains pending awaiting the outcome. On commit, the message is acked. On rollback the message is available for redelivery.
","Duplicated Code, Long Method, , "
"Rename Method,","Implement ""exactly once"" delivery with kahaDB and XA in the event of a failure post prepare With XA 2PC, a camel route, jms 'to' jdbc should ensure exactly once delivery to jdbc. In the event of a failure after prepare, where commit to jdbc is done, the jms message must remain with a pending ack till the commit outcome is relayed from the transaction manager.
Current versions of geronimo will correctly retry the commit in a timer thread, so activemq eventually gets the commit outcome after recovery. (btw: it looks like howl will not persist a commit outcome per NamedXAResource, so after a failure of the TM it may consier the transaction completed and the message may still be pending, need to check that!)

At the moment, ActiveMQ does a heuristic rollback after recovery which leads to message redelivery in error.
With the fix, an acked message remains pending awaiting the outcome. On commit, the message is acked. On rollback the message is available for redelivery.
",", "
"Move Class,Rename Class,Rename Method,Extract Method,Inline Method,","Usage of the temp store index by the PList needs the be improved Problem manifests when systemUsage memory limit is triggered and the pending message cursor for non persistent messages is flushing its in memory cache to temp store, this is taking a long time and in the mean time, other cursors that need to flush to disk are blocking on the temp store, and other sends to that destination are blocked.

In this scenario, the broker will come back to life once the flush completes. 
The problem is that the all cursors will see the same limit at the same time and try and flush.

The usage of the temp store index by the PList needs the be improved, it is not optimal at the moment as it uses too much space in the index. It uses a page per entry and reading from it, once the page cache is exhausted, is too slow.","Duplicated Code, Long Method, , , "
"Pull Up Method,Pull Up Attribute,","Broker does not check for expired persistent topic msgs. When using topics with durable subscriptions, where subscribers disconnect for a while, there is no task that checks for expired messages on the durable subscription.
In case where subscribers are disconnected for hours and message are still sent with a TTL, it may happen that either the producer gets blocked (in case of flow control) or the broker runs out of persistent storage (no flow control) just because of msgs that are already expired.

Similar to queues, there should be a periodic task that checks for expired msgs on durable topic subs. This task should also be configurable using a property similar to [expireMessagesPeriod|http://activemq.apache.org/per-destination-policies.html].",", Duplicated Code, Duplicated Code, "
"Move And Rename Class,Rename Method,Extract Method,","Admin commands which take --amqurl should accept --user and --pass options While using the fix for AMQ-3410, I realized that username and password are not available to be passed via command line.
I'll add a new parameter --user and --pass to allow username and password to be passed when --amqurl is passed.","Duplicated Code, Long Method, , "
"Rename Method,","Make use of remote port in Connection MBeanNames optional, useful when ephemeral range cycles quickly With fast connection close/creation (like stomp) the client side ephemeral port range can result in duplicate mbean names when close is async. Potential of failed mbean registration due to port reuse.
{code}DEBUG ManagedTransportConnection - Failure reason: javax.management.InstanceAlreadyExistsException: org.apache.activemq:BrokerName=XXXXXX,Type=Connection,ConnectorName=stomp,ViewType=address,Name=/X.X.X.X_52170{code}

Make the registration of an address type mbean configurable so that this case can be avoided. In the main, it is handy to see the remote address in the mbean name, so the default should be to use the remote port. Just the client Id (which default to the connection id, will be used in the mbean name) when use of remote port is not allowed.",", "
"Extract Superclass,Extract Method,","Add Stomp v1.1 support Stomp v1.1 spec was finalized so we should add support for it to AMQ. This adds support for connection inactivity monitoring for Stomp clients so there should be less stale stomp connections for Stomp v1.1 clients that enable heart beats. 

Also adds support for Queue browsing via stomp v1.1 clients that send the 'browser:true' header in their subscription command.

NACK Messages are also now supported in Stomp v1.1","Duplicated Code, Long Method, , Duplicated Code, Large Class, "
"Move Class,Rename Class,Move And Rename Class,","Rename rendezvous discovery scheme to zeroconf ActiveMQ implements 2 protocols to discover agents and broadcasts packets : multicast (default) and rendezvous.
It makes sense to use zeroconf as the name instead of rendezvous: 
- to avoid confusion with Tibco RendezVous technology,
- because the RendezVous name is not longer used by Apple. Due to Trademark/Copyright issues, Apple has changed this name into Bonjour (http://en.wikipedia.org/wiki/Bonjour_(software)).

It implements the zeroconf protocol so it is more exact.",", "
"Rename Method,Extract Method,","Modify MKahaDB To Support Using One Adapter Per Destination Without Explicity Listing Every Desintation In The Configuration We would like to have the ability to configure MKahaDB to use one adapter per queue without having to explicitly list every queue in our ActiveMQ configuration.

http://activemq.2283324.n4.nabble.com/MKahaDB-Configure-To-use-One-Adapter-Per-Queue-td4204743.html","Duplicated Code, Long Method, , "
"Extract Method,Pull Up Attribute,","Allow KahaDB to run without disk syncs, higher through put without the jms persistence guarantee When using the broker as a buffer for persistent data, large bursts or additions, periodic batch removes, we want to become disk bound. Waiting for disk syncs causes unnecessary pauses. If the JMS durability guarantee is not needed, it should be possible to execute without disk syncs. Syncing only on shutdown.
","Duplicated Code, Long Method, , Duplicated Code, "
"Rename Method,Extract Method,","Slave broker cannot be stopped in a JDBC Master/Slave configuration within OSGi A Blueprint container cannot be stopped while it is in the state ""Creating"" because both operations are synchronized in BlueprintContainerImpl.
The impact is that a slave broker cannot be stopped. Fortunately, before the broker itself is stopped, first the OSGi services are unregistered, which calls the configured OSGi unregistration listeners.
This patch provides a class which is a OSGi service unregistration listener, to allow to stop the database locker, while it is blocked in the ""Creating"" state.
","Duplicated Code, Long Method, , "
"Move Class,Move And Rename Class,Rename Method,Move Method,Inline Method,Move Attribute,","Recover scheduler database option I am not sure why, but Scheduler database got corrupted, and some messages couldn't be delivered to a broker. I got many exceptions similar to:

{code}
2012-03-02 03:26:08,234 | ERROR | JMS Failed to schedule job | org.apache.activemq.broker.scheduler.JobSchedulerImpl | JobScheduler:JMS 
java.io.IOException: Could not locate data file <correct-file-path>\db-2.log 
at org.apache.kahadb.journal.Journal.getDataFile(Journal.java:350) 
at org.apache.kahadb.journal.Journal.read(Journal.java:597) 
at org.apache.activemq.broker.scheduler.JobSchedulerStore.getPayload(JobSchedulerStore.java:315) 
at org.apache.activemq.broker.scheduler.JobSchedulerImpl.fireJob(JobSchedulerImpl.java:421) 
at org.apache.activemq.broker.scheduler.JobSchedulerImpl.mainLoop(JobSchedulerImpl.java:473) 
at org.apache.activemq.broker.scheduler.JobSchedulerImpl.run(JobSchedulerImpl.java:429) 
at java.lang.Thread.run(Unknown Source) 
{code}

The problem is that there is no way to restore the database like you can if you are working with the main ActiveMQ database. You can fix the main database by specifying the following configuration:

{code}
< persistenceAdapter> 
<kahaDB directory=""${activemq.base}/data/kahadb"" 
ignoreMissingJournalfiles=""true"" 
checkForCorruptJournalFiles=""true"" 
checksumJournalFiles=""true"" /> 
< /persistenceAdapter>
{code}

It would be nice to have the same feature for the scheduler database.",", , , , "
"Move Class,Move And Rename Class,Rename Method,Pull Up Method,Move Method,Extract Method,Pull Up Attribute,Move Attribute,","Recover scheduler database option I am not sure why, but Scheduler database got corrupted, and some messages couldn't be delivered to a broker. I got many exceptions similar to:

{code}
2012-03-02 03:26:08,234 | ERROR | JMS Failed to schedule job | org.apache.activemq.broker.scheduler.JobSchedulerImpl | JobScheduler:JMS 
java.io.IOException: Could not locate data file <correct-file-path>\db-2.log 
at org.apache.kahadb.journal.Journal.getDataFile(Journal.java:350) 
at org.apache.kahadb.journal.Journal.read(Journal.java:597) 
at org.apache.activemq.broker.scheduler.JobSchedulerStore.getPayload(JobSchedulerStore.java:315) 
at org.apache.activemq.broker.scheduler.JobSchedulerImpl.fireJob(JobSchedulerImpl.java:421) 
at org.apache.activemq.broker.scheduler.JobSchedulerImpl.mainLoop(JobSchedulerImpl.java:473) 
at org.apache.activemq.broker.scheduler.JobSchedulerImpl.run(JobSchedulerImpl.java:429) 
at java.lang.Thread.run(Unknown Source) 
{code}

The problem is that there is no way to restore the database like you can if you are working with the main ActiveMQ database. You can fix the main database by specifying the following configuration:

{code}
< persistenceAdapter> 
<kahaDB directory=""${activemq.base}/data/kahadb"" 
ignoreMissingJournalfiles=""true"" 
checkForCorruptJournalFiles=""true"" 
checksumJournalFiles=""true"" /> 
< /persistenceAdapter>
{code}

It would be nice to have the same feature for the scheduler database.","Duplicated Code, Long Method, , , , Duplicated Code, Duplicated Code, "
"Rename Class,Rename Method,Extract Method,",Add support for MQTT Support MQTT v3.1 protocol - see http://mqtt.org/,"Duplicated Code, Long Method, , "
"Move Method,Extract Method,Move Attribute,","WSS transport Feature request: wss transport for secure websocket connections

Cannot bind to the not secure websocket from https pages. Got JS security exception.","Duplicated Code, Long Method, , , , "
"Extract Method,Pull Up Attribute,","Expose transport connector URIs in uniform fashion Right now we have JMX methods like getOpenWireURL() for getting addresses of transport connectors. This API is not particularly nice and doesn't scale well when new transports are added.

So we should deprecate these methods and introduce new ones:

* Map<String, String> getTransportConnectors() - that returns all connectors with name as a key
* public String getTransportConnectorByType(String type) - that returns a connector address for a certain prefix/type (like tcp, stomp, ws and such)","Duplicated Code, Long Method, , Duplicated Code, "
"Rename Class,Rename Method,Pull Up Method,Pull Up Attribute,","Implement pluggable broker lockers Currently shared storage master/slave locking is a job of a persistence adapter.

It's hardcoded in KahaDB to use shared file locking, while JDBC allows at least some customization.

The idea is to create general locker interface and share it across available persistence adapters. So that for example, JDBC can use shared file lock or KahaDB use ZooKeeper lock when it's available.

We can also consider moving locking out of persistence layer and making it a job for a broker.",", Duplicated Code, Duplicated Code, "
"Rename Method,Extract Method,","Implement pluggable broker lockers Currently shared storage master/slave locking is a job of a persistence adapter.

It's hardcoded in KahaDB to use shared file locking, while JDBC allows at least some customization.

The idea is to create general locker interface and share it across available persistence adapters. So that for example, JDBC can use shared file lock or KahaDB use ZooKeeper lock when it's available.

We can also consider moving locking out of persistence layer and making it a job for a broker.","Duplicated Code, Long Method, , "
"Move Class,Inline Method,",Make better use of commons-pool in activemq-pool Currently activemq-pool uses only a tiny portion of the functionality that's available in commons-pool opting instead to reinvent a lot of things that now exists there such as keyed object pools. We can refactor the current codebase to better use common-pool. This allows for easily adding features like enabling async checks for Connections that have idled out and removing them from the pool as well as adding more diagnostic methods to our API and using a well tested pooling backend instead of our own custom code.,", , "
"Rename Method,Extract Method,","Refactor logic to shutdown thread pools using a single API to ensure better shutdown and offer logging et all In Apache Camel we have a centralized API for thread pools. This allows us to track this in Camel, and ensure thread pools is enlisted in JMX, and also unregistered again. As well having logs when thread pools is created / shutdown etc. Also better logic for shutdown graceful and then fallback to be aggressive etc. And to add a thread factory, that offers a naming pattern style, so end users can customize thread naming etc.

In activemq-core, there is pieces of logic for that. We should tidy this up, and especially ensure shutdown is happening consistent and more graceful, etc.

This can help make it possible to do as Camel to also enlist thread pools in JMX.","Duplicated Code, Long Method, , "
"Rename Method,","Job Scheduler Store Growth is Unrestricted When using scheduled delivery, it is possible to grow the job scheduler store indefinitely. As no quota can be set on the size of this store, a malfunctioning, malicious, or prodigious producer can easily consume all available storage with scheduled messages without any alerts being raised by the broker. If the operators do not have disk space monitoring in place outside of the broker, the broker can become innoperable without warning.

Provide a mechanism to set a usage quota for the job scheduler store. The mechanism should conform to the current resource quota model provided by SystemUsage as well as provide monitoring through JMX.

I have attached a basic patch to add management, enforcement, and configurability to the size of the job scheduler data store. Any guidance on things I missed or did not account for would be greatly appreciated.

While testing the size reporting in JMX, I noticed that the he Kaha persistence adapter seems to calculate its size differently than the job scheduler store. It appears that the job scheduler store is reporting the size of the data files and index while the Kaha persistence adapter is only reporting the size of the data files. What is the reason for this difference? I noticed the difference because the broker was reporting a 33% usage of the job scheduler store (100MB limit) immediately on a clean broker startup.",", "
"Rename Method,Extract Method,","Refactor network bridge start/stop to reduce async tasks and synchronisation - it is way to complicated atm sequence of events on a start is way to random as a result of many async tasks.
It should/could be very simple.

start remote transport
wait for remote broker info
start local transport and local bridge
start remote bridge

it is all or nothing w.r.t to success or failure.

we have sufficient tests in place to safely do the refactor at this stage.

There have been many bugs in this area and there are still some outstanding

https://issues.apache.org/jira/browse/AMQ-3993","Duplicated Code, Long Method, , "
"Rename Method,","PooledConnectionFactory should track Session checkouts and close associated resources When the user's code closes a Connection checked out from the pool, I would expect activemq-pool to close Sessions, MessageConsumers and MessageProducers that were created from it. 

Unfortunately, activemq-pool only cleans up Sessions on Connection.close() when no one else is referencing the Connection (referenceCount == 0). 

This makes Sessions, Consumers and Producers outlive the code that actually uses them, thus leading to increased resource consumption and messages being trapped in prefetch buffers that are no longer monitored.

Instead, we should keep track of the Sessions that were created from each specific Connection checkout, and close them when the borrowed Connection is closed.

Otherwise we bump into situations like [SPR-10092|https://jira.springsource.org/browse/SPR-10092] when using Spring's DefaultMessageListenerContainer. In some cases DMLC ""forgets"" to explicitly close MessageConsumers and Sessions, even though Connections are always closed, but the pool doesn't take care of cleaning up associated sessions.",", "
"Rename Method,Extract Method,","JMX ObjectNames do not follow JMX Best practices to use a Hierarchical format The Current JMX ObjectNames result in a disjointed view from JConsole of the managed objects and services of the ActiveMQ Broker. By following best practices, it will be easier to find and monitor all the endpoints associated with a Destination - for example.","Duplicated Code, Long Method, , "
"Rename Method,Extract Method,",Add mutual authentication (needClientAuth) to https transport Would like to be able to force clients to authenticate against he broker's trust store similar to how we have needClientAuth for the ssl:// transport.,"Duplicated Code, Long Method, , "
"Move And Rename Class,Pull Up Method,Extract Method,Pull Up Attribute,","Allow the Lease Locker to be used with out a JDBCPersistenceAdapter - so it can be a kahadb lock The locker interface needs another configure option to provide a broker service, or needs to be brokerService aware so that a locker can get identity and access to the io exception handlers.

The lease database locker is dependent on the jdbc pa to get statements and data source. It should be possible to configure these independently such that it can be used standalone as a broker lock. So setters for each.
This will help sort out some of the dependencies between broker and lock implementations. also making it possible to use a lease lock with kahadb for example.
some context: http://mail-archives.apache.org/mod_mbox/activemq-users/201303.mbox/%3CCAJ5znhurUZ+aEWsaaBAjtwBbpKWn06RyyyT6NqsDg_Su7vMOcg@mail.gmail.com%3E","Duplicated Code, Long Method, , Duplicated Code, Duplicated Code, "
"Rename Class,Move Method,Extract Method,","activemq-maven-plugin should have a stop goal The maven-activemq-plugin (aka activemq-maven-plugin) in a multi-module Maven project, we would like to stop and start ActiveMQ for each module where it is needed (a ""stop"" and ""start"" goal, rather than a ""run"" goal with a shutdown hook). We cannot run an individual module of our multi-module project because we can only start ActiveMQ once for our aggregate pom.xml. This approach would also resolve AMQ-1628 in a different way than was suggested. The approach we are suggesting is similar to how the cargo plugin handles tomcat.","Duplicated Code, Long Method, , , "
"Rename Method,Extract Method,",allow scheduled delivery of messages rather than be dispatched immediately it would be good to support a custom header to indicate the delivery some time in the future.,"Duplicated Code, Long Method, , "
"Move Method,Inline Method,Move Attribute,",Add EndpointCompleter functionality to ActiveMQ Camel component It will make it available for tools to autocomplete destinations names when creating routes.,", , , , "
"Rename Method,","network connectors - new messageTTL and consumerTTL - split usage of networkTTL for mesh topology currently networkTTL in a networkConnector (default=1) means that a message can go one hop and demand (or info about a consumer) can go one hop.
In a network (A<>B) messages and consumers can flow.

In a linear network (A<>B<>C) networkTTL needs to be 2 for messages and consumers to flow two hops from A to C.

In a mesh topology, (A<>B<>C<A>) a networkTTL=1 for consumers makes sense because there is at most one hop. However for messages, networkTTL > 1 is necessary if consumers need to hop around between brokers. Imagine a consumer on A which pulls messages to A from B, then the consumer moves to C, messages now need to hop again from A to C. This can repeat, essentially messageTTL(networkTTL) needs to be infinite.

With consumerTTL > 1 in a mesh, managing demand for proxy (demand) consumers and proxy proxy consumers becomes very difficult.",", "
"Rename Method,Move Method,","runtime configuration - allow selective application of changes to xml configuration without broker restart support on the fly configuration changes where appropriate.
Via JMX it is possible to make changes but they don't persist.
Via osgi we can restart the broker to pick up changes to xml config
but where it makes sense, we should be able to apply changes on the fly.

A first example would be the addition on a new network connector by
the addition of the relevant xml config (edit or copy over) that is
in use by the broker.",", , "
"Rename Method,Extract Method,","runtime configuration - allow selective application of changes to xml configuration without broker restart support on the fly configuration changes where appropriate.
Via JMX it is possible to make changes but they don't persist.
Via osgi we can restart the broker to pick up changes to xml config
but where it makes sense, we should be able to apply changes on the fly.

A first example would be the addition on a new network connector by
the addition of the relevant xml config (edit or copy over) that is
in use by the broker.","Duplicated Code, Long Method, , "
"Move Class,Extract Superclass,Rename Method,Pull Up Method,Move Method,Extract Method,Pull Up Attribute,Move Attribute,","Provide a generic jms XA connection pool When the jms bridge or camel is used with a third party jms provider the need for connection pooling and XA enlistment remains. The existing activemq-pool has hard dependencies on activemq so it cannot be used.
the idea is to extract the AMQ deps and make activemq-jms-pool a dependent of activemq-pool so we can leverage the shared pool impl on both ends of a jms bridge or in camel.","Duplicated Code, Long Method, , , , Duplicated Code, Large Class, Duplicated Code, Duplicated Code, "
"Rename Method,",Added JMX metics for networks per destination ,", "
"Rename Method,Move Method,","Secure the server from simple DoS attacks Originating from http://forums.logicblaze.com/posts/list/205.page

Simply start the 4.0 server (I used the stock config) 

in another window telnet to localhost 61616 

you will receieve: 
ActiveMQ^[[?1;2c 

type asdfasdf 

The connection will close by itself. 

All future TCP connections, either from telnet or from real JMS clients, will hang. 
",", , "
"Pull Up Method,Move Method,","Improve performance of composite topic fanout and persistent asyncSend We have publishers publishing to a topic which has 5 topic -> queue routings, and gets a max message rate attainable of ~833 messages/sec, with each message around 5k in size.

To test this i set up a JMS config with topic queues:
Topic
TopicRouted.1
...
TopicRouted.11

Each topic has an increasing number of routings to queues, and a client is set up to subscribe to all the queues.

Rough message rates:
routings messages/sec
0 2500
1 1428
2 2000
3 1428
4 1111
5 833

This occurs whether the broker config has producerFlowControl=""false"" set to true or false , and KahaDB disk synching is turned off. We also tried experimenting with concurrentStoreAndDispatch, but that didn't seem to help. LevelDB didn't give any notable performance improvement either.

We also have asyncSend enabled on the producer, and have a requirement to use persistent messages. We have also experimented with sending messages in a transaction, but that hasn't really helped.

It seems like producer throughput rate across all queue destinations, all connections and all publisher machines is limited by something on the broker, through a mechanism which is not producer flow control. I think the prime suspect is still contention on the index.

We did some test with Yourkit profiler.
Profiler was attached to broker at startup, allowed to run and then a topic publisher was started, routing to 5 queues. 
Profiler statistics were reset, the publisher allowed to run for 60 seconds, and then profiling snapshot was taken. During that time, ~9600 messages were logged as being sent for a rate of ~160/sec.
This ties in roughly with the invocation counts recorded in the snapshot (i think) - ~43k calls. 

From what i can work out, in the snapshot (filtering everything but org.apache.activemq.store.kahadb), 
For the 60 second sample period, 
24.8 seconds elapsed in org.apache.activemq.store.kahadb.KahaDbTransactionStore$1.removeAsyncMessage(ConnectionContext, MessageAck).
18.3 seconds elapsed in org.apache.activemq.store.kahadb.KahaDbTransactionStore$1.asyncAddQueueMessage(ConnectionContext, Message, boolean).

From these, a further large portion of the time is spent inside MessageDatabase:

org.apache.activemq.store.kahadb.MessageDatabase.process(KahaRemoveMessageCommand, Location) - 10 secs elapsed
org.apache.activemq.store.kahadb.MessageDatabase.process(KahaAddMessageCommand, Location) - 8.5 secs elapsed.

As both of these lock on indexLock.writeLock(), and both take place on the NIO transport threads, i think this accounts for at least some of the message throughput limits. As messages are added and removed from the index one by one, regardless of sync type settings, this adds a fair amount of overhead. 

While we're not synchronising on writes to disk, we are performing work on the NIO worker thread which can block on locks, and could account for the behaviour we've seen client side. 



To Reproduce:

1. Install a broker and use the attached configuration.

2. Use the 5.8.0 example ant script to consume from the queues, TopicQueueRouted.1 - 5. eg:

ant consumer -Durl=tcp://localhost:61616 -Dsubject=TopicQueueRouted.1 -Duser=admin -Dpassword=admin -Dmax=-1

3. Use the modified version of 5.8.0 example ant script (attached) to send messages to topics, TopicRouted.1 - 5, eg:

ant producer -Durl='tcp://localhost:61616?jms.useAsyncSend=true&wireFormat.tightEncodingEnabled=false&keepAlive=true&wireFormat.maxInactivityDuration=60000&socketBufferSize=32768' -Dsubject=TopicRouted.1 -Duser=admin -Dpassword=admin -Dmax=1 -Dtopic=true -DsleepTime=0 -Dmax=10000 -DmessageSize=5000

This modified version of the script prints the number of messages per second and prints it to the console.
",", , Duplicated Code, "
"Rename Method,Move Method,Extract Method,","MQTT clients using durable subscriptions on networked brokers received duplicates MQTT clients that create durable subscriptions and that operate in a network of Brokers can receive duplicate messages when they start failing back and forth between brokers in the network.

We should investigate using Virtual Destinations under the covers instead of durable topics subscriptions as this is a known problem with the durable topic case. With Virtual Destinations the client would be subscribed to a Queue that would receive all messages sent to the target Topic.","Duplicated Code, Long Method, , , "
"Rename Method,","MQTT clients using durable subscriptions on networked brokers received duplicates MQTT clients that create durable subscriptions and that operate in a network of Brokers can receive duplicate messages when they start failing back and forth between brokers in the network.

We should investigate using Virtual Destinations under the covers instead of durable topics subscriptions as this is a known problem with the durable topic case. With Virtual Destinations the client would be subscribed to a Queue that would receive all messages sent to the target Topic.",", "
"Move Method,Move Attribute,","MQTT clients using durable subscriptions on networked brokers received duplicates MQTT clients that create durable subscriptions and that operate in a network of Brokers can receive duplicate messages when they start failing back and forth between brokers in the network.

We should investigate using Virtual Destinations under the covers instead of durable topics subscriptions as this is a known problem with the durable topic case. With Virtual Destinations the client would be subscribed to a Queue that would receive all messages sent to the target Topic.",", , , "
"Rename Method,Inline Method,","Unify client and sampler timings in activemq-perf-maven-plugin Fundamentally there are 2 entities in the plugin managed by different
threads/threadpools:

* the client (generic term for producer of consumer)
* the samplers (throughput + cpu)

These entities can all be configured separately, and the plugin does not
complete until the slowest of these is done. Now, the problem is that
these things all have separate flags, which generally aren't overridden
unless you want to use a really long command line. For example, to get a
perf test that runs for 10 seconds, you have to do the following:

mvn activemq-perf:producer -Dproducer.sendDuration=10000
-DtpSampler.duration=10000 -DtpSampler.rampUpTime=0
-DtpSampler.rampDownTime=0 -DcpuSampler.duration=10000
-DcpuSampler.rampUpTime=0 -DcpuSampler.rampDownTime=0

And all this is before you start adding flags to do anything useful with
it. This is because:

1. sampler threads sleep for the ramp-up time (default 30s); if this
is longer than the producer's send duration then the sampler won't
even start doing anything until |the producer has well and truly
completed
2. the test runs until the longest configured sampler (by default both
throughput + cpu run)

So it's kind of a mess. I'd love to be able to use the principle of
least surprise (and configuration) so that you can just do this:

mvn activemq-perf:producer -Dproducer.sendDuration=10000

mvn activemq-perf:producer -Dproducer.sendType=count
-Dproducer.sendCount=10000",", , "
"Move Method,Extract Method,Inline Method,Move Attribute,","runtime configuration - allow changes to <destinations> configuration elements Extend the runtime configuration feature (AMQ-4682) to allow changes to the <destinations> configuration element.

As of 5.10.0 if you create a compositeTopic with queues/topics or filtered destinations within the <destinationInterceptors><virtualDestinationInterceptor><virtualDestinations> element the CompositeTopic will not showup in the Broker Manager Topics page.

{code:xml}
< destinationInterceptors>
<virtualDestinationInterceptor>
<virtualDestinations>
<!-- Add this compositeTopic after ActiveMQ start -->
< compositeTopic name=""CompositeTopic"" forwardOnly=""false"">
<forwardTo>
<queue physicalName=""ForwardedQueue"" />
</forwardTo>
</compositeTopic>
</virtualDestinations>
</virtualDestinationInterceptor>
</destinationInterceptors>
<destinations>
<!-- CompositeTopic and ForwardedQueue queue will not showup in the Broker Manager unless these are defined (or someone sends a message to the CompositeTopic. -->
<!--<topic physicalName=""CompositeTopic"" />-->
<!--<queue physicalName=""ForwardedQueue"" />-->
</destinations>
{code}

Note: This is more just a usability/management improvement.
The broker does correctly forward messages to the ForwardedQueue, if it is not defined in the <destinations> element.","Duplicated Code, Long Method, , , , , "
"Pull Up Method,Pull Up Attribute,","ignoreNetworkConsumers should be available in AbortSlowConsumerStrategy When AbortSlowAckConsumerStrategy.ignoreNetworkConsumers was introduced in https://git-wip-us.apache.org/repos/asf?p=activemq.git;a=commit;h=77bcffc9, its placement in AbortSlowAckConsumerStrategy means it can't be used with AbortSlowConsumerStrategy. Without it, abortConnection=""true"" causes the broker-to-broker network connectors to be aborted but not re-established, as described in http://irclogs.dankulp.com/logs/irclogger_log/activemq?date=2013-09-16,Mon&text=off, http://activemq.2283324.n4.nabble.com/Dropping-slow-consumers-td4671468.html, and http://activemq.2283324.n4.nabble.com/abortConnection-quot-true-quot-td4685674.html.

AbortSlowConsumerStrategy would benefit from being able to use this flag, so it should be moved up, and the AbortSlowConsumerStrategy code should be updated to use it, and to allow it to be set via the XML config.",", Duplicated Code, Duplicated Code, "
"Rename Method,","Add means to dynamically allocate port number for integration testing using maven plugin Port numbers for connectors can be dynamically allocated using special port number 0, but there is currently no way for integration clients to determine the correct port number.

Registering the connector URI's as maven project properties makes it easy to use the dynamically-allocated port numbers, making integration tests safer to run on a shared server by eliminating the possibility of port conflicts.",", "
"Move Method,Move Attribute,","Make some activemq jar executable and able to send/receive messages It would be nice to have basic verification/example tool builded directly in activemq-client.jar, so that folks can do something like 

{code}java -jar lib/activemq-client-xxx.jar producer
java -jar lib/activemq-client-xxx.jar consumer{code}
",", , , "
"Rename Class,Move Class,Move And Rename Class,Rename Method,Push Down Method,Move Method,Extract Method,Push Down Attribute,Move Attribute,",AMQP Implement the JMS Mapping spec as it evolves. The AMQP JMS Mapping specification is currently under development at OASIS. This issue covers work to be done to implement the mechanics necessary on the broker side to allow for JMS clients that operate over AMQP.,"Duplicated Code, Long Method, , , , , , "
"Move Class,Rename Method,Move Method,",AMQP Implement the JMS Mapping spec as it evolves. The AMQP JMS Mapping specification is currently under development at OASIS. This issue covers work to be done to implement the mechanics necessary on the broker side to allow for JMS clients that operate over AMQP.,", , "
"Extract Method,Inline Method,","Consider preallocation of journal files in batch increments Right now (as of ActiveMQ 5.12 release) we preallocate journal files, but the only scope is for entire journal file. The [potential] issue with that is if user configures large journal file sizes, we can end up stalling writes during log rotation because of the allocation process. There are two ways to do the allocation, configurable to do it in userspace, or defer to kernel space, but nevertheless it would be good to avoid this issue altogether by preallocating in small batch sizes regardless of the journal max file size.","Duplicated Code, Long Method, , , "
"Rename Method,","Allow advisory messages to traverse a broker network Currently the filter applied to forwarding consumers is very restrictive. It will suppress all advisory messages. But only two types of advisory are used by the network bridge.

Allowing the propagation of selective advisories, like new connection advisories is handy for monitoring at the application level.",", "
"Rename Method,","Time in queue statistics handy It would be very keen if the JMX console exposed queue statistics, such as average length of time in queue for current messages, longest/shortest time in queue for current messages, average time in queue for serviced messages (as opposed to those waiting), longestlshortest time for same, and a list of messages in the queue with when they were posted, and how long they have been waiting.",", "
"Rename Method,","Logging of ""Database ... is locked"" should be done on level DEBUG The SharedFileLocker tries to acquire a lock on the activemq lock file. Everytime it can not lock it outputs the logging message below at INFO level. On the slave it will try this forever till the master is down.

So I propose we only log on DEBUG level so the messages do not fill up a log with a global default info log level.


2015-04-07 12:35:36,522 | INFO | Database .../activemq/data/lock is locked... waiting 10 seconds for the database to be unlocked. Reason: java.io.IOException: File '.../activemq/data/lock' could not be locked. | org.apache.activemq.store.SharedFileLocker | main

",", "
"Rename Method,","Add the ability to get Message Size from a Message Store Currently, the {{MessageStore}} interface supports getting a count for messages ready to deliver using the {{getMessageCount}} method. It would also be very useful to be able to retrieve the message sizes for those counts as well for keeping track of metrics.

I've created a pull request to address this that adds a {{getMessageSize}} method that focuses specifically on KahaDB and the Memory store. The KahaDB store uses the same strategy as the existing {{getMessageCount}} method, which is to iterate over the index and total up the size of the messages. There are unit tests to show the size calculation and a unit test that shows a store based on version 5 working with the new version (the index is rebuilt)

One extra issue is that the size was not being serialized to the index (it was not included in the marshaller) so that required making a slight change and adding a new marshaller for {{Location}} to store the size in the location index of the store. Without this change, the size computation would not work when the broker was restarted since the size was not serialized.

Note that I wasn't sure the best way to handle the new marshaller and version compatibilities so I incremented the KahaDB version from 5 to 6. If an old version of the index is loaded, the index should be detected as corrupted and be rebuilt with the new format. If there is a better way to handle this upgrade let me know and the patch can certainly be updated.",", "
"Rename Method,Extract Method,","Add the ability to get Message Size from a Message Store Currently, the {{MessageStore}} interface supports getting a count for messages ready to deliver using the {{getMessageCount}} method. It would also be very useful to be able to retrieve the message sizes for those counts as well for keeping track of metrics.

I've created a pull request to address this that adds a {{getMessageSize}} method that focuses specifically on KahaDB and the Memory store. The KahaDB store uses the same strategy as the existing {{getMessageCount}} method, which is to iterate over the index and total up the size of the messages. There are unit tests to show the size calculation and a unit test that shows a store based on version 5 working with the new version (the index is rebuilt)

One extra issue is that the size was not being serialized to the index (it was not included in the marshaller) so that required making a slight change and adding a new marshaller for {{Location}} to store the size in the location index of the store. Without this change, the size computation would not work when the broker was restarted since the size was not serialized.

Note that I wasn't sure the best way to handle the new marshaller and version compatibilities so I incremented the KahaDB version from 5 to 6. If an old version of the index is loaded, the index should be detected as corrupted and be rebuilt with the new format. If there is a better way to handle this upgrade let me know and the patch can certainly be updated.","Duplicated Code, Long Method, , "
"Pull Up Method,Pull Up Attribute,",Add some tests for STOMP over WebSockets and fix and improve close handling Add some tests using the Jetty WebSocket client to cover STOMP over websockets and fix some issues with connection close and inactivity handling on the broker side. We want to ensure that on close or inactivity that we shut down the StompSocket resources.,", Duplicated Code, Duplicated Code, "
"Rename Method,",AMQP: Add support for heartbeats and inactivity monitoring. After we update to Proton-J 0.9.1 we will be able to take advantage if the idle processing added in that release to send empty keep alive frames in order to keep idle connections active and detect dropped connections.,", "
"Rename Method,","AMQP: Allow delivery transformer to fallback to lower level transformer when transformation fails If a client sends an AMQP that cannot be transformed using the configured transformer the broker shouldn't drop the message, instead it should attempt to fall-back to a less aggressive transformer. 

An example would be a broker configured to use the JMS transformer and the incoming message contains a body consisting of a DescribedType. The JMS Transformer would fail as there is no direct way to map that into a JMS message type. We could in this case fallback to the Native transformer and still process the message. An OpenWire client for instance would just receive a BytesMessage while other AMQP clients would get the message in the form it was sent.

This allows the message to round-trip for instance from AMQP -> OpenWire -> OpenWire -> AMQP (Broker network bridge) without losing its original payload or message properties.",", "
"Rename Method,",AMQP: Return a more complete Source when client looks up an existing durable subscription When a client is looking up an existing durable subscription to resubscribe we need to return a Source instance that contains as much of the original information used to create the durable sub. Things like noLocal flag and selector used should be returned to the client so that it can validate its request against the subscription that it is attempting to make and fail or otherwise respond if the old one does not match its expectations.,", "
"Rename Method,Extract Method,","Revisit topic statistics Currently, topic statistics can be confusing especially if you're using wildcard subscribers. In that case, inflight count and dequeue count is never updated when message is acked, so users can think that messages are not consumed.

Statistics are primarily developed for queues and then adapted for topics, which is why is some of them doesn't make sense in this use case.

To me, it'd make sense to keep only enqueue/dequeue properties on the topic, so that we can see general behaviour of the topic. 

Then every consumer, should keep it's own enqueue, dequeue, inflight (enqueue-dequeue) counts.
","Duplicated Code, Long Method, , "
"Move Method,Extract Method,Inline Method,Pull Up Attribute,","improve performance of TextFileCertificateLoginModule when many entries are in the ""textfiledn.user "" file With a large number of entries ( 200,000 ) in the ""org.apache.activemq.jaas.textfiledn.user"" file the performance seemed to degrade. 

To demonstrate the performance difference;
{code}

1) for 100 entries and calling initialize,login(),commit 10 times - Time taken is 73 miliseconds

2) for 200,000 entries and calling initialize,login(),commit 10 times - Time taken is 5020 miliseconds

{code}

Suggested improvements:
- avoid loading the org.apache.activemq.jaas.textfiledn.user file each time - in PropertiesLoginModule.java, the file is only read when it changes, using the file modification time.

- avoid iterating through the Properties object, using a Map instead to retrieve the userName
","Duplicated Code, Long Method, , , , Duplicated Code, "
"Move Class,Move Method,Move Attribute,","Support a single port for all wire protocols Both Apollo and Artemis support the ability to use a single port for all protocols and to have automatic detection for the protocol being used. It would be nice to be able to support at least a subset of this feature in the 5.x broker as well.

Ideally we should at least be able to detect OpenWire, MQTT, STOMP, and AMQP over a TCP, SSL, and NIO transport. Websockets and HTTP would be a bonus but could be more difficult to implement depending on how this could work with Jetty so that would take some investigation.

This is especially useful in environments where having to open up several new ports can be difficult because of firewall and security restrictions.",", , , "
"Rename Method,","Improve performance of virtual topic fanout Virtual topics provide a nice alternative to durable subs. Each durable sub is modeled as a separate queue.
There are performance implications however, because a message has to be sent to each of the (fanout) queues. For a durable subs, there is a single message in the journal and just index updates for each sub.

To improve performance there are three ways to improve the comparison between virtual topics and durable subs.
# avoid the disk sync associated with enqueue
# do parallel enqueues to get the store batching writes
# introduce message references in the journal to reduce the disk io

For 1, introducing a transaction (either client side or automatically, broker side) ensures there is a single disk sync on commit.
For 2, using an executor to do sends in parallel allows the journal to batch as seen in AMQ-5077
For 3, the implementation is a lot more involved; for recovery there needs to be a journal write per destination and reading the journal will require two reads because of the indirection. Tracking gc needs to be handled to ensure the referenced entry is maintained. In short this is a lot of work and will only be visible for large (> 8k) messages where the cost of a large v small journal write is noticeable. The messageId dataLocator provides an entry point for this work but considering that 1 & 2 combined can give a 3x improvement I don't think it is worth the effort (and added complexity) at the moment.",", "
"Pull Up Method,Extract Method,","Add Pending Message Size Metrics Right now the PendingMessageCursor keeps track of the number of pending messages (the size() method). It would be useful to also report back the total message size besides just the count so we know how much memory or disk space is being used by pending messages. 

Now that KahaDB is keeping track of the size of each message as of 5.12, it should be possible to report back this value in a non-blocking way. For a Queue this is pretty straightforward but for DurableSubscriptions some work will need to be done to keep track of the message size for unacked messages for each subscription.","Duplicated Code, Long Method, , Duplicated Code, "
"Extract Method,Inline Method,","Java runtime policy update should support a list of properties to apply retrospectively Right now when the {{JavaRuntimeConfigurationBroker}} is used to update a Policy entry, every property of the Policy is re-applied to matching destinations after update. It would also be nice if an optional list of properties could be provided during the update. 

The behavior of this option would be to still apply the entire Policy entry update (so all new destinations get the policy changes), but for existing destinations only properties that match what's in the property list would get applied.

The use case here would be wanting to apply only one or more properties that changed on a Policy to a destination, but not wanting to overwrite the other properties that might have been changed since creation of the destination.

For example, say through JMX someone updated the maxPageSize property on a specific Queue to something other than what is specified on the Policy. After the fact, it might be desirable to update a different property on the policy that matches that Queue and have it be applied to the Queue. Normally this policy update would overwrite all properties on the existing Queue, but in this case we would not want to overwrite the maxPageSize setting as it was changed after the fact. So providing a list of properties to apply would prevent that setting from being changed.","Duplicated Code, Long Method, , , "
"Rename Method,Extract Method,Inline Method,","KahaDB: Allow rewrite of message acks in older logs which prevent cleanup There are cases where a chain of journal logs can grow due to acks for messages in older logs needing to be kept so that on recovery proper state can be restored and older messages not be resurrected. 

In many cases just moving the acks from one log forward to a new log can free an entire chain during subsequent GC cycles. The 'compacted' ack log can be written during the time between GC cycles without the index lock being held meaning normal broker operations can continue.","Duplicated Code, Long Method, , , "
"Rename Method,","KahaDB does journal recovery for last append in error in normal restart case On a normal restart - the journal is replayed from the last append location in error. Reporting some unnecessary info logging of the form{code}INFO | Recovering from the journal @1:503
INFO | Recovery replayed 1 operations from the journal in 0.0 seconds.{code}

Recovery is only required when the last append location is different from the recovery location.",", "
"Move Class,Rename Method,Push Down Method,Push Down Attribute,",AMQP: Add support for AMQP over WebSockets Add support for AMQP connections over the WS and WSS transport.,", , , "
"Rename Method,Extract Method,",AMQP: Improve message transformation to provide better cross protocol messaging The current JMS message transformer does not always convert an incoming AMQP message to a type that is most appropriate for consumption from an OpenWire client and also can sometimes not transform the messages back to the expected AMQP type when sending a stored message back out to an AMQP client.,"Duplicated Code, Long Method, , "
"Rename Class,Move Method,","Introduce a periodic disk sync mode for KahaDB journal KahaDB has two modes for journal disk syncs, either always sync for each write or never sync. I'm proposing that we add a third option, a period disk sync. 

The intended behavior of this would be to run a task in the file appender that would sync the file (if necessary) at some periodic interval (such as every 500 ms, or 1 second, etc) instead of every write. The file would also be synced on close (on file rollover or shutdown)

In my testing, syncing every 1 second has been proven to be nearly indistinguishable performance as never disk syncing but is a safer option as you insure that a sync is performed at least once per interval.",", , "
"Rename Method,Pull Up Method,Extract Method,","Add a flag to allow forcing network subscriptions to be durable When there is a network bridge for two brokers and a topic is included, there is a conduit subscription created for the topic. This can cause issues if there is a mixture of durable and non-durable subscriptions.

The issue is that when the conduit subscription gets created, the type of the subscription across the bridge can be either durable or non-durable depending on which local subscription gets created first. This is a problem because you might end up with local durable subs but the network bridge conduit sub is actually non-durable. This can also cause issues if all of the consumers go away because then the conduit sub gets destroyed instead of being retained if it was durable.

To fix this scenario I think there should be an option to force the conduit subscription to be durable. This could be done on a per topic basis or topic hierarchy. Something like the following:

{code:xml}
<networkConnector name=""broker1"" duplex=""true"" uri=""static:(tcp://10.x.x.x:61616)"">
<dynamicallyIncludedDestinations>
<topic physicalName=""test.topic?forceDurable=true""/>
</dynamicallyIncludedDestinations>
</networkConnector>
{code}
","Duplicated Code, Long Method, , Duplicated Code, "
"Rename Method,","Implement JMX destination query API In an environment when there thousands of destinations on the broker, current way of exposing all MBeans and looking into them in the tools does not scale. We need to implement an API that can be used by tools to filter, sort and page destinations in this scenario.",", "
"Extract Method,Move Attribute,",AMQP: Add frame inspection capability to the test client. ,"Duplicated Code, Long Method, , , "
"Rename Method,Extract Method,Inline Method,",Refactoring of karaf itests The karaf itests can be improved quite a lot. I will prepare a bigger refactoring to bring the tests up to a good condition.,"Duplicated Code, Long Method, , , "
"Rename Method,Move Method,Extract Method,Move Attribute,",Refactoring of karaf itests The karaf itests can be improved quite a lot. I will prepare a bigger refactoring to bring the tests up to a good condition.,"Duplicated Code, Long Method, , , , "
"Extract Method,Inline Method,","Improve performance of KahaDB recovery check checkForCorruptJournalFiles=true The KahaDB checkForCorruptJournalFiles option validates the checksum of every journal batch record on startup. If a single producer writes many small messages, the batch sizes in the journal will be small. The current check implementation reads each batch at a time with a fseek/read sequence that can be very slow over shared disks.

The check can be a quick buffered sequential read using the maxBatchSize which should already be tuned to match the disk transfer rate.","Duplicated Code, Long Method, , , "
"Rename Method,","several functions spelled wrong, impairs code readability In the OpenWire marshalling code / scripts for Java, s/Unmarsal/Unmarshal/.
In OpenWireFormat.java, WireFormatNegotiator.java, and OpenWireFormatFactory.java, s/negociat/negotiate/.

Nitpicks, yes, but the cause of some wasted time looking around for functions that didn't exist.
",", "
"Rename Method,","Startup performance improvement when log contains prepared transactions. I have a KahaDB that's performing a recovery on each startup. 

Digging deeper into it I've found that the issue is that the db.log contains prepared transactions. 

The MessageDatabase will discard those entries in memory, however it does not remove transaction info from those messages (I believe that's by design). So on each restart, the broker will find those entries and again discard them in memory. 

If I access the broker via JMX, I can go to the prepared XAs and execute a clear on them one by one. 

When i restart my broker, i don't have a recovery attempted again. 

Performing a remove operation for each message can be very time consuming, so i'd like to introduce an optional parameter to allow all prepared XAs to be removed on recovery. 

Please see my forth coming patch with unit test.",", "
"Rename Method,",Add support for TLS hostname verification Add support to the transport on both server and client side to configure if hostname verification is enabled or disabled,", "
"Rename Method,","Suppress (optionally) warn logging of EOF or Reset exceptions when remote socket is closed when a load-balancer or health check pings any transport connector endpoint to verify that the broker is listening on a port; using socket.open/close, any subsequent read failure is treated as an error and logged as a WARN. 
This makes sense in general b/c it is indicative of a rogue client. 

However when it is the norm, ie: from a health check, then the logs get filled with these worrying messages that are in fact expected. 
For the somtp transport, where there is no protocol close method, we already suppress EOF and connection reset exceptions. 
This improvement would make that the default behaviour for all tcp transports and allow it to be enabled when required via configuration: 
{code:java} 
< transportConnector warnOnRemoteClose=""true"" ..>{code}",", "
"Rename Method,","ActiveMQ reads lots of index pages upon startup (after a graceful or ungraceful shutdown) Hi. 

We noticed that ActiveMQ reads lots of pages in the index file when is starting up to recover the destinations statistics: 

[https://github.com/apache/activemq/blob/master/activemq-kahadb-store/src/main/java/org/apache/activemq/store/kahadb/KahaDBStore.java#L819] 

Nowadays, in order to do that, activemq traverse the storedDestination.locationIndex to get the messageCount and totalMessageSize of each destination. For destinations with lots of messages this process can take a while making the startup process take long time. 

In a case of a master-slave broker, this prevent the broker to fast failover and does not meet what is stated on [http://activemq.apache.org/shared-file-system-master-slave.html.] 
{quote}If you have a SAN or shared file system it can be used to provide _high availability_ such that if a broker is killed, another broker can take over immediately.  
{quote} 
One solution for this is keep track of the destination statistics summary in the index file and doing so, we dont need to read all the locationIndex on the start up. 

The code change proposed is backward compatible but need a bump on the kahadb version. If this information is not in the index, the broker will fall back to the current implementation, which means that the first time people upgrade to the new version, it will still have to read the locationIndex, but subsequent restarts will be fast. 

This change should have a negligible performance impact during normal activemq operation, as this change introduce a few more bytes of data to the index and this information will be on checkpoints. Also, this new information is synchronized with the locationIndex as they are update at the same transaction.",", "
"Rename Method,","allow asynchronous dispatch to consumers in the broker for non-durable topics We typically use the current thread in the broker to dispatch to all the available non-durable consumers for performance - as this hugely reduces the context switching and increases performance. However (see AMQ-688) sometimes this can cause one dead consumer to block a producer.

Some folks may want to switch this strategy to use slower asynchronous dispatch with a thread pool to reduce the risk of blocking a producer at the expensive of lower performance",", "
"Rename Method,Extract Method,",Support a ?trace=true option on the http client side transport ,"Duplicated Code, Long Method, , "
"Rename Method,","AMQ-376 LDAP based authorization support Patch kindly added by ngcutura - discussion thread...

http://www.nabble.com/LDAP-Authorization-tf1851705.html#a5344494",", "
"Rename Method,","Allow MessageEvictionStrategy to evict more than one MessageReference in evictMessage(LinkedList message) method For slow consumers every time a single message is added to a TopicSubscription where the pending message limit is reached, a new call to evictMessage is made. To allow for more flexible and efficient means of evicting messages it would be nice to be able to evict multiple messages in one call to evictMessage. This allows new MessageEvictionStrategy implementations to evict based on age of messages (eg. evict all messages in the pending message list that are older than x ms), duplicate messages (evict all messages that are redundant based on newer messages currently in the pending message list) etc. As a single call to the evictMessage method may have the opportunity to reduce the size of the pending message list by more than one it means that the next message added to the TopicSubscription may not need to have to call the evictMessage again.",", "
"Extract Interface,Move Method,Extract Method,","Patch: refactoring to allow alternative (using different storage interface) Destinations implementations. We were looking at alternate message persistence mechanisms that can co-exist in current ActiveMQ code base and we are thinking of a mechanism that is somewhat incompatible with the current MessageStore and PersistenceAdapter APIs. Unfortunately, the current ActiveMQ broker doesn't allow for such a change as the PersistenceAdapter and MessageStore interfaces are referenced directly by the RegionBroker and by both the Queue and Topic region implementations. Therefore, we are proposing a relatively small backwards compatible refactoring of the broker code that would eliminate all dependencies on the PersistenceAdapter and MessageStore interfaces from those classes that do not use them directly. This refactoring would also allow creation of a custom Destination implementation that may use an alternative persistence mechanism on a destination by destination basis (which is exactly what we need to do).
The main idea behind the refactoring is to replace many references to PersistenceAdapter with a new interface: DestinationFactory:
public abstract class DestinationFactory { 
abstract public Destination createDestination(ConnectionContext context, ActiveMQDestination destination, DestinationStatistics destinationStatistics) throws Exception;
abstract public Set getDestinations(); 
abstract public SubscriptionInfo[] getAllDurableSubscriptions(ActiveMQTopic topic) throws IOException; 
abstract public long getLastMessageBrokerSequenceId() throws IOException; 
abstract public void setRegionBroker(RegionBroker regionBroker); 
} 
Note that DestinationFactory doesn't mandate any specific persistence mechanism. The classes that would reference it instead of PersistenceAdapter are: RegionBroker, AbstractRegion, QueueRegion, and TopicRegion. Also, the AbstractRegion.createDestination method would be changed from abstract to an implementation that uses DestinationFactory to create a destination.
BrokerService could be changed to use DestinationFactory if one is provided. If none is provided, it will create a DestinationFactory implementation that instantiates Queue and Topic using PersistenceAdapter as it does currently. Hence, full backwards compatibility will be maintained.

Patch is attached.
","Duplicated Code, Long Method, , , Large Class, "
"Move And Rename Class,Extract Interface,Rename Method,Extract Method,","Make the ActiveIO dependency and optional dependency. Need to move some core classes that are in activeio to activemq
so that it is not needed to run. Right now the only real
functionality that it provides that is optional is the journal
implementation.

Everything else that is use from activeio are just abstract interfaces, and I think
those need to be moved/copied to ActiveMQ.","Duplicated Code, Long Method, , Large Class, "
"Extract Superclass,Rename Method,Move Method,Extract Method,","Two TCP connection requirement for bidirectional message flow ... We noticed the following during our testing ....

When a broker A establishes connection to broker B, the message flow is unidirectional from A to B.
This is a an issue for us: For example, consider brokers associated with business critical services X and Y. There are many secondary services that either monitor/feed off of the messages coming from them.

A FOO service would like to process messages going from X to Y. So in FOO's broker configuration we add X's name. However, messages are not going to flow from X to FOO, till X initiates a connection to FOO. It may not be desirable/possible to change business critical brokers' configuration for usage scenarios like this.

TCP is bidirectional and asymmetry at connection establishment should not be translated to the higher level network connector. Is there a fundamental need/justification for this design that I may not be aware of ? Otherwise I would like to explore other design options.

Thanks
Regards
- Sridhar Komandur","Duplicated Code, Long Method, , , Duplicated Code, Large Class, "
"Move Method,Inline Method,Move Attribute,","Deal with NULL values of measures for inmem cubing Previously NULL values will be dealt for dimensions during both layered cubing and inmem cubing. However, NULL values of measures only are dealt for layered cubing. NULL values of measures for inmem cubing also should be dealt with.",", , , , "
"Rename Method,",Fix sonar reported static code issues phase 1 ,", "
"Pull Up Method,Inline Method,","Support MySQL as Kylin metadata storage Kylin uses HBase as the metastore; But in some cases user expects the metadata not in HBase. 

Sonny Heer from mailing list mentioned: 

""I'm fairly certain anyone using Kylin with AWS EMR will benefit from this.   Having multiple hbase clusters across AZs is a huge benefit.  BTW only thing blocking at the moment is write operations happening from kylin query nodes.""",", , Duplicated Code, "
"Rename Method,","Support prepare statement in Kylin server side Kylin use calcite as sql engine, when a sql comes to Kylin server, it requires to be parsed, optimized, code gen, and then query Kylin's cube storage, the previous 3 steps often take 50-150 ms to complete(depends on the complexity of the sql). If we support to cache the parsed result in Kylin server, the 3 steps will be saved. 

The idea is to cache calcite's PreparedStatement object and related OLAPContexts in the server side, when the prepare request comes with the same sql, reuse the PreparedStatement to do the execution. Since the PreparedStatement is not thread safe, so I planned to use ObjectPool to cache the PreparedStatement.(use apache commons-pool lib)",", "
"Rename Method,Move Method,","Improve the HTTP return code of Rest API Kylin returns 500 error for some invalid input; for example an invalid cube name, when the job exceeds the maximum number, etc. 

We need to refine the behavior; For invalid user input, return 4XX http code. ",", , "
"Rename Method,","User interface for hybrid model Hybrid model is useful for model change. While now there is no entry for it from GUI, this makes many users don't see such feature.",", "
"Rename Method,Extract Method,","Optimize spark cubing memory footprint Noticed that there are some steps can be optimized: 

1) Cuboid.findForMandatory() will create new object each time, which this can be cached; 

2) Executor repeatedly load dictionary, not hit the cache; 

3) NDCuboidBuilder and RowkeyEncoder, can optimize for less array copy;","Duplicated Code, Long Method, , "
"Move Method,Extract Method,Move Attribute,",Merge cube segments in Spark ,"Duplicated Code, Long Method, , , , "
"Rename Method,Move Method,Extract Method,","Enhance segment pruning 1.Compute and store all dimension's range in cubeSegment,; 

2.If the query condition don't satisfy dimension range, then no need to scan the segment.","Duplicated Code, Long Method, , , "
"Move Class,Extract Superclass,Pull Up Method,Move Method,Extract Method,","Convert to HFile in Spark Some references: 

https://www.opencore.com/blog/2016/10/efficient-bulk-load-of-hbase-using-spark/","Duplicated Code, Long Method, , , Duplicated Code, Large Class, Duplicated Code, "
"Move Class,Rename Method,Move Method,Extract Method,Inline Method,","Refactor the storage garbage clean up code Kylin will produce some garbage data in storage when it runs. 

Now, the clean up tool ""{{kylin.sh org.apache.kylin.tool.StorageCleanupJob}}"" can show what is garbage data or clean up the garbage by setting options ""–delete false"" or ""delete true"". 

But Kylin can't show the size of garbage data for users. 

This reconfiguration adds some member variables and methods recording the  garbage size in the detection process.  

After clean up job running over, Kylin can get the information about garbage size.","Duplicated Code, Long Method, , , , "
"Rename Method,Move Method,Inline Method,","Allow each project to set its own source at project level Currently, all projects connect to the same source which is set in kylin.properties with kylin.source.default property. 
It's better to allow each project to set its own source in project level override configuration. 
As the result, we can have project A connects to JDBC, and project B connects to Hive.",", , , "
"Move And Rename Class,Move Method,Move Attribute,","Util Class for encryption and decryption Extract methods of ""encrypt"" and ""decrypt"" in org.apache.kylin.rest.security.PasswordPlaceholderConfigurer to a Util class in core-common package. This will make them more reusable. ",", , , "
"Rename Method,","Only clean necessary cache for CubeMigrationCLI Currently, we simply clear ALL cache in CubeMigrationCLI. which will make a few of queries slower in prod env when we have many tables, models, cubes and migrate cube often.

So, we could only clean necessary cache for CubeMigrationCLI.

",", "
"Rename Class,Rename Method,Move Method,Extract Method,Move Attribute,",Refine Email Template for notification by freemarker ,"Duplicated Code, Long Method, , , , "
"Rename Method,","Metadata broadcast should only retry failed node In commit https://github.com/apache/kylin/commit/ecc01458c4ad361aaf863505884d51474a8fec9d, Kylin starts to retry failed metadata sync event, however it re-posts the failed event to all nodes. The retry SHOULD only apply to the failed node only.",", "
"Move Method,Move Attribute,","Make query visible and interruptible, improve server's stablility Problem:
1. Large query result will break kylin server, for example: select * from fact_table. Even when properties ""kylin.query.scan.threshold"" and ""kylin.query.mem.budget"" are set properly, OOM still happens, because the hbase rpc thread is not interrupted, the result will continually go to kylin server. And server will run OOM quickly when there are multiple such queries.
2. Tow many slow queries will occupy all tomcat threads, and make server unresponsed.
3. There's no corelation id for a specified query, so it is hard to find the rpc log for a specified query, if there are too many queries running concurrently.

Solution:
1. Interrupt the rpc thread and main query thread when return result size larger than the config limit size.
2. Make query visible. Admin can view all running queries, and detail of each query. 
Split the query into following steps:
1) sql parse
2) cube plan 
3) query cache
4) multiple cube segment query
a. for each segment request, have muliple endpoint range request.
b. for each endpoint range request, have multiple coprocessor request.
c. for each coprocessor request, have multiple region server rpc.

Admin can view the startTime/endTime of each step, and the thread stack trace if the step is running.
3. Add query id as corelation id in the rpc log.
4. Admin can interrupt a running query, to release the thread, memory, etc.
",", , , "
"Rename Method,Extract Method,","Support SQL Server as data source [KYLIN-1351|https://issues.apache.org/jira/browse/KYLIN-1351] has added Vertica as data source. Base on the work of KYLIN-1351, I'd like to enable SQL Server as data source of kylin.","Duplicated Code, Long Method, , "
"Rename Method,","Enlarge the reducer number for hyperloglog statistics calculation at step FactDistinctColumnsJob Currently only one reducer is assigned for hll stats calculation, which may become the bottleneck for slow down this step. Since the stats for different cuboids will not influence each other, it's better to divide the cuboid set into several and assign a reduce for each subset.
The strategy of this patch is to assign 100 cuboids into a subset. And there's a upper limit of reducers for hll stats calculation. Currently it's 50.",", "
"Move Method,Inline Method,","Enlarge the reducer number for hyperloglog statistics calculation at step FactDistinctColumnsJob Currently only one reducer is assigned for hll stats calculation, which may become the bottleneck for slow down this step. Since the stats for different cuboids will not influence each other, it's better to divide the cuboid set into several and assign a reduce for each subset.
The strategy of this patch is to assign 100 cuboids into a subset. And there's a upper limit of reducers for hll stats calculation. Currently it's 50.",", , , "
"Rename Method,",Use multiple threads to calculate HyperLogLogPlusCounter in FactDistinctColumnsMapper ,", "
"Move Method,Extract Method,Move Attribute,",Use multiple threads to calculate HyperLogLogPlusCounter in FactDistinctColumnsMapper ,"Duplicated Code, Long Method, , , , "
"Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,",Synchronize read/write operations on Managers To prevent abnormal behavior when reading cache (those getXXX methods) and writing cache (broadcast reload) happens at the same time in managers.,"Duplicated Code, Long Method, , , , , "
"Rename Method,Move Method,Extract Method,Inline Method,","Refactor to consolidate all caches and managers under KylinConfig Refactor to consolidate all caches and managers under KylinConfig. 

Such that there is a single place to clear (and GC) all cache under a KylinConfig.","Duplicated Code, Long Method, , , , "
"Rename Method,","Enable 'kylin.source.hive.flat-table-storage-format' for flat table storage format Flat table storage format is currently hard-coded as SEQUENCEFILE in the core-job/src/main/java/org/apache/kylin/job/JoinedFlatTable.java 
That prevents using Impala as a SQL engine while using beeline CLI (via custom JDBC URL), as Impala cannot write sequence files. 
Adding a parameter to kylin.properties to override the default setting would address the issue. 
Removing a hard-coded value for storage format might be good idea in and on itself.",", "
"Move Method,Extract Method,Inline Method,","Avoid doing useless work by checking query deadline Under high load, requests can spend a long time waiting in RPC queue, probably longer than query timeout. However, current coprocessor timeout mechanism doesn't take RPC queue time into account. As a result, handling these requests not only waste server resources while doing nothing useful, but can also cause cascading failure when server crashes and client retries. 

To recover from server overload ASAP, we should check query deadline at each stages of query processing, and avoid spending resources on query that will exceed their deadline.","Duplicated Code, Long Method, , , , "
"Rename Method,","Support SQL Server as data source [KYLIN-1351|https://issues.apache.org/jira/browse/KYLIN-1351] has added Vertica as data source. Base on the work of KYLIN-1351, I'd like to enable SQL Server as data source of kylin.",", "
"Rename Method,Push Down Method,Extract Method,Push Down Attribute,","Support user/group and role authentication for LDAP Currently, the user authentication interface that was provided by kylin to the third party only supports user and role authentication. However only user and group have authentication function when we use the LDAP authentication. In fact the authentication for user and role and the authentication for user and group have the same functional characteristics between different appplication system. So we should submit a new feature that it support the authentication for user and role and the authentication for user and group when the LDAP authentication was enabled. 

We supplied the checkPermission interface to implement the new feature. In the interface we set user groups information to the userRoles parameter when the LDAP was enabled, on the contrary we set user roles information to the userRoles parameter. The interface is as following: 
/** 
* Checks if a user has permission on an entity. 
* 
* @param user 
* @param userRoles 
* @param entityType String constants defined in AclEntityType 
* @param entityUuid 
* @param permission 
* 
* @return true if has permission 
*/ 
abstract public boolean checkPermission(String user, List<String> userRoles, // 
String entityType, String entityUuid, Permission permission); 
","Duplicated Code, Long Method, , , , "
"Rename Method,Move Method,Extract Method,","Table Level ACL We are planning to introduce table level ACL to kylin. Table level ACL control whether user can query data from a table. 
Table will go under project so that table level ACL will be set at project by project basis. When an instance starts for the first time or upgrades from lower version, every user by default has access to all tables that have been loaded to the project. Admin can modify table level ACL. by removing user from a table’s access list, admin can disable user’s query access to the table. 

In several cases, a user’s table level ACL will be checked to ensure ACL integrity
1. When user visit insight page and browse tables, table that user has no access to under current project, will not be visible to the user. 
2. When user’s query hit on a cube, the table level ACL on dependent tables for that user will be checked, if user has no access to any dependent table, query will be refused on that cube. 
","Duplicated Code, Long Method, , , "
"Move Class,Move Method,Extract Method,Move Attribute,","Global dict specific info should not use absolute path when saved it Currently, absolute path is used in the global dictionary for saving the location of specific info like the next json. 
{code} 
{ 
""uuid"" : ""b82505d8-5b40-4009-8839-8456500ea6a8"", 
""last_modified"" : 0, 
""version"" : ""2.3.0.20500"", 
""source_table"" : ""KYLIN_YIFEI_1011_TESTGLOBALDICTMIGRATION.KYLIN_SALES"", 
""source_column"" : ""PRICE"", 
""source_column_index"" : 6, 
""data_type"" : ""decimal(19,4)"", 
""input"" : { 
""path"" : ""hdfs://sandbox.hortonworks.com:8020/kylin/kylin_default/kylin-d213a963-b5e3-4e75-a1a2-8af93ca83a80/global_dict_cube/fact_distinct_columns/KYLIN_SALES.PRICE"", 
""size"" : 81150, 
""last_modified_time"" : 1508133611074 
}, 
""dictionary_class"" : ""org.apache.kylin.dict.AppendTrieDictionary"", 
""cardinality"" : 8878 
}^@hdfs://master:8020/kylin/kylin_yifei_1011_testGlobalDictMigration/resources/GlobalDict/dict/KYLIN_YIFEI_1011_TESTGLOBALDICTMIGRATION.KYLIN_SALES/PRICE/ 
{code} 
But if you change the path configuration or migrate the data to another cluster which node name is different or original host cannot be reached, you will meet problems in reusing it, like rebuilding and merge dictionary or transfer column from dictionary in building cube, for the dictionary info cannot be found.","Duplicated Code, Long Method, , , , "
"Rename Class,Move Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Refactor DataModelDesc Refactor DataModelDesc, such that it is more flexible and extensible. Splitting MetadataManager into a dedicated DataModelManager, and more.","Duplicated Code, Long Method, , , , , "
"Rename Method,","Move concept Table under Project Move concept Table under Project, such that reloading table in one project won't affect cubes in another project.",", "
"Rename Method,",Refactor CuboidScheduler to be extensible To allow other implementations like KYLIN-2727,", "
"Rename Method,Move Method,Extract Method,Pull Up Attribute,Move Attribute,",Refactor CuboidScheduler to be extensible To allow other implementations like KYLIN-2727,"Duplicated Code, Long Method, , , , Duplicated Code, "
"Rename Method,",Log pushdown query as a kind of BadQuery User will want to know the past pushdown queries and possibly create model/cube to enhance their speed.,", "
"Rename Method,","Inspect available memory during MR job Sometimes, it's hard to figure out the reason of OOM during MR job, especially when Kylin runs on different environments.

Hence, it is helpful to have memory info during steps of setup/cleanup.",", "
"Rename Method,","New metric framework based on dropwizard With https://issues.apache.org/jira/browse/KYLIN-2721.We are plan to release a new metric framework. 
New metric is different hadoop metric and based on dropwizard . which has the following advantage:
* Well-defined metric model for frequently-needed metrics (ie JVM metrics)
* Well-defined measurements for all metrics (ie max, mean, stddev, mean_rate, etc),
* Built-in pluggable reporting frameworks like JMX, Console, Log, JSON 

We refactored QueryMetric with new metrics, notice the exposed JMX MBeans have changed a little bit.

A new tool called perflog is also introduced. Perflog traces call duration time and current active calls by recording them to metric system.

Some snapshots of the new JMX MBeans can be seen in attachments",", "
"Rename Method,Move Method,Extract Method,Inline Method,","Refactor dateRange & sourceOffset on CubeSegment The two concepts are similar and confusing. Refactor, clean tech debt.","Duplicated Code, Long Method, , , , "
"Rename Method,",Make HBase 1.x the default of master ,", "
"Rename Method,","Refine Spark Cubing to reduce serialization's overhead In Spark Cubing, a lot of variables defined in driver and used in closures, which cause extra serialization's overhead.

Meanwhile, remove the method of reading KylinConfig from HDFS.",", "
"Rename Method,Extract Method,","Enhance Project Level ACL We are planning on enhancing the project level ACL, the plan as below:
There are five types of predefined user access: System Admin, Project Admin, Management, Operation and Query. 
When syncing an LDAP new user, admin has the option to set the new user as system admin. The other user access, however, need to be set at project level, that means other than system admin which is applied globally on an instance, the other four user access are granted on project by project basis. So that one user, say johndoe, can be project admin in Project A and management access in Project B. 

User role modeler and analyst that are currently set when syncing users, will be retired. Project level access can be assigned to user, but not on role any more. 
Cube level ACL will be retired too, user's access control on a cube will be inherited from user's project level ACL. 

The expected access level for those 5 types of user access are defined as attached.

!screenshot-1.png!","Duplicated Code, Long Method, , "
"Rename Method,","Support RDBMS as data source From v2.0, Kylin's plug-in architecture makes it possible to have multiple data sources, cube engines and storages. Some users ever aksed that whether Kylin support source data feeded from RDBMS like Oracle, MySQL, now it is possible to do that. Some tools like Apache Sqoop can easily export data from RDBMS to HDFS, that would help Kylin get the data and then build that into cubes.",", "
"Push Down Method,Extract Method,Push Down Attribute,",Refactor CuboidScheduler to be extensible To allow other implementations like KYLIN-2727,"Duplicated Code, Long Method, , , , "
"Move Method,Extract Method,","Spark Cubing read metadata from HDFS Currently, Spark cubing doesn't support HBase cluster with kerberos.
Temporarily，we could support HBase cluster with kerberos on Yarn client mode, because which is easy.
In the long term，we should avoid access HBase in Spark cubing.","Duplicated Code, Long Method, , , "
"Rename Method,Move Method,Extract Method,Inline Method,","Move concept Table under Project Move concept Table under Project, such that reloading table in one project won't affect cubes in another project.","Duplicated Code, Long Method, , , , "
"Rename Method,","Route unsupported query back to query its source directly When Kylin cannot support a query due to lack of prepared mode and cube, the query can be routed back to executing on its original source. It may integrate with Hive, or SparkSQL, or Drill, or anything if so desired.

There must be an interface defined first, to allow plug-in of any ad-hoc query engine.

This is a carry-on from KYLIN-742",", "
"Move Class,Rename Method,Push Down Method,Move Method,Extract Method,","Support RDBMS as data source From v2.0, Kylin's plug-in architecture makes it possible to have multiple data sources, cube engines and storages. Some users ever aksed that whether Kylin support source data feeded from RDBMS like Oracle, MySQL, now it is possible to do that. Some tools like Apache Sqoop can easily export data from RDBMS to HDFS, that would help Kylin get the data and then build that into cubes.","Duplicated Code, Long Method, , , , "
"Move Class,Move And Rename Class,","Route unsupported query back to query its source directly When Kylin cannot support a query due to lack of prepared mode and cube, the query can be routed back to executing on its original source. It may integrate with Hive, or SparkSQL, or Drill, or anything if so desired.

There must be an interface defined first, to allow plug-in of any ad-hoc query engine.

This is a carry-on from KYLIN-742",", "
"Move Class,Move Method,Inline Method,Move Attribute,",Use ResourceStore to manage ACL files ,", , , , "
"Rename Method,Move Method,Extract Method,","Prune cuboids by capping number of dimensions the scene like this: 

I have 20+ dimensions, However the query will only use at most 5 dimensions in all dimensions, so cuboid that contains 5+ dimensions(except base cuboid) is useless. 

I think we can add a configuration in cube, which limit the max dimensions that cuboid includes. 

What's more, we can config which level(number of dimension) need to calculate. in above scene, we only calculate leve 1,2,3,4,5. and skip level 5+ 

============================= 

The dimension capping is turned on by adding dim_cap property in aggregation_groups definition. 

For example, the following aggregation group sets the dimension cap to 3. All cuboids containing more than 3 dimensions are skipped in this aggregation group. 

{code:none} 

""aggregation_groups"" : [ { 
""includes"" : [ ""PART_DT"", ""META_CATEG_NAME"", ""CATEG_LVL2_NAME"", ""CATEG_LVL3_NAME"", ""LEAF_CATEG_ID"", ""LSTG_FORMAT_NAME"", ""LSTG_SITE_ID"", ""OPS_USER_ID"", ""OPS_REGION"", 
""BUYER_ACCOUNT.ACCOUNT_BUYER_LEVEL"", ""SELLER_ACCOUNT.ACCOUNT_SELLER_LEVEL"", ""BUYER_ACCOUNT.ACCOUNT_COUNTRY"", ""SELLER_ACCOUNT.ACCOUNT_COUNTRY"", ""BUYER_COUNTRY.NAME"", ""SELLER_COUNTRY.NAME"" ], 
""select_rule"" : { 
""hierarchy_dims"" : [ [ ""META_CATEG_NAME"", ""CATEG_LVL2_NAME"", ""CATEG_LVL3_NAME"", ""LEAF_CATEG_ID"" ] ], 
""mandatory_dims"" : [ ""PART_DT"" ], 
""joint_dims"" : [ [ ""BUYER_ACCOUNT.ACCOUNT_COUNTRY"", ""BUYER_COUNTRY.NAME"" ], [ ""SELLER_ACCOUNT.ACCOUNT_COUNTRY"", ""SELLER_COUNTRY.NAME"" ], 
[ ""BUYER_ACCOUNT.ACCOUNT_BUYER_LEVEL"", ""SELLER_ACCOUNT.ACCOUNT_SELLER_LEVEL"" ], [ ""LSTG_FORMAT_NAME"", ""LSTG_SITE_ID"" ], [ ""OPS_USER_ID"", ""OPS_REGION"" ] ], 
""dim_cap"" : 3 
} 
} ] 
{code}","Duplicated Code, Long Method, , , "
"Rename Method,Extract Method,","optimize determined case when filters currently calcite will not handle with determined filter like:

1. where 1 = 1 => where true
2. where ( 1= 1 or x = 2) => where true
3. where case when 'a' = 'a' then x > 1 else x < 1 => where x > 1

the first two cases have been handled in KYLIN-2539, however the third case is not handled yet. This JIRA is to track the third case.

In theory, this JIRA together with KYLIN-2539, KYLIN-2597 should be solved in calcite rather than KYLIN. However it's urgent demand so we'll fix in KYLIN first.","Duplicated Code, Long Method, , "
"Move Class,Rename Method,","Correct reporting of HBase errors Whenever HBase error occurs, metadata access fails, user must see a clear error message saying it is HBase failing, not Kylin.",", "
"Rename Class,Rename Method,Move Method,Extract Method,Move Attribute,",Move output(Hbase) related code from MR engine to outputside ,"Duplicated Code, Long Method, , , , "
"Rename Method,","Refactor DistributedLock The current {{DistributedLock}} could use some improvement:

- A {{lockClient}} is unnecessarily required.
- The {{watchPath}} is actually an on-unlock listener and the current name failed to make it clear.
- Could add a blocking version of {{lockPath}} and that will ease use cases like KYLIN-2557 and {{GlobalDictionaryBuilder.lock()}}
- Should add more javadoc on the interface",", "
"Rename Method,Move Method,Extract Method,Move Attribute,","Refactor DistributedLock The current {{DistributedLock}} could use some improvement:

- A {{lockClient}} is unnecessarily required.
- The {{watchPath}} is actually an on-unlock listener and the current name failed to make it clear.
- Could add a blocking version of {{lockPath}} and that will ease use cases like KYLIN-2557 and {{GlobalDictionaryBuilder.lock()}}
- Should add more javadoc on the interface","Duplicated Code, Long Method, , , , "
"Rename Class,Move Class,",Use ResourceStore to manage ACL files ,", "
"Move Class,Move Method,Move Attribute,",Use ResourceStore to manage ACL files ,", , , "
"Move Class,Move Method,Extract Method,Move Attribute,",Use ResourceStore to manage ACL files ,"Duplicated Code, Long Method, , , , "
"Move Class,Rename Class,Move And Rename Class,Rename Method,Move Method,Extract Method,Move Attribute,","Refactor Global Dictionary The main points of this refactor:
1 Fix the bug that the RemoveListener of LoadingCache swallowed any exceptions when building the GlobalDict.
2 Fix the bug that the HDFS filename of DictSliceKey had Illegal characters.
3 Fix the bug that the HDFS filename of DictSliceKey maybe longer than 255.
4 Fix the bug that DictNode split failed if value length greater than 255 bytes.
5 Decouple the build and query of GlobalDict: 
Abstract the builder of AppendTrieDictionary to AppendTrieDictionaryBuilder; Add LoadingCache to AppendTrieDictionary and make AppendTrieDictionary is only readable.
6 Remove dependence of LoadingCache when building the GlobalDict.
7 Abstract the HDFS operations to GlobalDictStore.
8 Abstract the metadata of GlobalDict to GlobalDictMetadata.
9 Delete CachedTreeMap.
10 Add distributed lock for GlobalDict.","Duplicated Code, Long Method, , , , "
"Rename Method,","Number2BytesConverter could tolerate malformed numbers Some malformed numbers like ""0100"", ""100.1200"" currently does not work with Number Dictionary. And could be improved.",", "
"Rename Method,Extract Method,","Use hive table statistics data to get the total count Kylin will count on the intermediate flat hive table to get the total row number, then to redistribute that.

From hive's wiki, hive will automatically collect the table statistics when run a ""insert overwrite"" statement, then the subsequent ""select count(*)"" will be very fast (see https://cwiki.apache.org/confluence/display/Hive/StatsDev). While, Kylin is executing ""INSERT OVERWRITE DIRECTORY '/kylin/row_count' SELECT count(*) from"", which still cause MR/Tez job be started, this will cause the step take longer time.

Just change the SQL to ""select count(*)"" or using Hive API to get the statistic, the cost will be saved. 

Here is a sample, the table 'kylin_intermediate_qq_dbe874d2_bb9a_4375_ba50_3dcf096a13c5' is an intermediate table :

If directly run ""count(*)"", it is pretty fast:
hive> select count(*) from kylin_intermediate_qq_dbe874d2_bb9a_4375_ba50_3dcf096a13c5;
OK
970033
Time taken: 0.112 seconds, Fetched: 1 row(s)


While today Kylin's SQL will cause a job be started:

hive> INSERT OVERWRITE DIRECTORY '/kylin/row_count' select count(*) from kylin_intermediate_qq_dbe874d2_bb9a_4375_ba50_3dcf096a13c5;
Query ID = root_20161106080808_0099b622-c0bd-41da-aee5-2321adf7bdda
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
set mapreduce.job.reduces=<number>
Starting Job = job_1463701915919_46208, Tracking URL =

","Duplicated Code, Long Method, , "
"Rename Class,Move Method,Inline Method,Move Attribute,","Stream Aggregate GTRecords at Query Server *Problem*
When query server needs to handle millions of records, CubeTupleConverter could become performance bottleneck.
An experiment shows that converting 5 millions records takes ~11s, which accounts for 50% of the total query time.

*Motivation*
Records returned from each storage partition is guaranteed to be ordered. Therefore we could reduce the number of records passed to CubeTupleConverter by
# merge sorted records from all partitions, similar to what we have done in KYLIN-1787
# use a [stream aggregate|https://blogs.msdn.microsoft.com/craigfr/2006/09/13/stream-aggregate/] algorithm on merged stream to aggregate those records with the same key

*Proposal*
# Add a new physical operator GTStreamAggregateScanner which implements the stream aggregate algorithm
# Refine SortedIteratorMergerWithLimit that was used to merge sort records from different partitions. The previous implementation has performance issues (KYLIN-2483) due to expensive record clone
# Leverage GTStreamAggregateScanner to aggregate records on merged stream

*Scope*
Stream aggregate has some good properties such as low memory usage and streamable ordered outputs, making it better than hash/sort based alternatives when input is already sorted. So I bet the new GTStreamAggregateScanner operator can also be used to accelerate cubing and coprocessor aggregation in certain cases. I'll focus on query server in this jira and leave those optimizations as future works.",", , , , "
"Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Stream Aggregate GTRecords at Query Server *Problem*
When query server needs to handle millions of records, CubeTupleConverter could become performance bottleneck.
An experiment shows that converting 5 millions records takes ~11s, which accounts for 50% of the total query time.

*Motivation*
Records returned from each storage partition is guaranteed to be ordered. Therefore we could reduce the number of records passed to CubeTupleConverter by
# merge sorted records from all partitions, similar to what we have done in KYLIN-1787
# use a [stream aggregate|https://blogs.msdn.microsoft.com/craigfr/2006/09/13/stream-aggregate/] algorithm on merged stream to aggregate those records with the same key

*Proposal*
# Add a new physical operator GTStreamAggregateScanner which implements the stream aggregate algorithm
# Refine SortedIteratorMergerWithLimit that was used to merge sort records from different partitions. The previous implementation has performance issues (KYLIN-2483) due to expensive record clone
# Leverage GTStreamAggregateScanner to aggregate records on merged stream

*Scope*
Stream aggregate has some good properties such as low memory usage and streamable ordered outputs, making it better than hash/sort based alternatives when input is already sorted. So I bet the new GTStreamAggregateScanner operator can also be used to accelerate cubing and coprocessor aggregation in certain cases. I'll focus on query server in this jira and leave those optimizations as future works.","Duplicated Code, Long Method, , , , , "
"Rename Method,",Smooth upgrade to 2.0.0 from older metadata ,", "
"Move And Rename Class,","Spark cubing step should show YARN app link A sample output is:

17/03/06 14:44:44 INFO Client: Submitting application 16 to ResourceManager
17/03/06 14:44:45 INFO YarnClientImpl: Submitted application application_1488805575687_0016
17/03/06 14:44:46 INFO Client: Application report for application_1488805575687_0016 (state: ACCEPTED)
17/03/06 14:44:46 INFO Client: 
client token: N/A
diagnostics: N/A
ApplicationMaster host: N/A
ApplicationMaster RPC port: -1
queue: default
start time: 1488811485001
final status: UNDEFINED
tracking URL: http://sandbox.hortonworks.com:8088/proxy/application_1488805575687_0016/
user: root",", "
"Rename Method,Push Down Attribute,","Optimize the performance of SelfDefineSortableKey In KYLIN-1851, we introduce a new sorting key ""SelfDefineSortableKey"" in kylin because the new dictionary needs ordered input value. But recently we found that when users apply dictionary encoding to a double type column, the sorting procedure of SelfDefineSortableKey is very slow. So some optimization need to be made to fix that.",", , "
"Move Class,Move Method,Move Attribute,","Refactor and refine Number Dictionary There are three kings of number dictionary: NumberDictionary.class NumberDictionary2.class and NumberDictionaryForest.class, which is too confused",", , , "
"Move Method,Extract Method,","Cleanup unnecessary shaded libraries for job/coprocessor/jdbc/server Kylin releases three libraries: kylin-coprocessor, kylin-jdbc, kylin-job and one web application: server. 
Currently, all libraries have shaded some used third party libraries into the package. For example, guava, curator, commons, kyro in kylin-job. The duplicate libraries in runtime classpath may have potential class loading conflicts and waste computing resource. We should leverage the hadoop provided libraries at runtime instead of the shaded one.","Duplicated Code, Long Method, , , "
"Rename Method,","add a configuration knob to disable spilling of aggregation cache Kylin's aggregation operator can spill intermediate results to disk when its estimated memory usage exceeds some threshold (kylin.query.coprocessor.mem.gb to be specific). While it's a useful feature in general to prevent RegionServer from OOM, there are times when aborting this kind of memory-hungry query immediately is a more suitable choice to users.

To accommodate this requirement, I suggest adding a new configuration named -*kylin.storage.hbase.coprocessor-spill-enabled*- +*kylin.storage.partition.aggr-spill-enabled*+. The default value would be true, which will keep the same behavior as before. If changed to false, query that uses more aggregation memory than threshold will fail immediately.",", "
"Rename Method,","Report coprocessor error information back to client When query aborts in coprocessor, the current error message (list below) doesn't carry any concrete reason. User has to check regionserver's log in order to figure out what's happening, which is a tedious work and not always possible in a cloud environment. 

{noformat}
< sub-thread for Query 4fb68974-de70-4f6e-a2ee-7048202e51a7 GTScanRequest 4d65f9bf>The coprocessor thread stopped itself due to scan timeout or scan threshold(check region server log), failing current query...
{noformat}

It would be better to report error message to client.",", "
"Rename Method,","replace scan threshold with max scan bytes In order to guard against bad queries that can consume lots of memory and potentially crash kylin / hbase server, kylin limits the maximum number of rows query can scan. The maximum value is chosen based on two configs
# *kylin.query.scan.threshold* is used if the query doesn't contain memory-hungry metrics
# *kylin.query.mem.budget* / estimated_row_size is used otherwise as the per region maximum.

This approach however has several deficiencies:
* It doesn't work with complex, varlen metrics very well. The estimated threshold could be either too small or too large. If it's too small, good queries are killed. If it's too large, bad queries are not banned.
* Row count doesn't correspond to memory consumption, thus it's difficult to determine how large scan threshold should be set to.
* kylin.query.scan.threshold can't be override at cube level.

In this JIRA, I propose to replace the current row count based threshold with a more intuitive size based threshold
* KYLIN-2437 will collect the number of bytes scanned at both region and query level
* A new configuration *kylin.query.max-scan-bytes* will be added to limits the maximum number of bytes query can scan
* *kylin.query.mem.budget* will be renamed to -*kylin.storage.hbase.coprocessor-max-scan-bytes*- +*kylin.storage.partition.max-scan-bytes*+, which limits at region level. No need to rely on estimations about row size any more.
* The above two configs scan be override at cube level
* the old *kylin.query.scan.threshold* will be deprecated",", "
"Move Class,Extract Superclass,Extract Method,Move Attribute,","collect number of bytes scanned to query metrics Besides scanned row count, it's useful to know how many bytes are scanned from HBase to fulfil a query. It is perhaps a better indicator than row count that shows how much pressure a query puts on HBase.","Duplicated Code, Long Method, , , Duplicated Code, Large Class, "
"Rename Method,","Distinguish UHC columns from normal columns in KYLIN-2217 In current implement, Kylin disable the whole feature of KYLIN-2217 when it founds that kylin.engine.mr.uhc-reducer-count>1. Actually, we can distinguish UHC dictionaries from normal dictionaries and only leave the UHC dictionaries for job node to build.",", "
"Rename Method,Extract Method,",Allow kylin to store metadata in HDFS instead of HBase ,"Duplicated Code, Long Method, , "
"Rename Method,",Create a branch for v1.5 with HBase 1.1 API Create a new branch for Kylin v1.5 compile with HBase v1.1 API.,", "
"Rename Class,Rename Method,Move Method,","A new BitmapCounter with better performance We found the old BitmapCounter does not perform very well on very large bitmap. The inefficiency comes from
* Poor serialize implementation: instead of serialize bitmap directly to ByteBuffer, it uses ByteArrayOutputStream as a temporal storage, which causes superfluous memory allocations
* Poor peekLength implementation: the whole bitmap is deserialized in order to retrieve its serialized size
* Extra deserialize cost: even if only cardinality info is needed to answer query, the whole bitmap is deserialize into MutableRoaringBitmap

A new BitmapCounter is designed to solve these problems
* It comes in tow flavors, mutable and immutable, which is based on Mutable/Immutable RoaringBitmap correspondingly
* ImmutableBitmapCounter has lower deserialize cost, as it just maps to a copied buffer. So we always deserialize to ImmutableBitmapCounter at first, and convert it to MutableBitmapCounter only when necessary
* peekLength is implemented using ImmutableRoaringBitmap.serializedSizeInBytes, which is very fast since only the header of roaring format is examined
* It can directly serializes to ByteBuffer, no intermediate buffer is allocated
* The wire format is the same as before ([RoaringFormatSpec|https://github.com/RoaringBitmap/RoaringFormatSpec/]). Therefore no cube rebuild is needed",", , "
"Rename Class,Rename Method,Extract Method,","Upgrade Calcite to 1.11 and Avatica to 1.9 Calcite has release 1.11.0 recently. In this JIRA, Kylin will upgrade the calcite to 1.11.0 and avatica to 1.9.0. 
Some potential refactor may be required, since protobuf 3.0(upgrade from 2.5) is needed by Calcite.","Duplicated Code, Long Method, , "
"Rename Method,","Improve resource utilization for DistributedScheduler Currently, in DistributedScheduler we lock segment in JobService, which will make the job of segment only schedule in jobServer that the job submitted and could not fully utilize the threadPool resource of all jobServers.

For example, we have two jobServer and the max concurrent jobs is 10, if we continuously submit 20 jobs to jobServer1, there will be only 10 jobs running at the same time not 20 and will no job running in jobServer2.",", "
"Rename Method,Inline Method,","Simplify Dictionary interface Remove byte[] related interface. Keep only

- int getIdFromValue(T value)
- int getIdFromValue(T value, int roundingFlag)
- T getValueFromId(int id)
",", , "
"Rename Method,Move Method,Inline Method,","A new BitmapCounter with better performance We found the old BitmapCounter does not perform very well on very large bitmap. The inefficiency comes from
* Poor serialize implementation: instead of serialize bitmap directly to ByteBuffer, it uses ByteArrayOutputStream as a temporal storage, which causes superfluous memory allocations
* Poor peekLength implementation: the whole bitmap is deserialized in order to retrieve its serialized size
* Extra deserialize cost: even if only cardinality info is needed to answer query, the whole bitmap is deserialize into MutableRoaringBitmap

A new BitmapCounter is designed to solve these problems
* It comes in tow flavors, mutable and immutable, which is based on Mutable/Immutable RoaringBitmap correspondingly
* ImmutableBitmapCounter has lower deserialize cost, as it just maps to a copied buffer. So we always deserialize to ImmutableBitmapCounter at first, and convert it to MutableBitmapCounter only when necessary
* peekLength is implemented using ImmutableRoaringBitmap.serializedSizeInBytes, which is very fast since only the header of roaring format is examined
* It can directly serializes to ByteBuffer, no intermediate buffer is allocated
* The wire format is the same as before ([RoaringFormatSpec|https://github.com/RoaringBitmap/RoaringFormatSpec/]). Therefore no cube rebuild is needed",", , , "
"Rename Method,Extract Method,","Refactor DbUnit assertions To tolerate some data type mismatch (between int / bigint).
To allow verify of query with limit.","Duplicated Code, Long Method, , "
"Rename Method,Move Method,Inline Method,Move Attribute,","By layer Spark cubing Using Apache Spark as the engine to build cube, with the by-layer iterative algorithm.",", , , , "
"Rename Method,","Reduce the size of metadata uploaded to distributed cache Currently, each MR job uploads all the metadata belonging to a cube to distributed cache. When the total size of metadata increases, the submission time (""MapReduce Waiting"" at Monitor UI) also increases and could become a significant problem.

We could actually optimize the amount of metadata uploaded according to the type of job, for example

* CuboidJob only needs dictionary of the building segment
* CubeHFileJob doesn't need any dictionary",", "
"Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Refine Table load/unload error message There is no exception handling in TableController, so most of exceptions will not be found in kylin.log, but kylin.out. The TableController should provide more useful messages, and be stable when exception happens.","Duplicated Code, Long Method, , , , , "
"Rename Method,Extract Method,","Reduce the size of metadata uploaded to distributed cache Currently, each MR job uploads all the metadata belonging to a cube to distributed cache. When the total size of metadata increases, the submission time (""MapReduce Waiting"" at Monitor UI) also increases and could become a significant problem.

We could actually optimize the amount of metadata uploaded according to the type of job, for example

* CuboidJob only needs dictionary of the building segment
* CubeHFileJob doesn't need any dictionary","Duplicated Code, Long Method, , "
"Rename Method,Inline Method,",Add Integration Test (IT) for snowflake ,", , "
"Rename Method,","Refactor CI, merge with_slr and without_slr cubes Try to reduce CI time, also prepare for snowflake cube in CI.",", "
"Move Class,Move Method,",Have a general purpose data generation tool To generate some random data according to a data model. Help troubleshooting and testing.,", , "
"Move Method,Extract Method,Move Attribute,","Add project config and make config priority become ""cube > project > server"" There are cases we want to override global kylin.properties in the scope of a project. E.g. the queue name of Hadoop job.

Finally, the config priority for Kylin should be ""cube > project > server"". I think which is reasonable.","Duplicated Code, Long Method, , , , "
"Rename Class,Move And Rename Class,Move Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","HyperLogLog codec performance improvement We have a cube with more than ten distinct count measure, and use hll15 store the value, we found it is too slow of HyperLogLogPlusCounter, there are three methods will called frequentlly: merge/writeRegisters/readRegisters.

I found in kylin-1.5.x add a parameter 'singleBucket' to store the only one bucket which can optimize base cuboid.

However, in other step of cuboid building, it will slow down. I has modify the code to speed up the speed of three operation.","Duplicated Code, Long Method, , , , , "
"Rename Method,","Display reasonable exception message if could not find kafka dependency for streaming build Kafka is optional dependency for Kylin install. But is mandatory for streaming build. Currently, if no KAFKA_HOME exported, the build will show ""Error"", but without any more detail message. It's not convenient for new user.",", "
"Extract Method,Inline Method,",Snowflake schema support ,"Duplicated Code, Long Method, , , "
"Rename Method,Extract Method,Inline Method,","More Robust Global Dictionary Global dictionary have been released over 2 months, I've received some feedbacks and bug reports. Here's the patch to make global dictionary more robust, including some functional improvements.
* Break through 255 bytes limitation for value, but still recommend value length less than 8K, avoiding stack overflow error;
* Fix 'Value not exists' or stack overflow error when dict size is larger than 1GB, the root cause is similar with KYLIN-1834; A check tool also provided for check corrupted or not of existing dict data;
* Support parallel dictionary building in one job server, used for parallel segments building;","Duplicated Code, Long Method, , , "
"Rename Method,Move Method,Extract Method,","Reducers build dictionaries locally In KYLIN-1851, we reduce the peek memory usage of the dictionary-building procedure by splitting a single Trie tree structure to Trie forest. But there still exist a bottleneck that all the dictionaries are built in Kylin client. In this issue, we want to use multi reducers to build different dictionaries locally and concurrently，which can further reduce the peek memory usage as well as speed up the dictionary-building procedure.","Duplicated Code, Long Method, , , "
"Rename Method,","Reducers build dictionaries locally In KYLIN-1851, we reduce the peek memory usage of the dictionary-building procedure by splitting a single Trie tree structure to Trie forest. But there still exist a bottleneck that all the dictionaries are built in Kylin client. In this issue, we want to use multi reducers to build different dictionaries locally and concurrently，which can further reduce the peek memory usage as well as speed up the dictionary-building procedure.",", "
"Rename Method,",Setup naming convention for kylin properties ,", "
"Rename Class,Rename Method,Extract Method,Inline Method,",Snowflake schema support ,"Duplicated Code, Long Method, , , "
"Rename Method,","Cannot support columns with same name under different table currently we implicitly assume all columns in the model have unique names, for example, in row key and aggregation group we use column name (without tablename) to identify each column",", "
"Rename Method,","Enhance TableExt metadata metadata ""table_ext"" is not very elegant and difficult to manage. Here will enrich it.",", "
"Rename Method,Extract Method,","Enhance TableExt metadata metadata ""table_ext"" is not very elegant and difficult to manage. Here will enrich it.","Duplicated Code, Long Method, , "
"Rename Method,",Mapper/Reducer cleanup() exception handling Or it could override the real exception happened in mapper() or reducer(),", "
"Move Method,Extract Method,","Cannot support columns with same name under different table currently we implicitly assume all columns in the model have unique names, for example, in row key and aggregation group we use column name (without tablename) to identify each column","Duplicated Code, Long Method, , , "
"Move Method,Inline Method,","Cannot support columns with same name under different table currently we implicitly assume all columns in the model have unique names, for example, in row key and aggregation group we use column name (without tablename) to identify each column",", , , "
"Rename Method,Extract Method,","Cannot support columns with same name under different table currently we implicitly assume all columns in the model have unique names, for example, in row key and aggregation group we use column name (without tablename) to identify each column","Duplicated Code, Long Method, , "
"Rename Method,",TopN counter merge performance improvement Observed the reduce phase of cube build is slow when there is TopN counter. There should be room for performance improvement.,", "
"Rename Method,Move Method,Move Attribute,","Support usage of schema name ""default"" in SQL Calcite will treat ""DEFAULT"" as internal keyword, while ""DEFAULT"" is also used as HIVE's default schema name. We need to escape such case of usage by quote it.",", , , "
"Rename Method,Extract Method,Inline Method,","Support intersect count for calculation of retention or conversion rates Retention or Conversion Rates is very important in data analyze. 
It can be calculated from two dataset of two different value of one dimension. For example, we have an count distinct measure, like uv(dataset of uuid), and one dimension, like date, and the retention of uv between '20161015' and '20161016' is the intersection of two uv datasets.
Fortunately, we have implement dataset in Kylin, as bitmap, for precisely count distinct. Only an UDAF is needed to calculate intersection of two or more bitmaps.
I'll try on this and post patch later.","Duplicated Code, Long Method, , , "
"Rename Method,","Scalable streaming cubing We try to achieve:

1. Scale streaming cubing workload on a computation cluster, e.g. YARN
2. Support Kafka as a formal data source
3. Guarantee no data loss reading from Kafka, even records are not strictly ordered by time",", "
"Move Class,Rename Method,Extract Method,","Scalable streaming cubing We try to achieve:

1. Scale streaming cubing workload on a computation cluster, e.g. YARN
2. Support Kafka as a formal data source
3. Guarantee no data loss reading from Kafka, even records are not strictly ordered by time","Duplicated Code, Long Method, , "
"Move And Rename Class,Rename Method,Move Method,Extract Method,Move Attribute,",Refactor broadcast of metadata change ,"Duplicated Code, Long Method, , , , "
"Move Class,Rename Method,","Scalable streaming cubing We try to achieve:

1. Scale streaming cubing workload on a computation cluster, e.g. YARN
2. Support Kafka as a formal data source
3. Guarantee no data loss reading from Kafka, even records are not strictly ordered by time",", "
"Rename Class,Move Class,Rename Method,","New Fix_length_Hex encoding to support hash value and better Integer encoding to support negative value 1. for some use cases string value represent hash code is used. The string only consist of [0~9A~F] (hex values), so two characters can be squashed into one byte
2. The current IntegerDimEnc does not support negative values, need another IntegerDimEnc that supports negative values",", "
"Move And Rename Class,Pull Up Method,Extract Method,Inline Method,",Improve the logic to decide whether to pre aggregate on Region server ,"Duplicated Code, Long Method, , , Duplicated Code, "
"Move Class,Extract Superclass,Rename Method,",Improve the logic to decide whether to pre aggregate on Region server ,", Duplicated Code, Large Class, "
"Rename Method,","Allow bigint(long) as a partition date column In many cases, hive tables may use long / bigint as date column. Currently Kylin only support 'yyyy-MM-dd' or 'yyyyMMdd' as date format. This jira is to support long / bigint format.",", "
"Rename Method,",Split kylin.properties into two files Split kylin.properties into two files: normal properties and security sensitive properties,", "
"Rename Method,","WebUI for GlobalDictionary Global Dictionary is introduced since v1.5.3. However, there's no Web UI to config with Global Dict, it's not convenience for users. 

The use case can be found in examples/test_case_data/localmeta/cube_desc/test_kylin_cube_without_slr_left_join_desc.json, the ""dictionaries"" arrays. 
One Global Dict config may contains three elements, ""column"", the column to generate dict, ""builder"", the builder to build dict, and ""reuse"", the column to be reused.",", "
"Move Method,Move Attribute,",Refactor IJoinedFlatTableDesc Make IJoinedFlatTableDesc more independent. Don't depend on CubeDesc and CubeSegment at the same time.,", , , "
"Move And Rename Class,","Collect Metrics to JMX As we all known, some performance metrics is important for enterprise applications. so we should support to collect metrics to JMX in Kylin.

The method I have done is As shown below:

1. use `org.apache.hadoop.metrics2` as the metrics collection framework.
2. define MBean Class for the metrics that we need to collect.
3. update metrics in right place.

The questions I have:
1. can I depend on `org.apache.hadoop.metrics2` directly?
2. how do you think about my method?",", "
"Rename Method,Inline Method,",Refactor IJoinedFlatTableDesc Make IJoinedFlatTableDesc more independent. Don't depend on CubeDesc and CubeSegment at the same time.,", , "
"Rename Method,Extract Method,Inline Method,",Split kylin.properties into two files Split kylin.properties into two files: normal properties and security sensitive properties,"Duplicated Code, Long Method, , , "
"Rename Method,Inline Method,",Split kylin.properties into two files Split kylin.properties into two files: normal properties and security sensitive properties,", , "
"Move And Rename Class,Rename Method,Move Method,Extract Method,Move Attribute,","Improve enable limit logic (exactAggregation is too strict) from zhaotianshuo@meizu.com:

recently I got the following error while execute query on a cube which is not that big( about 400mb, 20milion record)
==================
Error while executing SQL ""select FCRASHTIME,count(1) from UXIP.EDL_FDT_OUC_UPLOAD_FILES group by FCRASH_ANALYSIS_ID,FCRASHTIME limit 1"": Scan row count exceeded threshold: 10000000, please add filter condition to narrow down backend scan range, like where clause.

I guess what it scan were the intermediate result, but It doesn't any order by,also the result count is limit to just 1.so it could scan to find any record with those two dimension and wala.
","Duplicated Code, Long Method, , , , "
"Rename Class,Extract Method,Inline Method,","CubeTupleConverter.translateResult() is slow due to date conversion As attached, profiling shows that current filling tuple is slow especially about dates. The norm form ""yyyy-mm-dd"" is parsed into a Date, then converted into # since epic day. This is slow because of Java's slow SimpleDateFormat.

An idea is to refine the norm form of date/time to be the time millis. This will save a lot of str <--> date conversions. However the change impacts dictionary, lookup snapshot, and maybe data ingestion. Also need to think about backward compatibility.","Duplicated Code, Long Method, , , "
"Rename Method,Extract Method,","TopN measure should support non-dictionary encoding for ultra high cardinality TopN measure uses dictionary to encode the literal column, that may not work when the cardinality is ultra high. Need support other encoding like fixedLength.","Duplicated Code, Long Method, , "
"Rename Method,",Use KylinConfig inside coprocessor ,", "
"Move Class,Rename Method,Extract Method,Inline Method,","Introduce dictionary metadata To allow

- Special dictionary builder for specified column
- Two or more columns to share dictionary","Duplicated Code, Long Method, , , "
"Rename Method,Extract Method,",Use KylinConfig inside coprocessor ,"Duplicated Code, Long Method, , "
"Rename Class,Extract Method,","Grow ByteBuffer Dynamically in Cube Building and Query In Cube Building Mapper/Reducer and CubeVisitService, we use an allocated ByteBuffer to store encoded metrics value, with a constant size RowConstants.ROWVALUE_BUFFER_SIZE, which is 1MB by default.
If the metrics value is larger than 1MB, such as high cardinality bitmap, BufferOverflowException will be threw. We need grow the ByteBuffer if the exception occured.","Duplicated Code, Long Method, , "
"Move And Rename Class,Rename Method,","Distribute source data by certain columns when creating flat table Inspired by KYLIN-1656, Kylin can distribute the source data by certain columns when creating the flat hive table; Then the data assigned to a mapper will have more similarity, more aggregation can happen at mapper side, and then less shuffle and reduce is needed.

Columns can be used for the distribution includes: ultra high cardinality column, mandantory column, partition date/time column, etc.",", "
"Move Class,Extract Method,",Bring more information in diagnosis tool ,"Duplicated Code, Long Method, , "
"Rename Class,Rename Method,Pull Up Method,Move Method,Extract Method,Pull Up Attribute,Move Attribute,",Bring more information in diagnosis tool ,"Duplicated Code, Long Method, , , , Duplicated Code, Duplicated Code, "
"Rename Method,","Support Hive View as Lookup Table If we use a view as a lookup table the cube building job fails when executing the 3rd step (Build Dimension Dictionary) with this log:

java.io.IOException: java.lang.NullPointerException
at org.apache.kylin.dict.lookup.HiveTable.getSignature(HiveTable.java:72)
at org.apache.kylin.dict.DictionaryManager.buildDictionary(DictionaryManager.java:202)
at org.apache.kylin.cube.CubeManager.buildDictionary(CubeManager.java:166)
at org.apache.kylin.cube.cli.DictionaryGeneratorCLI.processSegment(DictionaryGeneratorCLI.java:52)
at org.apache.kylin.cube.cli.DictionaryGeneratorCLI.processSegment(DictionaryGeneratorCLI.java:41)
at org.apache.kylin.job.hadoop.dict.CreateDictionaryJob.run(CreateDictionaryJob.java:52)
at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)
at org.apache.kylin.job.common.HadoopShellExecutable.doWork(HadoopShellExecutable.java:62)
at org.apache.kylin.job.execution.AbstractExecutable.execute(AbstractExecutable.java:107)
at org.apache.kylin.job.execution.DefaultChainedExecutable.doWork(DefaultChainedExecutable.java:51)
at org.apache.kylin.job.execution.AbstractExecutable.execute(AbstractExecutable.java:107)
at org.apache.kylin.job.impl.threadpool.DefaultScheduler$JobRunner.run(DefaultScheduler.java:130)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException

result code:2",", "
"Rename Method,",Bring more information in diagnosis tool ,", "
"Rename Method,","Improve performance of converting data to hfile Supposed that we got 100GB data after cuboid building, and with setting that 10GB per region. For now, 10 split keys was calculated, and 10 region created, 10 reducer used in ‘convert to hfile’ step. 

With optimization, we could calculate 100 (or more) split keys, and use all them in ‘covert to file’ step, but sampled 10 keys in them to create regions. The result is still 10 region created, but 100 reducer used in ‘convert to file’ step. Of course, the hfile created is also 100, and load 10 files per region. That’s should be fine, doesn’t affect the query performance dramatically.",", "
"Rename Method,","Count distinct on any dimension should work even not a predefined measure Currently count distinct on a dimension does not work:

{""sql"":""select DATE'2015-07-18' , count(distinct country) as uniquecountry from pc_session e INNER JOIN pc_cal c ON e.part_date = c.cal_dt WHERE (part_date BETWEEN DATE'2015-07-18' AND DATE'2015-08-19') "",""offset"":0,""limit"":10,""acceptPartial"":true,""project"":""tracking""}

""exception"": ""Can't find any realization. Please confirm with providers. SQL digest: fact table DEFAULT.PC_SESSION,group by [],filter on [DEFAULT.PC_SESSION.PART_DATE],with aggregates[FunctionDesc [expression=COUNT_DISTINCT, parameter=ParameterDesc [type=column, value=COUNTRY], returnType=null]].
while executing SQL: ""select DATE'2015-07-18' , count(distinct country) as uniquecountry from pc_session e INNER JOIN pc_cal c ON e.part_date = c.cal_dt WHERE (part_date BETWEEN DATE'2015-07-18' AND DATE'2015-08-19') LIMIT 10""""
",", "
"Move Class,Move And Rename Class,Extract Method,",Tool to dump information for diagnosis ,"Duplicated Code, Long Method, , "
"Rename Method,","When cube is not empty, only signature consistent cube desc updates are allowed Currently when user update a cube desc, he will be warned that if the update causes inconsistency, existing segments might get purged. However users never knows ahead whether or not the update is consistent or not, in other words, users are not sure what are the effects if they change a specific cube desc field.

For cube desc updates, if the current cube is empty, we'll radically allow desc updates even if it will change the signature. On the other side, if the cube has existing segments, we'll check signature consistency before allowing the update. A failure in signature consistency will return error message and no update will be performed.

this will lead to a less-confusing interactivity with the user.",", "
"Rename Class,Move And Rename Class,","Metadata upgrade from 1.0~1.3 to 1.5, including metadata correction, relevant tools, etc. since 1.4.x branch is never released officially, users might need to upgrade to 1.5 compatible metadata format from 1.0.x, 1.1.x, 1.2.x or 1.3.x releases",", "
"Move Class,Rename Class,Move And Rename Class,Move Method,Move Attribute,","Metadata upgrade from 1.0~1.3 to 1.5, including metadata correction, relevant tools, etc. since 1.4.x branch is never released officially, users might need to upgrade to 1.5 compatible metadata format from 1.0.x, 1.1.x, 1.2.x or 1.3.x releases",", , , "
"Move Class,Move Method,Extract Method,Move Attribute,","Custom dimension encoding Currently dimension is encoded by either dictionary or fixed-length. There could be better custom encoding to support cases like bank account or card ID. This will help enabling some ultra high cardinality columns, and at the same time make data more compressed and save some storage.","Duplicated Code, Long Method, , , , "
"Rename Method,Extract Method,Inline Method,","Upgrade calcite version to 1.6 calcite is going to release 1.6, we'll upgrade to it as it contains bug fixes that blocks us","Duplicated Code, Long Method, , , "
"Move Method,Move Attribute,","Define stream config at table level, instead of on cube level In 2.0 streaming, user need enter the kafka information when create the cube, like the topic, the broker list, etc; while these info should be independent with cube, and can be reused across cubes which share the same table.

The expected design is, define kafka config when adding the table.",", , , "
"Move Method,Move Attribute,","Improve performance of converting data to hfile Supposed that we got 100GB data after cuboid building, and with setting that 10GB per region. For now, 10 split keys was calculated, and 10 region created, 10 reducer used in ‘convert to hfile’ step. 

With optimization, we could calculate 100 (or more) split keys, and use all them in ‘covert to file’ step, but sampled 10 keys in them to create regions. The result is still 10 region created, but 100 reducer used in ‘convert to file’ step. Of course, the hfile created is also 100, and load 10 files per region. That’s should be fine, doesn’t affect the query performance dramatically.",", , , "
"Rename Class,Rename Method,","cuboid sharding based on specific column currently cuboid sharding is based on the hash value of the row key. If we allow computing hash value on a specific column C, then we're actually partitioning the data w.r.t C. The benefit is that if we later a query with filter like ""where C = 'xyz'"", kylin can skip shards. Also for filter like ""where C IN {many candidates}"", kylin can prune candidates sent to different shards",", "
"Rename Method,Move Method,Move Attribute,","Tools to extract all cube/hybrid/project related metadata to facilitate diagnosing/debugging/sharing list of extracted items for a cube:
cube desc
cube instance
data model
all related table desc, and possibly with ext table info
each segment's dict, snapshot,statistics
cube related jobs and job outputs (optional)

list of extracted items for a hybrid:
hybrid instance
for each cube in the hybrid collect ""list of extracted items for a cube""

list of extracted items for a project:
project instance
for each cube in the project collect ""list of extracted items for a cube""
for each hybrid in the project collect ""list of extracted items for a hybrid""

",", , , "
"Rename Method,Extract Method,",Aggregation group validation To add more validations on aggregation group during the loading of cubes.,"Duplicated Code, Long Method, , "
"Extract Method,Inline Method,","Enable deriving dimensions on non PK/FK currently derived column has to be columns on look table, and the derived host column has to be PK/FK(It's also a problem when the lookup table grows every large). Sometimes columns on the fact exhibit deriving relationship too. Here's an example fact table:

(dt date, seller_id bigint, seller_name varchar(100) , item_id bigint, item_url varchar(1000), count decimal, price decimal)

seller_name is uniquely determined by each seller id, and item_url is uniquely determined by each item_id. The users does not expect to do filtering on columns like seller name or item_url, they just want to retrieve it when they do grouping/filtering on other dimensions like selller id, item id or even other dimensions like dt.
","Duplicated Code, Long Method, , , "
"Move Method,Move Attribute,",Upgrade tool to put old-version cube and new-version cube into a hybrid model ,", , , "
"Rename Method,Extract Method,","Enhance DeployCoprocessorCLI to support Cube level filter Currently DeployCoprocessorCLI can only filter on Tables, we need to enhance it to support Cube as filter.","Duplicated Code, Long Method, , "
"Move Class,Inline Method,","Replace GTScanRequest's SerDer form Kryo to manual Kryo greatly simplifies bject SerDer at the cost of performance. When there're tens of segments, such cost accumulates too big to accept. Going to serialize GTScanRequest's with manual serialization.",", , "
"Extract Superclass,Rename Method,Move Method,Extract Method,Move Attribute,","Switch between layer cubing and in-mem cubing according to stats Without the smart switch, in-mem cubing could be very slow in a worst case, though its best case is super fast. Layer cubing however is more stable, kinda always in the middle.","Duplicated Code, Long Method, , , , Duplicated Code, Large Class, "
"Move Method,Extract Method,","Switch between layer cubing and in-mem cubing according to stats Without the smart switch, in-mem cubing could be very slow in a worst case, though its best case is super fast. Layer cubing however is more stable, kinda always in the middle.","Duplicated Code, Long Method, , , "
"Rename Method,Move Method,Move Attribute,","Switch between layer cubing and in-mem cubing according to stats Without the smart switch, in-mem cubing could be very slow in a worst case, though its best case is super fast. Layer cubing however is more stable, kinda always in the middle.",", , , "
"Move Class,Move And Rename Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Support Custom Aggregation Types Currently, Kylin supports 6 basic aggregation measure functions:
Min/Max/Sum/Count/Avg/DistinctCount

But there are also many other cases require to support more complicate measure expression, for example:
COUNT(CASE WHEN so.ft = 'fv' THEN soi.sc ELSE NULL END) or Sum(if...)

Or even more complicated measures like TopN and RawRecords

To open this JIRA to tracking further implementation","Duplicated Code, Long Method, , , , , "
"Rename Method,","Support Custom Aggregation Types Currently, Kylin supports 6 basic aggregation measure functions:
Min/Max/Sum/Count/Avg/DistinctCount

But there are also many other cases require to support more complicate measure expression, for example:
COUNT(CASE WHEN so.ft = 'fv' THEN soi.sc ELSE NULL END) or Sum(if...)

Or even more complicated measures like TopN and RawRecords

To open this JIRA to tracking further implementation",", "
"Rename Method,Move Method,Inline Method,Move Attribute,","Support Custom Aggregation Types Currently, Kylin supports 6 basic aggregation measure functions:
Min/Max/Sum/Count/Avg/DistinctCount

But there are also many other cases require to support more complicate measure expression, for example:
COUNT(CASE WHEN so.ft = 'fv' THEN soi.sc ELSE NULL END) or Sum(if...)

Or even more complicated measures like TopN and RawRecords

To open this JIRA to tracking further implementation",", , , , "
"Move Class,Rename Method,Extract Method,","Support Custom Aggregation Types Currently, Kylin supports 6 basic aggregation measure functions:
Min/Max/Sum/Count/Avg/DistinctCount

But there are also many other cases require to support more complicate measure expression, for example:
COUNT(CASE WHEN so.ft = 'fv' THEN soi.sc ELSE NULL END) or Sum(if...)

Or even more complicated measures like TopN and RawRecords

To open this JIRA to tracking further implementation","Duplicated Code, Long Method, , "
"Rename Method,Move Method,","Support Custom Aggregation Types Currently, Kylin supports 6 basic aggregation measure functions:
Min/Max/Sum/Count/Avg/DistinctCount

But there are also many other cases require to support more complicate measure expression, for example:
COUNT(CASE WHEN so.ft = 'fv' THEN soi.sc ELSE NULL END) or Sum(if...)

Or even more complicated measures like TopN and RawRecords

To open this JIRA to tracking further implementation",", , "
"Move Class,Move And Rename Class,Move Method,Extract Method,Inline Method,Move Attribute,","Support Custom Aggregation Types Currently, Kylin supports 6 basic aggregation measure functions:
Min/Max/Sum/Count/Avg/DistinctCount

But there are also many other cases require to support more complicate measure expression, for example:
COUNT(CASE WHEN so.ft = 'fv' THEN soi.sc ELSE NULL END) or Sum(if...)

Or even more complicated measures like TopN and RawRecords

To open this JIRA to tracking further implementation","Duplicated Code, Long Method, , , , , "
"Move Class,Move Method,Extract Method,Move Attribute,","Support Custom Aggregation Types Currently, Kylin supports 6 basic aggregation measure functions:
Min/Max/Sum/Count/Avg/DistinctCount

But there are also many other cases require to support more complicate measure expression, for example:
COUNT(CASE WHEN so.ft = 'fv' THEN soi.sc ELSE NULL END) or Sum(if...)

Or even more complicated measures like TopN and RawRecords

To open this JIRA to tracking further implementation","Duplicated Code, Long Method, , , , "
"Move Class,Rename Method,Move Method,Extract Method,Move Attribute,","Redesign aggregation group Proposal from Hongbin about Cuboid White List:


* More detail, please refer to Kylin Dev mailing list: dev@kylin.incubator.apache.org 

Logically, a cube contains cuboids representing all combinations of
dimensions. Apparently, a naive cube building strategy that materializes
all cuboids will easily meet curse-of-dimension problems. Currently Kylin
leverages a strategy called ""aggregation groups"" to reduce the number of
cuboids need being materialized.

However, if the query pattern is simple and fixed, the ""aggregation group""
strategy is still not efficient enough. For example, suppose there're five
dimensions, namely A,B,C,D and E. The data modeler is sure that only
combinations (A,B,C), (D,E), (A,E) will be queried, so he’ll use the
aggregation group tool to optimize his cube definition. However, whatever
aggregation group he chooses, lots of useless combinations would be
materialized.

With a new strategy called ""cuboid whitelist"", data modelers can guide
Kylin to only materialize the cuboids he's interested in. Depending on the
whitelist, Kylin will materialize the minimal set of cuboids to cover each
cuboid in the whitelist. To support this, the following functionalities
should be added:

1. Front-end/UI for specifying whitelist members, and persistent them to
cube description.
2. Enhanced job engine scheduler that will calculate a minimal spanning
build tree based on the whitelist.
3. (OPTIONAL) Enhanced job engine to support dynamic whitelist, trigger new
builds for lately added whitelist members.

Hongbin Ma

---------------- Imported from GitHub ----------------
Url: https://github.com/KylinOLAP/Kylin/issues/263
Created by: [lukehan|https://github.com/lukehan]
Labels: enhancement, 
Milestone: Backlog
Assignee: [binmahone|https://github.com/binmahone]
Created at: Thu Dec 25 13:17:11 CST 2014
State: open
","Duplicated Code, Long Method, , , , "
"Rename Method,Extract Method,Inline Method,","Cube parallel scan on Hbase Currently cube is scanned in a sequential way, if endpoint is introduced, cube scanning can be parallelized too, this will lead to :

1. faster cube scanning
2. topN query made possible","Duplicated Code, Long Method, , , "
"Rename Method,","Distinguish fast build mode and complete build mode currently BuildCubeWithEngineTest is responsible for building 4 test cubes for other test cases to use. we intend to build the cubes by stages(use incremental cube building and cube merging, this meaning multiple rounds of MR jobs) so that we can make sure to cover both job engine's cube building and merging functions

On the other hand, sometimes when we're working on other modules, we don't care how a cube is built, we only need a queryable cube(like when we're developing query engine). This is when fast mode would be help full. In BuildCubeWithEngineTest fast mode, every test cube will be built with a single segment, this means only a single round of MR is required for each cube, this will make BuildCubeWithEngineTest much faster.
",", "
"Rename Method,Extract Method,Pull Up Attribute,","v2 storage(for parallel scan) backward compatibility with v1 storage As KYLIN-942 introduced a new storage format with shard put at the beginning of each rowkey(we call it v2 storage), cubes built with previous versions of job engines will no longer be compatible with current query engine. Need to refactor job engine& query engine to support both version of cube storage.","Duplicated Code, Long Method, , Duplicated Code, "
"Rename Method,",Support dictionary of cardinality over 10 millions Many use cases involve columns with high cardinality over 10 millions. Dictionary should be supported on these columns assuming mem is sufficient.,", "
"Move Class,Rename Class,Rename Method,Move Attribute,","Use local dictionary for InvertedIndex batch building InvertedIndex has two dictionary types: global dictionary and local dictionary; The batching building uses the global dict, and the streaming building uses the local dict; 

To make it be consistent and easy for maintain, should use only one type of dictionary: local.",", , "
"Move And Rename Class,Rename Method,Extract Method,","Cube parallel scan on Hbase Currently cube is scanned in a sequential way, if endpoint is introduced, cube scanning can be parallelized too, this will lead to :

1. faster cube scanning
2. topN query made possible","Duplicated Code, Long Method, , "
"Rename Class,Move And Rename Class,Rename Method,Move Method,Extract Method,","Cube parallel scan on Hbase Currently cube is scanned in a sequential way, if endpoint is introduced, cube scanning can be parallelized too, this will lead to :

1. faster cube scanning
2. topN query made possible","Duplicated Code, Long Method, , , "
"Rename Method,","ADD Streaming UI since we will enable streaming feature, we also need to enable streaming module UI for user to use on web.",", "
"Rename Class,Extract Method,","Route unsupported queries to Hive (on Spark) When Kylin can't serve coming SQL, it will be better to route it to enabled SQL on Hadoop like SparkSQL and execute there. 

Then get result and return to client through Kylin server.

","Duplicated Code, Long Method, , "
"Rename Method,","Approximate TopN supported by Cube SpaceSaving (TopN algorithm) code could copy from https://github.com/addthis/stream-lib/blob/master/src/main/java/com/clearspring/analytics/stream/StreamSummary.java
We don’t need the whole stream-lib, but just one (or two) classes is enough. Make sure you give credit to stream-lib in class comment.

In order to run SpaceSaving in parallel, the TopN has to be merged using http://arxiv.org/pdf/1401.0702.pdf. No existing impl as I searched, we have to implement ourselves.

Cheers
Yang

From: Li, Yang 
Sent: 2015年8月7日 12:43
To: DL-eBay-Kylin
Subject: Distributed TopN papers

The basic algorithm
[1] https://icmi.cs.ucsb.edu/research/tech_reports/reports/2005-23.pdf

Its application in distributed system
[2] http://www.cs.utah.edu/~jeffp/papers/merge-summ-TODS.pdf
[3] http://www.crm.umontreal.ca/pub/Rapports/3300-3399/3322.pdf


Cheers
Yang",", "
"Move Class,Move Method,Move Attribute,","Approximate TopN supported by Cube SpaceSaving (TopN algorithm) code could copy from https://github.com/addthis/stream-lib/blob/master/src/main/java/com/clearspring/analytics/stream/StreamSummary.java
We don’t need the whole stream-lib, but just one (or two) classes is enough. Make sure you give credit to stream-lib in class comment.

In order to run SpaceSaving in parallel, the TopN has to be merged using http://arxiv.org/pdf/1401.0702.pdf. No existing impl as I searched, we have to implement ourselves.

Cheers
Yang

From: Li, Yang 
Sent: 2015年8月7日 12:43
To: DL-eBay-Kylin
Subject: Distributed TopN papers

The basic algorithm
[1] https://icmi.cs.ucsb.edu/research/tech_reports/reports/2005-23.pdf

Its application in distributed system
[2] http://www.cs.utah.edu/~jeffp/papers/merge-summ-TODS.pdf
[3] http://www.crm.umontreal.ca/pub/Rapports/3300-3399/3322.pdf


Cheers
Yang",", , , "
"Rename Method,","Allow user to configure the region split size for cube Kylin hardcoded the region split size in RangeKeyDistributionReducer.java; It will use 10G, 20G and 100G as the split for a Small/Medium/Large cube; Besides, it set a limit on max region count to 500; Hardcode is not good; we should externalize these values to config file.
",", "
"Move Class,Rename Class,Move And Rename Class,",Split storage module to core-storage and storage-hbase for modularization purpose code refactor should be applied to storage module,", "
"Rename Method,Move Method,Extract Method,Move Attribute,","Adapt GTStore to hbase endpoint Adapt hbase cube to GTStore, cube in hbase will read by endpoint","Duplicated Code, Long Method, , , , "
"Rename Method,",Growing dictionary in streaming is potentially memory consuming for slowly increasing dictionaries ,", "
"Move Class,Push Down Method,Move Method,Move Attribute,","Decouple with Hadoop to allow alternative Input / Build Engine / Storage We've got many requests about how to use an input source other than hive; or how to use Spark instead of Hadoop MR; or how to use Cassandra in place of HBase.

In order to decouple with Hadoop, interfaces must be defined for Input Source / Build Engine / Storage respectively. And let hive / MR / hbase become a piece of plugin of implementation.
",", , , , "
"Move Class,Move Method,Move Attribute,","Decouple with Hadoop to allow alternative Input / Build Engine / Storage We've got many requests about how to use an input source other than hive; or how to use Spark instead of Hadoop MR; or how to use Cassandra in place of HBase.

In order to decouple with Hadoop, interfaces must be defined for Input Source / Build Engine / Storage respectively. And let hive / MR / hbase become a piece of plugin of implementation.
",", , , "
"Move Class,Move And Rename Class,Move Method,Move Attribute,","Decouple with Hadoop to allow alternative Input / Build Engine / Storage We've got many requests about how to use an input source other than hive; or how to use Spark instead of Hadoop MR; or how to use Cassandra in place of HBase.

In order to decouple with Hadoop, interfaces must be defined for Input Source / Build Engine / Storage respectively. And let hive / MR / hbase become a piece of plugin of implementation.
",", , , "
"Move Method,Move Attribute,",move streaming related parameters into StreamingConfig ,", , , "
"Rename Method,","Add ""retention_range"" attribute for cube instance, and automatically drop the oldest segment when exceeds retention Sometimes user want to only keep a certain range data in cube, for example only 1 month; Kylin should allow define such a ""retention_range"" at cube instance level; When a new segment is built, check and drop the oldest segment from the head if the retention is exceeded;",", "
"Rename Method,Extract Method,","growing dictionary for streaming case streaming cubes will generate a lot of dictionaries, and they may differ from each other only a little. growing dict swallows new entries and generate a bigger dict each time a new segment is being built. in this way the entries of dicts get controlled.","Duplicated Code, Long Method, , "
"Rename Class,Rename Method,",StreamingOLAP ,", "
"Move Method,Move Attribute,","IPartitionConditionBuilder, flexible range condition to match hive partitions ",", , , "
"Rename Method,",Streaming cubing allow multiple kafka clusters/topics ,", "
"Rename Class,Move Method,Move Attribute,",Streaming cubing allow multiple kafka clusters/topics ,", , , "
"Extract Method,Inline Method,","Out of memory in mapper when building cube in mem 2015-04-08 03:08:56,992 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.resourcemanager.keytab; Ignoring.
2015-04-08 03:08:56,996 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.jobhistory.http.policy; Ignoring.
2015-04-08 03:08:56,999 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval; Ignoring.
2015-04-08 03:08:57,003 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.keytab; Ignoring.
2015-04-08 03:08:57,004 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.resourcemanager.principal; Ignoring.
2015-04-08 03:08:57,006 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: net.topology.script.file.name; Ignoring.
2015-04-08 03:08:57,012 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.jobhistory.webapp.https.address; Ignoring.
2015-04-08 03:08:57,021 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.log-dirs; Ignoring.
2015-04-08 03:08:57,022 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: fs.defaultFS; Ignoring.
2015-04-08 03:08:57,029 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.jobhistory.keytab; Ignoring.
2015-04-08 03:08:57,034 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.http.policy; Ignoring.
2015-04-08 03:08:57,041 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.local-dirs; Ignoring.
2015-04-08 03:08:57,043 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.linux-container-executor.group; Ignoring.
2015-04-08 03:08:57,043 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.jobhistory.principal; Ignoring.
2015-04-08 03:08:57,047 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.container-executor.class; Ignoring.
2015-04-08 03:08:57,048 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts; Ignoring.
2015-04-08 03:08:57,050 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.admin.acl; Ignoring.
2015-04-08 03:08:57,053 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.principal; Ignoring.
2015-04-08 03:08:58,279 INFO [main] org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-04-08 03:08:58,347 INFO [main] org.apache.hadoop.metrics2.impl.MetricsSinkAdapter: Sink ganglia started
2015-04-08 03:08:58,492 INFO [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-04-08 03:08:58,492 INFO [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: MapTask metrics system started
2015-04-08 03:08:58,570 INFO [main] org.apache.hadoop.mapred.YarnChild: Executing with tokens:
2015-04-08 03:08:58,571 INFO [main] org.apache.hadoop.mapred.YarnChild: Kind: HDFS_DELEGATION_TOKEN, Service: 10.115.206.112:8020, Ident: (HDFS_DELEGATION_TOKEN token 8430348 for b_kylin)
2015-04-08 03:08:58,650 INFO [main] org.apache.hadoop.mapred.YarnChild: Kind: mapreduce.job, Service: job_1427705526386_110981, Ident: (org.apache.hadoop.mapreduce.security.token.JobTokenIdentifier@69607702)
2015-04-08 03:08:58,928 INFO [main] org.apache.hadoop.mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
2015-04-08 03:08:59,367 INFO [main] org.apache.hadoop.mapred.YarnChild: mapreduce.cluster.local.dir for child: /hadoop/1/scratch/local/usercache/b_kylin/appcache/application_1427705526386_110981,/hadoop/2/scratch/local/usercache/b_kylin/appcache/application_1427705526386_110981,/hadoop/3/scratch/local/usercache/b_kylin/appcache/application_1427705526386_110981,/hadoop/4/scratch/local/usercache/b_kylin/appcache/application_1427705526386_110981,/hadoop/5/scratch/local/usercache/b_kylin/appcache/application_1427705526386_110981,/hadoop/6/scratch/local/usercache/b_kylin/appcache/application_1427705526386_110981,/hadoop/7/scratch/local/usercache/b_kylin/appcache/application_1427705526386_110981,/hadoop/8/scratch/local/usercache/b_kylin/appcache/application_1427705526386_110981
2015-04-08 03:08:59,567 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.resourcemanager.keytab; Ignoring.
2015-04-08 03:08:59,568 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.jobhistory.http.policy; Ignoring.
2015-04-08 03:08:59,569 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval; Ignoring.
2015-04-08 03:08:59,570 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.keytab; Ignoring.
2015-04-08 03:08:59,571 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.resourcemanager.principal; Ignoring.
2015-04-08 03:08:59,571 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: net.topology.script.file.name; Ignoring.
2015-04-08 03:08:59,573 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: dfs.namenode.checkpoint.dir; Ignoring.
2015-04-08 03:08:59,573 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.jobhistory.webapp.https.address; Ignoring.
2015-04-08 03:08:59,576 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.log-dirs; Ignoring.
2015-04-08 03:08:59,577 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: fs.defaultFS; Ignoring.
2015-04-08 03:08:59,579 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.jobhistory.keytab; Ignoring.
2015-04-08 03:08:59,579 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: dfs.namenode.name.dir; Ignoring.
2015-04-08 03:08:59,582 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.http.policy; Ignoring.
2015-04-08 03:08:59,585 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.local-dirs; Ignoring.
2015-04-08 03:08:59,586 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.linux-container-executor.group; Ignoring.
2015-04-08 03:08:59,586 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.jobhistory.principal; Ignoring.
2015-04-08 03:08:59,587 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.container-executor.class; Ignoring.
2015-04-08 03:08:59,588 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts; Ignoring.
2015-04-08 03:08:59,589 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.admin.acl; Ignoring.
2015-04-08 03:08:59,591 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.principal; Ignoring.
2015-04-08 03:09:00,499 INFO [main] org.apache.hadoop.conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
2015-04-08 03:09:01,285 INFO [main] org.apache.hadoop.mapred.Task: Using ResourceCalculatorProcessTree : [ ]
2015-04-08 03:09:02,362 INFO [main] org.apache.hadoop.mapred.MapTask: Processing split: org.apache.hive.hcatalog.mapreduce.HCatSplit@47e7cecf
2015-04-08 03:09:02,500 INFO [main] org.apache.hadoop.mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2015-04-08 03:09:03,706 INFO [main] org.apache.hadoop.mapred.MapTask: (EQUATOR) 0 kvi 268435452(1073741808)
2015-04-08 03:09:03,706 INFO [main] org.apache.hadoop.mapred.MapTask: mapreduce.task.io.sort.mb: 1024
2015-04-08 03:09:03,706 INFO [main] org.apache.hadoop.mapred.MapTask: soft limit at 966367616
2015-04-08 03:09:03,706 INFO [main] org.apache.hadoop.mapred.MapTask: bufstart = 0; bufvoid = 1073741824
2015-04-08 03:09:03,706 INFO [main] org.apache.hadoop.mapred.MapTask: kvstart = 268435452; length = 67108864
2015-04-08 03:09:03,717 INFO [main] com.hadoop.compression.lzo.GPLNativeCodeLoader: Loaded native gpl library
2015-04-08 03:09:03,720 INFO [main] com.hadoop.compression.lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev dbd51f0fb61f5347228a7a23fe0765ac1242fcdf]
2015-04-08 03:09:03,846 INFO [main] org.apache.hadoop.io.compress.zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
2015-04-08 03:09:03,847 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.deflate]
2015-04-08 03:09:04,348 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.gz]
2015-04-08 03:09:04,348 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.gz]
2015-04-08 03:09:04,349 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.gz]
2015-04-08 03:09:04,349 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.gz]
2015-04-08 03:09:04,359 INFO [main] org.apache.hive.hcatalog.mapreduce.InternalUtil: Initializing org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe with properties {name=DEFAULT.kylin_intermediate_PC_SESSION_COPY_20150322000000_20150323000000_841ce77b_81c5_4c0c_941a_6f2e309a131f, numFiles=0, field.delim=, columns.types=string,int,string,string,string,string,string,string,string,string,string,string,string,string,bigint,bigint,bigint,bigint, serialization.format=, columns=default_pc_session_tenantname,default_pc_session_tenantsite,default_pc_session_devicefamily,default_pc_session_deviceclass,default_pc_session_osfamily,default_pc_session_osversion,default_pc_session_browserfamily,default_pc_session_browserversion,default_pc_session_trafficsource,default_pc_session_continent,default_pc_session_country,default_pc_session_region,default_pc_session_city,default_pc_session_streamid,default_pc_session_bounce,default_pc_session_retvisitor,default_pc_session_serveventcnt,default_pc_session_absduration, rawDataSize=50265185482, numRows=507325307, EXTERNAL=TRUE, serialization.lib=org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, COLUMN_STATS_ACCURATE=true, totalSize=0, serialization.null.format=\N, transient_lastDdlTime=1427989487}
2015-04-08 03:09:04,454 INFO [main] org.apache.kylin.job.hadoop.AbstractHadoopJob: The absolute path for meta dir is /hadoop/8/scratch/local/usercache/b_kylin/appcache/application_1427705526386_110981/container_1427705526386_110981_01_000026/meta
2015-04-08 03:09:04,456 INFO [main] org.apache.kylin.common.KylinConfig: Use KYLIN_CONF=/hadoop/8/scratch/local/usercache/b_kylin/appcache/application_1427705526386_110981/container_1427705526386_110981_01_000026/meta
2015-04-08 03:09:04,476 INFO [main] org.apache.kylin.cube.CubeManager: Initializing CubeManager with config /hadoop/3/scratch/local/usercache/b_kylin/filecache/1483/meta
2015-04-08 03:09:04,480 INFO [main] org.apache.kylin.common.persistence.ResourceStore: Using metadata url /hadoop/3/scratch/local/usercache/b_kylin/filecache/1483/meta for resource store
2015-04-08 03:09:04,955 INFO [main] org.apache.kylin.cube.CubeDescManager: Initializing CubeDescManager with config /hadoop/3/scratch/local/usercache/b_kylin/filecache/1483/meta
2015-04-08 03:09:04,956 INFO [main] org.apache.kylin.cube.CubeDescManager: Reloading Cube Metadata from folder /hadoop/3/scratch/local/usercache/b_kylin/filecache/1483/meta/cube_desc
2015-04-08 03:09:05,177 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Create base cuboid 8191
2015-04-08 03:09:08,171 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 100000 records!
2015-04-08 03:09:09,776 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 200000 records!
2015-04-08 03:09:11,903 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 300000 records!
2015-04-08 03:09:13,739 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 400000 records!
2015-04-08 03:09:15,243 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 500000 records!
2015-04-08 03:09:15,389 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Received 500000 rows, going to compress base cuboid table
2015-04-08 03:09:25,455 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: after scanAndAggregateGridTable cuboid 8191 has rows: 87680
2015-04-08 03:09:25,455 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Compress finished, it took 10 seconds.
2015-04-08 03:09:27,213 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 600000 records!
2015-04-08 03:09:28,391 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 700000 records!
2015-04-08 03:09:29,963 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 800000 records!
2015-04-08 03:09:31,408 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 900000 records!
2015-04-08 03:09:33,124 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 1000000 records!
2015-04-08 03:09:33,300 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Received 1000000 rows, going to compress base cuboid table
2015-04-08 03:09:45,069 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: after scanAndAggregateGridTable cuboid 8191 has rows: 183872
2015-04-08 03:09:45,070 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Compress finished, it took 11 seconds.
2015-04-08 03:09:46,535 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 1100000 records!
2015-04-08 03:09:47,942 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 1200000 records!
2015-04-08 03:09:49,166 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 1300000 records!
2015-04-08 03:09:50,398 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 1400000 records!
2015-04-08 03:09:51,170 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Totally handled 1453555 records!
2015-04-08 03:09:51,302 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: The source data has 1453555 rows
2015-04-08 03:09:51,303 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Received 1453555 rows, going to compress base cuboid table
2015-04-08 03:10:54,326 ERROR [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: stream build failed
java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: GC overhead limit exceeded
at java.util.concurrent.FutureTask.report(FutureTask.java:122)
at java.util.concurrent.FutureTask.get(FutureTask.java:188)
at org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper.cleanup(InMemCuboidMapper.java:96)
at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:148)
at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)
at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:415)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1650)
at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded
at org.apache.kylin.common.hll.HyperLogLogPlusCounter.<init>(HyperLogLogPlusCounter.java:64)
at org.apache.kylin.common.hll.HyperLogLogPlusCounter.<init>(HyperLogLogPlusCounter.java:55)
at org.apache.kylin.metadata.measure.HLLCAggregator.aggregate(HLLCAggregator.java:39)
at org.apache.kylin.metadata.measure.HLLCAggregator.aggregate(HLLCAggregator.java:27)
at org.apache.kylin.storage.gridtable.GTAggregateScanner$AggregationCache.aggregate(GTAggregateScanner.java:85)
at org.apache.kylin.storage.gridtable.GTAggregateScanner.iterator(GTAggregateScanner.java:57)
at org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder.scanAndAggregateGridTable(InMemCubeBuilder.java:193)
at org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder.compressBaseCuboid(InMemCubeBuilder.java:376)
at org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder.run(InMemCubeBuilder.java:318)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
at java.util.concurrent.FutureTask.run(FutureTask.java:262)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)
2015-04-08 03:10:54,329 INFO [main] org.apache.hadoop.mapred.MapTask: Starting flush of map output
2015-04-08 03:10:54,334 INFO [main] org.apache.hadoop.conf.Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available
2015-04-08 03:10:54,336 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.lzo_deflate]
2015-04-08 03:10:54,591 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.io.IOException: Failed to build cube in mapper 28
at org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper.cleanup(InMemCuboidMapper.java:99)
at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:148)
at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)
at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:415)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1650)
at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: GC overhead limit exceeded
at java.util.concurrent.FutureTask.report(FutureTask.java:122)
at java.util.concurrent.FutureTask.get(FutureTask.java:188)
at org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper.cleanup(InMemCuboidMapper.java:96)
... 8 more
Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded
at org.apache.kylin.common.hll.HyperLogLogPlusCounter.<init>(HyperLogLogPlusCounter.java:64)
at org.apache.kylin.common.hll.HyperLogLogPlusCounter.<init>(HyperLogLogPlusCounter.java:55)
at org.apache.kylin.metadata.measure.HLLCAggregator.aggregate(HLLCAggregator.java:39)
at org.apache.kylin.metadata.measure.HLLCAggregator.aggregate(HLLCAggregator.java:27)
at org.apache.kylin.storage.gridtable.GTAggregateScanner$AggregationCache.aggregate(GTAggregateScanner.java:85)
at org.apache.kylin.storage.gridtable.GTAggregateScanner.iterator(GTAggregateScanner.java:57)
at org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder.scanAndAggregateGridTable(InMemCubeBuilder.java:193)
at org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder.compressBaseCuboid(InMemCubeBuilder.java:376)
at org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder.run(InMemCubeBuilder.java:318)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
at java.util.concurrent.FutureTask.run(FutureTask.java:262)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)","Duplicated Code, Long Method, , , "
"Move Method,Extract Method,Move Attribute,","Out of memory in mapper when building cube in mem 2015-04-08 03:08:56,992 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.resourcemanager.keytab; Ignoring.
2015-04-08 03:08:56,996 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.jobhistory.http.policy; Ignoring.
2015-04-08 03:08:56,999 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval; Ignoring.
2015-04-08 03:08:57,003 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.keytab; Ignoring.
2015-04-08 03:08:57,004 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.resourcemanager.principal; Ignoring.
2015-04-08 03:08:57,006 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: net.topology.script.file.name; Ignoring.
2015-04-08 03:08:57,012 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.jobhistory.webapp.https.address; Ignoring.
2015-04-08 03:08:57,021 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.log-dirs; Ignoring.
2015-04-08 03:08:57,022 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: fs.defaultFS; Ignoring.
2015-04-08 03:08:57,029 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.jobhistory.keytab; Ignoring.
2015-04-08 03:08:57,034 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.http.policy; Ignoring.
2015-04-08 03:08:57,041 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.local-dirs; Ignoring.
2015-04-08 03:08:57,043 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.linux-container-executor.group; Ignoring.
2015-04-08 03:08:57,043 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.jobhistory.principal; Ignoring.
2015-04-08 03:08:57,047 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.container-executor.class; Ignoring.
2015-04-08 03:08:57,048 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts; Ignoring.
2015-04-08 03:08:57,050 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.admin.acl; Ignoring.
2015-04-08 03:08:57,053 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.principal; Ignoring.
2015-04-08 03:08:58,279 INFO [main] org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-04-08 03:08:58,347 INFO [main] org.apache.hadoop.metrics2.impl.MetricsSinkAdapter: Sink ganglia started
2015-04-08 03:08:58,492 INFO [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-04-08 03:08:58,492 INFO [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: MapTask metrics system started
2015-04-08 03:08:58,570 INFO [main] org.apache.hadoop.mapred.YarnChild: Executing with tokens:
2015-04-08 03:08:58,571 INFO [main] org.apache.hadoop.mapred.YarnChild: Kind: HDFS_DELEGATION_TOKEN, Service: 10.115.206.112:8020, Ident: (HDFS_DELEGATION_TOKEN token 8430348 for b_kylin)
2015-04-08 03:08:58,650 INFO [main] org.apache.hadoop.mapred.YarnChild: Kind: mapreduce.job, Service: job_1427705526386_110981, Ident: (org.apache.hadoop.mapreduce.security.token.JobTokenIdentifier@69607702)
2015-04-08 03:08:58,928 INFO [main] org.apache.hadoop.mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
2015-04-08 03:08:59,367 INFO [main] org.apache.hadoop.mapred.YarnChild: mapreduce.cluster.local.dir for child: /hadoop/1/scratch/local/usercache/b_kylin/appcache/application_1427705526386_110981,/hadoop/2/scratch/local/usercache/b_kylin/appcache/application_1427705526386_110981,/hadoop/3/scratch/local/usercache/b_kylin/appcache/application_1427705526386_110981,/hadoop/4/scratch/local/usercache/b_kylin/appcache/application_1427705526386_110981,/hadoop/5/scratch/local/usercache/b_kylin/appcache/application_1427705526386_110981,/hadoop/6/scratch/local/usercache/b_kylin/appcache/application_1427705526386_110981,/hadoop/7/scratch/local/usercache/b_kylin/appcache/application_1427705526386_110981,/hadoop/8/scratch/local/usercache/b_kylin/appcache/application_1427705526386_110981
2015-04-08 03:08:59,567 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.resourcemanager.keytab; Ignoring.
2015-04-08 03:08:59,568 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.jobhistory.http.policy; Ignoring.
2015-04-08 03:08:59,569 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval; Ignoring.
2015-04-08 03:08:59,570 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.keytab; Ignoring.
2015-04-08 03:08:59,571 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.resourcemanager.principal; Ignoring.
2015-04-08 03:08:59,571 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: net.topology.script.file.name; Ignoring.
2015-04-08 03:08:59,573 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: dfs.namenode.checkpoint.dir; Ignoring.
2015-04-08 03:08:59,573 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.jobhistory.webapp.https.address; Ignoring.
2015-04-08 03:08:59,576 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.log-dirs; Ignoring.
2015-04-08 03:08:59,577 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: fs.defaultFS; Ignoring.
2015-04-08 03:08:59,579 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.jobhistory.keytab; Ignoring.
2015-04-08 03:08:59,579 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: dfs.namenode.name.dir; Ignoring.
2015-04-08 03:08:59,582 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.http.policy; Ignoring.
2015-04-08 03:08:59,585 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.local-dirs; Ignoring.
2015-04-08 03:08:59,586 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.linux-container-executor.group; Ignoring.
2015-04-08 03:08:59,586 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.jobhistory.principal; Ignoring.
2015-04-08 03:08:59,587 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.container-executor.class; Ignoring.
2015-04-08 03:08:59,588 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts; Ignoring.
2015-04-08 03:08:59,589 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.admin.acl; Ignoring.
2015-04-08 03:08:59,591 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.principal; Ignoring.
2015-04-08 03:09:00,499 INFO [main] org.apache.hadoop.conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
2015-04-08 03:09:01,285 INFO [main] org.apache.hadoop.mapred.Task: Using ResourceCalculatorProcessTree : [ ]
2015-04-08 03:09:02,362 INFO [main] org.apache.hadoop.mapred.MapTask: Processing split: org.apache.hive.hcatalog.mapreduce.HCatSplit@47e7cecf
2015-04-08 03:09:02,500 INFO [main] org.apache.hadoop.mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2015-04-08 03:09:03,706 INFO [main] org.apache.hadoop.mapred.MapTask: (EQUATOR) 0 kvi 268435452(1073741808)
2015-04-08 03:09:03,706 INFO [main] org.apache.hadoop.mapred.MapTask: mapreduce.task.io.sort.mb: 1024
2015-04-08 03:09:03,706 INFO [main] org.apache.hadoop.mapred.MapTask: soft limit at 966367616
2015-04-08 03:09:03,706 INFO [main] org.apache.hadoop.mapred.MapTask: bufstart = 0; bufvoid = 1073741824
2015-04-08 03:09:03,706 INFO [main] org.apache.hadoop.mapred.MapTask: kvstart = 268435452; length = 67108864
2015-04-08 03:09:03,717 INFO [main] com.hadoop.compression.lzo.GPLNativeCodeLoader: Loaded native gpl library
2015-04-08 03:09:03,720 INFO [main] com.hadoop.compression.lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev dbd51f0fb61f5347228a7a23fe0765ac1242fcdf]
2015-04-08 03:09:03,846 INFO [main] org.apache.hadoop.io.compress.zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
2015-04-08 03:09:03,847 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.deflate]
2015-04-08 03:09:04,348 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.gz]
2015-04-08 03:09:04,348 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.gz]
2015-04-08 03:09:04,349 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.gz]
2015-04-08 03:09:04,349 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.gz]
2015-04-08 03:09:04,359 INFO [main] org.apache.hive.hcatalog.mapreduce.InternalUtil: Initializing org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe with properties {name=DEFAULT.kylin_intermediate_PC_SESSION_COPY_20150322000000_20150323000000_841ce77b_81c5_4c0c_941a_6f2e309a131f, numFiles=0, field.delim=, columns.types=string,int,string,string,string,string,string,string,string,string,string,string,string,string,bigint,bigint,bigint,bigint, serialization.format=, columns=default_pc_session_tenantname,default_pc_session_tenantsite,default_pc_session_devicefamily,default_pc_session_deviceclass,default_pc_session_osfamily,default_pc_session_osversion,default_pc_session_browserfamily,default_pc_session_browserversion,default_pc_session_trafficsource,default_pc_session_continent,default_pc_session_country,default_pc_session_region,default_pc_session_city,default_pc_session_streamid,default_pc_session_bounce,default_pc_session_retvisitor,default_pc_session_serveventcnt,default_pc_session_absduration, rawDataSize=50265185482, numRows=507325307, EXTERNAL=TRUE, serialization.lib=org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, COLUMN_STATS_ACCURATE=true, totalSize=0, serialization.null.format=\N, transient_lastDdlTime=1427989487}
2015-04-08 03:09:04,454 INFO [main] org.apache.kylin.job.hadoop.AbstractHadoopJob: The absolute path for meta dir is /hadoop/8/scratch/local/usercache/b_kylin/appcache/application_1427705526386_110981/container_1427705526386_110981_01_000026/meta
2015-04-08 03:09:04,456 INFO [main] org.apache.kylin.common.KylinConfig: Use KYLIN_CONF=/hadoop/8/scratch/local/usercache/b_kylin/appcache/application_1427705526386_110981/container_1427705526386_110981_01_000026/meta
2015-04-08 03:09:04,476 INFO [main] org.apache.kylin.cube.CubeManager: Initializing CubeManager with config /hadoop/3/scratch/local/usercache/b_kylin/filecache/1483/meta
2015-04-08 03:09:04,480 INFO [main] org.apache.kylin.common.persistence.ResourceStore: Using metadata url /hadoop/3/scratch/local/usercache/b_kylin/filecache/1483/meta for resource store
2015-04-08 03:09:04,955 INFO [main] org.apache.kylin.cube.CubeDescManager: Initializing CubeDescManager with config /hadoop/3/scratch/local/usercache/b_kylin/filecache/1483/meta
2015-04-08 03:09:04,956 INFO [main] org.apache.kylin.cube.CubeDescManager: Reloading Cube Metadata from folder /hadoop/3/scratch/local/usercache/b_kylin/filecache/1483/meta/cube_desc
2015-04-08 03:09:05,177 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Create base cuboid 8191
2015-04-08 03:09:08,171 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 100000 records!
2015-04-08 03:09:09,776 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 200000 records!
2015-04-08 03:09:11,903 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 300000 records!
2015-04-08 03:09:13,739 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 400000 records!
2015-04-08 03:09:15,243 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 500000 records!
2015-04-08 03:09:15,389 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Received 500000 rows, going to compress base cuboid table
2015-04-08 03:09:25,455 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: after scanAndAggregateGridTable cuboid 8191 has rows: 87680
2015-04-08 03:09:25,455 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Compress finished, it took 10 seconds.
2015-04-08 03:09:27,213 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 600000 records!
2015-04-08 03:09:28,391 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 700000 records!
2015-04-08 03:09:29,963 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 800000 records!
2015-04-08 03:09:31,408 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 900000 records!
2015-04-08 03:09:33,124 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 1000000 records!
2015-04-08 03:09:33,300 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Received 1000000 rows, going to compress base cuboid table
2015-04-08 03:09:45,069 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: after scanAndAggregateGridTable cuboid 8191 has rows: 183872
2015-04-08 03:09:45,070 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Compress finished, it took 11 seconds.
2015-04-08 03:09:46,535 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 1100000 records!
2015-04-08 03:09:47,942 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 1200000 records!
2015-04-08 03:09:49,166 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 1300000 records!
2015-04-08 03:09:50,398 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 1400000 records!
2015-04-08 03:09:51,170 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Totally handled 1453555 records!
2015-04-08 03:09:51,302 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: The source data has 1453555 rows
2015-04-08 03:09:51,303 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Received 1453555 rows, going to compress base cuboid table
2015-04-08 03:10:54,326 ERROR [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: stream build failed
java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: GC overhead limit exceeded
at java.util.concurrent.FutureTask.report(FutureTask.java:122)
at java.util.concurrent.FutureTask.get(FutureTask.java:188)
at org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper.cleanup(InMemCuboidMapper.java:96)
at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:148)
at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)
at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:415)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1650)
at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded
at org.apache.kylin.common.hll.HyperLogLogPlusCounter.<init>(HyperLogLogPlusCounter.java:64)
at org.apache.kylin.common.hll.HyperLogLogPlusCounter.<init>(HyperLogLogPlusCounter.java:55)
at org.apache.kylin.metadata.measure.HLLCAggregator.aggregate(HLLCAggregator.java:39)
at org.apache.kylin.metadata.measure.HLLCAggregator.aggregate(HLLCAggregator.java:27)
at org.apache.kylin.storage.gridtable.GTAggregateScanner$AggregationCache.aggregate(GTAggregateScanner.java:85)
at org.apache.kylin.storage.gridtable.GTAggregateScanner.iterator(GTAggregateScanner.java:57)
at org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder.scanAndAggregateGridTable(InMemCubeBuilder.java:193)
at org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder.compressBaseCuboid(InMemCubeBuilder.java:376)
at org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder.run(InMemCubeBuilder.java:318)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
at java.util.concurrent.FutureTask.run(FutureTask.java:262)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)
2015-04-08 03:10:54,329 INFO [main] org.apache.hadoop.mapred.MapTask: Starting flush of map output
2015-04-08 03:10:54,334 INFO [main] org.apache.hadoop.conf.Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available
2015-04-08 03:10:54,336 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.lzo_deflate]
2015-04-08 03:10:54,591 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.io.IOException: Failed to build cube in mapper 28
at org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper.cleanup(InMemCuboidMapper.java:99)
at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:148)
at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)
at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:415)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1650)
at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: GC overhead limit exceeded
at java.util.concurrent.FutureTask.report(FutureTask.java:122)
at java.util.concurrent.FutureTask.get(FutureTask.java:188)
at org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper.cleanup(InMemCuboidMapper.java:96)
... 8 more
Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded
at org.apache.kylin.common.hll.HyperLogLogPlusCounter.<init>(HyperLogLogPlusCounter.java:64)
at org.apache.kylin.common.hll.HyperLogLogPlusCounter.<init>(HyperLogLogPlusCounter.java:55)
at org.apache.kylin.metadata.measure.HLLCAggregator.aggregate(HLLCAggregator.java:39)
at org.apache.kylin.metadata.measure.HLLCAggregator.aggregate(HLLCAggregator.java:27)
at org.apache.kylin.storage.gridtable.GTAggregateScanner$AggregationCache.aggregate(GTAggregateScanner.java:85)
at org.apache.kylin.storage.gridtable.GTAggregateScanner.iterator(GTAggregateScanner.java:57)
at org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder.scanAndAggregateGridTable(InMemCubeBuilder.java:193)
at org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder.compressBaseCuboid(InMemCubeBuilder.java:376)
at org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder.run(InMemCubeBuilder.java:318)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
at java.util.concurrent.FutureTask.run(FutureTask.java:262)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)","Duplicated Code, Long Method, , , , "
"Move Class,Rename Method,Push Down Method,Move Method,Move Attribute,","Reorganize all test cases to unit test and integration tests All of the test case in current version are treated like unit test cases(meaning we have no integration tests), this is leading to many problems including:

1. At Jenkins, we have to separate the test process to buildcube, buildii, test,,, this is troublesome, and being the wrong way.
2. HbaseMeatadataTestscase will pollute the classpath of subsequent localmetadatatestcase, as described in https://issues.apache.org/jira/browse/KYLIN-694.
3. The vague boundary between unit test and integration tests makes it difficult for running unit test/integration test only.

Maven has provided a full stack of tool for cases like us. We could put all the unit tests in ""test"" phrase and all the integration tests in ""integration-test"" phrase. We can also put all the BuildCube/BuildII test cases to the ""pre-integration-test"" phrase, since they can be treated as part of environment preparation.",", , , , "
"Rename Method,Move Method,","Refactor storage layer cache currently storage layer cache is implemented as CacheFledgedStorageEngine, which is a logically independent storage engine. this design is unfriendly to test cases, and is hard to maintain. refactor it to make it as a wrapper of cube/ii storage engine",", , "
"Rename Method,Move Method,Inline Method,","IGTStore implementation which use disk when memory runs short The main idea is use disk when memory runs short. Right now both cuboid and intermediate AggregationCache stays in memory. At least the cuboid can go to disk if necessary. So at minimal we only need one AggregationCache in memory at a time. And the biggest AggregationCache is about the size of base cuboid.
Cuboid is stored in GTSimpleMemStore currently. Replace it with a flexible store that can switch between memory and disk. And you need to budget the total free memory to command the memory-to-disk switches smartly.",", , , "
"Rename Method,",support timestamp type in II and cube ,", "
"Rename Method,Extract Method,",IIEndpoint eliminate the non-aggregate routine the non-aggregate routine makes code difficult to maintain,"Duplicated Code, Long Method, , "
"Move Class,Rename Method,Move Method,Move Attribute,",Optimize endpoint's response structure to suit with no-dictionary data current endpoint response are designed when data in endpoint was assumed to be encoded by dict. This is no longer true for streaming cases. data structures like IIRow should be redesigned accordingly,", , , "
"Rename Method,Move Method,Move Attribute,",bundle statistics info in endpoint response bundle statistics info in endpoint response could help to spotlight the performance issues,", , , "
"Rename Class,Extract Method,",replace aliasMap in storage context with a clear specified return column list ,"Duplicated Code, Long Method, , "
"Move Class,Extract Method,",replace aliasMap in storage context with a clear specified return column list ,"Duplicated Code, Long Method, , "
"Extract Method,Inline Method,",Implement fine grained cache for cube and ii ,"Duplicated Code, Long Method, , , "
"Move Method,Extract Method,Inline Method,",Implement fine grained cache for cube and ii ,"Duplicated Code, Long Method, , , , "
"Rename Method,",Implement fine grained cache for cube and ii ,", "
"Move Method,Move Attribute,","replace BitSet for AggrKey inside endpoint, in order to get hashCode and compareTo, we use BitSet to store all the column that need to be calculated which is quite inefficient.",", , , "
"Move And Rename Class,Extract Method,",Migrate cube storage (query side) to use GridTable API ,"Duplicated Code, Long Method, , "
"Rename Method,Inline Method,","Out of memory in mapper when building cube in mem 2015-04-08 03:08:56,992 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.resourcemanager.keytab; Ignoring.
2015-04-08 03:08:56,996 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.jobhistory.http.policy; Ignoring.
2015-04-08 03:08:56,999 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval; Ignoring.
2015-04-08 03:08:57,003 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.keytab; Ignoring.
2015-04-08 03:08:57,004 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.resourcemanager.principal; Ignoring.
2015-04-08 03:08:57,006 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: net.topology.script.file.name; Ignoring.
2015-04-08 03:08:57,012 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.jobhistory.webapp.https.address; Ignoring.
2015-04-08 03:08:57,021 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.log-dirs; Ignoring.
2015-04-08 03:08:57,022 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: fs.defaultFS; Ignoring.
2015-04-08 03:08:57,029 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.jobhistory.keytab; Ignoring.
2015-04-08 03:08:57,034 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.http.policy; Ignoring.
2015-04-08 03:08:57,041 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.local-dirs; Ignoring.
2015-04-08 03:08:57,043 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.linux-container-executor.group; Ignoring.
2015-04-08 03:08:57,043 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.jobhistory.principal; Ignoring.
2015-04-08 03:08:57,047 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.container-executor.class; Ignoring.
2015-04-08 03:08:57,048 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts; Ignoring.
2015-04-08 03:08:57,050 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.admin.acl; Ignoring.
2015-04-08 03:08:57,053 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.principal; Ignoring.
2015-04-08 03:08:58,279 INFO [main] org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-04-08 03:08:58,347 INFO [main] org.apache.hadoop.metrics2.impl.MetricsSinkAdapter: Sink ganglia started
2015-04-08 03:08:58,492 INFO [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-04-08 03:08:58,492 INFO [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: MapTask metrics system started
2015-04-08 03:08:58,570 INFO [main] org.apache.hadoop.mapred.YarnChild: Executing with tokens:
2015-04-08 03:08:58,571 INFO [main] org.apache.hadoop.mapred.YarnChild: Kind: HDFS_DELEGATION_TOKEN, Service: 10.115.206.112:8020, Ident: (HDFS_DELEGATION_TOKEN token 8430348 for b_kylin)
2015-04-08 03:08:58,650 INFO [main] org.apache.hadoop.mapred.YarnChild: Kind: mapreduce.job, Service: job_1427705526386_110981, Ident: (org.apache.hadoop.mapreduce.security.token.JobTokenIdentifier@69607702)
2015-04-08 03:08:58,928 INFO [main] org.apache.hadoop.mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.
2015-04-08 03:08:59,367 INFO [main] org.apache.hadoop.mapred.YarnChild: mapreduce.cluster.local.dir for child: /hadoop/1/scratch/local/usercache/b_kylin/appcache/application_1427705526386_110981,/hadoop/2/scratch/local/usercache/b_kylin/appcache/application_1427705526386_110981,/hadoop/3/scratch/local/usercache/b_kylin/appcache/application_1427705526386_110981,/hadoop/4/scratch/local/usercache/b_kylin/appcache/application_1427705526386_110981,/hadoop/5/scratch/local/usercache/b_kylin/appcache/application_1427705526386_110981,/hadoop/6/scratch/local/usercache/b_kylin/appcache/application_1427705526386_110981,/hadoop/7/scratch/local/usercache/b_kylin/appcache/application_1427705526386_110981,/hadoop/8/scratch/local/usercache/b_kylin/appcache/application_1427705526386_110981
2015-04-08 03:08:59,567 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.resourcemanager.keytab; Ignoring.
2015-04-08 03:08:59,568 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.jobhistory.http.policy; Ignoring.
2015-04-08 03:08:59,569 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.retry.interval; Ignoring.
2015-04-08 03:08:59,570 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.keytab; Ignoring.
2015-04-08 03:08:59,571 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.resourcemanager.principal; Ignoring.
2015-04-08 03:08:59,571 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: net.topology.script.file.name; Ignoring.
2015-04-08 03:08:59,573 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: dfs.namenode.checkpoint.dir; Ignoring.
2015-04-08 03:08:59,573 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.jobhistory.webapp.https.address; Ignoring.
2015-04-08 03:08:59,576 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.log-dirs; Ignoring.
2015-04-08 03:08:59,577 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: fs.defaultFS; Ignoring.
2015-04-08 03:08:59,579 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.jobhistory.keytab; Ignoring.
2015-04-08 03:08:59,579 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: dfs.namenode.name.dir; Ignoring.
2015-04-08 03:08:59,582 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.http.policy; Ignoring.
2015-04-08 03:08:59,585 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.local-dirs; Ignoring.
2015-04-08 03:08:59,586 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.linux-container-executor.group; Ignoring.
2015-04-08 03:08:59,586 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.jobhistory.principal; Ignoring.
2015-04-08 03:08:59,587 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.container-executor.class; Ignoring.
2015-04-08 03:08:59,588 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: mapreduce.job.end-notification.max.attempts; Ignoring.
2015-04-08 03:08:59,589 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.admin.acl; Ignoring.
2015-04-08 03:08:59,591 WARN [main] org.apache.hadoop.conf.Configuration: job.xml:an attempt to override final parameter: yarn.nodemanager.principal; Ignoring.
2015-04-08 03:09:00,499 INFO [main] org.apache.hadoop.conf.Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
2015-04-08 03:09:01,285 INFO [main] org.apache.hadoop.mapred.Task: Using ResourceCalculatorProcessTree : [ ]
2015-04-08 03:09:02,362 INFO [main] org.apache.hadoop.mapred.MapTask: Processing split: org.apache.hive.hcatalog.mapreduce.HCatSplit@47e7cecf
2015-04-08 03:09:02,500 INFO [main] org.apache.hadoop.mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2015-04-08 03:09:03,706 INFO [main] org.apache.hadoop.mapred.MapTask: (EQUATOR) 0 kvi 268435452(1073741808)
2015-04-08 03:09:03,706 INFO [main] org.apache.hadoop.mapred.MapTask: mapreduce.task.io.sort.mb: 1024
2015-04-08 03:09:03,706 INFO [main] org.apache.hadoop.mapred.MapTask: soft limit at 966367616
2015-04-08 03:09:03,706 INFO [main] org.apache.hadoop.mapred.MapTask: bufstart = 0; bufvoid = 1073741824
2015-04-08 03:09:03,706 INFO [main] org.apache.hadoop.mapred.MapTask: kvstart = 268435452; length = 67108864
2015-04-08 03:09:03,717 INFO [main] com.hadoop.compression.lzo.GPLNativeCodeLoader: Loaded native gpl library
2015-04-08 03:09:03,720 INFO [main] com.hadoop.compression.lzo.LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev dbd51f0fb61f5347228a7a23fe0765ac1242fcdf]
2015-04-08 03:09:03,846 INFO [main] org.apache.hadoop.io.compress.zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
2015-04-08 03:09:03,847 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.deflate]
2015-04-08 03:09:04,348 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.gz]
2015-04-08 03:09:04,348 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.gz]
2015-04-08 03:09:04,349 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.gz]
2015-04-08 03:09:04,349 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.gz]
2015-04-08 03:09:04,359 INFO [main] org.apache.hive.hcatalog.mapreduce.InternalUtil: Initializing org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe with properties {name=DEFAULT.kylin_intermediate_PC_SESSION_COPY_20150322000000_20150323000000_841ce77b_81c5_4c0c_941a_6f2e309a131f, numFiles=0, field.delim=, columns.types=string,int,string,string,string,string,string,string,string,string,string,string,string,string,bigint,bigint,bigint,bigint, serialization.format=, columns=default_pc_session_tenantname,default_pc_session_tenantsite,default_pc_session_devicefamily,default_pc_session_deviceclass,default_pc_session_osfamily,default_pc_session_osversion,default_pc_session_browserfamily,default_pc_session_browserversion,default_pc_session_trafficsource,default_pc_session_continent,default_pc_session_country,default_pc_session_region,default_pc_session_city,default_pc_session_streamid,default_pc_session_bounce,default_pc_session_retvisitor,default_pc_session_serveventcnt,default_pc_session_absduration, rawDataSize=50265185482, numRows=507325307, EXTERNAL=TRUE, serialization.lib=org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, COLUMN_STATS_ACCURATE=true, totalSize=0, serialization.null.format=\N, transient_lastDdlTime=1427989487}
2015-04-08 03:09:04,454 INFO [main] org.apache.kylin.job.hadoop.AbstractHadoopJob: The absolute path for meta dir is /hadoop/8/scratch/local/usercache/b_kylin/appcache/application_1427705526386_110981/container_1427705526386_110981_01_000026/meta
2015-04-08 03:09:04,456 INFO [main] org.apache.kylin.common.KylinConfig: Use KYLIN_CONF=/hadoop/8/scratch/local/usercache/b_kylin/appcache/application_1427705526386_110981/container_1427705526386_110981_01_000026/meta
2015-04-08 03:09:04,476 INFO [main] org.apache.kylin.cube.CubeManager: Initializing CubeManager with config /hadoop/3/scratch/local/usercache/b_kylin/filecache/1483/meta
2015-04-08 03:09:04,480 INFO [main] org.apache.kylin.common.persistence.ResourceStore: Using metadata url /hadoop/3/scratch/local/usercache/b_kylin/filecache/1483/meta for resource store
2015-04-08 03:09:04,955 INFO [main] org.apache.kylin.cube.CubeDescManager: Initializing CubeDescManager with config /hadoop/3/scratch/local/usercache/b_kylin/filecache/1483/meta
2015-04-08 03:09:04,956 INFO [main] org.apache.kylin.cube.CubeDescManager: Reloading Cube Metadata from folder /hadoop/3/scratch/local/usercache/b_kylin/filecache/1483/meta/cube_desc
2015-04-08 03:09:05,177 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Create base cuboid 8191
2015-04-08 03:09:08,171 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 100000 records!
2015-04-08 03:09:09,776 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 200000 records!
2015-04-08 03:09:11,903 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 300000 records!
2015-04-08 03:09:13,739 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 400000 records!
2015-04-08 03:09:15,243 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 500000 records!
2015-04-08 03:09:15,389 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Received 500000 rows, going to compress base cuboid table
2015-04-08 03:09:25,455 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: after scanAndAggregateGridTable cuboid 8191 has rows: 87680
2015-04-08 03:09:25,455 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Compress finished, it took 10 seconds.
2015-04-08 03:09:27,213 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 600000 records!
2015-04-08 03:09:28,391 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 700000 records!
2015-04-08 03:09:29,963 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 800000 records!
2015-04-08 03:09:31,408 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 900000 records!
2015-04-08 03:09:33,124 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 1000000 records!
2015-04-08 03:09:33,300 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Received 1000000 rows, going to compress base cuboid table
2015-04-08 03:09:45,069 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: after scanAndAggregateGridTable cuboid 8191 has rows: 183872
2015-04-08 03:09:45,070 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Compress finished, it took 11 seconds.
2015-04-08 03:09:46,535 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 1100000 records!
2015-04-08 03:09:47,942 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 1200000 records!
2015-04-08 03:09:49,166 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 1300000 records!
2015-04-08 03:09:50,398 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Handled 1400000 records!
2015-04-08 03:09:51,170 INFO [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: Totally handled 1453555 records!
2015-04-08 03:09:51,302 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: The source data has 1453555 rows
2015-04-08 03:09:51,303 INFO [pool-5-thread-1] org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder: Received 1453555 rows, going to compress base cuboid table
2015-04-08 03:10:54,326 ERROR [main] org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper: stream build failed
java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: GC overhead limit exceeded
at java.util.concurrent.FutureTask.report(FutureTask.java:122)
at java.util.concurrent.FutureTask.get(FutureTask.java:188)
at org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper.cleanup(InMemCuboidMapper.java:96)
at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:148)
at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)
at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:415)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1650)
at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded
at org.apache.kylin.common.hll.HyperLogLogPlusCounter.<init>(HyperLogLogPlusCounter.java:64)
at org.apache.kylin.common.hll.HyperLogLogPlusCounter.<init>(HyperLogLogPlusCounter.java:55)
at org.apache.kylin.metadata.measure.HLLCAggregator.aggregate(HLLCAggregator.java:39)
at org.apache.kylin.metadata.measure.HLLCAggregator.aggregate(HLLCAggregator.java:27)
at org.apache.kylin.storage.gridtable.GTAggregateScanner$AggregationCache.aggregate(GTAggregateScanner.java:85)
at org.apache.kylin.storage.gridtable.GTAggregateScanner.iterator(GTAggregateScanner.java:57)
at org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder.scanAndAggregateGridTable(InMemCubeBuilder.java:193)
at org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder.compressBaseCuboid(InMemCubeBuilder.java:376)
at org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder.run(InMemCubeBuilder.java:318)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
at java.util.concurrent.FutureTask.run(FutureTask.java:262)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)
2015-04-08 03:10:54,329 INFO [main] org.apache.hadoop.mapred.MapTask: Starting flush of map output
2015-04-08 03:10:54,334 INFO [main] org.apache.hadoop.conf.Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available
2015-04-08 03:10:54,336 INFO [main] org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.lzo_deflate]
2015-04-08 03:10:54,591 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.io.IOException: Failed to build cube in mapper 28
at org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper.cleanup(InMemCuboidMapper.java:99)
at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:148)
at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)
at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:415)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1650)
at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: GC overhead limit exceeded
at java.util.concurrent.FutureTask.report(FutureTask.java:122)
at java.util.concurrent.FutureTask.get(FutureTask.java:188)
at org.apache.kylin.job.hadoop.cubev2.InMemCuboidMapper.cleanup(InMemCuboidMapper.java:96)
... 8 more
Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded
at org.apache.kylin.common.hll.HyperLogLogPlusCounter.<init>(HyperLogLogPlusCounter.java:64)
at org.apache.kylin.common.hll.HyperLogLogPlusCounter.<init>(HyperLogLogPlusCounter.java:55)
at org.apache.kylin.metadata.measure.HLLCAggregator.aggregate(HLLCAggregator.java:39)
at org.apache.kylin.metadata.measure.HLLCAggregator.aggregate(HLLCAggregator.java:27)
at org.apache.kylin.storage.gridtable.GTAggregateScanner$AggregationCache.aggregate(GTAggregateScanner.java:85)
at org.apache.kylin.storage.gridtable.GTAggregateScanner.iterator(GTAggregateScanner.java:57)
at org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder.scanAndAggregateGridTable(InMemCubeBuilder.java:193)
at org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder.compressBaseCuboid(InMemCubeBuilder.java:376)
at org.apache.kylin.job.hadoop.cubev2.InMemCubeBuilder.run(InMemCubeBuilder.java:318)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
at java.util.concurrent.FutureTask.run(FutureTask.java:262)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)",", , "
"Rename Method,Extract Method,",Push time condition down to ii endpoint ,"Duplicated Code, Long Method, , "
"Rename Class,Rename Method,",Push time condition down to ii endpoint ,", "
"Rename Method,Move Method,Move Attribute,",Migrate cube storage (query side) to use GridTable API ,", , , "
"Rename Method,","Create GridTable, a data structure that abstracts vertical and horizontal partition of a table ",", "
"Rename Class,Extract Method,","Create GridTable, a data structure that abstracts vertical and horizontal partition of a table ","Duplicated Code, Long Method, , "
"Move Method,Extract Method,Move Attribute,","Create GridTable, a data structure that abstracts vertical and horizontal partition of a table ","Duplicated Code, Long Method, , , , "
"Rename Class,Rename Method,Extract Method,Inline Method,","Create GridTable, a data structure that abstracts vertical and horizontal partition of a table ","Duplicated Code, Long Method, , , "
"Move Class,Rename Class,Move And Rename Class,Extract Method,","Create GridTable, a data structure that abstracts vertical and horizontal partition of a table ","Duplicated Code, Long Method, , "
"Move Method,Move Attribute,",clean useless code and improve code coverage ,", , , "
"Move And Rename Class,Move Class,Move Method,Move Attribute,",Data model upgrade for legacy cube descs ,", , , "
"Rename Method,Move Method,Move Attribute,",Data model upgrade for legacy cube descs ,", , , "
"Rename Method,Extract Method,","Support increment+merge job For Holistic count distinct, it requires the cube having only one segment. So every incremental segment must be followed by a merge.","Duplicated Code, Long Method, , "
"Extract Method,Inline Method,","Support increment+merge job For Holistic count distinct, it requires the cube having only one segment. So every incremental segment must be followed by a merge.","Duplicated Code, Long Method, , , "
"Move Method,Move Attribute,",Data model upgrade for legacy cube descs ,", , , "
"Pull Up Method,Inline Method,Pull Up Attribute,","Build job flow for Inverted Index building As Hongbin is going to finish the development on seperate job steps for inverted index, it is time to build a job flow to combine these steps and submit it to Job enigne.",", , Duplicated Code, Duplicated Code, "
"Rename Method,","implement group/artifactID looked by plugin prefix We have a default rule which will be used as a fallback, where the
prefix ""idea"" is turned into ""maven-idea-plugin"". This will be retained
if no other information is found.

Before applying the default rule, some more metadata from the repository
will be consulted. This will be stored at the group level, and named
""plugins.xml"".

/org/apache/maven/plugins/plugins.xml
/org.apache.maven.plugins/plugins.xml

While this could potentially include version information as well, it is
worth being able to check these on a plugin-by-plugin basis, and it also
fits with the potential RELEASE specifier on dependencies. This could be
reconsidered in future versions.

Format of the file for now is simple:
< prefixes>
<groupId>org.apache.maven.plugins</groupId>
<plugins>
<plugin>
<prefix>idea</prefix>
<artifactId>maven-idea-plugin</artifactId>
</plugin>
...
</plugins>
< /prefixes>

This particular file will also be updated at release time for a plugin
(though it should rarely be necessary as only new additions need to be
published back).

The list of group IDs to search will be configured from settings.xml,
with the default being just org.apache.maven.plugins. The process will
be to load the plugins.xml file from each of the configured groups, then
build the map of prefixes to plugin group/artifactIDs. If there is a
clash, initially it should fail. We might allow using the first
discovered or some other resolution mechanism, but would hope not to get
that situation as a goal representation might start taking on different
meanings in different contexts.",", "
"Rename Method,Pull Up Method,Extract Method,Pull Up Attribute,","improve transparent plugin downloading currently, it relies on the plugin being
maven:maven-XXX-plugin, version 1.0-SNAPSHOT (unless specified in plugin management in the super POM).

need to allow other groups/artifact IDs, perhaps again through repository metadata
","Duplicated Code, Long Method, , Duplicated Code, Duplicated Code, "
"Move Method,Move Attribute,",allow separate snapshot repositories ,", , , "
"Extract Method,Move Attribute,","Separate build strategies into their own implementations Current the different build strategies are conflated into Lifecycle starter and it's not very easy to create a new implementation of a strategy that builds the project in a different way and it just makes our code hard to read.

Make a first attempt to isolate the strategies and discuss what can be done further. I think it would make sense to remove the weave mode which would greatly simplify the code and allow another set of simplifications.","Duplicated Code, Long Method, , , "
"Rename Method,","<jdk></jdk> clause in the activation section has to provide more complex expressions. For now, <jdk></jdk> provides only one operator '!' which means negation, but it would be great if i can use '+' and ~ operator:

< jdk>1.5+</jdk> <!-- this will be activated when the current JDK version is 1.5 or above (e.g. 1.6) -->
< jdk>1.1 ~ 1.4</jdk> <!-- this will be activated when the current JDK version is between 1.1 and 1.4 -->
< jdk>~ 1.3</jdk> <!-- this will be activated when the current JDK version is 1.3 or below -->
< jdk>1.4 ~</jdk. <!-- the same with 1.5+ -->",", "
"Rename Method,Extract Method,","clean up of exception handling, error reporting and logging we need to get rid of the traces for non-fatal errors, and give something user friendly.

The traces for fatal errors need to be more descriptive.","Duplicated Code, Long Method, , "
"Rename Method,Extract Method,","Improve output readability of our MavenTransferListener implementations The current output of Downloading/Downladed/Uploading/Uploaded transfer notification has some flaws:

1. It does not scale numbers between 1 and 1000 with appropriate units
2. It should use correct size ({{kB}}, {{MB}}, {{GB}} and time units ({{s}}) but doesn't. (see https://en.wikipedia.org/wiki/Binary_prefix and https://en.wikipedia.org/wiki/Metric_prefix)
3. When Aether downloads in parallel (which applies for non-POM files) the progress interleaves due to race conditions to {{System.out}} and you do not know to which resource a progress belongs to.

Let's use an improved version of MPIR {{DependenciesRenderer}}'s [{{FileDecimalFormat}}|https://github.com/apache/maven-plugins/blob/trunk/maven-project-info-reports-plugin/src/main/java/org/apache/maven/report/projectinfo/dependencies/renderer/DependenciesRenderer.java#L1583] for it.

concrete examples:
before
{noformat}191/191 KB 27/48 KB 48/119 KB 80/87 KB 13/13 KB {noformat}
after:
{noformat}Progress (4): 500/800 B | 40/45 kB | 193 kB/315 kB | 1.3/9.0 MB | 12/30 MB{noformat}
if total size is unavailable or the file has already been downloaded but not removed from the list, the output will be:
{noformat}Progress (4): 800 B | 40/45 kB | 193 kB | 9.0 MB | 12 MB{noformat}
or in debug mode:
{noformat}Progress (5): xml-apis-1.3.04.jar (<progress>) | maven-shared-utils-0.6.jar (<progress>) | xercesImpl-2.9.1.jar (<progress>) | commons-digester-1.6.jar (<progress>) | maven-reporting-impl-2.3.jar (<progress>){noformat}

If the scale is between 1 and 10, one decimal place will be printed out. If it is between 10 and 1000+, it will be an integer.","Duplicated Code, Long Method, , "
"Rename Method,","Several small stylistic and spelling improvements to code and documentation The following can easily be squashed:

{noformat}
d99f9ef8c7ffe56966945d6f1b66f0280866ded5 Fix checkstyle error
1c9362be4328713386bd23b01f9e2c87674cb952 Use static final values instead of literals
52945a679ec8f3f571d77658eda2fce06d637aa7 Use proper spelling of ""e.g.""
a26cc1b9636e19c28cd32ed1c844fec64dec55b6 Use the proper term for char U+002D (-) hyphen(-minus) instead of dash
{noformat}",", "
"Move Method,Move Attribute,","ExecutionEvent give on the exception encountered (when having projectFailed , forkedProjectFailed) It could be usefull for having the exception details in ExecutionListener impls to have something like (to not wait the end of the maven execution) :
API change :
{code}
ExecutionEvent.getException().
{code}
",", , , "
"Rename Method,Extract Method,","Implement repository POM confidence levels let's add a source to the distributionManagement in the POM which is rewritten by the repository tool:
""none"" - there is no information about the POM's confidence level (the default)
""converted"" - converted from a Maven 1.x POM, so we can be sure the format is valid but the data within it may be incomplete
""partner"" - synced in directly from a partner site (and was a Maven2 POM, current partners will be converted instead)
""deployed"" - deployed to the repository directly using deploy:deploy
""verified"" - hand verified the information in the POM

I think this is a sliding scale of confidence in the data. I think each should be able to have an interval attached to it to check for metadata updates (but not updates to the JAR itself - this is just about redownloading the POM). By default, I would check none and converted daily and the rest never. Once again, a CLI switch could check them all again. Your releases could requires a certain level of confidence - if you accept anything less than verified, you might risk a reproducibility problem in the future. One change that might be needed is to get maven-proxy to recognise this.

There have been more than one instance of a jar getting corrupted in the repository too. Because once compromised this might be propogated to multiple levels we do need a way to do integrity checks of local and internal repositories against the main one by checking that the sha1's match up and match what is local. This can be something added at a later date, just wanted to keep it in mind.

","Duplicated Code, Long Method, , "
"Move Method,Extract Method,Move Attribute,",Remove console logging from DefaultMaven All the console based logging code needs to move to MavenCli.,"Duplicated Code, Long Method, , , , "
"Rename Method,Move Method,Inline Method,","improve release selection for ranges currently, ranges such as (,1.0) do not work, as it is incapable of locating the newest version < 1.0.
Also, ranges such as [1.0,) result in RELEASE, which may not be present for all artifacts.

We should replace or augment RELEASE with a full listing of versions for an artifact in the repository. This would be repository metadata of the same fashion as the plugin prefix -> id mapping.",", , , "
"Move Method,Move Attribute,","implement release-pom.xml from design docs 2 components to this:
- writing the pom during the release plugin and doing the scm operations
- using it instead when present at build time, instead of pom.xml

For the second, I'm not sure if this should be automatic or controlled by a -f switch. I think it should be automatic, and the -f switch should be implemented to allow pom.xml to be used when release-pom.xml is there. (As well as other pom files).

Design: http://docs.codehaus.org/display/MAVEN/Dependency+Mediation+and+Conflict+Resolution

",", , , "
"Rename Class,Rename Method,Move Method,Inline Method,Move Attribute,","implement group/artifactID looked by plugin prefix We have a default rule which will be used as a fallback, where the
prefix ""idea"" is turned into ""maven-idea-plugin"". This will be retained
if no other information is found.

Before applying the default rule, some more metadata from the repository
will be consulted. This will be stored at the group level, and named
""plugins.xml"".

/org/apache/maven/plugins/plugins.xml
/org.apache.maven.plugins/plugins.xml

While this could potentially include version information as well, it is
worth being able to check these on a plugin-by-plugin basis, and it also
fits with the potential RELEASE specifier on dependencies. This could be
reconsidered in future versions.

Format of the file for now is simple:
< prefixes>
<groupId>org.apache.maven.plugins</groupId>
<plugins>
<plugin>
<prefix>idea</prefix>
<artifactId>maven-idea-plugin</artifactId>
</plugin>
...
</plugins>
< /prefixes>

This particular file will also be updated at release time for a plugin
(though it should rarely be necessary as only new additions need to be
published back).

The list of group IDs to search will be configured from settings.xml,
with the default being just org.apache.maven.plugins. The process will
be to load the plugins.xml file from each of the configured groups, then
build the map of prefixes to plugin group/artifactIDs. If there is a
clash, initially it should fail. We might allow using the first
discovered or some other resolution mechanism, but would hope not to get
that situation as a goal representation might start taking on different
meanings in different contexts.",", , , , "
"Extract Interface,Rename Method,Extract Method,","for IDE embedding have ways of collecting model problems without failing the process Currently the IDE integrations need to perform 2 steps:
1. load the POM with no validation in place. Having a MavenProject instance in the most cases possible is important.
2. to display the warnings in pom editor and elsewhere one has to run at least the projectbuilder/modelbuilder with proper validation level and collect the results from either the result object or the exception thrown.

The proposed patch in https://github.com/mkleint/maven-3/commits/trunk makes it possible to have both a MAvenproject instance under minimal validation constraints and collect the validation problems for any higher validation levels.

Additional benefit of the patch is that it logs ""since what version of Maven is the problem valid"". Which can be further used in both cmd line and IDE error reporting.","Duplicated Code, Long Method, , Large Class, "
"Rename Method,Extract Method,",implement remaining lifecycle features http://docs.codehaus.org/display/MAVEN/Lifecycle,"Duplicated Code, Long Method, , "
"Extract Method,Inline Method,","Allow extension plugins to contribute non-core components to be reused by other plugins In Maven 2.x, build extensions can only contribute components that realize a custom impl of some API defined by the Maven core like artifact handlers and lifecycle mappings. However, to support things like Tycho in a stock Maven distro, we need a build extension to provide components that
a) realize APIs defined merely by the extension and are unknown to Maven itself
b) can be looked up and accessed via API from the extension and other plugins in the same project
c) can be shared among all projects in the reactor using the same extension, including state kept by singletons

See also https://issues.sonatype.org/browse/TYCHO-236.","Duplicated Code, Long Method, , , "
"Rename Class,Rename Method,Move Method,Extract Method,Move Attribute,","clean up of exception handling, error reporting and logging we need to get rid of the traces for non-fatal errors, and give something user friendly.

The traces for fatal errors need to be more descriptive.","Duplicated Code, Long Method, , , , "
"Move Method,Extract Method,Move Attribute,","implement group/artifactID looked by plugin prefix We have a default rule which will be used as a fallback, where the
prefix ""idea"" is turned into ""maven-idea-plugin"". This will be retained
if no other information is found.

Before applying the default rule, some more metadata from the repository
will be consulted. This will be stored at the group level, and named
""plugins.xml"".

/org/apache/maven/plugins/plugins.xml
/org.apache.maven.plugins/plugins.xml

While this could potentially include version information as well, it is
worth being able to check these on a plugin-by-plugin basis, and it also
fits with the potential RELEASE specifier on dependencies. This could be
reconsidered in future versions.

Format of the file for now is simple:
< prefixes>
<groupId>org.apache.maven.plugins</groupId>
<plugins>
<plugin>
<prefix>idea</prefix>
<artifactId>maven-idea-plugin</artifactId>
</plugin>
...
</plugins>
< /prefixes>

This particular file will also be updated at release time for a plugin
(though it should rarely be necessary as only new additions need to be
published back).

The list of group IDs to search will be configured from settings.xml,
with the default being just org.apache.maven.plugins. The process will
be to load the plugins.xml file from each of the configured groups, then
build the map of prefixes to plugin group/artifactIDs. If there is a
clash, initially it should fail. We might allow using the first
discovered or some other resolution mechanism, but would hope not to get
that situation as a goal representation might start taking on different
meanings in different contexts.","Duplicated Code, Long Method, , , , "
"Rename Method,","built in notion of a mirror repository this probably relates to repository work as much as Maven work.

Currently, if users wish to use a mirror of ibiblio, they specify that as an alternative remote repository. In m1, this causes a problem because if you add a new repository not equivalent to that in your project, it is not in the build.properties.

In m2, they are all added to the list, so the mirror will be used first, but the others may also be checked. This is better, but still requires the user specify it in their configuration and it may be used on projects it is never needed for.

It would be good if we could have a repository descriptor at the base of the repository that lists mirrors. On first use of the repository, the user could be prompted to select any mirrors they'd like to use, and it can be saved in their configuration for that repository ID.

That way any project listing ibiblio as a repository could actually use, say, planetmirror instead (and fall back to ibiblio last).",", "
"Rename Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Aether integration ""Aether aims to be the standard library for interacting with Maven repositories... Maven repositories play a critical role within JVM-based development infrastructures and Aether will provide the necessary interoperability, through a common set of tools and APIs, that's critical for happy users in the ecosystem"" (JVZ).

Full introduction can be read here:
http://www.sonatype.com/people/2010/08/introducing-aether/","Duplicated Code, Long Method, , , , , "
"Rename Method,Extract Method,","Add new requiresDependencyCollection mojo feature (with corresponding annotation) to grab dependency tree without files See thread [Add new mojo annotation @requiresDependencyCollection|http://markmail.org/message/ui2eywpqux4zi74f] for details.

this annotation will trigger a new plugin descriptor {{requiresDependencyCollection}} element: http://maven.apache.org/ref/3.1.0/maven-plugin-api/plugin.html#class_mojo","Duplicated Code, Long Method, , "
"Rename Class,Rename Method,Extract Method,","improved user-configurable core extensions mechanism As of version 3.2.5 maven provides two mechanisms to contribute additional components to maven core runtime. It is possible to add component jars to {{$M2_HOME/lib/ext directory}}. It is also possible to specify component jars using {{-Dmaven.ext.class.path}} command line parameter. Neither of the mechanisms is user friendly. In both cases, the user is expected to manually locate and download all required jar file. In both cases, this has to be done on all systems where the extensions are needed. In both cases, all extra jars are loaded into single classloader so all extensions must agree of the same set of dependencies.

This jira is to track changes needed to make it possible to configure core extensions in terms of groupId/artifactId/version and share set of required extensions across multiple systems.

More specifically, 

* introduce new {{$\{maven.projectBasedir\}/.mvn/extensions.xml}} descriptor to specify list of extensions. Initially, the descriptor will only allow specification of extension groupId/artifactId/version, but can be extended to support dependency includes/excludes mechanism and configuration parameters later
{code:xml}
< ?xml version=""1.0"" encoding=""UTF-8""?>
< extensions>
<extension>
<groupId>...</groupId>
<artifactId>...</artifactId>
<version>...</version>
</extension>
<extension>...</extension>
...
< /extensions>
{code}
* change maven to read and load core extensions in separate class realms as part of plexus container setup.
* provide mechanism for extensions to declare exported artifacts and packages using {{META-INF/maven/extension.xml}} descriptor.","Duplicated Code, Long Method, , "
"Rename Method,Extract Method,Inline Method,","implement release-pom.xml from design docs 2 components to this:
- writing the pom during the release plugin and doing the scm operations
- using it instead when present at build time, instead of pom.xml

For the second, I'm not sure if this should be automatic or controlled by a -f switch. I think it should be automatic, and the -f switch should be implemented to allow pom.xml to be used when release-pom.xml is there. (As well as other pom files).

Design: http://docs.codehaus.org/display/MAVEN/Dependency+Mediation+and+Conflict+Resolution

","Duplicated Code, Long Method, , , "
"Rename Method,","New resolution from local repository is very confusing I just discover the change introduced in Maven 3.x to try to improve the resolution mechanism and to avoid to use a local artifact which may not be available in its remote repository : 
https://cwiki.apache.org/confluence/display/MAVEN/Maven+3.x+Compatibility+Notes#Maven3.xCompatibilityNotes-ResolutionfromLocalRepository
Even if the feature is interesting it has several problems :
# When an artifact isn't accessible from its remote repository it isn't used by maven which replies a classical ""dependency not found error"". It is really annoying for a user with some Maven 2 skills which will have a look at his local repo, will find the artifact and won't understand why Maven doesn't use it. At least the error reported by Maven should be clear that even if the dependency is available locally, it isn't used because it's remote repository isn't available.
# This behavior cannot be configured to be only a warning for example. It is really annoying because it doesn't take care of some context and constraints we may have in a development team. Let's imagine that the remote artifact is really removed. Cool Maven broke the build to warn us. But it brakes the build of all the team whereas perhaps only one of them may try to solve the issue (and it can be a long resolution). Thus having the ability to configure if this control is blocker or warning may allow the team to configure it as blocker on the CI server and as warning on the development environment.
# This behavior may introduce some bad practices for example when we are using a staging feature on a repository manager. In our case my teams have a dedicated profile to activate a staging repository when we are validating a release. I recommend to not have this profile always activated but to do it only on-demand to avoid them to DL staging stuffs they don't need. With this new feature they need for all builds they run to activate this staging profile while binaries are stored in it. When you have to do it 20 times per day minimum let's imagine what the developer does : It adds it as an alwaysActive profile and then forget to remove it when the release is ended.

For all these reason I would like we improve this feature to make it more usable and before all bet understandable for ours users.",", "
"Rename Method,","Refactor release plugin to handle reactored build and release-pom.xml-SCM interactions. The release plugin should be refactored to:

1/ handle injection/deletion of release-pom.xml from SCM HEAD/trunk before and after tagging...we're waiting on maven-scm to settle down a little before implementing this. See MNG-607 for more information.

2/ handle reactored builds (rather, builds where the POM has a <modules/> section, which is NOT the same as using the '-r' tag). See MNG-521 for more information on this.",", "
"Move Method,Extract Method,Move Attribute,",reactor summary should have time taken next to SUCCESS message for each subproject ,"Duplicated Code, Long Method, , , , "
"Rename Method,Extract Method,Inline Method,",implement remaining lifecycle features http://docs.codehaus.org/display/MAVEN/Lifecycle,"Duplicated Code, Long Method, , , "
"Move Class,Move Method,Extract Method,Move Attribute,","Allow build lifecycle to execute projects in parallel One of the great advantages with maven over scripted build environments is that it can calculate the dependencies of the build, and it could execute items that are independent of each other in parallel.

Unfortunately it currently doesn't do this, which would be a big win over tools such as 'ant'. It also means that multicore machines have lots of idle capacity when running a serial build that could be utilised.

I had a quick shot at seeing what might be required. Bear in mind this is the first time I have looked at maven internally, and I was just trying to feel my way around and build a POC. I got some of the way there, but my build threads don't seem to have the correct classpath - I think this is something to do with plexus / classworlds - but I don't know enough.

It'd be great to get this feature in a future version, or a way of running my hack (figuring out why in a thread has not the plexus stuff) in the interim.","Duplicated Code, Long Method, , , , "
"Extract Superclass,Rename Method,Extract Method,","improve release selection for ranges currently, ranges such as (,1.0) do not work, as it is incapable of locating the newest version < 1.0.
Also, ranges such as [1.0,) result in RELEASE, which may not be present for all artifacts.

We should replace or augment RELEASE with a full listing of versions for an artifact in the repository. This would be repository metadata of the same fashion as the plugin prefix -> id mapping.","Duplicated Code, Long Method, , Duplicated Code, Large Class, "
"Move And Rename Class,Move Method,Move Attribute,","Refactor release plugin to handle reactored build and release-pom.xml-SCM interactions. The release plugin should be refactored to:

1/ handle injection/deletion of release-pom.xml from SCM HEAD/trunk before and after tagging...we're waiting on maven-scm to settle down a little before implementing this. See MNG-607 for more information.

2/ handle reactored builds (rather, builds where the POM has a <modules/> section, which is NOT the same as using the '-r' tag). See MNG-521 for more information on this.",", , , "
"Rename Method,Inline Method,","compile and package should be reactor-aware When compiling or packaging a project consisting of subprojects
where one depends on another, maven should first check the build environment
for existing jars or classes (in target/ directories) from the dependent subproject,
rather than always looking in the local and remote repositories for jar artifacts.

Attached is a very simple test-case with 2 subprojects, where one depends
on the other.

Only m2 install works; m2 compile and m2 package should also work.
",", , "
"Extract Interface,Pull Up Method,Pull Up Attribute,",Add module maven-builder-support There are some (almost) duplicate classes used by the {{ModelBuilder}} and {{SettingsBuilder}}. With the enhancements of ToolChains there's another need for these classes. So let's extract these classes to a separate module: maven-builder-support,", Duplicated Code, Large Class, Duplicated Code, "
"Rename Method,Push Down Method,Move Method,Extract Method,Move Attribute,","improve release selection for ranges currently, ranges such as (,1.0) do not work, as it is incapable of locating the newest version < 1.0.
Also, ranges such as [1.0,) result in RELEASE, which may not be present for all artifacts.

We should replace or augment RELEASE with a full listing of versions for an artifact in the repository. This would be repository metadata of the same fashion as the plugin prefix -> id mapping.","Duplicated Code, Long Method, , , , , "
"Move Method,Extract Method,","Allow goals such as ""clean:clean"" to be specified as ""clean"" It would be real nice if specifying a goal without a colon in it (e.g., ""clean"" or ""site"") would check to see if there is a plugin by that name with a goal by the same name. I think this would ease the transition for some people and greatly help people like me who can't type worth a darn.","Duplicated Code, Long Method, , , "
"Rename Method,Extract Method,","Build MavenProject instances incrementally Some embedding scenarios, like m2e workspace dependency resolution, can be implemented more efficiently if maven core allowed incremental construction of MavenProject instances. That is, build MavenProject with basic project information properly inherited and interpolated first, then populate project dependencies, then populate project plugins and plugins configuration. Attached is proposed implementation that supports such incremental MavenProject construction.
","Duplicated Code, Long Method, , "
"Rename Method,","Allow configuring the deployment of a timedstamped artifact when using SNAPSHOT in the version This is essentially the same issue as MPARTIFACT-59, but for the Maven 2 code base.

When the current version contains SNAPSHOT, every deploy creates a SNAPSHOT artifact and an artifact with a date/time. This causes a LOT of artifacts being created when using a continuous build. Would like to see this ""feature"" configurable, so I can turn it off at the very least or have it happen only when I want to or on a scheduled basis.

On a related note, I think it would be good for the date/timestamped version to still contain the word SNAPSHOT. This way, it's very easy to determine whether a project currently has dependencies on snapshot (unreleased) versions. I can parse for the date/time format as well, but the common SNAPSHOT keyword would be easier.",", "
"Rename Method,","Provide before/after callbacks for project and mojo execution As a build extension developer, I would like to be able to receive before/after callback events for project and mojo executions. For project-level events, I need MavenSession and ManveProject (obviously) as well as calculated project execution plain. For mojo executions I need MavenSession, ManveProject, MojoExecution and Mojo instances. The idea is to allow extensions observe and participate project build as a whole, not as set of independent mojo executions.",", "
"Move Class,Move Method,Extract Method,",Provide extension point for alternate implementations to construct build graph ,"Duplicated Code, Long Method, , , "
"Rename Method,","improve release selection for ranges currently, ranges such as (,1.0) do not work, as it is incapable of locating the newest version < 1.0.
Also, ranges such as [1.0,) result in RELEASE, which may not be present for all artifacts.

We should replace or augment RELEASE with a full listing of versions for an artifact in the repository. This would be repository metadata of the same fashion as the plugin prefix -> id mapping.",", "
"Rename Method,","Modify maven-toolchain to look in ${maven.home}/conf/toolchains.xml and in ${user.home}/.m2/toolchains.xml Actually, we can only specify the toolchains.xml in {{$\{user.home}/.m2/toolchains.xml.}}

However, like for the settings.xml, it would be very convenient to specify a default toolchains.xml in {{$\{maven.home}/conf/toolchains.xml}}

The idea is : If there is NO {{$\{user.home}/.m2/toolchains.xml}}, 
then uses {{$\{maven.home}/conf/toolchains.xml}}, 
otherwise NONE defined.

Merging both would also be good but not necessary.

The change is very simple. Edit the file
*maven-toolchain\src\main\java\org\apache\maven\toolchain\DefaultToolchainManager.java*

and replace 
{code:java}
private PersistedToolchains readToolchainSettings() throws MisconfiguredToolchainException {
File tch = new File(System.getProperty(""user.home""), "".m2/toolchains.xml"");
if (tch.exists()) {
MavenToolchainsXpp3Reader reader = new MavenToolchainsXpp3Reader();
...
{code}

by 

{code:java}
private PersistedToolchains readToolchainSettings() throws MisconfiguredToolchainException {
File tch = null;
tch = new File(System.getProperty(""user.home""), "".m2/toolchains.xml"");
if (tch == null || !tch.exists()) {
tch = new File(System.getProperty(""maven.home""), ""conf/toolchains.xml"");
}

if (tch.exists()) {
MavenToolchainsXpp3Reader reader = new MavenToolchainsXpp3Reader();
...
{code}

I did that on my local environment by compiling this 2.0.11-SNAPSHOT class and integrating it in my maven-2.0.9-uber.jar and it works perfectly.


",", "
"Move Class,Rename Method,Move Method,Extract Method,Move Attribute,","Make plugin discovery reactor aware If plugin discovery is reactor aware, plugins that are part of a reactor build and are used to aid the build
can be used in one go. Else first the plugin should be installed, and then the rest of the project can
be built.

But mainly this aids in integration testing for plugins: the maven-it-plugin in sandbox can't add the current plugin
+ artifact to the lifecycle without making some methods public. If the plugin were to be discovered using the reactor,
those methods can remain private.","Duplicated Code, Long Method, , , , "
"Rename Method,Move Method,Inline Method,Move Attribute,","improve release selection for ranges currently, ranges such as (,1.0) do not work, as it is incapable of locating the newest version < 1.0.
Also, ranges such as [1.0,) result in RELEASE, which may not be present for all artifacts.

We should replace or augment RELEASE with a full listing of versions for an artifact in the repository. This would be repository metadata of the same fashion as the plugin prefix -> id mapping.",", , , , "
"Rename Method,","project-specific default jvm options and command line parameters Some of the projects builds I work on require special jvm options, like minimal -Xmx value, and specific command line parameters, like --builder. Currently, I have to manually configure these every time run the build, which is rather annoying and error prone. This manual configuration also makes it harder for new or external developers to build the projects and many simply give up trying after ""mvn package"" does not work from the first try.

This enhancement request proposes to introduce two new optional configuration files .mvn/jvm.config and .mvn/maven.config, located at the base directory of project source tree. If present, these files will provide default jvm and maven options. Because these files are part of the project source tree, they will be present in all project checkouts and will be automatically used every time the project is build.",", "
"Rename Method,Extract Method,","API to calculate execution plan without full mojo execution configuration m2e uses project execution plan to determine how to configure the project in eclipse workspace and what to do during workbench build. Not all mojo execution bound to execution plan are relevant for this (think enforcer or deploy plugin), so m2e needs an API to analyse execution plan and perform full configuration of interesting mojo execution only. 


Original m2e jira https://issues.sonatype.org/browse/MNGECLIPSE-2724

Proposed patch attached.","Duplicated Code, Long Method, , "
"Rename Method,","Improve output readability of our MavenTransferListener implementations The current output of Downloading/Downladed/Uploading/Uploaded transfer notification has some flaws:

1. It does not scale numbers between 1 and 1000 with appropriate units
2. It should use correct size ({{kB}}, {{MB}}, {{GB}} and time units ({{s}}) but doesn't. (see https://en.wikipedia.org/wiki/Binary_prefix and https://en.wikipedia.org/wiki/Metric_prefix)
3. When Aether downloads in parallel (which applies for non-POM files) the progress interleaves due to race conditions to {{System.out}} and you do not know to which resource a progress belongs to.

Let's use an improved version of MPIR {{DependenciesRenderer}}'s [{{FileDecimalFormat}}|https://github.com/apache/maven-plugins/blob/trunk/maven-project-info-reports-plugin/src/main/java/org/apache/maven/report/projectinfo/dependencies/renderer/DependenciesRenderer.java#L1583] for it.

concrete examples:
before
{noformat}191/191 KB 27/48 KB 48/119 KB 80/87 KB 13/13 KB {noformat}
after:
{noformat}Progress (4): 500/800 B | 40/45 kB | 193 kB/315 kB | 1.3/9.0 MB | 12/30 MB{noformat}
if total size is unavailable or the file has already been downloaded but not removed from the list, the output will be:
{noformat}Progress (4): 800 B | 40/45 kB | 193 kB | 9.0 MB | 12 MB{noformat}
or in debug mode:
{noformat}Progress (5): xml-apis-1.3.04.jar (<progress>) | maven-shared-utils-0.6.jar (<progress>) | xercesImpl-2.9.1.jar (<progress>) | commons-digester-1.6.jar (<progress>) | maven-reporting-impl-2.3.jar (<progress>){noformat}

If the scale is between 1 and 10, one decimal place will be printed out. If it is between 10 and 1000+, it will be an integer.",", "
"Rename Method,","Update slf4j and simplify its color integration Update dependences for maven build 

Slf4j 1.7.22 -> 1.7.25 ([SLF4J-394|https://jira.qos.ch/browse/SLF4J-394] and [SLF4J-395|https://jira.qos.ch/browse/SLF4J-395] for [SLF4J-389|https://jira.qos.ch/browse/SLF4J-389]) 

With slf4j update we can simplify maven-slf4j-provider implementation given [SLF4J-394|https://jira.qos.ch/browse/SLF4J-394] 
",", "
"Move Method,Extract Method,","m2 eclipse plugin improvements (source download and attachment, customization of natures/builders/conclasspath, flexible project dupport) and refactoring This patch adds the following to the M2 eclipse plugin:
- downloading of source attachments and configuration in .classpath
- customization of project builders and natures in .project (like in the m1 plugin)
- additional conclasspath entries in .classpath (like in the m1 plugin)
- fix: don't add duplicate directories if main/resources directories overlap (like in the m1 plugin)
- support for flexible projects (.wtpmodules file generation for utility modules, wars, ejbs)

Along with these new features the plugin has been refactored, splitting the single big EclipseWriter class to several specific classes and all the messages have been externalized in a property file.
There are still some todos in the code, which probably some M2 guru could look at, but anyway all the existing functionalities continue to work and some other tests have been added.

Due to the refactoring the patch looks more like a complete rewrite: sorry for that, but adding new features without splitting the existing file was ugly


","Duplicated Code, Long Method, , , "
"Move Method,Move Attribute,","Don't deserialize the ViewState-ID if the state saving method is server Currently the ViewState-ID provided by the user is deserialized via Java deserialization even when the {{javax.faces.STATE_SAVING_METHOD}} is set to {{server}} (the default).

The deserialization in this case is unecessary and most likely even slower than just sending the ViewState Id directly.
If a developer now disables the ViewState encryption by setting {{org.apache.myfaces.USE_ENCRYPTION}} to {{false}} (against the [MyFaces security advice|https://wiki.apache.org/myfaces/Secure_Your_Application]) he might have unintentionally introduced a dangerous remote code execution (RCE) vulnerability as described [here|https://www.alphabot.com/security/blog/2017/java/Misconfigured-JSF-ViewStates-can-lead-to-severe-RCE-vulnerabilities.html].

This has been discussed before on [Issue MYFACES-4021|https://issues.apache.org/jira/browse/MYFACES-4021].


",", , , "
"Rename Class,Pull Up Method,","Make JSP 2.1 optional Myfaces 1.2.0 requires JSP 2.1 to be present. Otherwise the StartupServletContextListener fails with this error:

Exception sending context initialized event to listener instance of class org.apache.myfaces.webapp.StartupServletContextListener
java.lang.NoSuchMethodError: javax.servlet.jsp.JspFactory.getJspApplicationContext(Ljavax/servlet/ServletContext;)Ljavax/servlet/jsp/JspApplicationContext;
at org.apache.myfaces.webapp.DefaultFacesInitializer.initFaces(DefaultFacesInitializer.java:102)
at org.apache.myfaces.webapp.StartupServletContextListener.contextInitialized(StartupServletContextListener.java:57) 

JSP version 2.1 or better JSP in general should be an optional dependency, like discussed here and implemented in Sun RI too:
http://www.nabble.com/Does-MyFaces-1.2-require-JSP-2.1--tf4112432.html#a11693501

Use cases: 
- Run Myfaces 1.2.0 in JEE 1.4 environment (Tomcat 5.x). 
- Easier setup and smaller distributions when running Myfaces in an embedded servlet container (jetty) with facelets. For example I'm using jetty for junit tests. I haven't found a working setup which solves the jsp problems.




",", Duplicated Code, "
"Pull Up Method,Extract Method,","javax.faces.validator - DoubleRangeValidator, LengthValidator, LongRangeValidator are very similar, refactor common behaviour The 3 classes are very similar to each other except the type of minimum and maximum (and value of course). Therefore I'll suggest extracting the common behaviour in common parent class. 
","Duplicated Code, Long Method, , Duplicated Code, "
"Rename Method,","SearchExpression API There is a proposal from PrimeFaces guys to include a search expression api to locate components in cases like f:ajax execute/render attributes, h:message for attribute and others.

The idea comes from here:

http://blog.primefaces.org/?p=2740

The idea is support a syntax like this in the attributes:

< clientId>
:<id>
< id>:<id>:<id>
@<keyword>:<id>
id:@<keyword>
@<keyword>(<param1>)
@<keyword>(<param1>,<param2>)

There is a patch for this I have been working over the last weeks, but still require more tests and update the components in MyFaces Core 2.3 to use the new API.",", "
"Rename Method,",Improve the plugin mechanism forgot to add the version,", "
"Rename Method,","[perf] Additional performance improvements Some performance improvements in : 

1) ApplicationImpl.java 

2) ServletExternalContextImpl.java 

3) HtmlResponseWriterImpl.java 

4) HTMLEncoder.java, we also discussed on the mailing list changing encodeURIAtributte to encodeUriAttribute to fix the typo in the method name so I'll do that here as well. 

5) ResourceValidationUtils.java 

The following changes were made: 
- Skip calling ConcurrentHashMap.containsKey since we will call get 
afterward if containsKey == true. 
- Stop using Boolean for variables that don't have a null meaning. If 
null == false, then just use boolean with a default of false. 
- Don't call String.length() constantly for String variables that aren't re-assigned. 
- Change conditional order to avoid calling validateResourceName unless the other conditions are true",", "
"Rename Method,",Support for valueChangeListener method without ValueChangeEvent parameter A ValueChangeListener method can now also take no arguments (see javadoc for MethodExpressionValueChangeListener for details),", "
"Extract Superclass,Move Method,Extract Method,Move Attribute,","enable 'standard' checkstyle checks in myfaces-core We currently only have the 'minimal' checks enabled in core, which actually only checks the correct license headers.

We should go for the 'standard' checkstyle rules, even if this would take some time to fix (found 1111 errors only in the first module).
","Duplicated Code, Long Method, , , , Duplicated Code, Large Class, "
"Move Class,Rename Method,","[perf] Implement o.a.m.SUPPORT_MANAGED_BEANS When running stress test this is one of the most BLOCKED thread (blocked on some ArrayList monitor in tomcat internals):
org.apache.catalina.core.ContainerBase.fireContainerEvent(String, Object)
org.apache.catalina.session.StandardSession.fireContainerEvent(Context, String, Object)
org.apache.catalina.session.StandardSession.setAttribute(String, Object, boolean)
org.apache.catalina.session.StandardSession.setAttribute(String, Object)
org.apache.catalina.session.StandardSessionFacade.setAttribute(String, Object) 
org.apache.myfaces.context.servlet.SessionMap.setAttribute(String, Object)
org.apache.myfaces.util.AbstractThreadSafeAttributeMap.put(String, Object)
org.apache.myfaces.util.AbstractThreadSafeAttributeMap.put(Object, Object)

This happens when someone puts a attribute into httpSession:
org.apache.myfaces.util.AbstractThreadSafeAttributeMap.put(Object, Object)
org.apache.myfaces.renderkit.ServerSideStateCacheImpl.nextViewSequence(FacesContext)
org.apache.myfaces.view.facelets.FaceletViewDeclarationLanguage.getResponseEncoding(FacesContext, String) 
org.apache.myfaces.renderkit.ServerSideStateCacheImpl.saveSerializedViewInServletSession(FacesContext, 

then Servlet container delivers event HttpSessionBindingEvent.

in myfaces HttpSessionAttributeListener in implemented by oam.StartupServletContextListener and handles some stuff for managed beans. 
Review if this is needed - ideally remove it.
",", "
"Move Method,Move Attribute,","[perf] optimize UILeaf UILeaf is a facelets internal class that acts as a wrapper for html markup. Since this is a stateless transient class, used intensively by JSF it is better to reduce the size and overhead caused by this class.

1. Make this class extends from UIComponent instead UIComponentBase to reduce the overall size of the object in memory.
2. Do not use an extra object for implement its attribute map.
3. Use a variable for ComponentSupport.MARK_CREATED instead store it into a HashMap. 

With these optimizations we can reduce object size to less than a half and replace a lot of calls to HashMap.get() by simple variable assignments.",", , , "
"Rename Method,",Google App Engine Support for Myfaces 2 Google App Engine Support for Myfaces 2,", "
"Rename Method,",TODO 65: Partial View Lifecycle Add a partial view lifecycle as described in 13.4 Partial View Traversal of the EDR2,", "
"Move Class,Move Method,Move Attribute,","Add annotation processing logic JSF 2.0 specifies the use of the following annotations for Managed Bean configuration:
- ManagedBean
- ManagedBeans
- ManagedProperty
- RequestScoped
- SessionScoped
- ViewScoped
- ApplicationScoped
- NoneScoped

The annotations are already there, it now needs processing logic.
I'm on it.",", , , "
"Rename Method,","[perf] UIForm#invokeOnComponent with prependId=true In case when invokeOnComponent is used and the form has prependId=true, we can early skip the whole component tree if the baseClientId != form clientId. 
Will also check other components, UIData already contains this enhancement",", "
"Rename Method,Move Method,","Lifecycle phase executions repetitions Every phase in LifecycleImpl looks like:

private boolean applyRequestValues(FacesContext facesContext, PhaseListenerManager phaseListenerMgr)
throws FacesException
{
boolean skipFurtherProcessing = false;
if (log.isTraceEnabled()) log.trace(""entering applyRequestValues in "" + LifecycleImpl.class.getName());

try {
phaseListenerMgr.informPhaseListenersBefore(PhaseId.APPLY_REQUEST_VALUES);

if(isResponseComplete(facesContext, ""applyRequestValues"", true))
{
// have to return right away
return true;
}
if(shouldRenderResponse(facesContext, ""applyRequestValues"", true))
{
skipFurtherProcessing = true;
}

facesContext.getViewRoot().processDecodes(facesContext);
} finally {
phaseListenerMgr.informPhaseListenersAfter(PhaseId.APPLY_REQUEST_VALUES);
}


if (isResponseComplete(facesContext, ""applyRequestValues"", false)
|| shouldRenderResponse(facesContext, ""applyRequestValues"", false))
{
// since this phase is completed we don't need to return right away even if the response is completed
skipFurtherProcessing = true;
}

if (!skipFurtherProcessing && log.isTraceEnabled())
log.trace(""exiting applyRequestValues in ""
+ LifecycleImpl.class.getName());
return skipFurtherProcessing;
}


And that is repeated as many times as phases are. The fix will be to extract the common behavior in a method, that receives one additional parameter - PhaseExecutor and delegate to it the real execution.",", , "
"Rename Method,Pull Up Method,Pull Up Attribute,","Implementation of new JSR-252 class: UIComponentClassicTagBase A new class for JSR-252 must be implemented, the UIComponentClassicTagBase, which is the superclass of all JSP tags

http://java.sun.com/javaee/javaserverfaces/1.2/docs/api/index.html",", Duplicated Code, Duplicated Code, "
"Rename Method,",[perf] use shared StringBuilder instance method javax.faces.component.UIComponentBase._getSharedStringBuilder(FacesContext) already provide this. Add same method to public API and use request-shared StringBuilder instance in renderers too.,", "
"Rename Method,","Make those add*** methods public in WebXml In the Geronimo integration work, we have an internal structure for the parsed web.xml file, and we hope to use that instance to fill in the org.apache.myfaces.shared.webapp.webxml.WebXml, so that myfaces does not need to parse the web.xml file again, But those add*** method are package scope. 
Is it possible to make those methods public, I did not see it will break anyting.
Thanks",", "
"Pull Up Method,Pull Up Attribute,",Implement ViewHandler.getViews(...) ,", Duplicated Code, Duplicated Code, "
"Rename Method,Pull Up Method,Extract Method,","javax.faces.convert - refactor common behaviour + DateTimeConverter changes All available converters look very similar. Extract the common behavior in base class.

Also DateTimeConverter can be migrated to work with type safe enums for style and type properties.
There are comments in source like //TODO: validate timeStyle. According to java doc of DateTimeConverter on sun there should not have validation. The validation of these will be performed when asString/asObject methods are called.","Duplicated Code, Long Method, , Duplicated Code, "
"Rename Method,Inline Method,","Make a way to get the FacesConfig from a provider Currently, MyFaces startup listener will parse the all the faces configuration files and sort them on each startup time, and it will be better to do it once in the deployment time, and get those data structure instances from a provider. One possible way is to make those FacesConfig class serializable.
",", , "
"Rename Method,","Implement CDI changes for JSF 2.3 The idea is implement the following annotations:

ApplicationMap
FlowMap
HeaderMap
HeaderValuesMap
InitParameterMap
RequestCookieMap
RequestMap
RequestParameterMap
RequestParameterValuesMap
SessionMap
ViewMap

The tricky part here is some objects are managed by JSF and others by CDI.",", "
"Move Method,Move Attribute,","Unify the ReflectUtil and ClassUtils classes Currently we have historically grown 2 different classes which are responsible for Class loading one being the ReflectUtil class which originates out of the Facelets Codebase the other one being our own homegrown ClassUtils.
Both have their advantages and disadvantages, we probably should unify both classes under shared ClassUtils in the long run (post 2.0)
",", , , "
"Push Down Method,Push Down Attribute,","cdi support for converters and validators with
<context-param>
<param-name>org.apache.myfaces.CDI_MANAGED_CONVERTERS_ENABLED</param-name>
<param-value>true</param-value>
</context-param>
and
<context-param>
<param-name>org.apache.myfaces.CDI_MANAGED_VALIDATORS_ENABLED</param-name>
<param-value>true</param-value>
</context-param>
it should be possible to enable cdi support for converters/validators.
we need the config, because it was postponed for the spec.",", , , "
"Rename Method,Extract Method,","Myfaces 1.2 doesn't start without web.xml Myfaces 1.2 fails with one of these messages if no web.xml can be found or no mapping for the FacesServlet is defined:

""Couldn't find web.xml. Abort initializing MyFaces.""
""No mappings of FacesServlet found. Abort initializing MyFaces.""

That's a quite strict interpretation of the spec which says 
'Implementations may check for the presence of a servlet-class definition of class
javax.faces.webapp.FacesServlet in the web application deployment descriptor
as a means to abort the configuration process and reduce startup time for applications that do
not use JavaServer Faces Technology.'

It would be helpful for unit tests if the web.xml and a servlet-mapping is not mandatory (maybe configurable?).
In my unit tests I start an embedded jetty server whose configuration is build programmatically:

Server jettyServer = new Server();
Context webappContext = new Context(jettyServer, contextPath, Context.SESSIONS);
webappContext.addEventListener(new StartupServletContextListener());
ServletHolder facesServletHolder = new ServletHolder(new FacesServlet());
webappContext.addServlet(facesServletHolder, ""*.faces"");
webappContext.getServer().start();

That code is working fine with Myfaces 1.1 but broken with 1.2.








","Duplicated Code, Long Method, , "
"Move Class,Move Method,Move Attribute,","review/refactor/document ViewState handling We currently have a few things in our ViewState handling which could get even further improved.
There are 3 main goals we achieve (in order of importance):

1.) security - it should not be easily possible to create state key clashes
2.) performance - we still use java 1.3 tricks and e.g. barely use java.util.concurrent
3.) memory - we shall keep the mem footprint as low as possible

",", , , "
"Rename Method,Extract Method,","REGRESSION - Detect when to wpdate head or body target when content has been updated dynamically Related to topic sent on jsr344-experts list:

[jsr344-experts] Facelet page with dynamic content and update ajax content does not work as user expects

Now take a look at this example:

include.xhtml

< h:commandLink ...>
<f:ajax render=""content""/>
< /h:commandLink>
...
< f:subview id=""content"">
< ui:include src=""#{testManagedBean.page}""/>
< /f:subview>

page1.xhtml

< ui:composition
xmlns:h=""http://java.sun.com/jsf/html""
xmlns:f=""http://java.sun.com/jsf/core""
xmlns:ui=""http://java.sun.com/jsf/facelets"">
< h:outputText id=""component1"" value=""Page 1""/>
< !-- ... more components ... -->
< /ui:composition>

page2.xhtml

< ui:composition
xmlns:h=""http://java.sun.com/jsf/html""
xmlns:f=""http://java.sun.com/jsf/core""
xmlns:ui=""http://java.sun.com/jsf/facelets"">
< h:outputStylesheet ... />
< h:outputText id=""component2"" value=""Page 2""/>
< !-- ... more components ... -->
< /ui:composition>

Here the problem is if the dynamic content changes and add a resource under
""head"" target (h:outputStylesheet does that), shouldn't be added a section
on the ajax payload to update the <head> section? In theory yes, because
this breaks encapsulation principle. If the user says render all inside
content if the <head> section changes it is responsability of the framework
(in this case PartialViewContext) to detect that an send the correct
payload, right?. Here we have two options:

a. Keep track of the resources rendered and save that on the state, then use
that information to check if the head should be rendered. 
b. Use PostAddToViewEvent to check when a change on the component tree has 
triggered a change on the head.

Option b. save some bytes on the state but it could cause render <head> 
section more than necessary (for example a dynamic change but the head
has already rendered the resource, so it is not necessary). Option a.
impose that you need a way to check if the <head> was changed, and
require changes on the spec. 

I'll solve this problem adding a web config param:

org.apache.myfaces.STRICT_JSF_2_REFRESH_TARGET_AJAX

on MyFaces and doing some
changes on the algorithm, adding a flag to indicate if a view is being built
by first time. 
","Duplicated Code, Long Method, , "
"Rename Method,","Create new module for JUnit Mock Testing using MyFaces Core, MyFaces Test and CDI This issue is the next step in the work started in MYFACES-3376 ""Create abstract test classes that runs MyFaces Core as in a container"".",", "
"Rename Method,","Allow ELResolvers filtering related to topic
http://www.mail-archive.com/dev@myfaces.apache.org/msg49177.html


Problem: 
how to disable ELResolver smartly? Adding a context-param for each is an
overkill.

But we have https://cwiki.apache.org/MYFACES/elresolver-ordering.html in
codebase already. I propose to add new feature ""ELResolver filtering""
and new context-param:

< context-param>
<param-name>org.apache.myfaces.EL_RESOLVER_PREDICATE</param-name>
<param-value>org.foo.bazz.ELResolverPredicate</param-value>
< /context-param>

Filter is simple instance of org.apache.commons.collections.Predicate.


For application where no ManagedBean(Resolver) is used or no Flash, user
can simply return false from Predicate.evaluate and ELResolver won't be
installed.


See mail thread here: http://www.mail-archive.com/dev@myfaces.apache.org/msg52082.html
",", "
"Move Method,Extract Method,","f:facet can have more than one child MIchael Kurz tested Mojarra and found out that <f:facet> now can have more than one child. If so, the children will automaticall be put in a UIPanel to serve facet requirements.

However, this improvement is not mentioned in the spec, filed spec issue: https://javaserverfaces-spec-public.dev.java.net/issues/show_bug.cgi?id=677","Duplicated Code, Long Method, , , "
"Extract Method,Move Attribute,","JSF View Pooling (going beyond JSF Stateless Mode) In the last months, I have been doing some investigations around ""stateless JSF"" ideas. The intention is try to find ways to improve MyFaces Core performance as much as possible, without lose all those nice features we all are used to.

In summary, the justification around stateless JSF is that, if it is possible to cut the build view time from a request, there will be an improvement from both speed and memory perspective. This is true, but only to some point, because the response time for a request is given by the build view, validation/invoke application and render response time. 

To get to the same goal, without sacrifice JSF stateful behavior, other improvements has been already done (cache EL expressions, cache ids, make tree structure lighter, ...). The idea is cache that ""stateless information"" into a place where it can be reused effectively, which in this case is inside Facelet abstract syntax tree (AST). This has worked well so far. The side effects of enable these optimizations has been analysed, and there is a good understanding about this.

In few words, the basic idea about stateless JSF as proposed originally by Rudi Simic in his blog is this:

Mark the view as stateless using some attribute.
Use a pool of views, because views are not thread safe.
Before store the view in the pool, use a visitTree call to reset the fields.

Unfortunately, it was quickly found that the implementation proposed requires a better view pool and try to reset the fields is not fail-safe, because the component tree also stores more than just the input field values. Additionally, it doesn't provide a way to use it for dynamic views.

Provide a thread safe implementation of UIComponent that can be reused across threads is not a good solution, because anyway there is some information that is inside UIComponent and should be stored per thread, and precisely UIComponent is a place specifically designed to store that information.

Based on the previous background, the big question is if a solution based on object pooling pattern can be done effectively for a web framework like JSF. A good description of the technique and its trade-off can be found at:

http://en.wikipedia.org/wiki/Object_pool_pattern 

In few words, the proposal is go ""Beyond JSF Stateless Mode"", and instead blame the state, make it your friend. Let's just take advantage of the stateful nature of JSF to allow reuse views fully or partially. 

How? 

- PSS algorithm can be used to check if a view has been modified or not, checking its state. So, it can be used to check which components has state, and if it is possible to provide a way to reset the state of a component to the initial state set by the first markInitialState(), restore the state is possible.

-If the view cannot be reset fully, it is possible to use facelets refreshing algorithm and reuse a view partially.

- Add some additional code to recover a view instance when it is discarded, and store it into the view pool. This requires some changes over NavigationHandlerImpl, because it is not possible to reuse a view and store it in the pool that is still on usage, so it is necessary to do a ""deferred navigation"", changing the default ActionListenerImpl and ensure handleNavigation() is called before end invoke application phase but outside the visitTree() call.

- In MyFaces there exists the concept of FaceletState. It is possible to use this concept and cache even dynamic views, because each different FaceletState can identify an specific view structure. 
","Duplicated Code, Long Method, , , "
"Rename Method,","Ajax behavior and renderer need improvements, and new implementations There was no delta state saving in our existing ajax behavior and the renderer has not been implemented
",", "
"Move Method,Extract Method,","findComponent slow for a high amount of calls Profiling showed that findComponent takes a considerable amount of time to complete if called quite often (and many components do call findComponent). The proposed solution will store children not only in a list, but also in a map keyed by their id.

regards,

Martin","Duplicated Code, Long Method, , , "
"Rename Method,",Implement f:websocket and related api Implement f:websocket proposal as described in the latest javadoc.,", "
"Rename Class,Rename Method,","cdi support for converters and validators with
<context-param>
<param-name>org.apache.myfaces.CDI_MANAGED_CONVERTERS_ENABLED</param-name>
<param-value>true</param-value>
</context-param>
and
<context-param>
<param-name>org.apache.myfaces.CDI_MANAGED_VALIDATORS_ENABLED</param-name>
<param-value>true</param-value>
</context-param>
it should be possible to enable cdi support for converters/validators.
we need the config, because it was postponed for the spec.",", "
"Rename Method,","Add alwaysRecompile mode for EL Expression Cache Mode In MYFACES-3160, EL Expression Cache Mode was introduced but soon it was seen a
problem found on MYFACES-3169 (ui:param and c:set implementations does not 
work as expected).

There are two problems that limit the scope where EL Expression Cache can 
be used:

1. Facelets user tags cannot cache EL Expressions.
2. Inclusions using ui:param must always contains the same number of 
parameters.

To understand the reasons it is worth to remember this example:

a.xhtml
< ui:composition template=""c.xhtml"">
<ui:param name=""var1"" value=""value1""/>
< /ui:composition>

b.xhtml
< ui:composition template=""c.xhtml"">
<ui:param name=""var1"" value=""value1""/>
<ui:param name=""var2"" value=""value2""/>
< /ui:composition>

c.xhtml
< ui:composition>
<h:outputText value=""#{var1}/>
<h:outputText value=""#{var2}/>
< /ui:composition>

When facelet c.xhtml is constructed from a.xhtml, ""var2"" is not recognized as
a parameter so all EL expressions inside c.xhtml holding refereces to ""var2""
will be cached. Later, facelet c.xhtml is reused from b.xhtml but since 
some EL expressions are cached the passed value in ""var2"" is not taken into 
account and the error arise.

In this point it is good to remember that ui:include or ui:decorate or user 
tags are build view time tags, so they are executed only when the view is
built. Parameters or attributes passed by ui:param or as user tag attributes
follows the same principle, they are calculated on build view time through
VariableMapper and the evaluation is stored inside the EL Expression. This
means all EL Expressions holding references to these variables cannot be
cached and needs to be generated each time the view is built.

There is no way to know beforehand which references are affected, because
in a template or an user tag there is no declaration of the parameters or
attributes. But from user point of view that's good, because in this context
a declaration of the parameters is just not necessary.

The problem is ui:param and user tags are very useful features, widely used.
A solution to this problem will improve performance in those cases.

I have been thinking for a long time how to solve this, trying different 
strategies. Use some kind of concurrency algorithm inside TagAttributeImpl
does not work because it is too expensive, or use a central storage for 
cache the expressions by the cost involved in the comparisons.

The objective of cache EL expressions inside facelets abstract syntax tree 
(AST) is minimize the calculations required to get a valid expression. EL
implementations has already an internal map that cache that information,
but that code usually has synchronized blocks or similar things. In that
sense, the idea is rely on that storage in those EL expressions where 
there is no choice and they need to be recreated.

After doing many experiments in this part, I came up with a solution, which
involves the following points:

1. Associate to a facelet, the parameters that were considered as passed 
through ui:param or as a user tag attribute. If in some point of time
we know for example c.xhtml uses var1, just consider it as c.xhtml(var1).

2. Use DefaultVariableMapper to track the parameters that are passed through
ui:param or as a user tag attribute. When the EL expression is created, if
it uses at least one parameter, mark the expression as not cacheable.

3. Override FaceletCache implementation and force a recompilation of a 
facelet if a new parameter is detected that was not considered the first 
time the template was created.

4. A facelet stored in the cache can be used if and only if all the 
parameters used for the template where considered when it was compiled at
first time.

In the example proposed, when facelet c.xhtml is constructed from a.xhtml,
we say that c.xhtml was built with var1 as a known parameter, or 
c.xhtml(var1). when we try to reuse facelet c.xhtml from b.xhtml, we discover
that var2 is also a parameter, but since the cached facelet is c.xhtml(var1),
the algorithm discard the facelet and create a new one, but taking into
account var2 too, so the new facelet becomes c.xhtml(var1,var2). If there
is a call to c.xhtml with no params, it is considered that c.xhtml(var1,var2)
can be used in that case.

The final effect is just some extra compilations of the same facelet at
startup but in the medium/long term, the information we need is calculated 
and associated with the facelet url. Nice!. Facelet is very fast doing those
extra compilation steps, and the final effect over performance really pays 
off. We could even set this mode as default.

The only disadvantage of this strategy is the current contract of FaceletCache
is insuficient. As it has been described in MYFACES-3705, there are 
implementation details inside MyFaces Core and in our facelets implementation,
that needs to be exposed in a proper way. We need to create a custom
AbstractFaceletCache and specify how to implement it.
",", "
"Rename Method,","Use the same key in server side state saving for ajax requests The current code for server side state saving creates one key per request to store the view state. This is ok, but it is not necessary for ajax requests. 

The reason why is not necessary is because you can never go back to a page when using ajax. If you are on page A and the current request is an ajax request and it returns to the same page and the view is the same that the one that has been restored, the key or the token sent does not need to change, what changes is the internal state of the view. From the client side the page is the same. We can take advantage of this fact and just update the state stored in SerializedViewCollection for the view. 

The challenge here is detect when this strategy is applicable. For example, what happen if there is an ajax redirect? It looks is a good idea for implement in 2.2, because it avoids to store unnecessary information into session and optimize the use of each view slot.",", "
"Rename Method,Extract Method,Inline Method,","Refactor UIRepeat code to implement PSS algorithm like UIData and fix state behavior Right now, the code in org.apache.myfaces.view.facelets.component.UIRepeat has not been reviewed more than to fix issues related to JSF 2 spec. It works, but it can be done better.

The code has the following opportunities:

- Implement PSS algorithm, to prevent store data in the state.
- The algorithm used in UIData to handle state is better.
- Fix MYFACES-3415","Duplicated Code, Long Method, , , "
"Move And Rename Class,Extract Superclass,","Don't deserialize the ViewState-ID if the state saving method is server Currently the ViewState-ID provided by the user is deserialized via Java deserialization even when the {{javax.faces.STATE_SAVING_METHOD}} is set to {{server}} (the default).

The deserialization in this case is unecessary and most likely even slower than just sending the ViewState Id directly.
If a developer now disables the ViewState encryption by setting {{org.apache.myfaces.USE_ENCRYPTION}} to {{false}} (against the [MyFaces security advice|https://wiki.apache.org/myfaces/Secure_Your_Application]) he might have unintentionally introduced a dangerous remote code execution (RCE) vulnerability as described [here|https://www.alphabot.com/security/blog/2017/java/Misconfigured-JSF-ViewStates-can-lead-to-severe-RCE-vulnerabilities.html].

This has been discussed before on [Issue MYFACES-4021|https://issues.apache.org/jira/browse/MYFACES-4021].


",", Duplicated Code, Large Class, "
"Rename Method,Move Method,Extract Method,Move Attribute,","Create a new package org.apache.myfaces.spi and implement Providers for integration points with application containers On jsf 1.2 and earlier, it was only necessary one point for integrate an application container, an that was LifecycleProvider2, to handle @PostConstruct and @PreDestroy annotations.

But on jsf 2.0, many stuff has been introduced that requires provide spi interfaces, so application containers could integrate better.

One problem is how to handle jsf libraries outside WEB-INF/lib directory, or how to customize some algorithms that requires knowledge of the container protocols.

It is also known protocols like ""jar:"" to cause problems, when you try to scan files, because it is necessary to open a jar file and scan all entries to find just one file (see MYFACES-2583 and MYFACES-2833 for details).

I think we should introduce two new packages called org.apache.myfaces.spi and org.apache.myfaces.spi.impl to deal with this stuff.

Based on the class names found on com.sun.faces.spi package, we should provide the following points:

- Handling of @PostConstruct and @PreDestroy (done in LifecycleProvider2)
- Annotation scanning: containers could have this code duplicated or a framework to deal with stuff like that, so it should be a possibility to override/extend.
- faces-config.xml and facelet-taglib.xml additions: It should be possible to add resource files to be included in the process.
- Serialization (partially done on SerialFactory interface) : Jboss provide its own serialization solution.

At this time it is not very clear how this interfaces should looks like. I'll provide a proposal for this stuff, but it will take some time, because it is necessary to think carefully each interface.","Duplicated Code, Long Method, , , , "
"Rename Method,","enable 'standard' checkstyle checks in myfaces-core We currently only have the 'minimal' checks enabled in core, which actually only checks the correct license headers.

We should go for the 'standard' checkstyle rules, even if this would take some time to fix (found 1111 errors only in the first module).
",", "
"Move And Rename Class,Rename Method,","SearchExpression API There is a proposal from PrimeFaces guys to include a search expression api to locate components in cases like f:ajax execute/render attributes, h:message for attribute and others.

The idea comes from here:

http://blog.primefaces.org/?p=2740

The idea is support a syntax like this in the attributes:

< clientId>
:<id>
< id>:<id>:<id>
@<keyword>:<id>
id:@<keyword>
@<keyword>(<param1>)
@<keyword>(<param1>,<param2>)

There is a patch for this I have been working over the last weeks, but still require more tests and update the components in MyFaces Core 2.3 to use the new API.",", "
"Rename Method,Extract Method,","Create new module for JUnit Mock Testing using MyFaces Core, MyFaces Test and CDI This issue is the next step in the work started in MYFACES-3376 ""Create abstract test classes that runs MyFaces Core as in a container"".","Duplicated Code, Long Method, , "
"Rename Method,","Use Java8 instead of commons e.g. Base64 - will check other used parts

Collections:
* EmptyIterator (/)
* LRUMap (/)
* CollectionUtils.filter (/)
* Predicate (/)

Coded: (/)
* Hex (Could be replaced by javax/xml/bind/DatatypeConverter) (/)
* DecoderException (/)
* Base64 (/)

Digester:
* Digester (x)

BeanUtils:
* BeanUtils (x)
* PropertyUtils (x)",", "
"Extract Superclass,Move Method,Extract Method,Move Attribute,","Make a way to get the FacesConfig from a provider Currently, MyFaces startup listener will parse the all the faces configuration files and sort them on each startup time, and it will be better to do it once in the deployment time, and get those data structure instances from a provider. One possible way is to make those FacesConfig class serializable.
","Duplicated Code, Long Method, , , , Duplicated Code, Large Class, "
"Push Down Method,Push Down Attribute,Move Attribute,","Don't deserialize the ViewState-ID if the state saving method is server Currently the ViewState-ID provided by the user is deserialized via Java deserialization even when the {{javax.faces.STATE_SAVING_METHOD}} is set to {{server}} (the default).

The deserialization in this case is unecessary and most likely even slower than just sending the ViewState Id directly.
If a developer now disables the ViewState encryption by setting {{org.apache.myfaces.USE_ENCRYPTION}} to {{false}} (against the [MyFaces security advice|https://wiki.apache.org/myfaces/Secure_Your_Application]) he might have unintentionally introduced a dangerous remote code execution (RCE) vulnerability as described [here|https://www.alphabot.com/security/blog/2017/java/Misconfigured-JSF-ViewStates-can-lead-to-severe-RCE-vulnerabilities.html].

This has been discussed before on [Issue MYFACES-4021|https://issues.apache.org/jira/browse/MYFACES-4021].


",", , , , "
"Pull Up Method,Extract Method,Pull Up Attribute,","Allow to use different ExpressionFactory implementation It should be possible to use a different ExpressionFactory implementation. This feature is required
to use 3rd party EL implementations like JBoss EL. Mojarra already supports this with the
'com.sun.faces.expressionFactory' context parameter.

MyFaces Core 1.2.x already supports a context parameter 'org.apache.myfaces.EXPRESSION_FACTORY'
but it is only evaluated in a JSP 2.0 environment (see MYFACES-1693 for details).

It should be possible to use this parameter with JSP 2.1 as well. The corresponding code
should be refactored from Jsp20FacesInitializer to AbstractFacesInitializer to be usable in both
JSP 2.0 and 2.1.

Discussion on myfaces-users:

http://www.nabble.com/Replacing-expression-factory-td18867420.html
","Duplicated Code, Long Method, , Duplicated Code, Duplicated Code, "
"Rename Method,Move Method,Move Attribute,",Processor Counters should be included in the Status Reports This would allow a Processor's Status HIstory to show counters that were maintained over time periods instead of having only a single count since system start.,", , , "
"Rename Method,","Allow ConvertCharacterSet to accept expression language This issue arose from a user on the mailing list. It demonstrates the need to be able to use expression language to set the incoming (and potentially outgoing) character sets:

I'm looking to process many files into common formats. The source files are coming in various character sets, mime types, and new line terminators.

My thinking for a data flow was along these lines:

GetFile (from many sub directories) -> 
ExecuteStreamCommand (file -i) ->
ConvertCharacterSet (from previous command to utf8) ->
ReplaceText (to change any \r\n into \n) ->
PutFile (into a directory structure based on values found in the original file path and filename)

Additional steps would be added for archiving a copy of the original, converting xml files, etc.

Attempting to process these with Nifi leaves me confused as to how to process within the tool. If I want to ConvertCharacterSet, I have to know the input type. I setup a ExecuteStreamCommand to file -i ${absolute.path:append(${filename})} which returned the expected values. I don't see a way to turn these results into input for the processor, which doesn't accept expression language for that field.

I also considered ConvertCSVToAvro as an interim step but notice the same issue. Any suggestions what this dataflow should look like?",", "
"Rename Method,Extract Method,","Support AttributesToJSON processor Add a standard processor for creating a new attribute on a Flowfile which is the JSON representation of all (default) or a user-defined list of attributes.

Flowfile:
foo: bar
content: sometext

AttributesToJSON output:
foo: bar
content: sometext
JSON: {foo: 'bar', content: 'sometext'}","Duplicated Code, Long Method, , "
"Rename Method,Extract Method,","DFM should be allowed to inspect/interact with FlowFiles on a Connection User should be able to see the attributes, as well as download the content for flowfiles in the top of the active queue.

Additional tickets have been created and linked for searching, removing, and uploading flowfiles in a given queue.
","Duplicated Code, Long Method, , "
"Move Class,Move Method,Inline Method,Move Attribute,","Refactor InvokeHttp InvokeHttp currently uses Java's HttpUrlConnection, which is lacking in it's features and ease-of-use. In order to support all the current InvokeHttp pending tickets it's clear that a new underlying library is needed.

OkHttp looks to be a promising library that focusing on individual transactions (as opposed to Apache's HttpClient that focuses more on sessions).",", , , , "
"Extract Method,Move Attribute,","TailFile ""File to Tail"" property should support Wildcards Because of challenges around log rotation of high volume syslog and app producers, it is customary to logging platform developers to promote file variables based file names such as DynaFiles (rsyslog), Macros(syslog-ng)as alternatives to getting SIGHUPs being sent to the syslog daemon upon every file rotation.

(To certain extent, used even NiFi's has similar patterns, like for example, when one uses Expression Language to set PutHDFS destination file).

The current TailFile strategy suggests rotation patterns like:
{code}
log_folder/app.log
log_folder/app.log.1
log_folder/app.log.2
log_folder/app.log.3
{code}

It is possible to fool the system to accept wildcards by simply using a strategy like:

{code}
log_folder/test1
log_folder/server1
log_folder/server2
log_folder/server3
{code}

And configure *Rolling Filename Pattern* to * but it feels like a hack, rather than catering for an ever increasingly prevalent use case (DynaFile/macros/etc).

It would be great if instead, TailFile had the ability to use wildcards on File to Tail property","Duplicated Code, Long Method, , , "
"Pull Up Method,Move Method,Extract Method,Pull Up Attribute,Move Attribute,","Create a Put HBase processor that can put multiple cells We recently added a PutHBaseCell processor which works great for writing one individual cell at a time, but it can require a significant amount of work in a flow to create a row with multiple cells. 

We should support a variation of this processor that can accept a flow file with key/value pairs in the content of the flow file (possibly json). The key/value pairs then turned into the cells for the given row and get added in one put operation.","Duplicated Code, Long Method, , , , Duplicated Code, Duplicated Code, "
"Move And Rename Class,Move Method,Extract Method,Move Attribute,","Allow ReplaceText expression language functions to access matching group values Chanru from the user's mailing list asked how we'd support a pretty simple use case of converting 

{quote}
col1,col2,col3
2006,10-01-2004,10may2004
2007,15-05-2006,10jun2005
2009,8-8-2008,10aug2008
{quote}

to this

{quote}
col1,col2,col3
2006,10-01-2004,2004-05-10
2007,15-05-2006,2005-06-10
2009,8-8-2008,2008-08-10
{quote}

And we can do it today but it is surprising more effort than it should be. The problem is that ReplaceText as-is gets us sooo close. But what it cannot do is convert the original column three into a formatted Date object which can then be written out as a formatted string. If it could then this would be easy. So for example if we extracted each column as a matching group and the replacement value could be

{code}
${ $3:toDate('ddMMMyyy'):format('yyyy/MM/ddd')}
{code}

Then we'd be all set. But right now there is no way to take that third matching group and do anything fun with expression language on it because it is not a subject passed into the EL on which functions could be applied. If instead we simply added those matching groups as keys/subjects available to the EL then this becomes a quite powerful tool.
","Duplicated Code, Long Method, , , , "
"Move Class,Move Method,Extract Method,Move Attribute,","Provide additional KDFs for EncryptContent Currently, the two key derivation functions (KDF) supported are NiFi Legacy (1000 iterations of MD5 digest over a password and optional salt) and OpenSSL PKCS#5 v1.5 (a single iteration of MD5 digest over a password and optional salt). 

Both of these are very weak -- they use a deprecated cryptographic hash function (CHF) with known weakness and susceptibility to collisions (with demonstrated attacks) and a non-configurable and tightly coupled iteration count to derive the key and IV. 

Current best practice KDFs (with work factor recommendations) are as follows:
* PBKDF2 with variable hash function (SHA1, SHA256, SHA384, SHA512, or ideally HMAC variants of these functions) and variable iteration count (in the 10k - 1M range). 
* bcrypt with work factor of 12 - 16
* scrypt with work factor of (2^14 - 2^20, 8, 1)

The salt and iteration count should be stored alongside the hashed record (bcrypt handles this natively). 

Notes:

* http://wildlyinaccurate.com/bcrypt-choosing-a-work-factor/
* http://blog.ircmaxell.com/2012/12/seven-ways-to-screw-up-bcrypt.html
* http://security.stackexchange.com/questions/17207/recommended-of-rounds-for-bcrypt
* http://security.stackexchange.com/questions/3959/recommended-of-iterations-when-using-pkbdf2-sha256/3993#3993
* http://security.stackexchange.com/questions/4781/do-any-security-experts-recommend-bcrypt-for-password-storage/6415 
* http://web.archive.org/web/20130407190430/http://chargen.matasano.com/chargen/2007/9/7/enough-with-the-rainbow-tables-what-you-need-to-know-about-s.html
* https://www.nccgroup.trust/us/about-us/newsroom-and-events/blog/2015/march/enough-with-the-salts-updates-on-secure-password-schemes/
* http://www.tarsnap.com/scrypt.html
* http://www.tarsnap.com/scrypt/scrypt.pdf","Duplicated Code, Long Method, , , , "
"Rename Method,Extract Method,","Provide additional KDFs for EncryptContent Currently, the two key derivation functions (KDF) supported are NiFi Legacy (1000 iterations of MD5 digest over a password and optional salt) and OpenSSL PKCS#5 v1.5 (a single iteration of MD5 digest over a password and optional salt). 

Both of these are very weak -- they use a deprecated cryptographic hash function (CHF) with known weakness and susceptibility to collisions (with demonstrated attacks) and a non-configurable and tightly coupled iteration count to derive the key and IV. 

Current best practice KDFs (with work factor recommendations) are as follows:
* PBKDF2 with variable hash function (SHA1, SHA256, SHA384, SHA512, or ideally HMAC variants of these functions) and variable iteration count (in the 10k - 1M range). 
* bcrypt with work factor of 12 - 16
* scrypt with work factor of (2^14 - 2^20, 8, 1)

The salt and iteration count should be stored alongside the hashed record (bcrypt handles this natively). 

Notes:

* http://wildlyinaccurate.com/bcrypt-choosing-a-work-factor/
* http://blog.ircmaxell.com/2012/12/seven-ways-to-screw-up-bcrypt.html
* http://security.stackexchange.com/questions/17207/recommended-of-rounds-for-bcrypt
* http://security.stackexchange.com/questions/3959/recommended-of-iterations-when-using-pkbdf2-sha256/3993#3993
* http://security.stackexchange.com/questions/4781/do-any-security-experts-recommend-bcrypt-for-password-storage/6415 
* http://web.archive.org/web/20130407190430/http://chargen.matasano.com/chargen/2007/9/7/enough-with-the-rainbow-tables-what-you-need-to-know-about-s.html
* https://www.nccgroup.trust/us/about-us/newsroom-and-events/blog/2015/march/enough-with-the-salts-updates-on-secure-password-schemes/
* http://www.tarsnap.com/scrypt.html
* http://www.tarsnap.com/scrypt/scrypt.pdf","Duplicated Code, Long Method, , "
"Move Class,Move And Rename Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Add support for RELP in ListenSyslog Add support for listening for syslog events using The Reliable Event Logging Protocol (RELP) [1]
http://www.rsyslog.com/doc/relp.html","Duplicated Code, Long Method, , , , , "
"Rename Class,Rename Method,Inline Method,","Add processor(s) support for Elasticsearch Request to add processors to NiFi for Elasticsearch for the following capabilities:

- Bulk Insert of flowfile content
- Fetch files by document id
- Support for secure clusters (if the Shield plugin is available)",", , "
"Rename Class,Move Class,Rename Method,Move Method,Extract Method,Move Attribute,","Create QueryFlowFile Processor We should have a Processor that allows users to easily filter out specific columns from CSV data. For instance, a user would configure two different properties: ""Columns of Interest"" (a comma-separated list of column indexes) and ""Filtering Strategy"" (Keep Only These Columns, Remove Only These Columns).

We can do this today with ReplaceText, but it is far more difficult than it would be with this Processor, as the user has to use Regular Expressions, etc. with ReplaceText.

Eventually a Custom UI could even be built that allows a user to upload a Sample CSV and choose which columns from there, similar to the way that Excel works when importing CSV by dragging and selecting the desired columns? That would certainly be a larger undertaking and would not need to be done for an initial implementation.","Duplicated Code, Long Method, , , , "
"Move And Rename Class,Move Method,Move Attribute,","Create QueryFlowFile Processor We should have a Processor that allows users to easily filter out specific columns from CSV data. For instance, a user would configure two different properties: ""Columns of Interest"" (a comma-separated list of column indexes) and ""Filtering Strategy"" (Keep Only These Columns, Remove Only These Columns).

We can do this today with ReplaceText, but it is far more difficult than it would be with this Processor, as the user has to use Regular Expressions, etc. with ReplaceText.

Eventually a Custom UI could even be built that allows a user to upload a Sample CSV and choose which columns from there, similar to the way that Excel works when importing CSV by dragging and selecting the desired columns? That would certainly be a larger undertaking and would not need to be done for an initial implementation.",", , , "
"Rename Class,Rename Method,Move Method,Extract Method,Move Attribute,","Add capabilities to Kafka NAR to use new Kafka API (0.9) Not sure when can we address this, but the interesting comment in https://github.com/apache/nifi/pull/143. The usage of new API may introduce issues with running against older Kafka brokers (e.g., 0.8). Need to investigate.","Duplicated Code, Long Method, , , , "
"Move Method,Move Attribute,","Create Avro Schema by inferring CSV and JSON data There are several situations where the ability to dynamically create an Avro schema is desired. Kite provides the ability to dynamically infer an Avro schema from both CSV and JSON data. Since NiFi already contains a kite bundle for converting csv and json to Avro this feature should be an easy add. 

I propose 2 new processors ""InferAvroSchemaFromCSV"" and ""InferAvroSchemaFromJSON"". These processors will reside inside of the existing ""nifi-kite-bundle"" and extend upon the already present third party libraries. Each processor will accept either CSV or JSON and produce as output the Avro Schema JSON and the original data that was presented to them. The processors will rely on kite to perform the actually inferring of the schema.",", , , "
"Rename Method,","Splunk Processors To continue improving NiFi's ability to collect logs, a good integration point would be to have a processor that could listen for data from a Splunk forwarder (https://docs.splunk.com/Splexicon:Universalforwarder). Being able to push log messages to Splunk would also be useful.

Splunk provides an SDK that may be helpful:
https://github.com/splunk/splunk-sdk-java",", "
"Move Method,Extract Method,","Splunk Processors To continue improving NiFi's ability to collect logs, a good integration point would be to have a processor that could listen for data from a Splunk forwarder (https://docs.splunk.com/Splexicon:Universalforwarder). Being able to push log messages to Splunk would also be useful.

Splunk provides an SDK that may be helpful:
https://github.com/splunk/splunk-sdk-java","Duplicated Code, Long Method, , , "
"Rename Class,Extract Method,Move Attribute,","Add scriptable ReportingTask Now that NIFI-210 adds scriptable Processors (and scripted onTrigger bodies), a great extension would be to add a scriptable ReportingTask. This would enable users to script their own ReportingTasks using the various supported scripting languages.","Duplicated Code, Long Method, , , "
"Rename Method,","Refactor lifecycle code for Processors and other components Similar lifecycle handling improvements that went in as part of the NIFI-1164 for ControllerServices, could/should be applied to other components (e..g, Processors).
The improvements may also help to address NIFI-78 (may be without killing the thread).",", "
"Pull Up Method,Extract Method,Pull Up Attribute,","Refactor lifecycle code for Processors and other components Similar lifecycle handling improvements that went in as part of the NIFI-1164 for ControllerServices, could/should be applied to other components (e..g, Processors).
The improvements may also help to address NIFI-78 (may be without killing the thread).","Duplicated Code, Long Method, , Duplicated Code, Duplicated Code, "
"Rename Method,Extract Method,","Add Kerberos Support to HBase processors Our current HBase integration does not support communicating with a Kerberized HBase install. We should support this just like we do for the HDFS processors.

","Duplicated Code, Long Method, , "
"Move Class,Rename Method,Move Method,Extract Method,Move Attribute,","Remove storage of components' stats and bulletins from NCM Currently, each node in a cluster sends a period heartbeat that contains the stats for all components in the node. This happens every 5 seconds by default. This results in quite a lot of chatter between the NCM and nodes. It made sense to take this approach when the clustering concept was designed because there were no Process Groups, and we had no notion of merging responses from nodes for a web request.

However, now we should replicate the request to all nodes and then merge the responses on demand, rather than storing this information on the NCM. This requires far less bandwidth because we only need to pull the stats for a particular Process Group and only on demand instead of every 5 seconds. 

Additionally, this is laying groundwork for the Zero-Master clustering that we want to have in place for 1.0.0.

In order to remove the stats from the NCM, we will also need to remove the bulletins and stats history. These requests will all need to be federated and the responses merged on-demand.","Duplicated Code, Long Method, , , , "
"Rename Class,Rename Method,Move Method,Extract Method,","Improve Expression Language to Enable Working with Decimals Currently the math operations in Expression Language use Longs to evaluate numbers. This leads to any decimal places getting truncated when performing operations like divide. 

NiFi should support working with decimals","Duplicated Code, Long Method, , , "
"Rename Method,Extract Method,","Improve the Provenance Events emitted by PutKafka The Provenance SEND events emitted by PutKafka should include the amount of time that it took to send the data. Also, if some of the messages were sent but not all (when using a delimiter), then we should have a SEND event that indicates the number of events that were sent. The Transit URI should also include the ""kafka://"" protocol name at the beginning and should include the topic, such as kafka://leaderBroker:9092/topics/topicDataWasPlacedOn","Duplicated Code, Long Method, , "
"Rename Method,Move Method,Extract Method,Move Attribute,","Nodes in cluster should use ZooKeeper to store heartbeat messages instead of sending to NCM Currently, nodes send heartbeats to the NCM periodically in order to indicate that they are actively participating in the cluster. As we move away from using an NCM, we need these heartbeats to go somewhere else. ZooKeeper is a reasonable location to push the heartbeats to, as it provides the HA that we need","Duplicated Code, Long Method, , , , "
"Rename Method,Push Down Attribute,","Allow concurrent execution of ExecuteScript Currently ExecuteScript is annotated as TriggerSerially, meaning only one task can be running at a time. This causes issues for throughput, and can become a severe bottleneck in a flow.

Originally this was done because the bindings for the session, context, etc. were put on a single script engine, so multiple tasks would clobber each other's bindings. However as a tradeoff (memory for capability), it would be better to have a ""pool"" of Script Engine instances, whose size is perhaps the max number of concurrent tasks.",", , "
"Rename Method,","Allow encrypted passwords in configuration files Storing passwords in plaintext in configuration files is not a security best practice. While file access can be restricted through OS permissions, these configuration files can be accidentally checked into source control, shared or deployed to multiple instances, etc. 

NiFi should allow a deployer to provide an encrypted password in the configuration file to minimize exposure of the passwords. On application start-up, NiFi should decrypt the passwords in memory. NiFi should also include a utility to encrypt the raw passwords (and optionally populate the configuration files and provide additional metadata in the configuration files). 

I am aware this simply shifts the responsibility/delegation of trust from the passwords in the properties file to a new location on the same system, but mitigating the visibility of the raw passwords in the properties file can be one step in a defense in depth approach and is often mandated by security policies within organizations using NiFi. 

The key used for encryption should not be hard-coded into the application source code, nor should it be universally consistent. The key could be determined by reading static information from the deployed system and feeding it to a key derivation function based on a cryptographically-secure hash function, such as PBKDF2, bcrypt, or scrypt. However, this does introduce upgrade, system migration, and portability issues. These challenges will have to be kept in consideration when determining the key derivation process. 

Manual key entry is a possibility, and then the master key would only be present in memory, but this prevents automatic reboot on loss of power or other recovery scenario. 

This must be backward-compatible to allow systems with plaintext passwords to continue operating. Options for achieving this are to only attempt to decrypt passwords when a sibling property is present, or to match a specific format. 

For these examples, I have used the following default values:

{code}
password: thisIsABadPassword
key: 0123456789ABCDEFFEDCBA98765432100123456789ABCDEFFEDCBA9876543210
iv: 0123456789ABCDEFFEDCBA9876543210
algorithm: AES/CBC 256-bit
{code}

**Note: These values should not be used in production systems -- the key and IV are common test values, and an AEAD cipher is preferable to provide cipher text integrity assurances, however OpenSSL does not support the use of AEAD ciphers for command-line encryption at this time**

Example 1: *here the sibling property indicates the password is encrypted and with which implementation; the absence of the property would default to a raw password*

{code}
hw12203:/Users/alopresto/Workspace/scratch/encrypted-passwords (master) alopresto
🔓 0s @ 16:25:56 $ echo ""thisIsABadPassword"" > password.txt
hw12203:/Users/alopresto/Workspace/scratch/encrypted-passwords (master) alopresto
🔓 0s @ 16:26:47 $ ossl aes-256-cbc -e -nosalt -p -K 0123456789ABCDEFFEDCBA98765432100123456789ABCDEFFEDCBA9876543210 -iv 0123456789ABCDEFFEDCBA9876543210 -a -in password.txt -out password.enc
key=0123456789ABCDEFFEDCBA98765432100123456789ABCDEFFEDCBA9876543210
iv =0123456789ABCDEFFEDCBA9876543210
hw12203:/Users/alopresto/Workspace/scratch/encrypted-passwords (master) alopresto
🔓 0s @ 16:27:09 $ xxd password.enc
0000000: 5643 5856 6146 6250 4158 364f 5743 7646 VCXVaFbPAX6OWCvF
0000010: 6963 6b76 4a63 7744 3854 6b67 3731 4c76 ickvJcwD8Tkg71Lv
0000020: 4d38 6d32 7952 4776 5739 413d 0a M8m2yRGvW9A=.
hw12203:/Users/alopresto/Workspace/scratch/encrypted-passwords (master) alopresto
🔓 0s @ 16:27:16 $ more password.enc
VCXVaFbPAX6OWCvFickvJcwD8Tkg71LvM8m2yRGvW9A=
hw12203:/Users/alopresto/Workspace/scratch/encrypted-passwords (master) alopresto
🔓 0s @ 16:27:55 $
{code}

In {{nifi.properties}}: 
{code}
nifi.security.keystorePasswd=VCXVaFbPAX6OWCvFickvJcwD8Tkg71LvM8m2yRGvW9A=
nifi.security.keystorePasswd.encrypted=AES-CBC-256
{code}

Example 2: *here the encrypted password has a header tag indicating both that it is encrypted and the algorithm used*

{code:java}
@Test
public void testShouldDecryptPassword() throws Exception {
// Arrange
KeyedCipherProvider cipherProvider = new AESKeyedCipherProvider()

final String PLAINTEXT = ""thisIsABadPassword""
logger.info(""Expected: ${Hex.encodeHexString(PLAINTEXT.bytes)}"")

final byte[] IV = Hex.decodeHex(""0123456789ABCDEFFEDCBA9876543210"" as char[])
final byte[] LOCAL_KEY = Hex.decodeHex(""0123456789ABCDEFFEDCBA9876543210"" * 2 as char[])
// Generated via openssl enc -a
final String CIPHER_TEXT = ""VCXVaFbPAX6OWCvFickvJcwD8Tkg71LvM8m2yRGvW9A=""
byte[] cipherBytes = Base64.decoder.decode(CIPHER_TEXT)

SecretKey localKey = new SecretKeySpec(LOCAL_KEY, ""AES"")

EncryptionMethod encryptionMethod = EncryptionMethod.AES_CBC
logger.info(""Using algorithm: ${encryptionMethod.getAlgorithm()}"")
logger.info(""Cipher text: \$nifipw\$${CIPHER_TEXT} ${cipherBytes.length + 8}"")

// Act
Cipher cipher = cipherProvider.getCipher(encryptionMethod, localKey, IV, false)
byte[] recoveredBytes = cipher.doFinal(cipherBytes)

// OpenSSL adds a newline character during encryption
String recovered = new String(recoveredBytes, ""UTF-8"").trim()
logger.info(""Recovered: ${recovered} ${Hex.encodeHexString(recoveredBytes)}"")

// Assert
assert PLAINTEXT.equals(recovered)
}
{code}

In {{nifi.properties}}: 
{code}
nifi.security.keystorePasswd=$nifipw$VCXVaFbPAX6OWCvFickvJcwD8Tkg71LvM8m2yRGvW9A=
{code}

Ideally, NiFi would use a pluggable implementation architecture to allow users to integrate with a variety of secret management services. There are both commercial and open source solutions, including CyberArk Enterprise Password Vault [1], Hashicorp Vault [2], and Square Keywhiz [3]. In the future, this could also be extended to Hardware Security Modules (HSM) like SafeNet Luna [4] and Amazon CloudHSM [5]. 

[1] http://www.cyberark.com/products/privileged-account-security-solution/enterprise-password-vault/
[2] https://www.vaultproject.io/
[3] https://square.github.io/keywhiz/
[4] http://www.safenet-inc.com/data-encryption/hardware-security-modules-hsms/luna-hsms-key-management/luna-sa-network-hsm/
[5] https://aws.amazon.com/cloudhsm/",", "
"Move Class,Rename Class,","Add support for Azure Blob Storage and Table Storage It would be useful to have an Azure equivalent of the current S3 capability. Azure also provides a Table storage mechanism, providing simple key value storage. Since the Azure SDKs are Apache Licensed, this should be reasonably straightforward. A first cut is available as an addition to the existing azure bundle.",", "
"Extract Superclass,Move Method,Extract Method,Move Attribute,",Create PutTCP Processor Create a PutTCP Processor to send FlowFile content over a TCP connection to a TCP server.,"Duplicated Code, Long Method, , , , Duplicated Code, Large Class, "
"Rename Class,Move And Rename Class,Rename Method,Extract Method,","JSON-to-JSON Schema Converter Editor With NIFI-361 implemented, users can embed a JOLT spec into the TransformJSON processor. However, building the spec is not intuitive enough without the ability to obtain real time feedback on spec syntax and preview output data. Would like to have a specification editor and transform preview view added to the processor configuration screen to allow users to immediately see the impact on the output JSON data.","Duplicated Code, Long Method, , "
"Move Class,Move And Rename Class,Extract Superclass,Extract Interface,Pull Up Method,Move Method,Extract Method,Pull Up Attribute,Move Attribute,","Support HTTP(S) as a transport mechanism for Site-to-Site We should add support for using HTTP(S) for site-to-site to be an alternative to the current socket based approach.

This would support the same push based or pull based approach site-to-site offers now but it would use HTTP(S) for all interactions to include learning about ports, learning about NCM topology, and actually exchanging data. This mechanism should also support interaction via an HTTP proxy.

This would also require some UI work to allow the user to specify which protocol for site-to-site to use such as 'raw' vs 'http'. We also need to document any limitations with regard to SSL support for this mode and we'd need to provide 'how-to' when using proxies like http_proxy or something else.","Duplicated Code, Long Method, , , , Duplicated Code, Large Class, Duplicated Code, Large Class, Duplicated Code, "
"Move Class,Move Method,Extract Method,Move Attribute,","Add support for Hive Streaming Traditionally adding new data into Hive requires gathering a large amount of data onto HDFS and then periodically adding a new partition. This is essentially a “batch insertion”. Insertion of new data into an existing partition is not permitted. Hive Streaming API allows data to be pumped continuously into Hive. The incoming data can be continuously committed in small batches of records into an existing Hive partition or table. Once data is committed it becomes immediately visible to all Hive queries initiated subsequently.

This case is to add a PutHiveStreaming processor to NiFi, to leverage the Hive Streaming API to allow continuous streaming of data into a Hive partition/table.","Duplicated Code, Long Method, , , , "
"Push Down Method,Push Down Attribute,","Add Elasticsearch processors that use the REST API The current Elasticsearch processors use the Transport Client, and as a result there can be some compatibility issues between multiple versions of ES clusters. The REST API is much more standard between versions, so it would be nice to have ES processors that use the REST API, to enable things like migration from an Elasticsearch cluster with an older version to a cluster with a newer version.",", , , "
"Move Method,Move Attribute,","Move Controller level bulletins out of Controller Status Controller level bulletins need to be removed from the controller status endpoint and into some endpoint that is accessed through the Controller resource for authorization consistency.

All status requests are authorized under the flow but the bulletins are more sensitive.",", , , "
"Rename Method,Extract Method,","Convert Azure Event Hub Processors to use correct Azure Event Hub client library Currently NiFi uses the Event Hub client found here: https://github.com/hdinsight/eventhubs-client/ however, it should use the client library found here: https://github.com/Azure/azure-event-hubs/tree/master/java

The Azure Event Hub client targets JDK 1.8 and therefore, the resolution of this JIRA is only applicable to NiFi 1.0.0 or later.","Duplicated Code, Long Method, , "
"Rename Method,Extract Method,","Decouple users/user groups and policies Currently, users, user groups, and policies are all persisted in the same file. Because the policies are associated with a particular NiFi instance it makes this file not portable. However, the users and user groups configuration is completely independent of a particular instance and should be persisted in a separate file to promote portable of this configuration between NiFi instances.","Duplicated Code, Long Method, , "
"Pull Up Attribute,Push Down Attribute,",New processor GetIgniteCache for getting values from Apache Ignite New component for getting value from Ignite Cache,", , Duplicated Code, "
"Rename Class,Rename Method,","Allow for configuration of Controller Services and Reporting Tasks in UI Currently, Controller Services and Reporting Tasks are specified in configuration files where NiFi is installed. We need to make this configuration more accessible by moving into the UI and not requiring a restart of the application.",", "
"Rename Method,","Allow for configuration of Controller Services and Reporting Tasks in UI Currently, Controller Services and Reporting Tasks are specified in configuration files where NiFi is installed. We need to make this configuration more accessible by moving into the UI and not requiring a restart of the application.",", "
"Rename Method,Extract Method,","Allow for configuration of Controller Services and Reporting Tasks in UI Currently, Controller Services and Reporting Tasks are specified in configuration files where NiFi is installed. We need to make this configuration more accessible by moving into the UI and not requiring a restart of the application.","Duplicated Code, Long Method, , "
"Move Method,Inline Method,Move Attribute,","Allow for configuration of Controller Services and Reporting Tasks in UI Currently, Controller Services and Reporting Tasks are specified in configuration files where NiFi is installed. We need to make this configuration more accessible by moving into the UI and not requiring a restart of the application.",", , , , "
"Move And Rename Class,","Allow for configuration of Controller Services and Reporting Tasks in UI Currently, Controller Services and Reporting Tasks are specified in configuration files where NiFi is installed. We need to make this configuration more accessible by moving into the UI and not requiring a restart of the application.",", "
"Move Class,Move Method,Move Attribute,","Allow for configuration of Controller Services and Reporting Tasks in UI Currently, Controller Services and Reporting Tasks are specified in configuration files where NiFi is installed. We need to make this configuration more accessible by moving into the UI and not requiring a restart of the application.",", , , "
"Move Method,Move Attribute,","Allow for configuration of Controller Services and Reporting Tasks in UI Currently, Controller Services and Reporting Tasks are specified in configuration files where NiFi is installed. We need to make this configuration more accessible by moving into the UI and not requiring a restart of the application.",", , , "
"Extract Method,Pull Up Attribute,","Add ability to write Row Identifier as binary in hbase using the PutHbaseCell Today the PutHBaseCell processor makes the assumption that all row keys are text. However, this does not work when the row key in the HBase table is binary. 

If the row key is specified in the binary string format, such as:

\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x 
00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00 
\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x 
00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00 
\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x 
00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00 
\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x 
00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00 
\x00\x00\x00\x00\x00\x00\x00\x01\x01\x00\x 
00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00 
\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x 
00\x00\x00\x00\x00\x00\x01\x00\x00\x01\x00 
\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x 
00\x00\x01\x00\x00\x01\x00\x00\x00\x00\x01 
\x01\x01\x00\x01\x00\x01\x01\x01\x00\x00\x 
00\x00\x00\x00\x01\x01\x01\x01\x00\x00\x00 
\x00\x00\x00\x01\x01\x00\x01\x00\x01\x00\x 
00\x01\x01\x01\x01\x00\x00\x01\x01\x01\x00 
\x01\x00\x00

Which is the textual representation that the HBase CLI would return, NiFi calls getBytes on that string. Appropriately HBase will encode the '\' with the hex code: x5C, resulting in an output string that looks like:

x5Cx00\x5Cx00\ ...........

To address this the proposed solution would be to:

* Add ""toBytesBinary"" method to HBaseClientService similar to the ones already added [1]. 

* Update the PutFlowFile and PutColumn to pass around mostly byte[] and not strings that they do today.

For this JIRA only support for Text and Binary will be added for the RowKey

[1] https://github.com/apache/nifi/blob/master/nifi-nar-bundles/nifi-standard-services/nifi-hbase_1_1_2-client-service-bundle/nifi-hbase_1_1_2-client-service/src/main/java/org/apache/nifi/hbase/HBase_1_1_2_ClientService.java#L427","Duplicated Code, Long Method, , Duplicated Code, "
"Rename Method,",Add JMS properties to FlowFile attributes on receive in ConsumeJMS ConsumeJMS currently adds JMS headers to the FlowFile attributes when it receives a message but ignores any JMS properties coming through. It should be reading both the headers and properties and merging them into the FlowFile attributes.,", "
"Rename Method,",The great typo cleanup ,", "
"Rename Class,Rename Method,",The great typo cleanup ,", "
"Move Method,Extract Method,Inline Method,Move Attribute,","Provide ability for a FlowFile to be migrated from one Process Session to another Currently, the MergeContent processor creates a separate ProcessSession for each FlowFile that it pulls. This is done so that we can ensure that we can commit all Process Sessions when a bin is full. Unfortunately, this means that MergeContent is required to call ProcessSession.get() many times, which adds a lot of contention on the FlowFile Queue. If we allow FlowFiles to be migrated from 1 session to another, we can have a session per bin, and then use ProcessSession.get(100) to greatly reduce lock contention. This will likely have benefits in other processors as well.","Duplicated Code, Long Method, , , , , "
"Rename Method,Extract Method,","Improve performance of SplitText SplitText is fairly CPU-intensive and quite slow. A simple flow that splits a 1.4 million line text file into 5k line chunks and then splits those 5k line chunks into 1 line chunks is only capable of pushing through about 10k lines per second. This equates to about 10 MB/sec. JVisualVM shows that the majority of the time is spent in the locateSplitPoint() method. Isolating this code and inspecting how it works, and using some micro-benchmarking, it appears that if we refactor the calls to InputStream.read() to instead read into a byte array, we can improve performance.","Duplicated Code, Long Method, , "
"Move And Rename Class,Extract Superclass,Rename Method,Pull Up Method,Move Method,Extract Method,Pull Up Attribute,Move Attribute,","Enable repositories to support upgrades and rollback in well defined scenarios The flowfile, swapfile, provenance, and content repositories play a very important roll in NiFi's ability to be safely upgraded and rolled back. We need to have well documented behaviors, designs, and version adherence so that users can safely rely on these mechanisms.

Once this is formalized and in place we should update our versioning guidance to reflect this as well.

The following would be true from NiFi 1.2.0 onward

* No changes to how the repositories are persisted to disk can be made which will break forward/backward compatibility and specifically this means that things like the way each is serialized to disk cannot change.
* If changes are made which impact forward or backward compatibility they should be reserved for major releases only and should include a utility to help users with pre-existing data convert from some older format to the newer format. It may not be feasible to have rollback on major releases.
* The content repository should not be changed within a major release cycle in any way that will harm forward or backward compatibility.
* The flow file repository can change in that new fields can be added to existing write ahead log record types but no fields can be removed nor can any new types be added. Once a field is considered required it must remain required. Changes may only be made across minor version changes - not incremental.
* Swap File storage should follow very similar rules to the flow file repository. Adding a schema to the swap file header may allow some variation there but the variation should only be hints to optimize how they're processed and not change their behavior otherwise. Changes are only permitted during minor version releases.
* Provenance repository changes are only permitted during minor version releases. These changes may include adding or removing fields from existing event types. If a field is considered required it must always be considered required. If a field is removed then it must not be a required field and there must be a sensible default an older version could use if that value is not found in new data once rolled back. New event types may be added. Fields or event types not known to older version, if seen after a rollback, will simply be ignored.

The following also would be true:
* Apache NiFi 1.0.0 repositories should work just fine when applied to an Apache NiFi 1.1.0 installation.
* Repositories made/updated in Apache NiFi 1.1.0 onward would not work in older Apache NiFi releases (such as 1.0.0)","Duplicated Code, Long Method, , , , Duplicated Code, Large Class, Duplicated Code, Duplicated Code, "
"Rename Method,","NiFi Site-To-Site with port forwarding It would be useful to be able to use port forwarding with NiFi Site-To-Site. This would allow NiFi to appear externally to be listening on a privileged port without having been granted elevated permissions.

For example, an administrator could configure iptables to forward traffic from port 443 to port 9443. Then users could use NiFi at port 443. This provides more flexibility as far as firewall configuration is concerned.

The above scenario causes problems with Site-To-Site though because in a clustered scenario, the nodes will still advertise themselves with port 9443. This would prevent a Site-To-Site client from being able to talk to them from outside the firewall.

We need a way (probably a nifi property) to tell NiFi to listen on one port (9443) and advertise another (443) for Site-To-Site purposes to enable this usecase.",", "
"Rename Method,Pull Up Method,Inline Method,Pull Up Attribute,",Refactor TextLineDemarcator and StreamDemarcator into a common abstract class Based on the work that has been performed as part of the NIFI-2851 we now have a new class with a significantly faster logic to perform demarcation of the InputStream (TextLineDemarcator). This new class's initial starting point was the existing LineDemarcator. They both now share ~60-70% of common code which would be important to extract into a common abstract class as well as incorporate the new (faster) demarcation logic int StreamDemarcator.,", , Duplicated Code, Duplicated Code, "
"Rename Method,Extract Method,","Provide a framework mechanism for loading additional classpath resources We currently have several components with a property for specifying additional classpath resources (DBCP connection pool, scripting processors, JMS). Each of these components is responsible for handling this in its own way. 

The framework should provide a more integrated solution to make it easier for component developers to deal with this scenario. Some requirements that need to be met by this solution:

- Multiple instances of the same component with different resources added to the classpath and not interfering with each other (i.e. two DBCP connection pools using different drivers)

- Ability to modify the actual ClassLoader of the component to deal with frameworks that use Class.forName() without passing in a ClassLoader, meaning if a processor loads class A and class A calls Class.forName(classBName), then class B needs to be available in the ClassLoader that loaded the processor's class which in turn loaded class A

- A component developer should be able to indicate that a given PropertyDescriptor represents a classpath resource and the framework should take care of the ClassLoader manipulation","Duplicated Code, Long Method, , "
"Rename Class,Rename Method,Pull Up Method,Extract Method,","Remote input/output ports and local process groups should be treated as completely different components With the addition of Multi-tenancy users can restrict users to particular process groups. What these users cannot do is create input and output ports on the root canvas. Users should be able to create remote input/output ports within process groups and assign S2S policies to them. The only thing they should need an admin to do is add Servers as users and add the global ""retrieve site-to-site details"" policy. This allows for a better separation between dataflow designer/implementor/DFM and NiFi Admin. The added benefit of treating remote and local input/output ports as unique components is that you could add them anywhere in your flow including imbedded within process groups.

Perhaps making them configurable as local or remote ports (defaulting to remote when added to root canvas and local when added within process group). This way we preserve backwards compatibility while still improving their usability in a multi-tenancy environment.","Duplicated Code, Long Method, , Duplicated Code, "
"Rename Method,","In TLS-Toolkit, allow users to specify separate input and output locations for client configuration settings Currently when using the tls-toolkit to generate client certificate artifacts (keystore/truststore etc) users have the option to provide the location of a configuration file that will provide the information necessary to create those items (using the ""F"" argument). Another option can be used to allow toolkit to write out all of the settings generated back to the indicated input file (using the ""f"" argument). For scenarios where users may want to pipe in input using stdin, vs referring to a file on disk, this is not optimal since toolkit will attempt to write out to stdin causing an error.

To prevent this error proposing to have the ""f"" argument also support an output location, that is separate from the location provided with the ""F"" argument.",", "
"Move Method,Move Attribute,",Add PGP and GPG support to EncryptContent Processor ,", , , "
"Rename Method,Extract Method,","LDAP - Support configurable user identity The current LDAP provider supports a configurable search filter that will allow the user specified login name to be matched against any LDAP entry attribute. We should offer a configuration option that will indicate if we should use the LDAP entry DN or if we should use the login name that was used in the search filter. For instance, this would allow an admin to configure a user to login with their sAMAccountName and subsequently use that name as their user's identity.

Note: we should default this option to be the user DN in order to ensure backwards compatibility.","Duplicated Code, Long Method, , "
"Extract Method,Inline Method,","S2S initial connection behavior enhancement s2s client behavior and initial connection improvement is needed.
Current experience is this: I, as a client (e.g. minifi), connect to a nifi cluster of e.g. 10 nodes. but i need to specify 1 node URL to establish this connection. this node may not be available 100% and go down, in which case my initial connection won't work.
Once S2S makes the first connection, it then has a list of all nodes, and can check their status. But first connection failure would be a concern if the specified URL is somehow not working. Usually for these problems, the client should be able to specify multiple urls (according to multiple target cluster nodes), comma-separated.","Duplicated Code, Long Method, , , "
"Rename Method,Pull Up Method,Pull Up Attribute,","Refactor base class from MergeContent The binning logic in MergeContent is extremely useful, and could be pulled out into an abstract superclass. This would allow other processors to perform binning logic without being tied to a merged flow file. For example, a processor may want to submit a batch request to a database like Solr, or to a REST endpoint. 

The vast majority of the code in MergeContent would remain in the concrete class, but there are several points, such as the BinManager usage, that could be abstracted for easy extensibility.",", Duplicated Code, Duplicated Code, "
"Rename Method,","Refactor base class from MergeContent The binning logic in MergeContent is extremely useful, and could be pulled out into an abstract superclass. This would allow other processors to perform binning logic without being tied to a merged flow file. For example, a processor may want to submit a batch request to a database like Solr, or to a REST endpoint. 

The vast majority of the code in MergeContent would remain in the concrete class, but there are several points, such as the BinManager usage, that could be abstracted for easy extensibility.",", "
"Move Method,Extract Method,","Add a method to DistributedMapCacheClient to replace existing key only if it hasn't been changed It would be helpful for some processors to be able to -get a list of all the keys in the distributed map cache that match some pattern, or even to get a list of all the keys. We should update DistributedMapCacheClient to have a method like getKeys(String pattern).- replace an existing key in the distributed map cache with concurrency control so that the key will be replaced only if it hasn't been updated by other operations since updating client program fetched the key.

Updated JIRA description as mentioned in [NIFI-3216|https://issues.apache.org/jira/browse/NIFI-3216?focusedCommentId=15812535&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15812535].","Duplicated Code, Long Method, , , "
"Rename Method,","Allow PublishAMQP to use NiFi expression language Enable the use of NiFi expression language for the PublishAMQP processors, Routing Key value to allow it to be better used within the NiFi workflows.

PublishAMQP fields to enable:
""Routing Key""",", "
"Rename Method,","AttributesToJson performance improvements AttributesToJson does a lot of work in every onTrigger() that it doesn't need to.

A lot of the attributes map logic can be done at schedule time so that it doesn't need to be done on every trigger.

Also, all of the properties gotten from the process context can be fetched in onSchedule()",", "
"Move And Rename Class,Extract Interface,Rename Method,Move Method,Extract Method,Move Attribute,","Provide a newly refactored provenance repository The Persistent Provenance Repository has been redesigned a few different times over several years. The original design for the repository was to provide storage of events and sequential iteration over those events via a Reporting Task. After that, we added the ability to compress the data so that it could be held longer. We then introduced the notion of indexing and searching via Lucene. We've since made several more modifications to try to boost performance.

At this point, however, the repository is still the bottleneck for many flows that handle large volumes of small FlowFiles. We need a new implementation that is based around the current goals for the repository and that can provide better throughput.","Duplicated Code, Long Method, , , , Large Class, "
"Rename Method,","MoveHDFS Processor Today the PutHDFS Processor merely places a file into HDFS from NiFi. There are times when we may want to move Files/Directories around on HDFS as part of a workflow. This could be after the PutHDFS processor has placed the file, or from some other trigger. 

Initially we are targeting to take a flow file attribute of an absolute HDFS path, and be able to move it to a target HDFS Path using the Filesystem RENAME API.",", "
"Move Class,Rename Class,Rename Method,Extract Method,Inline Method,Move Attribute,","Multiple Versions of the Same Component This ticket is to track the work for supporting multiple versions of the same component within NiFi. The overall design for this feature is described in detail at the following wiki page:

https://cwiki.apache.org/confluence/display/NIFI/Multiple+Versions+of+the+Same+Extension

This ticket will track only the core NiFi work, and a separate ticket will be created to track enhancements for the NAR Maven Plugin.","Duplicated Code, Long Method, , , , "
"Extract Interface,Rename Method,Move Method,","Add ""Rollback on Failure"" property to PutHiveStreaming, PutHiveQL, PutDatabaseRecord, and PutSQL Many Put processors (such as PutHiveStreaming, PutHiveQL, and PutSQL) offer ""failure"" and ""retry"" relationships for flow files that cannot be processed, perhaps due to issues with the external system or other errors.

However there are use cases where if a Put fails, then no other flow files should be processed until the issue(s) have been resolved. This should be configurable for said processors, to enable both the current behavior and a ""stop on failure"" type of behavior.

I propose a property be added to the Put processors (at a minimum the PutHiveStreaming, PutHiveQL, and PutSQL processors) called ""Rollback on Failure"", which offers true or false values. If set to true, then the ""failure"" and ""retry"" relationships should be removed from the processor instance, and if set to false, those relationships should be offered.

If Rollback on Failure is false, then the processor should continue to behave as it has. If set to true, then if any error occurs while processing a flow file, the session should be rolled back rather than transferring the flow file to some error-handling relationship.

It may also be the case that if Rollback on Failure is true, then the incoming connection must use a FIFO Prioritizer, but I'm not positive. The documentation should be updated to include any such requirements.",", , Large Class, "
"Move Method,Move Attribute,","Add DetectDuplicateUsingHBase processor The DetectDuplicate processor makes use of a distributed map cache for maintaining a list of unique file identifiers (such as hashes).

The distributed map cache functionality could be provided by an HBase table, which then allows for reliably storing a huge volume of file identifiers and auditing information. The downside of this approach is of course that HBase is required.

Storing the unique file identifiers in a reliable, query-able manner along with some audit information is of benefit to several use cases.",", , , "
"Rename Method,Move Method,Extract Method,Move Attribute,","Create PolicyBasedAuthorizer interface to allow authorization chain Rather than using AbstractPolicyBasedAuthorizer to trigger policy management, refactor to use a new interface. New implementations of this interface can then create an authorization chain with existing AbstractPolicyBasedAuthorizer sub-classes.

----
While investigating alternate implementations of the Authorizer interface, I see the AbstractPolicyBasedAuthorizer is meant to be extended. It's authorize() method is final, however, and does not have an abstract doAuthorize() method that sub-classes can extend.

In particular, the existing AbstractPolicyBasedAuthorizer authorize() method does not take into account the AuthorizationRequest ""resourceContext"" in its authorization decision. This is especially important when authorizing access to events in Provenance, which places attributes in resouceContext of its AuthorizationRequest when obtaining an authorization decision. I would like to use attributes to authorize access to Provenance download & view content feature.

If I had my own sub-class of AbstractPolicyBasedAuthorizer, with the availability of a doAuthorize() method, then I could maintain my own user policies for allowing access to flowfile content via Provenance.","Duplicated Code, Long Method, , , , "
"Move Class,Rename Class,Push Down Method,Move Method,Extract Method,Push Down Attribute,Move Attribute,","Add ""Schema Access Strategy"" to Record Readers and Writers Currently the record readers are mostly configured with a Schema Registry service and the name of the schema. We should instead allow user to choose one of several strategies for determining the schema: Schema Registry + schema.name attribute, Schema Registry + identifier and version embedded at start of record/stream, avro.schema attribute, embedded schema for cases like Avro where the schema can be embedded in the content itself.

On the writer side, we should also expose these options in order to convey the schema information to others.","Duplicated Code, Long Method, , , , , , "
"Move Method,Extract Method,Inline Method,","Standard metadata property names in the ParseData metadata Currently, people are free to name their string-based properties anything that they want, such as having names of ""Content-type"", ""content-TyPe"", ""CONTENT_TYPE"" all having the same meaning. Stefan G. I believe proposed a solution in which all property names be converted to lower case, but in essence this really only fixes half the problem right (the case of identifying that ""CONTENT_TYPE""
and ""conTeNT_TyPE"" and all the permutations are really the same). What about
if I named it ""Content Type"", or ""ContentType""?

I propose that a way to correct this would be to create a standard set of named Strings in the ParseData class that the protocol framework and the parsing framework could use to identify common properties such as ""Content-type"", ""Creator"", ""Language"", etc.

The properties would be defined at the top of the ParseData class, something like:

public class ParseData{

.....

public static final String CONTENT_TYPE = ""content-type"";
public static final String CREATOR = ""creator"";

....

}


In this fashion, users could at least know what the name of the standard properties that they can obtain from the ParseData are, for example by making a call to ParseData.getMetadata().get(ParseData.CONTENT_TYPE) to get the content type or a call to ParseData.getMetadata().set(ParseData.CONTENT_TYPE, ""text/xml""); Of course, this wouldn't preclude users from doing what they are currently doing, it would just provide a standard method of obtaining some of the more common, critical metadata without pouring over the code base to figure out what they are named.

I'll contribute a patch near the end of the this week, or beg. of next week that addresses this issue.
","Duplicated Code, Long Method, , , , "
"Rename Method,Extract Method,Inline Method,",Nutch 2.x upgrade to Gora 0.4 Nutch upgrade for GORA_94 branch has to be implemented. We can discuss the details in this issue.,"Duplicated Code, Long Method, , , "
"Rename Method,Move Method,Extract Method,Move Attribute,","Alternative Generator which can generate several segments in one parse of the crawlDB When using Nutch on a large scale (e.g. billions of URLs), the operations related to the crawlDB (generate - update) tend to take the biggest part of the time. One solution is to limit such operations to a minimum by generating several fetchlists in one parse of the crawlDB then update the Db only once on several segments. The existing Generator allows several successive runs by generating a copy of the crawlDB and marking the URLs to be fetched. In practice this approach does not work well as we need to read the whole crawlDB as many time as we generate a segment.

The patch attached contains an implementation of a MultiGenerator which can generate several fetchlists by reading the crawlDB only once. The MultiGenerator differs from the Generator in other aspects: 
* can filter the URLs by score
* normalisation is optional
* IP resolution is done ONLY on the entries which have been selected for fetching (during the partitioning). Running the IP resolution on the whole crawlDb is too slow to be usable on a large scale
* can max the number of URLs per host or domain (but not by IP)
* can choose to partition by host, domain or IP

Typically the same unit (e.g. domain) would be used for maxing the URLs and for partitioning; however as we can't count the max number of URLs by IP another unit must be chosen while partitioning by IP. 
We found that using a filter on the score can dramatically improve the performance as this reduces the amount of data being sent to the reducers.

The MultiGenerator is called via : nutch org.apache.nutch.crawl.MultiGenerator ...
with the following options :
MultiGenerator <crawldb> <segments_dir> [-force] [-topN N] [-numFetchers numFetchers] [-adddays numDays] [-noFilter] [-noNorm] [-maxNumSegments num]

where most parameters are similar to the default Generator - apart from : 
-noNorm (explicit)
-topN : max number of URLs per segment
-maxNumSegments : the actual number of segments generated could be less than the max value select e.g. not enough URLs are available for fetching and fit in less segments

Please give it a try and less me know what you think of it

Julien Nioche
http://www.digitalpebble.com




","Duplicated Code, Long Method, , , , "
"Rename Method,","Add alias capability in parse-plugins.xml file that allows mimeType->extensionId mapping Jerome and I have been talking about an idea to address the current issue raised by Stefan G. about having a mapping of mimeType->list of pluginIds rather than mimeType->list of extensionIds in the parse-plugins.xml file. We've come up with the following proposed update that would seemingly fix this problem.

We propose to have the concept of ""aliases"" in the parse-plugins.xml file, defined at the end of the file, something lie:

<parse-plugins>
....

<mimeType name=""text/html"">
<plugin id=""parse-html""/>
</mimeType>

.....

<aliases>
<alias name=""parse-html""
extension-point=""org.apache.nutch.parse.html.HtmlParser""/>

....
<alias name=""parse-html2"" extension-point=""my.other.html.Parser""/>

....
</aliases>
< /parse-plugins>



What do you guys think? This approach would be flexible enough to allow the mapping of extensionIds to mimeTypes, but without impacting the current ""pluginId"" concept.

Comments welcome.",", "
"Extract Interface,Move Method,Extract Method,Move Attribute,","Integrate Solr/Nutch Hi:

After trying out Sami's patch regarding Solr/Nutch. Can be found here (http://blog.foofactory.fi/2007/02/online-indexing-integrating-nutch-with.html) and I can confirm it worked :-) And that lead me to request the following :

I would be very very great full if this could be included in nutch 0.9 as I am trying to eliminate my python based crawler which post documents to solr. As I am in the corporate enviornment I can't install trunk version in the production enviornment thus I am asking this to be included in 0.9 release. I hope my wish would be granted.

I look forward to get some feedback.

Thank you.

","Duplicated Code, Long Method, , , , Large Class, "
"Rename Method,Extract Method,","Bad language identifier plugin performances As reported by Stefan Groschupf (http://www.mail-archive.com/nutch-developers@lists.sourceforge.net/msg04090.html) the language identifier plugin consumes a lot of processing time.
Some optimizations and/or configuration options are required.","Duplicated Code, Long Method, , "
"Rename Method,Extract Method,","nutchgora Configure minimum throughput for fetcher Like trunk, nutchgora should also have a feature to configure the fetcher with a minimum throughput. (See NUTCH-1067 for the work done by Markus).

It's implemented in almost the same way, except that the number of times throughput falls below threshold is measured sequentially. (The counter is reset when throughput is healthy again; this should work even better against temporary dips).

Defaults to disabled. Will commit later today if there is no objection.","Duplicated Code, Long Method, , "
"Rename Class,Rename Method,Move Method,Extract Method,",Fetcher improvements Fetcher improvements.,"Duplicated Code, Long Method, , , "
"Move Class,Move Method,","Refactor Fetcher in trunk Put simply [Fetcher|https://github.com/apache/nutch/blob/trunk/src/java/org/apache/nutch/fetcher/Fetcher.java] is too big.
This is kinda strange as the size of this file is unique (I think) from every other class within Nutch. The others are reasonably well modularized and split into constituent classes which make sense.",", , "
"Rename Class,Extract Method,",Upgrade Carrot2 clustering plugin to the newest stable release (2.1) This issue upgrades Carrot2 search results clustering plugin to the newest stable version.,"Duplicated Code, Long Method, , "
"Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","remove static NutchConf Removing the static NutchConf.get is required for a set of improvements and new features.
+ it allows a better integration of nutch in j2ee or other systems.
+ it allows the management of nutch from a web based gui (a kind of nutch appliance) which will improve the usability and also increase the user acceptance of nutch
+ it allows to change configuration properties until runtime
+ it allows to implement NutchConf as a abstract class or interface to provide other configuration value sources than xml files. (community request)
","Duplicated Code, Long Method, , , , , "
"Rename Method,","Upgrade all instances of commons logging to slf4j (with log4j backend) Whilst working on another issue, I noticed that some classes still import and use commons logging for example HttpBase.java

{code}
import java.util.*;

// Commons Logging imports
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;

// Nutch imports
import org.apache.nutch.crawl.CrawlDatum;
{code}

At this stage I am unsure how many (if any others) still import and reply upon commons logging, however they should be upgraded to slf4j for branch-1.4.",", "
"Rename Method,Extract Method,","Use MultipleInputs in Injector to make it a single mapreduce job Currently Injector creates two mapreduce jobs:
1. sort job: get the urls from seeds file, emit CrawlDatum objects.
2. merge job: read CrawlDatum objects from both crawldb and output of sort job. Merge and emit final CrawlDatum objects.

Using MultipleInputs, we can read CrawlDatum objects from crawldb and urls from seeds file simultaneously and perform inject in a single map-reduce job.

Also, here are additional things covered with this jira:
1. Pushed filtering and normalization above metadata extraction so that the unwanted records are ruled out quickly.
2. Migrated to new mapreduce API
3. Improved documentation 
4. New junits with better coverage

Relevant discussion over nutch-dev can be found here:
http://mail-archives.apache.org/mod_mbox/nutch-dev/201401.mbox/%3CCAFKhtFyXO6WL7gyUV+a5Y1pzNtdCoqPz4jz_up_bkp9cJe80kg@mail.gmail.com%3E","Duplicated Code, Long Method, , "
"Rename Method,Move Method,","Upgrade to most recent JUnit 4.x to improve test flexibility I wanted to try using the @Ignore functionality within JUnit, however I don't think it is available in the current JUnit version we use in Nutch. We should upgrade.",", , "
"Move Class,Rename Method,Move Method,Extract Method,Move Attribute,","REST API refactoring I'd reviewed REST API code, and realized that it is old and clunky. I want make some refactoring of this part and propose these changes as patch.","Duplicated Code, Long Method, , , , "
"Rename Method,","Reduce dependency of Nutch on config files Currently many components in Nutch rely on reading their configuration from files. These files need to be on the classpath (or packed into a job jar). This is inconvenient if you want to manage configuration via API, e.g. when embedding Nutch, or running many jobs with slightly different configurations.

This issue tracks the improvement to make various components read their config directly from Configuration properties.",", "
"Rename Method,Extract Method,","REST API for Nutch This issue is for discussing a REST-style API for accessing Nutch.

Here's an initial idea:

* I propose to use org.restlet for handling requests and returning JSON/XML/whatever responses.
* hook up all regular tools so that they can be driven via this API. This would have to be an async API, since all Nutch operations take long time to execute. It follows then that we need to be able also to list running operations, retrieve their current status, and possibly abort/cancel/stop/suspend/resume/...? This also means that we would have to potentially create & manage many threads in a servlet - AFAIK this is frowned upon by J2EE purists...
* package this in a webapp (that includes all deps, essentially nutch.job content), with the restlet servlet as an entry point.

Open issues:

* how to implement the reading of crawl results via this API
* should we manage only crawls that use a single configuration per webapp, or should we have a notion of crawl contexts (sets of crawl configs) with CRUD ops on them? this would be nice, because it would allow managing of several different crawls, with different configs, in a single webapp - but it complicates the implementation a lot.","Duplicated Code, Long Method, , "
"Rename Method,Move Method,Pull Up Attribute,Move Attribute,",Refactor *Checker classes to use base class for common code The various Checker class implementations have quite a bit of duplicated code in them. This should be refactored for cleanliness and maintainability.,", , , Duplicated Code, "
"Rename Class,Extract Interface,Rename Method,Move Method,Extract Method,Move Attribute,",Hbase Integration This issue will track nutch/hbase integration,"Duplicated Code, Long Method, , , , Large Class, "
"Rename Class,Rename Method,","Precise data parsing using Jsoup CSS selectors As far as I know, currently Nutch 1.x and 2.x has no features to extract/parse exact contents for specific websites. I've developed a plugin {{parse-jsoup}} using Jsoup for my current project to extract precise content for site specific crawling using detailed XML configuration(field name, CSS-selector, attribute, extraction rules, data-type, default-value etc).

Please let me know if this feature seems relevant and currently not present in Nutch. I have also plan to export it into Nutch 1.x.",", "
"Move Class,Rename Class,Rename Method,Move Method,Extract Method,Move Attribute,",port pluggable indexing architecture to 2.x I would like to port the work done by Julien on NUTCH-1047 to 2.x. This issue should track that. It would be nice to do the upgrade in NUTCH-1486 before we do the upgrade so that people can get using with solr 4.x ASAP.,"Duplicated Code, Long Method, , , , "
"Rename Class,Extract Method,","Implement SSL Connection Test at TestNutchAPI Currently, testing of SSL is ignored at TestNutchAPI. We should complete the implementation.","Duplicated Code, Long Method, , "
"Rename Method,","Document webpage.avsc and host.avsc We can easily document Avro schema files as defined in the current Avro specification
http://avro.apache.org/docs/current/spec.html#schema_complex
We should document both of the above files to provide more meaningful comments in generated source.",", "
"Rename Method,","Enhance ParserFactory plugin selection policy The ParserFactory choose the Parser plugin to use based on the content-types and path-suffix defined in the parsers plugin.xml file.
The selection policy is as follow:
Content type has priority: the first plugin found whose ""contentType"" attribute matches the beginning of the content's type is used. 
If none match, then the first whose ""pathSuffix"" attribute matches the end of the url's path is used.
If neither of these match, then the first plugin whose ""pathSuffix"" is the empty string is used.

This policy has a lot of problems when no matching is found, because a random parser is used (and there is a lot of chance this parser can't handle the content).
On the other hand, the content-type associated to a parser plugin is specified in the plugin.xml of each plugin (this is the value used by the ParserFactory), AND the code of each parser checks itself in its code if the content-type is ok (it uses an hard-coded content-type value, and not uses the value specified in the plugin.xml => possibility of missmatches between content-type hard-coded and content-type delcared in plugin.xml).

A complete list of problems and discussion aout this point is available in:
* http://www.mail-archive.com/nutch-user%40lucene.apache.org/msg00744.html
* http://www.mail-archive.com/nutch-dev%40lucene.apache.org/msg00789.html
",", "
"Rename Method,","Add protocol-htmlunit HtmlUnit is, opposed to other Javascript enabled headless browsers, a portable library and should therefore be better suited for very large scale crawls. This issue is an attempt to implement protocol-htmlunit.",", "
"Extract Method,Move Attribute,","Upgrade Nutch to Hadoop 0.7 Upgrade Nutch to Hadoop 0.7, and replace all occurences of UTF8 with Text. UTF8 is deprecated and its use is discouraged due to its limitations.

This change will break API, in the sense that all third-party additions will have to be updated to use new APIs that use Text instead of UTF8 in method parameters.

This change also breaks backward compatibility of data in CrawlDb, LinkDb and segments. A tool to upgrade CrawlDb, LinkDb and segments can be created to facilitate the upgrade path.","Duplicated Code, Long Method, , , "
"Extract Method,Move Attribute,","Jexl support in generator job Generator should support Jexl expressions. This would make it much easier to implement focussing crawlers that rely on information stored in the CrawlDB. With the HostDB it is possible to restrict the generator to select only interesting records but it is very cumbersome and involves domainblacklist-urlfiltering.

With Jexl support, it is no hassle!

Crawl only english records:
{code}
bin/nutch generate crawl/crawldb/ crawl/segments/ -expr ""(lang == 'en'')""
{code}

Crawl only HTML records:
{code}
bin/nutch generate crawl/crawldb/ crawl/segments/ -expr ""(Content_Type == 'text/html' || Content_Type == 'application/xhtml+xml')""
{code}

Keep in mind:
* Jexl doesn't allow a hyphen/minus in field identifier, they are transformed to underscores
* string literals must be in quotes, only surrounding qoute needs to be escaped by backslash
","Duplicated Code, Long Method, , , "
"Rename Class,Rename Method,Move Method,","speed up indexing by eliminating the indexreducer Currently the indexer in Nutchgora consists of both mappers and reduces. But the reduce code does not actually iterate over any (grouped/sorted) values. It simply indexes individual key/value (String/Webpage) pairs. Therefore by moving this indexing code to the mapper we can eliminate the reduce step therefore making the indexing job much faster. (No more unnecessary spilling to disk/network and no cpu wasted to sorting).

Note this is not (directly) applicable to trunk because trunk uses a quite different approach. Different types of input are combined to a single value in the reducer. Although I think it is possible to implement a similar optimization I am not sure how to do this. So if anyone wants this for trunk too feel free to implement a similar patch.",", , "
"Move Method,Move Attribute,",Improved Tokenization for Similarity Scoring plugin This patch would add Lucene based tokenization to the cosine similarity plugin and clean up the code currently present.,", , , "
"Rename Method,","Scoring filter should distribute score to all outlinks at once Currently ScoringFilter.distributeScoreToOutlink, as its name implies, takes only a single outlink and works on that. I would suggest that we change it to distributeScoreToOutlink_s_ so that it would take all the outlinks of a page at once. This has several advantages:

1) A ScoringFilter plugin returns a single adjust datum to set its score instead of returning several.
2) A ScoringFilter plugin can change the score of the original page (via adjust datum) even if there are no outlinks. This is useful if you have a ScoringFilter plugin that, say, scores pages based on content instead of outlinks.
3) Since the ScoringFilter plugin recieves all outlinks at once, it can make better decisions on how to distribute the score. For example, right now it is not possible to create a plugin that always distributes exactly a page's 'cash' to outlinks(that is, if a page has score 5, it will always distribute exactly 5 points to its outlinks no matter what the internal/external factors are) if internal / external score factors are not 1.",", "
"Rename Method,","Redirected urls should be handled more cleanly (more like an outlink url) This is specifically for Nutch2.x. Handling a redirects url like an outlink is much more cleaner because this makes it more simple to trace how new urls are added to the webpage database. Instant fetching of redirects won't work, but this is a small price to pay. (Note that this currently does not work at all, because the http.max.redirect property has no effect). Will be attaching a patch in the upcoming days.",", "
"Move Method,Extract Method,","needs 'character encoding' detector transferred from:
http://sourceforge.net/tracker/index.php?func=detail&aid=995730&group_id=59548&atid=491356
submitted by:
Jungshik Shin

this is a follow-up to bug 993380 (figure out 'charset'
from the meta tag).

Although we can cover a lot of ground using the 'C-T'
header field in in the HTTP header and the
corresponding meta tag in html documents (and in case
of XML, we have to use a similar but a different
'parsing'), in the wild, there are a lot of documents
without any information about the character encoding
used. Browsers like Mozilla and search engines like
Google use character encoding detectors to deal with
these 'unlabelled' documents. 

Mozilla's character encoding detector is GPL/MPL'd and
we might be able to port it to Java. Unfortunately,
it's not fool-proof. However, along with some other
heuristic used by Mozilla and elsewhere, it'll be
possible to achieve a high rate of the detection. 

The following page has links to some other related pages.

http://trainedmonkey.com/week/2004/26

In addition to the character encoding detection, we
also need to detect the language of a document, which
is even harder and should be a separate bug (although
it's related).","Duplicated Code, Long Method, , , "
"Move Class,Rename Class,Rename Method,Move Method,Extract Method,Pull Up Attribute,Move Attribute,",Upgrade the code base from org.apache.hadoop.mapred to org.apache.hadoop.mapreduce Nutch is still using the deprecated org.apache.hadoop.mapred dependency which has been deprecated. It need to be updated to org.apache.hadoop.mapreduce dependency.,"Duplicated Code, Long Method, , , , Duplicated Code, "
"Move Class,Move And Rename Class,Rename Method,Move Method,Extract Method,Move Attribute,","Pluggable indexing backends One possible feature would be to add a new endpoint for indexing-backends and make the indexing plugable. at the moment we are hardwired to SOLR - which is OK - but as other resources like ElasticSearch are becoming more popular it would be better to handle this as plugins. Not sure about the name of the endpoint though : we already have indexing-plugins (which are about generating fields sent to the backends) and moreover the backends are not necessarily for indexing / searching but could be just an external storage e.g. CouchDB. The term backend on its own would be confusing in 2.0 as this could be pertaining to the storage in GORA. 'indexing-backend' is the best name that came to my mind so far - please suggest better ones.

We should come up with generic map/reduce jobs for indexing, deduplicating and cleaning and maybe add a Nutch extension point there so we can easily hook up indexing, cleaning and deduplicating for various backends.","Duplicated Code, Long Method, , , , "
"Rename Class,Move And Rename Class,Move Class,Move Attribute,","Flexible URL normalization This patch is a heavily restructured version of the patch in NUTCH-253, so much that I decided to create a separate issue. It changes the URL normalization from a selectable single class to a flexible and context-aware chain of normalization filters.

Highlights:

* rename all *UrlNormalizer* to *URLNormalizer* for consistency.

* use a ""chained filter"" pattern for running several normalizers in sequence

* the order in which normalizers are executed is defined by ""urlnormalizer.order"" property, which lists space-separated implementation classes. If there are more normalizers active than explicitly named on this list, they will be run in random order after the ones specified on the list are executed.

* define a set of contexts (or scopes) in which normalizers may be called. Each scope can have its own list of normalizers (via ""urlnormalizer.scope.<scope_name>"" property) and its own order (via ""urlnormalizer.order.<scope_name>"" property). If any of these properties are missing, default settings are used.

* each normalizer may further select among many configurations, depending on the context in which it is called, using a modified API:

URLNormalizer.normalize(String url, String scope);

* if a config for a given scope is not defined, then the default config will be used.

* several standard contexts / scopes have been defined, and various applications have been modified to attempt using appropriate normalizer in their context.

* all JUnit tests have been modified, and run successfully.

NUTCH-363 suggests to me that further changes may be required in this area, perhaps we should combine urlfilters and urlnormalizers into a single subsystem of url munging - now that we have support for scopes and flexible combinations of normalizers we could turn URLFilters into a special case of normalizers (or vice versa, depending on the point of view) ...",", , "
"Move Method,Move Attribute,","DistributedSearch does not update search servers added to search-servers.txt on the fly DistributedSearch client updates the search servers added to the search-servers.txt file on the fly. 
This patch will updates the search servers on the fly and the client does not need a restart.",", , , "
"Move Class,Rename Method,Move Method,Extract Method,Move Attribute,",Hbase Integration This issue will track nutch/hbase integration,"Duplicated Code, Long Method, , , , "
"Rename Method,Move Method,Extract Method,Move Attribute,","Bulk REST API to retrieve crawl results as JSON It would be useful to be able to retrieve results of a crawl as JSON. There are a few things that need to be discussed:

* how to return bulk results using Restlet (WritableRepresentation subclass?)

* what should be the format of results?

I think it would make sense to provide a single record retrieval (by primary key), all records, and records within a range. This incidentally matches well the capabilities of the Gora Query class :)","Duplicated Code, Long Method, , , , "
"Rename Method,","Nutch should delegate compression to Hadoop Some data structures within nutch (such as Content, ParseText) handle their own compression. We should delegate all compressions to Hadoop. 

Also, nutch should respect io.seqfile.compression.type setting. Currently even if io.seqfile.compression.type is BLOCK or RECORD, nutch overrides it for some structures and sets it to NONE (However, IMO, ParseText should always be compressed as RECORD because of performance reasons).",", "
"Push Down Method,Move Method,Push Down Attribute,","Increase fetching speed There have been some discussion on nutch mailing lists about fetcher being slow, this patch tried to address that. the patch is just a quich hack and needs some cleaning up, it also currently applies to 0.8 branch and not trunk and it has also not been tested in large. What it changes?

Metadata - the original metadata uses spellchecking, new version does not (a decorator is provided that can do it and it should perhaps be used where http headers are handled but in most of the cases the functionality is not required)

Reading/writing various data structures - patch tries to do io more efficiently see the patch for details.

Initial benchmark:

A small benchmark was done to measure the performance of changes with a script that basically does the following:
-inject a list of urls into a fresh crawldb
-create fetchlist (10k urls pointing to local filesystem)
-fetch
-updatedb

original code from 0.8-branch:
real 10m51.907s
user 10m9.914s
sys 0m21.285s

after applying the patch
real 4m15.313s
user 3m42.598s
sys 0m18.485s

",", , , , "
"Move Class,Move Method,Extract Method,Move Attribute,","Upgrade parse-tika to use Tika 1.18 Tika 1.18 is released and NUTCH-2583 includes and upgrade of tika-core. 
See [howto_upgrade_tika|https://github.com/apache/nutch/blob/master/src/plugin/parse-tika/howto_upgrade_tika.txt].","Duplicated Code, Long Method, , , , "
"Move Class,Move Method,Move Attribute,","Consolidate code for Fetcher and Fetcher2 I'd like to consolidate a lot of the common code between Fetcher and Fetcher2.java.

It seems to me like there are the following differences:
- Fetcher relies on the Protocol to obey robots.txt and crawl delay settings whereas Fetcher2 implements them itself
- Fetcher2 uses a different queueing model (queue per crawl host) to accomplish the per-host limiting without making the Protocol do it.

I've begun work on this but want to check with people on the following:

- What reason is there for Fetcher existing at all since Fetcher2 seems to be a superset of functionality?

- Is it on the road map to remove the robots/delay logic from the Http protocol and make Fetcher2's delegation of duties the standard?

- Any other improvements wanted for Fetcher while I am in and around the code?",", , , "
"Move Method,Extract Method,",Support of Sitemaps in Nutch 2.x Sitemap support has to be implemented for 2.x branch. It is being discussed in NUTCH-1465 for trunk.,"Duplicated Code, Long Method, , , "
"Rename Method,","Improve forkjoin validation to allow same errorTo transitions It seems common that users will have the ""error to"" transition from every action go to the same action node (e.g. email action), which then goes to the kill node instead of just going to the kill node directly. When this is done in action nodes within a forkjoin path, the forkjoin validation doesn't allow it. We should improve the forkjoin validation code to allow the same ""error to"" transition, as long as it eventually leads to a kill node.",", "
"Rename Method,",Add a dryrun option for workflows Add a dryrun option for Workflows that would do all of the validation/parsing/checking/etc that normally happens when you submit a workflow but without actually submitting it.,", "
"Move Method,Move Attribute,","improve logic of purge service The current logic of the purge service is flat. I.e., WF purging only takes into account WF end time, it does not take into account that the WF was started by a COORD job. This means that completed WFs of a running COORD job could be purge if the COORD job runs for longer that the purge age.

One way of addressing this would be:

* WF purging only purges WF jobs started directly by a client call.
* COORD purging purges COORD jobs started directly by a client call. It also purges the WF jobs created by the COORD jobs being purged.
* BUNDLE purging purges BUNDLE jobs, and the corresponding COORD jobs and WF jobs.

This could be handled by a new property in the job beans 'job-owner'. Set to 'self' it would mean it can be purged by the same job type purger. If set to other value, then it is a higher level purger the one responsible for purging it.

This means that for a WF job started by COORD job started by a BUNDLE job, the WF job and the COORD job would have the BUNDLE job as owner, while the BUNDLE with have 'self' as owner.

This ownership propagation would also have
A caveat here would be how to handle sub-workflows. 
I guess we should check if the wf was created from coord, and if then let the coord purge take care of that, meaning wf purge does not purge wf started by coords.

Similarly, the same should also apply for sub-WFs.
",", , , "
"Move Class,Rename Class,Rename Method,Pull Up Method,Move Method,Extract Method,","SLA Support in Oozie Would like to have the following features in Oozie
- JMS notifications on SLA met, SLA start miss, SLA end miss and SLA duration miss
- Email alerting for SLA start miss, SLA end miss and SLA duration miss
- API to query SLA met/miss information. Currently the SLA information that can be queried is only SLA registration event and job status events. One has to calculate the actual misses from those. 
- A simple dashboard to view and query the SLA met/miss information built on the API mentioned above.","Duplicated Code, Long Method, , , Duplicated Code, "
"Push Down Method,Move Method,","Refactor action Main classes into sharelibs We should refactor {{PigMain}}, {{HiveMain}}, and {{SqoopMain}} classes into their respective sharelib so we can remove all of their dependencies from oozie-core. This will help prevent dependency issues in the future (e.g. different versions of antlr). These Main classes would then end up in the sharelib instead of the launcher jar. Their tests would also have to be moved to the sharelib, but not the ActionExecutors.",", , , "
"Rename Method,Move Method,Extract Method,Move Attribute,","Refactor classes from launcher jar into Oozie sharelib We should look into refactoring the classes that get put into the launcher jar into the Oozie sharelib (i.e. share/lib/oozie), which currently only has a json jar. Doing this would allow us to get rid of the launcher jar.","Duplicated Code, Long Method, , , , "
"Rename Method,Inline Method,","Compress lob columns before storing in database Storing huge data in lobs is very inefficient. Making Oozie compress the data before storing will reduce size of data to be stored in lobs and help in reducing the time for queries. Also most databases like oracle, mysql support storing lob data in tablerow (inline) if the data is of smaller size. Inline storage has much better performance compared to outline storage (storage outside of tablerow)",", , "
"Rename Method,Move Method,","Add ability to issue kill on Coordinator Action directly with id and nominal daterange If we want to kill a coordinator action in particular, there is no way to do this currently. Kill has to be issued on the Coordinator Job level and percolates through to all its actions. This JIRA is enhancing the kill command for coord action specifically.",", , "
"Rename Method,Extract Method,","Cut down on number of small files created to track a running action Oozie creates multiple files while running a action. This has been observed to be an overkill and can be consolidated (as applicable) into a lesser files. Advantages involve not only staying within user storage quotas but also reducing Namenode pressure in a large production environment.

static final String ACTION_CONF_XML = ""action.xml"";
public static final String ACTION_PREPARE_XML = ""oozie.action.prepare.xml"";
private static final String ACTION_OUTPUT_PROPS = ""output.properties"";
private static final String ACTION_STATS_PROPS = ""stats.properties"";
private static final String ACTION_EXTERNAL_CHILD_IDS_PROPS =
""externalChildIds.properties"";
private static final String ACTION_NEW_ID_PROPS = ""newId.properties"";
private static final String ACTION_ERROR_PROPS = ""error.properties"";

Consolidate and reduce the number of files required.","Duplicated Code, Long Method, , "
"Move Class,Extract Method,",Make sure HA works with a secure ZooKeeper We need to make sure that HA works with a secure ZooKeeper. This includes the SASL ACL setting that will prevent someone else from deleting the oozie znodes.,"Duplicated Code, Long Method, , "
"Rename Method,","Workflow performance optimizations Creating a combo JIRA for small performance optimizations.
1. changing from asynchronous action start to a synchronous one, to overcome the undue delay in transitioning from ::start:: control node to the actual first node, owing to a loaded queue. This delay has been observed to be close to 30 min at times in stress conditions.",", "
"Rename Method,Extract Method,","When an ApplicationMaster restarts, it restarts the launcher job When using Yarn, there are some situations in which the ApplicationMaster can be restarted (e.g. RM failover, the AM dies and another attempt is made, etc). 

When this happens, it starts the launcher job again, which will start over. So, if that launcher has already launched a job, we'll end up with two instances of the same job, which can be problematic. For example, if you have a Pig action, the Pig client might run a job, but then the launcher gets restarted by an AM restart and launches that same job again. 

We don't have a way of ""re-attaching"" to previously launched jobs; however, with YARN-1461 and MAPREDUCE-5699, we can use yarn tags to find anything the launcher previously launched that's running and kill them. We still have to start over, but at least we're not running two instances of a job at the same time.

Here's what we can do for each action type:
- Pig, Sqoop, Hive
-- Kill previously launched jobs and start over
- MapReduce (different because of the optimization)
-- Exit launcher if a previously launched job already exists
- Java, Shell
-- No out-of-the-box support for this
-- Like with other things, the Java action can take advantage of this like Pig, Sqoop, and Hive if the user adds some code
- DistCp
-- Not supported
- SSH, Email
-- N/A

The yarn tags won't be available until Hadoop 2.4.0, but is in the nightly (i.e. Hadoop 3.0.0-SNAPSHOT); and its obviously not in Hadoop 1.x. To be able to use the Yarn methods and the new methods for tagging, we can add a new type of Hadooplib called ""Hadoop Utils"" where we can put classes that are specific to a specific version of Hadoop; the other implementations can have dummy versions. For example, in the Hadoop-2 Hadoop Utils, we can put a method foo() that calls some yarn stuff but in the Hadoop-1 Hadoop Utils, the foo() method would either do the equivalent in MR1 or a no-op. So for now, I put some methods in the Hadoop-3 Hadoop Utils that use the tags and the Hadoop-1, Hadoop-2, and Hadoop-23 Hadoop Utils all have dummy implementations that don't do anything (so the existing behavior is preserved). The Hadoop Utils modules will allow us to take advantage of Hadoop 2 only features in the future, while still being able to compile against Hadoop 1; so it's not just limited to this feature.","Duplicated Code, Long Method, , "
"Rename Class,Move Class,Rename Method,Push Down Method,Move Method,Extract Method,Move Attribute,","Create Oozie Application Master for YARN After the first release of oozie on hadoop 2, it will be good if users can set execution engine in oozie conf, be it YARN AM or traditional MR. We can target this for post oozie 4.1 release.","Duplicated Code, Long Method, , , , , "
"Move Method,Extract Method,Move Attribute,","Improvement in Purge service Current purge service of oozie have some performance issues and it might help to look at the queries and indexes to improve the the purge service.
","Duplicated Code, Long Method, , , , "
"Rename Method,","use pom properties rather than specific version numbers in the pom files of hbaselibs, hcataloglibs, sharelib, etc version numbers (hbase, hive, hcatalog, sqoop, etc) are hard coded in the pom files.",", "
"Rename Class,Rename Method,Move Method,Extract Method,Move Attribute,","Devise a way to turn off SLA alerts for bundle/coordinator flexibly From user:
Need to turn off the SLA miss alerts in jobs when the bundle is suspended for
grid upgrades and similar work so that when it's resumed we aren't flooded with a bunch of alerts.","Duplicated Code, Long Method, , , , "
"Rename Method,","Use Curator leader latch instead of checking the order of Oozie servers We currently have a few tasks (e.g. Purging old jobs) that we only want to do in one Oozie server. We currently simply check which Oozie server is first in ZooKeeper's list of servers (i.e. the order they connected). We haven't seen any problems with this, but it might be a good idea to replace this with Curator's leader-latch, which sounds more robust. 

The leader path should probably be something like ""/services/leader"".

Make sure errors and edge cases are handled properly, including what happens when the leader dies without unregistering, etc.

http://curator.apache.org/curator-recipes/leader-latch.html",", "
"Rename Method,Move Method,Extract Method,","Specifying coordinator input datasets in more logical ways All dataset instances specified as input to coordinator, currently work on AND logic i.e. ALL of them should be available for workflow to start. We should enhance this to include more logical ways of specifying availability criteria e.g.
* OR between instances
* minimum N out of K instances
* delta datasets (process data incrementally)

Use-cases for this:
* Different datasets are BCP, and workflow can run with either, whichever arrives earlier.
* Data is not guaranteed, and while $coord:latest allows skipping to available ones, workflow will never trigger unless mentioned number of instances are found.
* Workflow is like a ‘refining’ algorithm which should run after minimum required datasets are ready, and should only process the delta for efficiency.

This JIRA is to discuss the design and then the review the implementation for some or all of the above features.","Duplicated Code, Long Method, , , "
"Extract Method,Move Attribute,","bulk kill, suspend, resume jobs using existing filter, offset, len, and jobtype params Currently, there is no bulk write operations in ""jobs"" API. We would like to first introduce a bulk kill operation that kills all jobs which satisfy the filter. The desired usage will be {noformat}oozie jobs -oozie http://localhost:11000/oozie -kill -filter name=something{noformat}","Duplicated Code, Long Method, , , "
"Rename Method,Extract Method,","Add a way to specify a default JT/RM and NN Oozie is cluster agnostic, which is why we require an RM/JT and NN per action in your workflow (or once via the <global> section). In practice, many users use one Oozie server per cluster, so it's an extra burden for them to have to specify this all the time. It would be convenient if we added configuration properties to oozie-site that would let you specify a default RM/JT and NN to use. 

This way, these users could completely omit the {{<job-tracker>}} and {{<name-node>}} fields from their workflows; as an added benefit, they can easily update these values if they ever rename/move their RM/JT or NN. We'd of course still allow specifying {{<job-tracker>}} and {{<name-node>}} in each action and {{<global>}} to allow individual workflows or actions to override the default.","Duplicated Code, Long Method, , "
"Rename Method,Extract Method,",Add support for bundle:conf() function ,"Duplicated Code, Long Method, , "
"Rename Method,Extract Method,Inline Method,",Add ability to provide Hive and Hive 2 Action queries inline in workflows I'd like the ability to specify the hive query within the workflow.xml for the HS2 action.,"Duplicated Code, Long Method, , , "
"Rename Method,Pull Up Method,Extract Method,","[fluent-job] Minimum Viable Fluent Job API Users often complain about the XML they have to write for Oozie jobs. It would be nice if they could write them in something like Java, but we don't want to have to maintain a separate Java API for this. I was looking around and saw that JAXB might be the right thing here. From what I can tell, it lets you create Java classes from XSD schemas. So, we should be able to auto-generate a Java API for writing Oozie jobs, without having to really maintain it. 

We should investigate if this is feasible and, if so, implement it. 

Some useful looking links: 
* [JAXB overview|https://en.wikipedia.org/wiki/Java_Architecture_for_XML_Binding] 
* [JAXB description|https://jaxb.java.net/2.2.11/docs/ch03.html] 
* [Maven JAXB plugin|https://java.net/projects/maven-jaxb2-plugin/pages/Home] 
* [Apache Falcon|https://falcon.apache.org] 

Key features: 
* must have: 
** inside a {{fluent-job-api}} artifact 
** able to create workflow / coordinator / bundle definitions programmatically 
** synchronizing each and every XSD change on rebuild 
** can write {{workflow.xml}}, {{coordinator.xml}}, {{bundle.xml}}, and {{jobs.properties}} artifacts of every XSD version 
** cloneability of workflow etc. {{Object}} s 
** perform cross checks, e.g. that the workflow graph is a DAG 
** only latest XSD versions should be supported as must have 
* nice to have: 
** XSD version(s) can be provided. When not provided, latest ones are considered as valid 
** implement a [*fluent API*|https://en.wikipedia.org/wiki/Fluent_interface] 
** have a Python / Jython / Py4J REPL to make it easy to experiment with also for data engineers / data scientists 
** create documentation about usage 
** can read {{workflow.xml}}, {{coordinator.xml}}, {{bundle.xml}}, and {{jobs.properties}} artifacts of every XSD version 
** can convert between XSD versions 
** support XSD change on the fly (within REPL) 
** support HDFS reads / writes 
** support dry run on an Oozie server to perform checks","Duplicated Code, Long Method, , Duplicated Code, "
"Rename Method,","Allow table drop in hcat prepare The hcat prepare only allows to drop partitions. It would be nice to also allow dropping of table, depending on the URL. The current format of the URL is
{code}hcat://[metastore server]:[port]/[database name]/[table name]/[partkey1]=[value];[partkey2]=[value]{code}
where at least one partition must be provided, otherwise the prepare step fails with the fololwing exception.
{code}
Starting the execution of prepare actions
Creating HCatClient for user=ehsan.haq (auth:SIMPLE) and server=thrift://datavault-prod-app2.internal.machines:9083
Prepare execution in the Launcher Mapper has failed
Failing Oozie Launcher, Main class [org.apache.oozie.action.hadoop.SqoopMain], exception invoking main(), Error trying to drop hcat://datavault-prod-app2.internal.machines:9083/test_rdbms_import_2015110600/test
org.apache.oozie.action.hadoop.LauncherException: Error trying to drop hcat://datavault-prod-app2.internal.machines:9083/test_rdbms_import_2015110600/test
at org.apache.oozie.action.hadoop.LauncherMapper.map(LauncherMapper.java:178)
at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)
at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)
at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:415)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: org.apache.oozie.action.hadoop.LauncherException: Error trying to drop hcat://datavault-prod-app2.internal.machines:9083/test_rdbms_import_2015110600/test
at org.apache.oozie.action.hadoop.HCatLauncherURIHandler.delete(HCatLauncherURIHandler.java:64)
at org.apache.oozie.action.hadoop.PrepareActionsDriver.execute(PrepareActionsDriver.java:89)
at org.apache.oozie.action.hadoop.PrepareActionsDriver.doOperations(PrepareActionsDriver.java:67)
at org.apache.oozie.action.hadoop.LauncherMapper.executePrepare(LauncherMapper.java:446)
at org.apache.oozie.action.hadoop.LauncherMapper.map(LauncherMapper.java:174)
... 8 more
Caused by: java.net.URISyntaxException: URI path is not in expected format: hcat://datavault-prod-app2.internal.machines:9083/test_rdbms_import_2015110600/test
at org.apache.oozie.util.HCatURI.parse(HCatURI.java:66)
at org.apache.oozie.util.HCatURI.<init>(HCatURI.java:52)
at org.apache.oozie.util.HCatURI.<init>(HCatURI.java:48)
at org.apache.oozie.action.hadoop.HCatLauncherURIHandler.delete(HCatLauncherURIHandler.java:52)
... 12 more

Oozie Launcher failed, finishing Hadoop job gracefully
{code}

h3. Suggestion
In the URL if the partition is not provided then it should delete the entire table.",", "
"Rename Method,Extract Method,","oozie-setup.sh sharelib create takes a long time on large clusters When cluster has 256+ nodes it can take up to 5 minutes to create a sharelib. 
Copy the tarball itself takes only around 10 seconds. It seems like performance could be improved by loading files concurrently in many threads.","Duplicated Code, Long Method, , "
"Move And Rename Class,Move Method,Extract Method,Move Attribute,","Completely rewrite GraphGenerator code The Web UI currently generates a graph of the workflow DAG as a png image that it can show to the user ({{GraphGenerator}} class). Unfortunately, there are a number of downsides to the current implementation:
# The image is generated server-side, which doesn't scale well and eats up lots of memory
#- To help combat this issue, we only generate graphs for workflows with less than 25 nodes and we disabled the refresh button in the UI
# It's slow
# It uses a library ({{net.sf.jung}} from http://jung.sourceforge.net), which hasn't been updated since 2010. 
#- This library also has a dependency on a fork of Commons-Collections ({{net.sourceforge.collections}} from http://sourceforge.net/projects/collections/), which similarly hasn't been updated since 2010. This is a problem because we can't update Commons-Collections when there are security concerns such as COLLECTIONS-580 (though Oozie is *not* susceptible to this attack).

It would be good to do a complete rewrite, using a different library and remove Jung and the Commons-Collections fork. Whatever we choose, it should an svg which will draw the image on the browser.","Duplicated Code, Long Method, , , , "
"Rename Method,Extract Method,","Ability to use local paths for the sharelib With OOZIE-2590, as part of OOZIE-1770 Oozie on Yarn work, Oozie now has full control over the classpath given to the Launcher AM. In a cluster where all nodes have everything installed locally (in the same paths), it should be possible to have the Launcher AM reference the local jars instead of having to localize them from HDFS.

For example, if you have Hive installed on all nodes at {{/usr/lib/hive/}} and all Hive jars under {{/usr/lib/hive/lib/}}, we could have the Launcher AM add {{/usr/lib/hive/lib}} to its classpath. This saves on the overhead of localizing the same jars from the hive sharelib in HDFS. 

I think the best way to implement this is to augment the [Sharelib Mapping File|https://oozie.apache.org/docs/4.2.0/AG_Install.html#Oozie_Share_Lib] feature to accept {{file:///}} paths.

If we had this also work with the ""oozie"" sharelib and the Oozie jars in the individual sharelibs (e.g. have the Mapping file take comma-separated dirs/jars), then in a cluster with everything installed on all of the nodes, you wouldn't need to bother with the sharelib at all!","Duplicated Code, Long Method, , "
"Rename Method,","Deprecate Instrumentation in favor of Metrics OOZIE-1817 added the option to use DropWizard Metrics instead of our homegrown Instrumentation. We left the Instrumentation as the default for compatibility; in Oozie 5, we should drop Instrumentation and only have Metrics. 

We can also use this opportunity to clean up the code and interface for Metrics, which currently has to conform to Instrumentation for pluggability. 

---- 

Update: for 5.0.0 we only deprecate {{InstrumentationService}}, and make {{MetricsInstrumentationService}} the default.",", "
"Rename Method,","Extend HTTPS configuration settings for embedded Jetty Regarding HTTPS settings, currently Oozie only support {{oozie.https.include.protocols}} and {{oozie.https.exclude.cipher.suites}} (introduced by OOZIE-2666).

However, Jetty SslContextFactory supports the following configurations:
* excludeProtocols
* includeProtocols
* excludeCipherSuites
* includeCipherSuites

To have more control over employed protocols and cipher suites, we should extend current implementation to allow users to configure {{excludeProtocols}} and {{includeCipherSuites}}. Sensible defaults are also needed.",", "
"Rename Class,Rename Method,Extract Method,","Oozie should handle transient database problems There can be problems when Oozie cannot update the database properly. Recently, we have experienced erratic behavior with two setups:

* MySQL with the Galera cluster manager. Galera uses cluster-wide optimistic locking which might cause a transaction to rollback if there are two or more parallel transaction running and one of them cannot complete because of a conflict.

* MySQL with Percona XtraDB Cluster. If one of the MySQL instances is killed, Oozie might get ""Communications link failure"" exception during the failover.

The problem is that failed DB transactions later might cause a workflow (which are started/re-started by RecoveryService) to get stuck. It's not clear to us how this happens but it has to do with the fact that certain DB updates are not executed.

The solution is to use some sort of retry logic with exponential backoff if the DB update fails. We could start with a 100ms wait time which is doubled at every retry. The operation can be considered a failure if it still fails after 10 attempts. These values could be configurable. We should discuss initial values in the scope of this JIRA.

Note that this solution is to handle *transient* failures. If the DB is down for a longer period of time, we have to accept that the internal state of Oozie is corrupted.","Duplicated Code, Long Method, , "
"Move Class,Move Method,Extract Method,Move Attribute,","Improve Spark options parsing There are two issues w/ Spark action's argument parsing within {{SparkMain}}

h5. Driver and executor extra classpaths: equals sign used

When the user specifies {{\-\-conf spark.executor.extraClassPath=XYZ}} or {{\-\-conf spark.driver.extraClassPath=ABC}}, the option {{\-\-conf}} will be added to {{sparkArgs}}. Then when the code tries to evaluate {{spark.executor.extraClassPath=XYZ}}, it uses special logic and set {{addToSparkArgs = false}}. As a result there will be a extra {{\-\-conf}} in the {{sparkArgs}} eventually.

For example: {{\-\-conf spark.executor.extraClassPath=XYZ \-\-conf otherProperty=ABC}} will become {{\-\-conf \-\-conf otherProperty=ABC}}, which will cause spark job submit failure later.

We might need to remove one prior {{\-\-conf}} in {{sparkArgs}} if the current evaluated {{opt}} is {{EXECUTOR_CLASSPATH}} or {{DRIVER_CLASSPATH}}.

h5. User provided files and archives: equals sign used

For the following workflow XML snippet:
{code:xml}
< spark-opts>--files=${nameNode}/home/share/hive-site.xml --num-executors 4 --executor-memory 7g --driver-memory 7g</spark-opts>
{code}

the {{\-\-files=$\{nameNode\}/home/share/hive-site.xml}} {{opt}} will be placed into {{sparkArgs}} in previous Oozie version without any modification, because we don't have special handling for {{\-\-files}} {{opt}}.

If the user specifies {{\-\-files=$\{nameNode\}/home/share/hive-site.xml --num-executor 4}}, then {{SparkMain}} code treats {{\-\-num-executor}} as a file path / name. That caused the issue as I described in my previous comment. We might need to change the handling logic for {{FILES_OPTION}} and {{ARCHIVES_OPTION}} to be the same to {{DRIVER_CLASSPATH_OPTION}}.","Duplicated Code, Long Method, , , , "
"Rename Method,","Enable definition of admin users using oozie-site.xml Currently the list of admin users is defined in the {{adminusers.txt}} file hard coded to the Oozie config dir. For a more streamlined solution, we could define the list of admin users via {{oozie-site.xml}} by introducing the following configuration, which receives the comma separated values of the users that are admins. 

{{oozie.service.AuthorizationService.admin.users}}",", "
"Rename Class,Pull Up Method,Extract Method,","[client] [ui] Improved SLA filtering options Currently we can apply a range of filters on top of {{V2SLAServlet}} that can be used in a rich but undocumented set of ways: 
* {{id}} 
* {{parent_id}} 
* {{event_status}} 
* {{app_name}} 
* {{nominal_start}} 
* {{nominal_end}} 

Need to refactor {{V2SLAServlet}} to feature: 
* a richer set of {{SLAEvent}}, {{SLARegistration}}, and {{SLASummary}} filtering based on their attributes 
* filter options will always be {{AND}}-ed, never {{OR}}-ed to each other 
* maintain compatibility with the parameter names and behavior used thus far 
* remove {{SLASummaryFilter}} and refactor {{SLASummaryGetForFilterJPAExecutor}} as just another possibility for confusion 
* document new functionality with rich use case / example library so that users can leverage","Duplicated Code, Long Method, , Duplicated Code, "
"Move Method,Inline Method,Move Attribute,",Upgrade Derby to 10.14.1.0 We should upgrade Derby to 10.14.1.0,", , , , "
"Rename Method,","[fluent-job] Create error handler ACTION only if needed The Shell and MultipleShellActions example of the Fluent Job API generates multiple actions with the same name ({{email-on-error}}) which gives {{E0705}} error code. 

For MultipleShellActions the generated XML: 
{noformat} 
Workflow job definition generated from API jar: 
< ?xml version=""1.0"" encoding=""UTF-8"" standalone=""yes""?> 
< workflow:workflow-app xmlns:email=""uri:oozie:email-action:0.2"" xmlns:workflow=""uri:oozie:workflow:1.0"" xmlns:shell=""uri:oozie:shell-action:1.0"" name=""shell-example""> 
<workflow:start to=""parent""/> 
<workflow:kill name=""kill""> 
<workflow:message>Action failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</workflow:message> 
</workflow:kill> 
<workflow:action name=""email-on-error""> 
<email:email> 
<email:to>somebody@apache.org</email:to> 
<email:subject>Workflow error</email:subject> 
<email:body>Shell action failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</email:body> 
</email:email> 
<workflow:ok to=""kill""/> 
<workflow:error to=""kill""/> 
</workflow:action> 
<workflow:action name=""parent""> 
... 
</workflow:action> 
<workflow:decision name=""decision1""> 
... 
</workflow:decision> 
<workflow:action name=""email-on-error""> 
... 
</workflow:action> 
<workflow:action name=""happy-path-0""> 
... 
</workflow:action> 
<workflow:decision name=""decision2""> 
... 
</workflow:action> 
... 
{noformat} 

The error message: 

{noformat} 
bin/oozie job -oozie http://localhost:11000/oozie -runjar fluenttest.jar -config job.properties -verbose 
... 
Error: E0705 : E0705: Nnode already defined, node [email-on-error] 
{noformat} 

The Shell example also creates an XML with multiple {{email-on-error}} actions.",", "
"Move Method,Extract Method,","[SSH action] Optimize process streams draining OOZIE-3354 improved {{SshActionExecutor}} to avoid {{Process#waitFor()}} blocks and modified the {{drainBuffers}} method to keep draining the standard output (and standard error) continuously. 

Right now the speed of the drain is hardwired. As long as the process is running the method only reads 1024 bytes in each cycle (half a second) which can take very long time if we want to drain several megabytes (for instance {{oozie.servlet.CallbackServlet.max.data.len}} is increased). 

Let's optimize the draining. 

We can either read 1024 bytes multiple times in each cycle (as long as there are data in the buffer), or we can increase the value of the buffer size (1024). 

In the latter case the default of the buffer size could be half of the {{oozie.servlet.CallbackServlet.max.data.len}} value, but we also need an additional property to specify the buffer size (to avoid memory problems because of using a very big buffer). We can keep 1024 as a minimum buffer size. 

It would be also useful to refactor the code and put the buffer draining into a separate class and create unit tests for the class. Using this class in {{ShellMain}} to avoid code duplication would also be very useful, but we have to fix OOZIE-3359 first.","Duplicated Code, Long Method, , , "
"Rename Method,Extract Method,","Oozie to allow drill down to hadoop job's details High-level Requirements:
-----------------------------------
Since Oozie is designed as the gateway to grid, we need to support WS API for most common hadoop commands through Oozie. User doesn't want to go to multiple system to get the required data. Based on these, we propose to implement the following requirements into Oozie.

R1: Oozie will provide WS endpoints to get hadoop job details (including job counters).

R2: It will support both types of hadoop jobs : MR job created for MR action, MR jobs created as part of pig script.

R3: In addition, for pig action, oozie will provide a way to query the pig stats.

Proposed design:
----------------------
D1: Oozie will store the *summary* jobcounter /pigstats into oozie DB. The items in the summary stats will be determined by oozie to limit the size. However,the commonly used stats will be include into the summary. It is important to note that summary information will be collected *after* the job finished.

D2: If the user asks for *details* hadoop job stats , the user needs to query using different WS API. In this query, a user will specify *a* hadoop job id. Oozie will directly query the hadoop JT/RM/HS. Since it is an external call with undetermined response time, Oozie will provide only one hadoop job id per-request to avoid the timeout in WS call. Caveats: If hadoop is down or the job is not in JT/RM/History Server, Oozie will fail to collect the details. 

D3: For pig, Oozie will store the pig-generated hadoop ids in it DB and will expose that to user throw the ""verbose"" query.

D4: Oozie will need to collect those summary pig stats and corresponding job counters and store it in Oozie DB. PigStats has a way of getting job counter for each hadoop job that it submits. We could use that API to collect summary counters for pig-created jobs.

D5: The complete/detail pigstats will be stored into Pig Launcher Mapper as job counter. So that if a user wants to get the detail pig stats, we could get it from the LM directly.



Open questions:
----------------------
* What should be in the summary counters/stats? 
* What is the max size of stats?

Advanced planning: <Not in the scope of this task, but might required for design to support later>
--------------------------
* Some users are asking to query the job stats when the job is RUNNING. They need it to decide for subsequent job submissions.
* By the above design , user could use D2, to get the counter when MR action is running.
* However, for pig, it is not that straight forward. Because Pig submits the jobs during execution. But the new PigRunner provide a listener concept where user can get the notifications such as when a new MR job submitted and its ID.
* By using this, Oozie could get the running hadoop job id instantly. In future, user might want this to query using D2.

","Duplicated Code, Long Method, , "
"Rename Method,","Oozie to allow drill down to hadoop job's details High-level Requirements:
-----------------------------------
Since Oozie is designed as the gateway to grid, we need to support WS API for most common hadoop commands through Oozie. User doesn't want to go to multiple system to get the required data. Based on these, we propose to implement the following requirements into Oozie.

R1: Oozie will provide WS endpoints to get hadoop job details (including job counters).

R2: It will support both types of hadoop jobs : MR job created for MR action, MR jobs created as part of pig script.

R3: In addition, for pig action, oozie will provide a way to query the pig stats.

Proposed design:
----------------------
D1: Oozie will store the *summary* jobcounter /pigstats into oozie DB. The items in the summary stats will be determined by oozie to limit the size. However,the commonly used stats will be include into the summary. It is important to note that summary information will be collected *after* the job finished.

D2: If the user asks for *details* hadoop job stats , the user needs to query using different WS API. In this query, a user will specify *a* hadoop job id. Oozie will directly query the hadoop JT/RM/HS. Since it is an external call with undetermined response time, Oozie will provide only one hadoop job id per-request to avoid the timeout in WS call. Caveats: If hadoop is down or the job is not in JT/RM/History Server, Oozie will fail to collect the details. 

D3: For pig, Oozie will store the pig-generated hadoop ids in it DB and will expose that to user throw the ""verbose"" query.

D4: Oozie will need to collect those summary pig stats and corresponding job counters and store it in Oozie DB. PigStats has a way of getting job counter for each hadoop job that it submits. We could use that API to collect summary counters for pig-created jobs.

D5: The complete/detail pigstats will be stored into Pig Launcher Mapper as job counter. So that if a user wants to get the detail pig stats, we could get it from the LM directly.



Open questions:
----------------------
* What should be in the summary counters/stats? 
* What is the max size of stats?

Advanced planning: <Not in the scope of this task, but might required for design to support later>
--------------------------
* Some users are asking to query the job stats when the job is RUNNING. They need it to decide for subsequent job submissions.
* By the above design , user could use D2, to get the counter when MR action is running.
* However, for pig, it is not that straight forward. Because Pig submits the jobs during execution. But the new PigRunner provide a listener concept where user can get the notifications such as when a new MR job submitted and its ID.
* By using this, Oozie could get the running hadoop job id instantly. In future, user might want this to query using D2.

",", "
"Move Class,Rename Class,Move And Rename Class,Extract Interface,Rename Method,Move Method,Extract Method,","Support high availability for the Oozie service As Oozie becomes a critical component in the Hadoop ecosystem users needs assured availability of the services provided by Oozie. To support this need Oozie should include a new feature to support high availability. This feature needs to take into consideration that Oozie provides RESTful APIs, Java APIs, and a command line API that should all be insensitive to the availability of any specific server or components. At Yahoo! it is not required that there be session fail-over from the client. It is acceptable for the client to reconnect if a session is lost as long as the state data managed by the Oozie service is not lost.","Duplicated Code, Long Method, , , Large Class, "
"Rename Method,Extract Method,","Add libraries which can be used by all actions in oozie In Oozie - 610, action specific share libs are added. This JIRA proposes an enhancement to it where libraries are added which can be shared by all actions. 
For e.g, I am modifying Pig to store the stats. But Json library is required to do so. By putting the json lib at a common location, other actions like hive, distcp can use this library in future.

For e.g, all common libs can be stored in
/share/lib/oozie/* 

Also, all action specific share libs will override the common libs

","Duplicated Code, Long Method, , "
"Move Method,Move Attribute,","Simplify Kerberos/HadoopAccessorService code and remove Kerberos/DoAs code The KerberosHadoopAccessorService subclass was implemented as a separate implementation to provide support for pre-UGI and UGI versions of Hadoop at the same time. The DoAs/KerberosDoAs classes were there to make testcases run with pre-UGI and UGI versions of Hadoop at the same time.

As the versions of Hadoop supported by trunk (3.2.0) all have UGI, we don't need these classes anymore.

The KerberosHadoopAccessorService logic can be folded into the HadoopAccessorService & the doAs classes can be removed.

This will simplify the code and avoid a common source of confusion for users.
",", , , "
"Extract Method,Inline Method,","enhance ooziedb tool not to require manual upgrade steps and not to require the -sqlfile option Currently the upgrade tool requires a manual step to alter a column length after doing and upgrade.

It was done this way because OpenJPA schematool ignores the column length modification.

the ooziedb tool should use direct JDBC with the corresponding syntax for each DB.

In addition, the -sqlfile FILE option should not be required, and if not provide the sql scripts should be written to a file in /tmp.","Duplicated Code, Long Method, , , "
"Rename Method,","add support for multiple/configurable sharelibs for each action type Currently there is a fixed sharelib per action type. I.e.:

{code}
/share/lib/mapreduce-streaming/
pig/
hive/
sqoop/
{code}

It many situations it would be desirable to support multiple versions of sharelib per component, have a system default, and allow users to override the default for a specific version. I.e.:

{code}
/share/lib/mapreduce-streaming/
pig-0_8/ (default)
pig-0_9/
sqoop/
{code}



",", "
"Rename Method,Pull Up Method,",support for hive in Oozie CLI add support for 'oozie hive' in the CLI,", Duplicated Code, "
"Rename Method,Extract Method,","Add support for Oozie coordinator to work in an UTC offset Current Oozie coordinator expects and resolves dates in UTC (ie {{2009-08-10T00:00Z}}). UTC datetimes are used for start/end/pause of jobs, datasets initial-instance and to resolve dataset instance URI templates.

Adding support for a non UTC timezone it would enable deployments where they use a timezone different than UTC as standard. This seems quite common in countries where they don't observe DST changes.
","Duplicated Code, Long Method, , "
"Rename Class,Move Method,",Refactoring the Command Line Parameter interfaces Refactoring the Command Line Parameter interfaces as described at https://cwiki.apache.org/OPENNLP/command-line-parameter-interfaces.html,", , "
"Rename Method,Extract Method,Inline Method,",Merge TrainingParameters and PluggableParameters The PluggableParameters class was added to pull out the get(Int/String/Boolean)Parameters() methods from the AbstractTrainer. Merge the functionality of the PluggableParameters into the TrainingParameters.,"Duplicated Code, Long Method, , , "
"Move Class,Rename Class,Move And Rename Class,Move Method,","Small refactoring of Arvores Deitadas Format classes. Some small refactoring to make it clear how to use the Arvores Deitadas formatters.
- Rename the class ContractionUtility to PortugueseContractionUtility and move it to 'ad' package;
- Rename the class ADParagraphStream to ADSentenceStream. Improved the corpus parsing;
- Move tests related to Arvores Deitadas to 'ad' package.
",", , "
"Move Method,Move Attribute,",Move the Porter Stemmer to OpenNLP Tools The similarity package contribution contains a Porter Stemmer this stemmer should be moved to opennlp tools and a test for it needs to be written.,", , , "
"Pull Up Method,Move Attribute,","Absence of logging and usage of System.out There seems to be no concept of logging used by the libraries. Instead System.out.println is hard-coded in many places where debug information using a logging framework would do it.
This makes awkward to use the modules integrated into a different application (as it spams our logs or console). 

Is the usage of System.out in core classes (like GISTrainer) by choice? Or is it simply a technical debt? I am happy to work on it and provide a patch if this is a technical debt.",", , Duplicated Code, "
"Push Down Method,Move Method,Move Attribute,","CLI tools and formats refactored Proposed patch refactors CLI tools and simplifies the code by introducing hierarchy and removing a lot of code duplication. It also introduces better error and help messages, including help for formats and listing available formats in various tools, which are now able to work with formats directly. This, in turn, eliminates the need to keep converted files on disk.",", , , , "
"Extract Method,Inline Method,","Fix remaining issue in L-BFGS parameter estimation to get it stable Enhance the L-BFGS parameter estimation code to at least perform as well as the GIS training. The work on this issue should bring the implementation to a level where the experimental flag can be removed.

For remaining problems see this issue: OPENNLP-338.","Duplicated Code, Long Method, , , "
"Rename Class,Extract Interface,Rename Method,Pull Up Method,Extract Method,Inline Method,","CLI tools and formats refactored Proposed patch refactors CLI tools and simplifies the code by introducing hierarchy and removing a lot of code duplication. It also introduces better error and help messages, including help for formats and listing available formats in various tools, which are now able to work with formats directly. This, in turn, eliminates the need to keep converted files on disk.","Duplicated Code, Long Method, , , Duplicated Code, Large Class, "
"Rename Method,","Improve OSGi support for OpenNLP extensions We have very basic OSGi support currently. We simply export all the packages we have and don't use any other OSGi features. This works well for anything we do, expect the places where we try to access classes by class name, e.g. to load custom factories via Class.forName(...). Most users will just be happy with that.

Such calls do not work in an OSGi environment because the class we try to load is not on ""our"" class path.

In OSGi this is done via services and we need to use them if we are running in an OSGI environment.

Anyway, OpenNLP needs to work with and without OSGi.

I suggest that we make OSGi an optional dependency and write code which can detect if the OSGi classes are there or not.

To instantiate a user class we would need to do something like this:
- Try to load via Class.forName(...)
- If cannot be found, check if running in an OSGi environment
- If so try to get an OSGi service which provides an instance to the user class",", "
"Move Method,Inline Method,Move Attribute,",Evaluator CLI tools should use the Parameters interface Some CLI evaluation tools are not using the Parameters interface to describe arguments. It is easier to set optional parameters if the tool is using the interface.,", , , , "
"Rename Class,Rename Method,",Evaluators should allow tools to register a report interface OPENNLP-220 introduced the -misclassified argument that enables evaluators to print misclassified items while using the command line evaluators. We should expand it to allow any other tool that uses evaluators to register an interface to get that information.,", "
"Rename Method,","Add Pluggable Machine Learning support The OpenNLP Tools can currently only use the classifiers inside the Maxent library. It should be possible to plugin 3rd party machine learning libraries which can be integrated as seamlessly as the Maxent library.

To achieve this two these tasks need to be solved:

- Define a MachineLearningFactory which is capable of instantiating a Trainer and Classifer based on a given parameter properties file. The Algorithm name could be the name of the factory to use. Additional the code in OpenNLP Tools need to be refactored to use the factory interface instead of the TrainUtil.

- Refactor the OpenNLP Tools to use an interface instead of the AbstractModel the interface can be identical to the current MaxentModel with additional support for serialization.

- To avoid an interface layer between OpenNLP Tools and Maxent the maxent classes should be moved to opennlp.tools.ml.",", "
"Rename Method,","Add a cli tool for the doccat evaluation support There should be a command line tool which can be used to evaluate the document categorizer model
on a test file.",", "
"Move Method,Extract Method,","Create a Factory to customize the POS Tagger Should provide a mechanism to customize the POS Tagger using a factory. The component should get the following objects from the factory:

- Context Generator
- Sequence Validator
- POS Dictionary implementation

One issue to solve is how to initialize the objects. For example, the Sequence Validator might be initialized using a POS Dictionary.","Duplicated Code, Long Method, , , "
"Rename Method,","facilitating the specialization of POSDictionary The train method in POSTaggerME receives in input a POSDictionary. This makes the implementation of custom dictionaries painful.
I suggest to replace the POSDictionary input as a TagDictionary.

Another improvement may also be the declaration of POSDictionary fields as protected, to help the extension of this class.",", "
"Move Method,Extract Method,",Add name finder factory support to instantiate a highly modified name finder via a model Most components in OpenNLP support a user defined factory to customize most aspects of it. The name finder does not support a user defined factory yet. To change this implement a name finder factory in the style of the already existing factories.,"Duplicated Code, Long Method, , , "
"Rename Method,","Remove deprecated iteration and cutoff params The deprecated iterations and cutoff parameters have been replaced by a properties file (or TrainingParameters object) which can contain all necessary parameters for a certain machine learning implementation.

Remove all deprecated API which is still using them. Also remove the parameters from the command line interface.",", "
"Rename Method,Extract Method,",Deprecate the methods which take a List in the Chunker interfaces All the methods in the Chunker interface which take a List should be deprecated.,"Duplicated Code, Long Method, , "
"Move And Rename Class,","Add OntoNotes format support Add native formats support for OntoNotes to OpenNLP. It should be possible to train the POS Tagger, Parser and Name Finder on the OntoNotes data.",", "
"Rename Method,Extract Method,","Refactor the PerceptronTrainer class to address a couple of problems - Changed the update to be the actual perceptron update: when a label
that is not the gold label is chosen for an event, the parameters
associated with that label are decremented, and the parameters
associated with the gold label are incremented. I checked this
empirically on several datasets, and it works better than the
previous update (and it involves fewer updates).

- stepsize is decreased by stepsize/1.05 on every iteration, ensuring
better stability toward the end of training. This is actually the
main reason that the training set accuracy obtained during parameter
update continued to be different from that computed when parameters
aren't updated. Now, the parameters don't jump as much in later
iterations, so things settle down and those two accuracies converge
if enough iterations are allowed.

- Training set accuracy is computed once per iteration.

- Training stops if the current training set accuracy changes less
than a given tolerance from the accuracies obtained in each of the
previous three iterations.

- Averaging is done differently than before. Rather than doing an
immediate update, parameters are simply accumulated after iterations
(this makes the code much easier to understand/maintain). Also, not
every iteration is used, as this tends to give to much weight to the
final iterations, which don't actually differ that much from one
another. I tried a few things and found a simple method that works
well: sum the parameters from the first 20 iterations and then sum
parameters from any further iterations that are perfect squares (25,
36, 49, etc). This gets a good (diverse) sample of parameters for
averaging since the distance between subsequent parameter sets gets
larger as the number of iterations gets bigger.

- Added ListEventStream to make a stream out of List<Event>

- Added some helper methods, e.g. maxIndex, to simplify the code in
the main algorithm.

- The training stats aren't shown for every iteration. Now it is just
the first 10 and then every 10th iteration after that.

- modelDistribution, params, evalParams and others are no longer class
variables. They have been pushed into the findParameters
method. Other variables could/should be made non-global too, but
leaving as is for now.","Duplicated Code, Long Method, , "
"Rename Method,Extract Method,","Adding new functionality to know all possible lemmas given a word and pos tag pair Currently the various lemmatizers (DictionaryLemmatizer, LemmatizerME and MorfologikLemmatizer) do not allow to obtain all posible lemmas given a word and postag pair. This functionality is useful and should be added.","Duplicated Code, Long Method, , "
"Move Method,Move Attribute,","Add formats support for the French Treebank The OpenNLP formats package should have support for the French TreeBank. It can be obtained without paying for it for research purposes.

Here is more information about it.
http://www.llf.cnrs.fr/Gens/Abeille/French-Treebank-fr.php",", , , "
"Rename Method,","Fix remaining issue in L-BFGS parameter estimation to get it stable Enhance the L-BFGS parameter estimation code to at least perform as well as the GIS training. The work on this issue should bring the implementation to a level where the experimental flag can be removed.

For remaining problems see this issue: OPENNLP-338.",", "
"Move And Rename Class,",Evaluator CLI tools should use the Parameters interface Some CLI evaluation tools are not using the Parameters interface to describe arguments. It is easier to set optional parameters if the tool is using the interface.,", "
"Rename Method,","Use stupid backoff by default in NGramLanguageModel {{NGramLanguageModel}} is already using [Stupid Backoff|http://www.aclweb.org/anthology/D07-1090.pdf] discounting when it contains more than 1M ngrams.
However since the not very good performance of Laplace smoothing for smaller models, it'd be better to simply use Stupid Backoff in all cases.
",", "
"Rename Method,","Add Concatenate Stream method for Collections of streams Minor change to opennlp.tools.util.ObjectStreamUtls. First change the signature of the createObjectStream(final ObjectStream<T>... streams) to concatenateObjectStream(final ObjectStream<T>... streams), and add a method concatenateObjectStream(final Collection<ObjectStream<T>> streams)

The reason behind this is that I often pull data from multiple files, whereas it is possible to create an array of ObjectStreams, it is easier to work with Lists. Also, the name of the method is clearer. It concatenates a list/array of ObjectStreams as opposed the the createObjectStream(final Collection<T> collection) which makes an obectstream of items in the collection.",", "
"Move Method,Move Attribute,",Remove deprecated leipzig doccat format support ,", , , "
"Rename Method,",Evaluators should allow tools to register a report interface OPENNLP-220 introduced the -misclassified argument that enables evaluators to print misclassified items while using the command line evaluators. We should expand it to allow any other tool that uses evaluators to register an interface to get that information.,", "
"Rename Class,Inline Method,",Feature cutoff should only be done by data indexers Currently the data indexers and the maxent training code can cutoff features. The feature cutoff should be removed from the maxent training code and only be done by the data indexers.,", , "
"Rename Method,","Add support for custom feature generator configuration embedded in the model package Add support for custom feature generator configuration embedded in the model package.

The configuration of the feature generators for the name finder component can be quite complex and the configuration must
be always done twice once for training and once for tagging. Doing it twice at two different points in time makes
the feature generation very error prone. Small mistakes lead to a drop in detection performance which might
be difficult to notice. 

To solve this issue add the configuration to the model, then it must only be specified during training and
can be loaded from the model during tagging.

Another advantage is that custom feature generation is difficult to use otherwise, because the integration
code must deal itself with setting up the feature generators. In some cases the user even does not have control
over the code, or does not want to change it, e.g. in the UIMA wrappers.

The same logic should be used for the POS Tagger and Chunker.

The issues is migrated from SourceForge:
https://sourceforge.net/tracker/?func=detail&aid=1941380&group_id=3368&atid=353368",", "
"Rename Class,Move And Rename Class,Rename Method,Move Method,Extract Method,Move Attribute,","Add L1-regularization into L-BFGS L1-regularization is useful during training Maximum Entropy models since it pushes parameters of irrelevant features to zero. Hence, the parameter vector will be sparse and the trained model will be compact. 

When the number of features is much larger than the number of training examples, L1 often gives better accuracy than L2.

The implementation of L1-regularization for L-BFGS will follow the method described in the paper:

http://research.microsoft.com/en-us/um/people/jfgao/paper/icml07scalable.pdf","Duplicated Code, Long Method, , , , "
"Rename Method,",Organize imports according to new order It would be nice to do this for the code base and enforce it via checkstyle. We can tell people to make sure their IDE is configured correctly and everything will be fine.,", "
"Rename Method,",Extend eval tests to run more ml algorithms ,", "
"Move Method,Extract Method,","Add Pluggable Machine Learning support The OpenNLP Tools can currently only use the classifiers inside the Maxent library. It should be possible to plugin 3rd party machine learning libraries which can be integrated as seamlessly as the Maxent library.

To achieve this two these tasks need to be solved:

- Define a MachineLearningFactory which is capable of instantiating a Trainer and Classifer based on a given parameter properties file. The Algorithm name could be the name of the factory to use. Additional the code in OpenNLP Tools need to be refactored to use the factory interface instead of the TrainUtil.

- Refactor the OpenNLP Tools to use an interface instead of the AbstractModel the interface can be identical to the current MaxentModel with additional support for serialization.

- To avoid an interface layer between OpenNLP Tools and Maxent the maxent classes should be moved to opennlp.tools.ml.","Duplicated Code, Long Method, , , "
"Extract Superclass,Extract Method,","move ArrayMath to a more general package In OPENNLP-1195, [~joern] mentioned this. 

{quote} 
There are more usages of argmax in the OpenNLP source code. 
I propose we create one common method and then try to only use that one. 

We could move the ArrayMath to a more general package and place a common method there, or keep the existing one 
{quote} 

I want to solve this before OPENNLP-1195.","Duplicated Code, Long Method, , Duplicated Code, Large Class, "
"Rename Method,Move Method,Extract Method,",Add test data verification to tests under opennlp.tools.eval Add test data verification to tests under opennlp.tools.eval. Each class should verify the test data prior to executing the tests in the class.,"Duplicated Code, Long Method, , , "
"Extract Method,Move Attribute,","Refactor BratNameSampleStream Create a BratAnnotationParser that parses a BratDocument and creates a List<NameSample> The NameSampleStream,read() method would call this directly.

Consider Making the changes for the other formats as well.","Duplicated Code, Long Method, , , "
"Move Method,Move Attribute,",Add a cmdline interface for the Entity Linker The Entity Linker should have a command line interface. The command line tool should be capable of running an entity linker over some sample data.,", , , "
"Move Method,Extract Method,","Addition of prepositional phrase attachment dataset and unit test for it I have obtained permission from Adwait Ratnaparkhi to include his prepositional phrase attachment dataset in the distribution as a test case. Jorn correctly points out that we need to see whether this is ASF compliant. Here is the original dataset:

http://sites.google.com/site/adwaitratnaparkhi/publications/ppa.tar.gz?attredirects=0","Duplicated Code, Long Method, , , "
"Rename Method,",Improve resource loading for custom feature generators Currently the feature generators are matched in the TokenNameFinderTool which is part of the cmd line interface. To improve this the logic should be moved to the GeneratorFactory.,", "
"Rename Method,Move Method,",Integration with Hadoop 20 New API Hadoop 21 is not yet released but we know that switch to new MR API is coming there. This JIRA is for early integration with the portion of this API that has been implemented in Hadoop 20.,", , "
"Extract Interface,Rename Method,Extract Method,","[Zebra] to support record(row)-based file split in Zebra's TableInputFormat TFile currently supports split by record sequence number (see Jira HADOOP-6218). We want to utilize this to provide record(row)-based input split support in Zebra.
One prominent benefit is that: in cases where we have very large data files, we can create much more fine-grained input splits than before where we can only create one big split for one big file.

In more detail, the new row-based getSplits() works by default (user does not specify no. of splits to be generated) as follows: 
1) Select the biggest column group in terms of data size, split all of its TFiles according to hdfs block size (64 MB or 128 MB) and get a list of physical byte offsets as the output per TFile. For example, let us assume for the 1st TFile we get offset1, offset2, ..., offset10; 
2) Invoke TFile.getRecordNumNear(long offset) to get the RecordNum of a key-value pair near a byte offset. For the example above, say we get recordNum1, recordNum2, ..., recordNum10; 
3) Stitch [0, recordNum1], [recordNum1+1, recordNum2], ..., [recordNum9+1, recordNum10], [recordNum10+1, lastRecordNum] splits of all column groups, respectively to form 11 record-based input splits for the 1st TFile. 
4) For each input split, we need to create a TFile scanner through: TFile.createScannerByRecordNum(long beginRecNum, long endRecNum). 

Note: conversion from byte offset to record number will be done by each mapper, rather than being done at the job initialization phase. This is due to performance concern since the conversion incurs some TFile reading overhead.","Duplicated Code, Long Method, , Large Class, "
"Rename Method,","[zebra] Provide streaming support in Zebra. Hadoop streaming is very popular among Hadoop users. The main attraction is the simplicity of use. A user can write the application logic in any language and process large amounts of data using Hadoop framework. As more people start to use Zebra to store their data, we expect users would like to run Hadoop streaming scripts to easily process Zebra tables. 

The following lists a simple example of using Hadoop streaming to access Zebra data. It loads data from foo table using Zebra's TableInputFormat and then writes the data into output using default TextOutputFormat. 

$ hadoop jar hadoop-streaming.jar -D mapred.reduce.tasks=0 -input foo -output output -mapper 'cat' -inputformat org.apache.hadoop.zebra.mapred.TableInputFormat 

More detailed, Zebra uses Pig DefaultTuple implementation of Tuple for its records. Currently, when Zebra's TableInputFormat is used for input, the user script sees each line containing "" key_if_any\tTuple.toString() "". We plan to generate CSV format representation of our Pig tuples. To this end, we plan to do the following: 

1) Derive a sub class ZupleTuple from pig's DefaultTuple class and override its toString() method to present the data into CSV format. 

2) On Zebra side, the tuple factory should be changed to create ZebraTuple objects, instead of DefaultTuple objects. 

Note that we can only support streaming on the input side - ability to use streaming to read data from Zebra tables. For the output side, the streaming support is not feasible, since the streaming mapper or reducer only emits ""Text\tText"", the output collector has no way of knowing how to convert this to (BytesWritable,Tuple).
",", "
"Rename Method,","[zebra] Use of Hadoop 2.0 APIs Currently, Zebra is still using already deprecated Hadoop 1.8 APIs. Need to upgrade to its 2.0 APIs.",", "
"Rename Method,Extract Method,","LogicalPlan and Optimizer are too complex and hard to work with The current implementation of the logical plan and the logical optimizer in Pig has proven to not be easily extensible. Developer feedback has indicated that adding new rules to the optimizer is quite burdensome. In addition, the logical plan has been an area of numerous bugs, many of which have been difficult to fix. Developers also feel that the logical plan is difficult to understand and maintain. The root cause for these issues is that a number of design decisions that were made as part of the 0.2 rewrite of the front end have now proven to be sub-optimal. The heart of this proposal is to revisit a number of those proposals and rebuild the logical plan with a simpler design that will make it much easier to maintain the logical plan as well as extend the logical optimizer. 

See http://wiki.apache.org/pig/PigLogicalPlanOptimizerRewrite for full details.","Duplicated Code, Long Method, , "
"Move Class,Rename Class,Move And Rename Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","LogicalPlan and Optimizer are too complex and hard to work with The current implementation of the logical plan and the logical optimizer in Pig has proven to not be easily extensible. Developer feedback has indicated that adding new rules to the optimizer is quite burdensome. In addition, the logical plan has been an area of numerous bugs, many of which have been difficult to fix. Developers also feel that the logical plan is difficult to understand and maintain. The root cause for these issues is that a number of design decisions that were made as part of the 0.2 rewrite of the front end have now proven to be sub-optimal. The heart of this proposal is to revisit a number of those proposals and rebuild the logical plan with a simpler design that will make it much easier to maintain the logical plan as well as extend the logical optimizer. 

See http://wiki.apache.org/pig/PigLogicalPlanOptimizerRewrite for full details.","Duplicated Code, Long Method, , , , , "
"Rename Class,Rename Method,Move Method,Extract Method,","LogicalPlan and Optimizer are too complex and hard to work with The current implementation of the logical plan and the logical optimizer in Pig has proven to not be easily extensible. Developer feedback has indicated that adding new rules to the optimizer is quite burdensome. In addition, the logical plan has been an area of numerous bugs, many of which have been difficult to fix. Developers also feel that the logical plan is difficult to understand and maintain. The root cause for these issues is that a number of design decisions that were made as part of the 0.2 rewrite of the front end have now proven to be sub-optimal. The heart of this proposal is to revisit a number of those proposals and rebuild the logical plan with a simpler design that will make it much easier to maintain the logical plan as well as extend the logical optimizer. 

See http://wiki.apache.org/pig/PigLogicalPlanOptimizerRewrite for full details.","Duplicated Code, Long Method, , , "
"Rename Method,Extract Method,","Use distributed cache to store samples Currently, in the case of skew join and order by we use sample that is just written to the dfs (not distributed cache) and, as the result, get opened and copied around more than necessary. This impacts query performance and also places unnecesary load on the name node","Duplicated Code, Long Method, , "
"Rename Class,Extract Method,","Binary comparator for secondary sort When hadoop framework doing the sorting, it will try to use binary version of comparator if available. The benefit of binary comparator is we do not need to instantiate the object before we compare. We see a ~30% speedup after we switch to binary comparator. Currently, Pig use binary comparator in following case:

1. When semantics of order doesn't matter. For example, in distinct, we need to do a sort in order to filter out duplicate values; however, we do not care how comparator sort keys. Groupby also share this character. In this case, we rely on hadoop's default binary comparator
2. Semantics of order matter, but the key is of simple type. In this case, we have implementation for simple types, such as integer, long, float, chararray, databytearray, string

However, if the key is a tuple and the sort semantics matters, we do not have a binary comparator implementation. This especially matters when we switch to use secondary sort. In secondary sort, we convert the inner sort of nested foreach into the secondary key and rely on hadoop to sorting on both main key and secondary key. The sorting key will become a two items tuple. Since the secondary key the sorting key of the nested foreach, so the sorting semantics matters. It turns out we do not have binary comparator once we use secondary sort, and we see a significant slow down.

Binary comparator for tuple should be doable once we understand the binary structure of the serialized tuple. We can focus on most common use cases first, which is ""group by"" followed by a nested sort. In this case, we will use secondary sort. Semantics of the first key does not matter but semantics of secondary key matters. We need to identify the boundary of main key and secondary key in the binary tuple buffer without instantiate tuple itself. Then if the first key equals, we use a binary comparator to compare secondary key. Secondary key can also be a complex data type, but for the first step, we focus on simple secondary key, which is the most common use case.

We mark this issue to be a candidate project for ""Google summer of code 2010"" program.","Duplicated Code, Long Method, , "
"Rename Method,Extract Method,Inline Method,","API interface to Pig It would be nice to make Pig more friendly for applications like workflow that would be executing pig scripts on user behalf.

Currently, they would have to use pig command line to execute the code; however, this has limitation on the kind of output that would be delivered. For instance, it is hard to produce error information that is easy to use programatically or collect statistics.

The proposal is to create a class that mimics the behavior of the Main but gives users a status object back. The the main code of pig would look somethig like:

public static void main(String args[])
{
PigStatus ps = PigMain.exec(args);
exit (PigStatus.rc);
}

We need to define the following:

- Content of PigStatus. It should at least include
* return code
* error string
* exception 
* statistics
- A way to propagate the status class through pig code","Duplicated Code, Long Method, , , "
"Rename Method,Extract Method,","Map-side outer joins Pig already has couple of map-side join implementations: Merge Join and Fragmented-Replicate Join. But both of them are pretty restrictive. Merge Join can only join two tables and that too can only do inner join. FR Join can join multiple relations, but it can also only do inner and left outer joins. Further it restricts the sizes of side relations. It will be nice if we can do map side joins on multiple tables as well do inner, left outer, right outer and full outer joins. 

Lot of groundwork for this has already been done in PIG-1309. Remaining will be tracked in this jira.","Duplicated Code, Long Method, , "
"Move Method,Move Attribute,","[Zebra] To support writing multiple Zebra tables through Pig In Zebra, we already have multiple outputs support for map/reduce. But we do not support this feature if users use Zebra through Pig.

This jira is to address this issue. We plan to support writing to multiple output tables through Pig as well.

We propose to support the following Pig store statements with multiple outputs:

store relation into 'loc1,loc2,loc3....' using org.apache.hadoop.zebra.pig.TableStorer('storagehint_string',
'complete name of your custom partition class', 'some arguments to partition class'); /* if certain partition class arguments is needed */

store relation into 'loc1,loc2,loc3....' using org.apache.hadoop.zebra.pig.TableStorer('storagehint_string',
'complete name of your custom partition class'); /* if no partition class arguments is needed */

Note that users need to specify up to three arguments - storage hint string, complete name of partition class and partition class arguments string.",", , , "
"Rename Method,","Need a way for Pig to take an alternative property file Currently, Pig read the first ever pig.properties in CLASSPATH. Pig has a default pig.properties and if user have a different pig.properties, there will be a conflict since we can only read one. There are couple of ways to solve it:

1. Give a command line option for user to pass an additional property file
2. Change the name for default pig.properties to pig-default.properties, and user can give a pig.properties to override
3. Further, can we consider to use pig-default.xml/pig-site.xml, which seems to be more natural for hadoop community. If so, we shall provide backward compatibility to also read pig.properties, pig-cluster-hadoop-site.xml.",", "
"Rename Method,","Implement Pig counter to track number of rows for each input files A MR job generated by Pig not only can have multiple outputs (in the case of multiquery) but also can have multiple inputs (in the case of join or cogroup). In both cases, the existing Hadoop counters (e.g. MAP_INPUT_RECORDS, REDUCE_OUTPUT_RECORDS) can not be used to count the number of records in the given input or output. PIG-1299 addressed the case of multiple outputs. We need to add new counters for jobs with multiple inputs.",", "
"Rename Method,","Allow casting relations to scalars This jira is to implement a simplified version of the functionality described in https://issues.apache.org/jira/browse/PIG-801.

The proposal is to allow casting relations to scalar types in foreach.

Example:

A = load 'data' as (x, y, z);
B = group A all;
C = foreach B generate COUNT(A);
.....
X = ....
Y = foreach X generate $1/(long) C;

Couple of additional comments:

(1) You can only cast relations including a single value or an error will be reported
(2) Name resolution is needed since relation X might have field named C in which case that field takes precedence.
(3) Y will look for C closest to it.

Implementation thoughts:

The idea is to store C into a file and then convert it into scalar via a UDF. I believe we already have a UDF that Ben Reed contributed for this purpose. Most of the work would be to update the logical plan to
(1) Store C
(2) convert the cast to the UDF",", "
"Move Class,Rename Method,Push Down Method,Push Down Attribute,","Embed Pig in scripting languages It should be possible to embed Pig calls in a scripting language and let functions defined in the same script available as UDFs.
This is a spin off of https://issues.apache.org/jira/browse/PIG-928 which lets users define UDFs in scripting languages.",", , , "
"Rename Method,","multi file input format for loaders We frequently run in the situation where Pig needs to deal with small files in the input. In this case a separate map is created for each file which could be very inefficient. 

It would be greate to have an umbrella input format that can take multiple files and use them in a single split. We would like to see this working with different data formats if possible.

There are already a couple of input formats doing similar thing: MultifileInputFormat as well as CombinedInputFormat; howevere, neither works with ne Hadoop 20 API. 

We at least want to do a feasibility study for Pig 0.8.0.",", "
"Rename Method,","Optimize scalar to consolidate the part file Current scalar implementation will write a scalar file onto dfs. When Pig need the scalar, it will open the dfs file directly. Each scalar file contains more than one part file though it contains only one record. This puts a huge load to namenode. We should consolidate part file before open it. Another optional step is put the consolicated file into distributed cache. This further bring down the load of namenode.",", "
"Rename Class,Rename Method,","Switch to new parser generator technology There are many bugs in Pig related to the parser, particularly to bad error messages. After review of Java CC we feel these will be difficult to address using that tool. Also, the .jjt files used by JavaCC are hard to understand and maintain. 

ANTLR is being reviewed as the most likely choice to move to, but other parsers will be reviewed as well.

This JIRA will act as an umbrella issue for other parser issues.",", "
"Push Down Method,Extract Method,Push Down Attribute,","Switch to new parser generator technology There are many bugs in Pig related to the parser, particularly to bad error messages. After review of Java CC we feel these will be difficult to address using that tool. Also, the .jjt files used by JavaCC are hard to understand and maintain. 

ANTLR is being reviewed as the most likely choice to move to, but other parsers will be reviewed as well.

This JIRA will act as an umbrella issue for other parser issues.","Duplicated Code, Long Method, , , , "
"Rename Method,Extract Method,","Switch to new parser generator technology There are many bugs in Pig related to the parser, particularly to bad error messages. After review of Java CC we feel these will be difficult to address using that tool. Also, the .jjt files used by JavaCC are hard to understand and maintain. 

ANTLR is being reviewed as the most likely choice to move to, but other parsers will be reviewed as well.

This JIRA will act as an umbrella issue for other parser issues.","Duplicated Code, Long Method, , "
"Rename Method,","Switch to new parser generator technology There are many bugs in Pig related to the parser, particularly to bad error messages. After review of Java CC we feel these will be difficult to address using that tool. Also, the .jjt files used by JavaCC are hard to understand and maintain. 

ANTLR is being reviewed as the most likely choice to move to, but other parsers will be reviewed as well.

This JIRA will act as an umbrella issue for other parser issues.",", "
"Move Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Switch to new parser generator technology There are many bugs in Pig related to the parser, particularly to bad error messages. After review of Java CC we feel these will be difficult to address using that tool. Also, the .jjt files used by JavaCC are hard to understand and maintain. 

ANTLR is being reviewed as the most likely choice to move to, but other parsers will be reviewed as well.

This JIRA will act as an umbrella issue for other parser issues.","Duplicated Code, Long Method, , , , , "
"Rename Method,Move Method,Extract Method,",Removal of old logical plan The new logical plan will only be used and the old logical plan will be removed once the new one is stable enough. It is scheduled for the 0.9 release.,"Duplicated Code, Long Method, , , "
"Rename Method,","""0"" value seen in PigStat's map/reduce runtime, even when the job is successful Pig runtime calls JobClient.getMapTaskReports(jobId) and JobClient.getReduceTaskReports(jobId) to get statistics about numbers of maps/reducers, as well as max/min/avg time of these tasks. But from time to time, these calls return empty lists. When that happens pig is reports 0 values for the stats. 

The jobtracker keeps the stats information only for a limited duration based on the configuration parameters mapred.jobtracker.completeuserjobs.maximum and mapred.job.tracker.retiredjobs.cache.size. Since pig collects the stats after jobs have finished running, it is possible that the stats for the initial jobs are no longer available. To have better chances of getting the stats, it should be collected as soon as the job is over. 
",", "
"Rename Class,Move Method,Move Attribute,","Make Pig work with hadoop .NEXT We need to make Pig work with hadoop .NEXT, the svn branch currently is: https://svn.apache.org/repos/asf/hadoop/common/branches/MR-279",", , , "
"Extract Superclass,Pull Up Method,Move Method,Extract Method,Pull Up Attribute,Move Attribute,","Make PigStorage optionally store schema; improve docs. I'd like to propose that we allow for a greater degree of customization in PigStorage.

An incomplete list features that we might want to add:

- flag to tell it to overwrite existing output if it exists
- flag to tell it to compress output using gzip|bzip|lzo (currently this can be achieved by setting the directory name to end in .gz or .bz2, which is a bit awkward)
- flag to tell it to store the schema and header (perhaps by merging in PigStorageSchema work?)
","Duplicated Code, Long Method, , , , Duplicated Code, Large Class, Duplicated Code, Duplicated Code, "
"Move Method,Extract Method,Move Attribute,","support partial aggregation in map task h3. Introduction

Pig does (sort based) partial aggregation in map side through the use of combiner. MR serializes the output of map to a buffer, sorts it on the keys, deserializes and passes the values grouped on the keys to combiner phase. The same work of combiner can be done in the map phase itself by using a hash-map on the keys. This hash based (partial) aggregation can be done with or without a combiner phase.

h3. Benefits

It will send fewer records to combiner and thereby -
* Save on cost of serializing and de-serializing
* Save on cost of lock calls on the combiner input buffer. (I have found this to be a significant cost for a query that was doing multiple group-by's in a single MR job. -Thejas) 
* The problem of running out of memory in reduce side, for queries like COUNT(distinct col) can be avoided. The OOM issue happens because very large records get created after the combiner run on merged reduce input. In case of combiner, you have no way of telling MR not to combine records in reduce side. The workaround is to disable combiner completely, and the opportunity to reduce map output size is lost.
* When the foreach after group-by has both algebraic and non-algebraic functions, or if a bag is being projected, the combiner is not used. This is because the data size reduction in typical cases are not significant enough to justify the additional (de)serialization costs. But hash based aggregation can be used in such cases as well.
* It is possible to turn off the in-map combine automatically if there is not enough 'combination' that is taking place to justify the overhead of the in-map combiner. (Idea borrowed from Hive jira.) 
* If input data is sorted, it is possible to do efficient map side (partial) aggregation with in-map combiner.

Design proposal is here - https://cwiki.apache.org/confluence/display/PIG/PigInMapCombinerProposal","Duplicated Code, Long Method, , , , "
"Rename Method,",Set default number of reducers for S3N filesystem Currently pig only estimates default reducers based on input file size for the HDFS and local file systems. This patch adds support for the S3N file system as well.,", "
"Extract Superclass,Pull Up Method,Move Method,","Reduce code duplication in SUM, MAX, MIN udfs The current type-specific implementations of Max, Min, and Sum have a lot of duplicated code. We can reduce this significantly.",", , Duplicated Code, Large Class, Duplicated Code, "
"Rename Class,Move Method,Extract Method,Move Attribute,","Streaming UDFs - allow users to easily write UDFs in scripting languages with no JVM implementation. The goal of Streaming UDFs is to allow users to easily write UDFs in scripting languages with no JVM implementation or a limited JVM implementation. The initial proposal is outlined here: https://cwiki.apache.org/confluence/display/PIG/StreamingUDFs.

In order to implement this we need new syntax to distinguish a streaming UDF from an embedded JVM UDF. I'd propose something like the following (although I'm not sure 'language' is the best term to be using):

{code}define my_streaming_udfs language('python') ship('my_streaming_udfs.py'){code}

We'll also need a language-specific controller script that gets shipped to the cluster which is responsible for reading the input stream, deserializing the input data, passing it to the user written script, serializing that script output, and writing that to the output stream.

Finally, we'll need to add a StreamingUDF class that extends evalFunc. This class will likely share some of the existing code in POStream and ExecutableManager (where it make sense to pull out shared code) to stream data to/from the controller script.

One alternative approach to creating the StreamingUDF EvalFunc is to use the POStream operator directly. This would involve inserting the POStream operator instead of the POUserFunc operator whenever we encountered a streaming UDF while building the physical plan. This approach seemed problematic because there would need to be a lot of changes in order to support POStream in all of the places we want to be able use UDFs (For example - to operate on a single field inside of a for each statement).
","Duplicated Code, Long Method, , , , "
"Rename Method,","Speed up TestBuiltin On our build, TestBuiltin takes over 4 minutes. No reason for that.",", "
"Rename Method,Extract Method,","Support for multiple input schemas in AvroStorage This is a barebones patch for AvroStorage which enables support of multiple input schemas. The assumption is that the input consists of avro files having different schemas that can be unioned, e.g., flat records. 

A simple illustrative example is attached (avro_storage_union_schema_test.tar.gz): run create_avro1.pig, followed by create_avro2.pig, followed by read_avro.pig.","Duplicated Code, Long Method, , "
"Rename Method,Extract Method,","add source location of the aliases in the physical plan The goal is to provide better information about what is actually running in a job.
In particular when alias names are being reused.

For example with the following script:
{code}
A = LOAD 'foo' using PigStorage();
B = GROUP A BY $0;
A = FOREACH B GENERATE COUNT(A);
STORE A INTO 'bar';
{code}

The job conf will contain the following information
{code}
pig.alias.location: M: A[1,4],A[3,4],B[2,4] C: A[3,4],B[2,4] R: A[3,4]
{code}

A caveat is that the Logical Plan Optimizer throws away the original information when merging Logical Operators.
this is already the case today with pig.alias","Duplicated Code, Long Method, , "
"Move Method,Extract Method,","Improve PlanHelper to allow finding any PhysicalOperator in a plan PlanHelper has a few handy methods for finding Loads, Stores, and Native MR operators in a PhysicalPlan. With a bit of refactoring, we can make PlanHelper find any PhysicalOperator, instead of hardcoding which POs we can find.","Duplicated Code, Long Method, , , "
"Rename Method,Extract Method,","Add recursive record support to AvroStorage Currently, AvroStorage does not allow recursive records in Avro schema because it is not possible to define Pig schema for recursive records. (i.e. records that have self-referencing fields cause an infinite loop, so they are not supported.)

Even though there is no natural way of handling recursive records in Pig schema, I'd like to propose the following workaround: mapping recursive records to bytearray.

Take for example the following Avro schema:
{code}
{
""type"" : ""record"",
""name"" : ""RECURSIVE_RECORD"",
""fields"" : [ {
""name"" : ""value"",
""type"" : [ ""null"", ""int"" ]
}, {
""name"" : ""next"",
""type"" : [ ""null"", ""RECURSIVE_RECORD"" ]
} ]
}
{code}

and the following data:

{code}
{""value"":1,""next"":{""RECURSIVE_RECORD"":{""value"":2,""next"":{""RECURSIVE_RECORD"":{""value"":3,""next"":null}}}}} 
{""value"":2,""next"":{""RECURSIVE_RECORD"":{""value"":3,""next"":null}}} 
{""value"":3,""next"":null}
{code}

Then, we can define Pig schema as follows:

{code}
{value: int,next: bytearray}
{code}

Even though Pig thinks that the ""next"" fields are bytearray, they're actually loaded as tuples since AvroStorage uses Avro schema when loading files.

{code}
grunt> in = LOAD 'test_recursive_schema.avro' USING org.apache.pig.piggybank.storage.avro.AvroStorage ();
grunt> dump in;
(1,(2,(3,)))
(2,(3,))
(3,)
{code}

At this point, we have discrepancy between Avro schema and Pig schema; nevertheless, we can still refer to each field of tuples as follows:

{code}
grunt> first = FOREACH in GENERATE $0;
grunt> dump first;
(1)
(2)
(3)

or

grunt> second = FOREACH in GENERATE $1.$0;
grunt> dump second;
(2)
(3)
()
{code}

Lastly, we can store these tuples as Avro files by specifying schema. Since we can no longer construct Avro schema from Pig schema, it is required for the user to provide Avro schema via the 'schema' parameter in STORE function.

{code}
grunt> STORE first INTO 'output' USING org.apache.pig.piggybank.storage.avro.AvroStorage ( 'schema', '[ ""null"", ""int"" ]' );

or

grunt> STORE in INTO 'output' USING org.apache.pig.piggybank.storage.avro.AvroStorage ( 'schema', '
{
""type"" : ""record"",
""name"" : ""recursive_schema"",
""fields"" : [ { 
""name"" : ""value"",
""type"" : [ ""null"", ""int"" ]
}, {
""name"" : ""next"",
""type"" : [ ""null"", ""recursive_schema"" ]
} ] 
}
' );
{code}

To implement this workaround, the following work is required:
- Update the current generic union check so that it can handle recursive records. Currently, AvroStorage checks if the Avro schema contains 1) recursive records and 2) generic unions, and fails if so. But since I am going to remove the 1st check, the 2nd check should be able to handle recursive records without stack overflow.
- Update AvroSchema2Pig so that recursive records can be detected and mapped to bytearrays in Pig schema.
- Add the 'no_schema_check' parameter to STORE function so that results can be stored even though there exists discrepancy between Avro schema and Pig schema. Since Avro schema for STORE function cannot be constructed from Pig schema, it has to be specified by the user via the 'schema' parameter, and schema check has to be disabled by 'no_schema_check'.
- Update AvroStorage wiki.
- Add unit tests.

I do not think that any incompatibility issues will be introduced by this.

P.S. The reason why I chose to map recursive records to bytearray instead of empty tuple is because I cannot refer to any field if I use empty tuple. For example, if Pig schema is defined as follows:

{code}
{value: int,next: ()}
{code}

I get an exception when I attempt to refer to any field in loaded tuples since their schema is not defined (i.e. empty tuple).

{code}
ERROR 1127: Index 0 out of range in schema
{code}

This is all what I found by trials and errors, so there might be something that I am missing here. If so, please let me know.

Thanks!","Duplicated Code, Long Method, , "
"Extract Method,Move Attribute,","Improve performance of POPartialAgg During performance testing, we found that POPartialAgg can cause performance degradation for Pig jobs when the Algebraic UDFs it's being applied to aren't well suited to the operator's assumptions. Changing the implementation to a more flexible hash-based model can provide significant performance improvements.","Duplicated Code, Long Method, , , "
"Rename Method,Move Method,Move Attribute,","Modernize a chunk of the tests A lot of the tests use antiquated patterns. My goal was to refactor them in a couple ways:
- get rid of the annotation specifying Junit 4. All should use JUnit 4 (question: where is the Junit 3 dependency even being pulled in?)
- Nothing should extend TestCase. Everything should be annotation driven.
- Properly use asserts. There was a lot of assertTrue(null==thing), so I replaced it with assertNull(thing), and so on.
- Get rid of MiniCluster use in a handful of cases.

I've run every test and they pass, EXCEPT TestLargeFile which is failing on trunk anyway.",", , , "
"Rename Method,Extract Method,","Support for Credentials for UDF,Loader and Storer Pig does not have a clean way (APIs) to support adding Credentials (hbase token, hcat/hive metastore token) to Job and retrieving it.","Duplicated Code, Long Method, , "
"Rename Method,Extract Method,",Simplify Logical Plans By Removing Unneccessary Identity Projections The TypeCastInserter inserts a LOForeach into a logical plan even if it doesn't need to make any casts. The attached patch adds a special case to the TypeCastInserter to skip this behaviour if possible. The patch also refactors some of the if-instanceof-x-else-if-instanceof-y behaviour into subclasses.,"Duplicated Code, Long Method, , "
"Rename Method,","Giving CSVExcelStorage an option to handle header rows Adds an argument to CSVExcelStorage to skip the header row when loading. This works properly with multiple small files each with a header being combined into one split, or a large file with a single header being split into multiple splits.

Also fixes a few bugs with CSVExcelStorage, including PIG-2470 and a bug involving quoted fields at the end of a line not escaping properly.",", "
"Rename Method,Extract Method,","Add log4j.properties for unit tests Currently, debug level messages are not logged for unit tests. It is helpful to enable them to debug unit tests.","Duplicated Code, Long Method, , "
"Move And Rename Class,Rename Method,","Allow Pig use Hive UDFs It would be nice if Pig provide some interoperability with Hive. We can wrap Hive UDF in Pig so we can use Hive UDF in Pig.

This is a candidate project for Google summer of code 2013. More information about the program can be found at https://cwiki.apache.org/confluence/display/PIG/GSoc2013",", "
"Rename Method,","Refactor physical operators to remove methods parameters that are always null The physical operators are sometimes overly complex. I'm trying to cleanup some unnecessary code.
in particular there is an array of getNext(*T* v) where the value v does not seem to have any importance and is just used to pick the correct method.
I have started a refactoring for a more readable getNext*T*().
",", "
"Rename Method,Extract Method,","Strict datetime parsing and improve performance of loading datetime values The performance of loading datetime values can be improved by about 25% by moving a single line in ToDate.java:

public static DateTimeZone extractDateTimeZone(String dtStr) {
Pattern pattern = Pattern.compile(""(Z|(?<=(T[0-9\\.:]{0,12}))((\\+|-)\\d{2}(:?\\d{2})?))$"");;

should become:

static Pattern pattern = Pattern.compile(""(Z|(?<=(T[0-9\\.:]{0,12}))((\\+|-)\\d{2}(:?\\d{2})?))$"");
public static DateTimeZone extractDateTimeZone(String dtStr) {

There is no need to recompile the regular expression for every value. I'm not sure if this function is ever called concurrently, but Pattern objects are thread-safe anyways.

As a test, I created a file of 10M timestamps:

for i in 0..10000000
puts '2000-01-01T00:00:00+23'
end

I then ran this script:

grunt> A = load 'data' as (a:datetime); B = filter A by a is null; dump B;

Before the change it took 160s.
After the change, the script took 120s.

----------------

Another performance improvement can be made for invalid datetime values. If a datetime value is invalid, an exception is created and thrown, which is a costly way to fail a validity check. To test the performance impact, I created 10M invalid datetime values:

for i in 0..10000000
puts '2000-99-01T00:00:00+23'
end

In this test, the regex pattern was always recompiled. I then ran this script:

grunt> A = load 'data' as (a:datetime); B = filter A by a is not null; dump B;

The script took 190s.

I understand this could be considered an edge case and might not be worth changing. However, if there are use cases where invalid dates are part of normal processing, then you might consider fixing this.

","Duplicated Code, Long Method, , "
"Rename Method,Extract Method,","Register Statements and Param Substitution in Macros There are some gaps in the functionality of macros that I've made a patch to address. The goal is to provide everything you'd need to make reusable algorithms libraries.

1. You can't register udfs inside a macro
2. Paramater substitutions aren't done inside macros
3. Resources (including macros) should not be redundantly acquired if they are already present.

Rohini's patch https://issues.apache.org/jira/browse/PIG-3204 should address problem 3 where Pig reparses everything every time it reads a line, but there still would be a problem if two separate files import the same macro / udf file.

To get this working, I moved methods for registering jars/udfs and param substitution from PigServer to PigContext so they can be accessed in QueryParserDriver which processes macros (QPD was already passed a PigContext reference). Is that ok?","Duplicated Code, Long Method, , "
"Move Class,Rename Class,Rename Method,Push Down Method,Inline Method,Push Down Attribute,","Pluggable Execution Engine In an effort to adapt Pig to work using Apache Tez (https://issues.apache.org/jira/browse/TEZ), I made some changes to allow for a cleaner ExecutionEngine abstraction than existed before. The changes are not that major as Pig was already relatively abstracted out between the frontend and backend. The changes in the attached commit are essentially the barebones changes -- I tried to not change the structure of Pig's different components too much. I think it will be interesting to see in the future how we can refactor more areas of Pig to really honor this abstraction between the frontend and backend. 

Some of the changes was to reinstate an ExecutionEngine interface to tie together the front end and backend, and making the changes in Pig to delegate to the EE when necessary, and creating an MRExecutionEngine that implements this interface. Other work included changing ExecType to cycle through the ExecutionEngines on the classpath and select the appropriate one (this is done using Java ServiceLoader, exactly how MapReduce does for choosing the framework to use between local and distributed mode). Also I tried to make ScriptState, JobStats, and PigStats as abstract as possible in its current state. I think in the future some work will need to be done here to perhaps re-evaluate the usage of ScriptState and the responsibilities of the different statistics classes. I haven't touched the PPNL, but I think more abstraction is needed here, perhaps in a separate patch.",", , , , "
"Rename Method,","Move FileLocalizer.setR() calls to unit tests Currently, temporary paths are generated by FileLocalizer using Random.nextInt(). To provide strong randomness, MapReduceLauncher resets the Random object every time when compiling physical plan to MR plan:
{code}
MRCompiler comp = new MRCompiler(php, pc); 
comp.randomizeFileLocalizer(); // This in turn calls FileLocalizer.setR(new Random()).
{code}

Besides, there are a couple of places calling FileLocalizer.setR() (e.g. MRCompiler) with some random seed.

I think-
# Randomizing Random seed is unnecessary if we switch to UUID.
# Setting Random objects in code like this is error-prone because it can be easily broken by having or missing a FileLocalizer.setR() somewhere else. See an example [here|http://search-hadoop.com/m/2nxTzQXfHw1].

So I propose that we remove all this ""randomizing Random seed"" code and use UUID instead in temporary paths.

For unit tests that compare the results against gold files, we should still allow to set Random seed through FileLocalizer.setR(). But this method will be annotated as ""VisibleForTesting"" to ensure it is not used nowhere else other than in unit tests.

Regarding the existing gold files, they can be easily regenerated by TestMRCompiler as follows-
{code}
FileOutputStream fos = new FileOutputStream(expectedFile + ""_new"");
PrintWriter pw = new PrintWriter(fos);
pw.write(compiledPlan);
{code}

I assume there won't be any kind of regressions due to this change. But please let me know if I am wrong.",", "
"Extract Interface,Rename Method,","Ability to disable Pig commands and operators This is an admin feature providing ability to blacklist or/and whitelist certain commands and operations. Pig exposes a few of these that could be not very safe in a multitenant environment. For example, ""sh"" invokes shell commands, ""set"" allows users to change non-final configs. While these are tremendously useful in general, having an ability to disable would make Pig a safer platform. The goal is to allow administrators to be able to have more control over user scripts. Default behaviour would still be the same - no filters applied on commands and operators.",", Large Class, "
"Move Method,Extract Method,","Remodel the XMLLoader to work to be faster and more maintainable I recreated the XMLLoader in PiggyBank to work line by line instead of character by character. This makes it more efficient as it uses precompiled regular expressions on each line instead of doing checks on a character by character basis. The code is also significantly smaller which makes it more maintainable.

Just to put you in perspective. I'm a PhD student in University of Minnesota. I built SpatialHadoop [http://spatialhadoop.cs.umn.edu] which is an extension to Hadoop that adds spatial data types and indexes in HDFS. The system is open source and have been downloads more than 75,000 times so far. Part of it is to provide a simple high level language that works with spatial data.

I proposed Pigeon [http://spatialhadoop.cs.umn.edu/pigeon] as a spatial extension to Pig. My case study is the planet file from OpenStreetMap. This is a 450GB XML file that contains all the information about the whole planet. I previously used XMLLoader to parse it. I found some bugs and fixed it in previous issues. Now, I found that it takes a lot of time to parse the XML file. To be a good citizen, I remodeled the XMLLoader to work line by line and use precompiled regular expressions which makes it faster. The parsing time of the compressed OSM planet file drops from 5:30 hours to 3:30 hours in my cluster setup with Hadoop 1.2.1. By the way, Pigeon was presented in ICDE 2014 [http://ieee-icde2014.eecs.northwestern.edu/program.html], a top conference in data engineering.

The code is now more maintainable. For example, I can easily modify it to add to accept a regular expression for the XML identifier so that it matches all tags that satisfy the regular expression instead of just returning a fixed static tag. In this version, I didn't add any new features but they can be added in the future.","Duplicated Code, Long Method, , , "
"Pull Up Method,Pull Up Attribute,",Move multi store counters to PigStatsUtil from MRPigStatsUtil Multistore counters are applicable to other frameworks (Tez) as well which can support multiquery. So moving them to PigStatsUtil. Related Tez jira PIG-3842 contains these changes in tez branch.,", Duplicated Code, Duplicated Code, "
"Rename Method,","Add LoadCaster to EvalFunc(UDF) this ticket was very close to http://stackoverflow.com/questions/8828839/how-can-correct-data-types-on-apache-pig-be-enforced.
To reproduce the issue, first, we have an UDF to cast map to bag, code almost like(http://stackoverflow.com/questions/12476929/group-key-value-of-map-in-pig?answertab=votes#tab-top)

{code:title=test.pig}
$ cat test.pig
register polisan/maptobag.jar;
define MAPTOBAG maptobag.MAPTOBAG();
A = load 'polisan/input1.txt' using PigStorage(' ') as (id:chararray, kv:[]);
B = foreach A generate id, MAPTOBAG(kv) as to_bag;
C = foreach B generate id, flatten(to_bag) as (key:chararray, value:chararray);
D = group C by (id, key);
E = foreach D generate group, MIN(C.value);
dump E;
{code}

{code:title=polisan/input1.pig}
1 [x#1,y#ab]
1 [x#2,y#cd]
{code}

then run the pig, I got exception as following:
{noformat}
2014-05-15 19:44:52,944 [Thread-2] WARN org.apache.hadoop.mapred.LocalJobRunner - job_local_0001
org.apache.pig.backend.executionengine.ExecException: ERROR 0: Exception while executing (Name: D: Local Rearrange[tuple]{tuple}(false) - scope-42 Operator Key: scope-42): org.apache.pig.backend.executionengine.ExecException: ERROR 2106: Error while computing min in Initial
at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:289)
at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POLocalRearrange.getNextTuple(POLocalRearrange.java:263)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.runPipeline(PigGenericMapBase.java:282)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:277)
at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.map(PigGenericMapBase.java:1)
at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)
at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:212)
Caused by: org.apache.pig.backend.executionengine.ExecException: ERROR 2106: Error while computing min in Initial
at org.apache.pig.builtin.StringMin$Initial.exec(StringMin.java:81)
at org.apache.pig.builtin.StringMin$Initial.exec(StringMin.java:1)
at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNext(POUserFunc.java:352)
at org.apache.pig.backend.hadoop.executionengine.physicalLayer.expressionOperators.POUserFunc.getNextTuple(POUserFunc.java:391)
at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.getNext(PhysicalOperator.java:334)
at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.processPlan(POForEach.java:378)
at org.apache.pig.backend.hadoop.executionengine.physicalLayer.relationalOperators.POForEach.getNextTuple(POForEach.java:298)
at org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator.processInput(PhysicalOperator.java:281)
... 8 more
Caused by: java.lang.ClassCastException: org.apache.pig.data.DataByteArray cannot be cast to java.lang.String
at org.apache.pig.builtin.StringMin$Initial.exec(StringMin.java:73)
... 15 more
{noformat}


",", "
"Move Class,Rename Method,Move Method,Extract Method,Move Attribute,","Merge Tez branch into trunk After months of development, I feel tez branch reaches to a point we can merge back into trunk. The merge will not introduce any known regression, that is: 
1. All existing unit tests pass in mr mode
2. All existing e2e tests pass in mr mode
3. No backward-incompitable changes

The tez branch is stable enough, that is:
1. All migrated unit tests pass in tez mode
2. Vast majority of e2e tests pass in tez mode, minor number of failures are properly investigated

Ongoing tez work can continue after the merge. It is unlikely major changes to other part of Pig code will be involved going forward.

Known limitations for Pig on tez at this moment:
1. Some unit tests are not ported (PIG-3840)
2. Several operators are missing: native, merge-sparse join, collected cogroup
3. auto-parallelism is not in (PIG-3846)","Duplicated Code, Long Method, , , , "
"Rename Method,Extract Method,","Do not create job.jar when submitting job Currently Pig creates job.jar per job when submitting mapreduce job. There are several disadvantages:
1. job.jar varies job by job, job.jar will not get reused even if jar cache is used (PIG-2672).
2. Before job submission, we need to pack a job.jar which are mostly repacking of existing jars, this is a waste of time
3. job.jar is a uber jar which makes debug harder and could lead to jar conflicting issue (eg, PIG-3039)

On tez side, situation is similar, the consequence is worse since container will not be reused.

So instead of job.jar, I would like to ship individual jar to distributed cache. Note this issue is in essence independent of PIG-4047, however, PIG-4047 would make the picture more complete in that we don't have any uber jars.","Duplicated Code, Long Method, , "
"Extract Superclass,Move Method,Extract Method,","Pig on Spark Setting up your development environment:
0. download spark release package(currently pig on spark only support spark 1.6).
1. Check out Pig Spark branch.

2. Build Pig by running ""ant jar"" and ""ant -Dhadoopversion=23 jar"" for hadoop-2.x versions

3. Configure these environmental variables:
export HADOOP_USER_CLASSPATH_FIRST=""true""
Now we support “local” and ""yarn-client"" mode, you can export system variable “SPARK_MASTER” like:
export SPARK_MASTER=local or export SPARK_MASTER=""yarn-client""

4. In local mode: ./pig -x spark_local xxx.pig
In yarn-client mode: 
export SPARK_HOME=xx; 
export SPARK_JAR=hdfs://example.com:8020/xxxx (the hdfs location where you upload the spark-assembly*.jar)
./pig -x spark xxx.pig

","Duplicated Code, Long Method, , , Duplicated Code, Large Class, "
"Move Method,Extract Method,",Some fixes and e2e test for OrcStorage ,"Duplicated Code, Long Method, , , "
"Move Method,Inline Method,","Ship UDF/LoadFunc/StoreFunc dependent jar automatically When user use AvroStorage/JsonStorage/OrcStorage, they need to register dependent jars manually. It would be much convenient if we can provide a mechanism for UDF/LoadFunc/StoreFunc to claim the dependency and ship jars automatically.",", , , "
"Rename Method,Extract Method,",HBaseStorage should implement getShipFiles HBaseStorage.initializeHBaseClassLoaderResources() uses TableMapReduceUtil APIs to add dependency jars. That sets the tmpjars setting which makes JobClient ship the jars to hdfs and use that path in distributed cache. That bypasses the optimizations in PIG-2672 and PIG-3861 which avoid shipping the jars to hdfs. Instead it should implement the getShipFiles() API introduced in PIG-4141 so that PIG-2672 or PIG-3861 avoid shipping the same jar multiple times to hdfs for a job.,"Duplicated Code, Long Method, , "
"Rename Class,Rename Method,",Add a UniqueID UDF ,", "
"Rename Method,","Add pattern matching to PluckTuple PluckTuple is useful when cleaning up long prefixes in a lengthy Pig script. Currently, the udf filters out fields only with exact match, but it would be useful if it could filter based on regex/wildcard.",", "
"Rename Method,Extract Method,",Support for vertex level configuration like speculative execution Need to add code in Pig to translate user setting of mapreduce.map/reduce.speculative and enable speculative execution (tez.am.speculation.enabled) in Tez. Need to take this up after TEZ-1788 - Allow vertex level disabling of speculation is done.,"Duplicated Code, Long Method, , "
"Rename Class,Extract Method,",Print Job stats information in Tez like mapreduce Job stats information in mapreduce is extremely useful while debugging or looking at performance bottlenecks on which of the mapreduce jobs is taking time. It is hard to figure out the same and what aliases are being processed in vertices of Tez without that.,"Duplicated Code, Long Method, , "
"Rename Method,","Do not turn off UnionOptimizer for unsupported storefuncs in case of no vertex groups We turn of UnionOptimizer for unsupported storefuncs as writing from two vertices may overwrite data. But in the case where there is only one unique union member we don't create vertex groups and merge the union operators into the Split vertex, we can turn it on.",", "
"Rename Method,","Provide option to disable DAG recovery Tez 0.7 has lot of issues with DAG recovery with auto parallelism causing hung dags in many cases as it was not writing auto parallelism decisions to recovery history. Rewrite was done in Tez 0.8 to handle that.
Code was added to Tez to automatically disable recovery if there was auto parallelism so that it would benefit both Pig and Tez. It works fine and the second AM attempt fails with DAG cannot be recovered error when it sees there are vertices with auto parallelism. But problem is it is hard to see what the actual problem is for the users and is hard to debug as well as the whole UI state is rewritten with the partial recovery information.
Doing the disabling of recovery in Pig itself by setting tez.dag.recovery.enabled=false will make it not go for the second attempt at all which will eventually fail. It also makes it easy to debug the original failure.",", "
"Move Class,Move And Rename Class,Rename Method,Move Method,","Drop Hadoop 1.x support in Pig 0.17 To facilitate the future development, we want to get rid of the legacy Hadoop 1.x support and reduce the code complexity.",", , "
"Rename Method,","Add a Bloom join In PIG-4925, added option to pass BloomFilter as a scalar to bloom function. But found that actually using it for big data which required huge vector size was very inefficient and led to OOM.
I had initially calculated that it would take around 12MB bytearray for 100 million vectorsize (100000000 + 7) / 8 = 12500000 bytes) and that would be the scalar value broadcasted and would not take much space. But problem is 12MB was written out for every input record with BuildBloom$Initial before the aggregation happens and we arrive at the final BloomFilter vector. And with POPartialAgg it runs into OOM issues. 

If we added a bloom join implementation, which can be combined with hash or skewed join it would boost performance for a lot of jobs. Bloom filter of the smaller tables can be sent to the bigger tables as scalar and data filtered before hash or skewed join is used.",", "
"Rename Class,Extract Method,","Does pig need a NATIVE keyword? Assume a user had a job that broke easily into three pieces. Further assume that pieces one and three were easily expressible in pig, but that piece two needed to be written in map reduce for whatever reason (performance, something that pig could not easily express, legacy job that was too important to change, etc.). Today the user would either have to use map reduce for the entire job or manually handle the stitching together of pig and map reduce jobs. What if instead pig provided a NATIVE keyword that would allow the script to pass off the data stream to the underlying system (in this case map reduce). The semantics of NATIVE would vary by underlying system. In the map reduce case, we would assume that this indicated a collection of one or more fully contained map reduce jobs, so that pig would store the data, invoke the map reduce jobs, and then read the resulting data to continue. It might look something like this:

{code}
A = load 'myfile';
X = load 'myotherfile';
B = group A by $0;
C = foreach B generate group, myudf(B);
D = native (jar=mymr.jar, infile=frompig outfile=topig);
E = join D by $0, X by $0;
...
{code}

This differs from streaming in that it allows the user to insert an arbitrary amount of native processing, whereas streaming allows the insertion of one binary. It also differs in that, for streaming, data is piped directly into and out of the binary as part of the pig pipeline. Here the pipeline would be broken, data written to disk, and the native block invoked, then data read back from disk.

Another alternative is to say this is unnecessary because the user can do the coordination from java, using the PIgServer interface to run pig and calling the map reduce job explicitly. The advantages of the native keyword are that the user need not be worried about coordination between the jobs, pig will take care of it. Also the user can make use of existing java applications without being a java programmer.","Duplicated Code, Long Method, , "
"Rename Method,",Support FLATTEN of maps I have come across users asking for this quite a few times. Don't see why we should not support it with FLATTEN instead of users having to write a UDF for that,", "
"Rename Class,Rename Method,Move Method,Move Attribute,",Upgrade to Spark 2.0 Upgrade to Spark 2.0 (or latest),", , , "
"Rename Class,Rename Method,","Customizable Error Handling for Loaders in Pig Add Error Handling for Loaders in Pig, so that user can choose to allow errors when load data, and set error numbers / rate
Ideas based on error handling on store func see https://issues.apache.org/jira/browse/PIG-4704",", "
"Rename Method,Extract Method,","PERFORMANCE: multi-query optimization Currently, if your Pig script contains multiple stores and some shared computation, Pig will execute several independent queries. For instance:

A = load 'data' as (a, b, c);
B = filter A by a > 5;
store B into 'output1';
C = group B by b;
store C into 'output2';

This script will result in map-only job that generated output1 followed by a map-reduce job that generated output2. As the resuld data is read, parsed and filetered twice which is unnecessary and costly.","Duplicated Code, Long Method, , "
"Rename Method,","PERFORMANCE: multi-query optimization Currently, if your Pig script contains multiple stores and some shared computation, Pig will execute several independent queries. For instance:

A = load 'data' as (a, b, c);
B = filter A by a > 5;
store B into 'output1';
C = group B by b;
store C into 'output2';

This script will result in map-only job that generated output1 followed by a map-reduce job that generated output2. As the resuld data is read, parsed and filetered twice which is unnecessary and costly.",", "
"Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","PERFORMANCE: multi-query optimization Currently, if your Pig script contains multiple stores and some shared computation, Pig will execute several independent queries. For instance:

A = load 'data' as (a, b, c);
B = filter A by a > 5;
store B into 'output1';
C = group B by b;
store C into 'output2';

This script will result in map-only job that generated output1 followed by a map-reduce job that generated output2. As the resuld data is read, parsed and filetered twice which is unnecessary and costly.","Duplicated Code, Long Method, , , , , "
"Rename Method,Move Method,Extract Method,","PERFORMANCE: multi-query optimization Currently, if your Pig script contains multiple stores and some shared computation, Pig will execute several independent queries. For instance:

A = load 'data' as (a, b, c);
B = filter A by a > 5;
store B into 'output1';
C = group B by b;
store C into 'output2';

This script will result in map-only job that generated output1 followed by a map-reduce job that generated output2. As the resuld data is read, parsed and filetered twice which is unnecessary and costly.","Duplicated Code, Long Method, , , "
"Move Method,Extract Method,Move Attribute,","PERFORMANCE: multi-query optimization Currently, if your Pig script contains multiple stores and some shared computation, Pig will execute several independent queries. For instance:

A = load 'data' as (a, b, c);
B = filter A by a > 5;
store B into 'output1';
C = group B by b;
store C into 'output2';

This script will result in map-only job that generated output1 followed by a map-reduce job that generated output2. As the resuld data is read, parsed and filetered twice which is unnecessary and costly.","Duplicated Code, Long Method, , , , "
"Rename Method,","Autocompletion doesn't complete aliases Autocompletion only knows about keywords, but in different contexts, it would be nice if it completed aliases where an alias is expected.",", "
"Rename Method,Extract Method,","Use combiner when algebraic UDFs are used in expressions Currently Pig uses combiner when all a,b, c,... are algebraic (e.g. SUM, AVG etc.) in foreach:

foreach X generate a,b,c,... 

It's a performance improvement if it uses combiner when a mix of algebraic and non-algebraic functions are used as well.","Duplicated Code, Long Method, , "
"Rename Method,","Error reporting for failed MR jobs If we have multiple MR jobs to run and some of them fail the behavior of the system is to not stop on the first failure but to keep going. That way jobs that do not depend on the failed job might still succeed.

The question is to how best report this scenario to a user. How do we tell which jobs failed and which didn't?

One way could be to tie jobs to stores and report which store locations won't have data and which ones do.",", "
"Rename Class,Extract Method,",PERFORMANCE: Merge Join Thsi join would work if the data for both tables is sorted on the join key.,"Duplicated Code, Long Method, , "
"Rename Method,","support cast of chararray to other simple types Pig should support casting of chararray to integer,long,float,double,bytearray. If the conversion fails for reasons such as overflow, cast should return null and log a warning.",", "
"Move Method,Extract Method,","Logical optimizer: push up project This is a continuation work of [PIG-697|https://issues.apache.org/jira/browse/PIG-697]. We need to add another rule to the logical optimizer: Push up project, ie, prune columns as early as possible.","Duplicated Code, Long Method, , , "
"Rename Method,","UDFs in scripting languages It should be possible to write UDFs in scripting languages such as python, ruby, etc. This frees users from needing to compile Java, generate a jar, etc. It also opens Pig to programmers who prefer scripting languages over Java.",", "
"Rename Method,Move Method,","[zebra] Sorted Table Support by Zebra This new feature is for Zebra to support sorted data in storage. As a storage library, Zebra will not sort the data by itself. But it will support creation and use of sorted data either through PIG or through map/reduce tasks that use Zebra as storage format.

The sorted table keeps the data in a ""totally sorted"" manner across all TFiles created by potentially all mappers or reducers.

For sorted data creation through PIG's STORE operator , if the input data is sorted through ""ORDER BY"", the new Zebra table will be marked as sorted on the sorted columns;

For sorted data creation though Map/Reduce tasks, three new static methods of the BasicTableOutput class will be provided to allow or help the user to achieve the goal. ""setSortInfo"" allows the user to specify the sorted columns of the input tuple to be stored; ""getSortKeyGenerator"" and ""getSortKey"" help the user to generate the key acceptable by Zebra as a sorted key based upon the schema, sorted columns and the input tuple.

For sorted data read through PIG's LOAD operator, pass string ""sorted"" as an extra argument to the TableLoader constructor to ask for sorted table to be loaded;

For sorted data read through Map/Reduce tasks, a new static method of TableInputFormat class, requireSortedTable, can be called to ask for a sorted table to be read. Additionally, an overloaded version of the new method can be called to ask for a sorted table on specified sort columns and comparator.

For this release, sorted table only supported sorting in ascending order, not in descending order. In addition, the sort keys must be of simple types not complex types such as RECORD, COLLECTION and MAP. 

Multiple-key sorting is supported. But the ordering of the multiple sort keys is significant with the first sort column being the primary sort key, the second being the secondary sort key, etc.

In this release, the sort keys are stored along with the sort columns where the keys were originally created from, resulting in some data storage redundancy.",", , "
"Rename Method,Pull Up Method,Extract Method,",Ranger tagsync to support Atlas notification for HDFS path Currently Ranger tagsync supports Atlas notifications for Hive resources i.e. entity-types hive_db/hive_table/hive_column. This should be updated to support HDFS path as well.,"Duplicated Code, Long Method, , Duplicated Code, "
"Rename Method,","Optimize tag-download to include only tags that have policies For the calls to download tags from plugins, Ranger Admin returns all the service-resources that have one or more tags associated. This can be optimized to include only service-resources that have tags for which policies exists.

For example, if tag-based policies exists for tags PII and PCI, Ranger Admin should return service-resources that are associated with PII or PCI tags only; any service-resource that is not associated with either of these tags should be excluded. In addition to reducing the size of the tag-download, this can improve policy-engine performance by not having to deal with tags that don't have policies.",", "
"Rename Method,","Ranger policies should support notion of OWNER user Components like HDFS have the notion of an owner for the resource being accessed. For such components, it should be possible to setup Ranger policies to grant specific permissions for owners of accessed resources.

Example usecase: users should have read/write/execute/delegate-admin privileges on files and directories they own under /home/. Ranger policy should be like:

{noformat}
{ path=/home/*; users=[ {OWNER} ]; permission=[ read, write, execute ]; isDelegateAdmin=true}
{noformat}
",", "
"Move Method,Extract Method,Move Attribute,","Consolidate XML configuration parsing Currently, duplicate code appears in several places to parse XML. Instead it should all be consolidated in a single place.","Duplicated Code, Long Method, , , , "
"Rename Method,","Upgrade Ranger to support Apache Hadoop 3.0.0 This task is to upgrade Ranger to support Apache Hadoop 3.0.0. Here are some notes about the upgrade: 

a) The Hive plugin needs the Hadoop 3.0.0 jars to run the tests properly, as Hive only supports the older Hadoop version, so an exclusion and some additional 3.0.0 dependencies need to be added. 
b) The Storm plugin bundles the hadoop-auth jars in storm-core (although they really should be renamed here). Therefore, we have no option but to package Storm with the Hadoop 2.7.x jars, until such time that Storm upgrades the Hadoop dependency. 

This is an initial patch to get some feedback. If there is broad agreement on the upgrade I will test the distributions properly.",", "
"Move Class,Rename Class,Move And Rename Class,Rename Method,Move Method,Move Attribute,",Rename packages from xasecure to apache ranger ,", , , "
"Rename Method,","Auditing for Ranger Usersync operations During every sync cycle, ranger usersync should audit some basic information like number of users, number of groups that are sync'd for that cycle. Also provide details on sync source like the unix, file, or ldap with relevant configuration like ldap filters applied for that sync cycle, ldap host url, etc... 

Add a new tab in the ranger admin UI audits for usersync and show the above information.",", "
"Rename Method,","Policy effective dates to support time-bound and temporary authorization Currently Ranger policies have effectiveness period that is permanent i.e. once authored they can only be disabled or enabled. There are many use cases where such policies or even a policy condition needs to be time bound. For example certain financial information about earnings that is sensitive and restricted only until the earnings release date.  

it would be great to have the ability to specify with each policy a time horizon when it is effective (i.e.) either be effective after a certain date and/or expire after a specific date or only valid within a certain time window and have Ranger check whether the policy is effective before evaluating in the policy engine. Therefore, policy authoring can be simplified and does not require any subsequent action from the user, basically making policy authoring a one time effort and users do not have to go back disable the policies once it is past the expiration date. 

This means that: 
# Ranger policy engine needs to be able to recognize the start and end times for policies  and enforce them based on period of validity specified by the user. 
# Active policies should be checked not only based on the resource, user and environment context but also whether the policy is effective.",", "
"Rename Class,Extract Method,","Policy effective dates to support time-bound and temporary authorization Currently Ranger policies have effectiveness period that is permanent i.e. once authored they can only be disabled or enabled. There are many use cases where such policies or even a policy condition needs to be time bound. For example certain financial information about earnings that is sensitive and restricted only until the earnings release date.  

it would be great to have the ability to specify with each policy a time horizon when it is effective (i.e.) either be effective after a certain date and/or expire after a specific date or only valid within a certain time window and have Ranger check whether the policy is effective before evaluating in the policy engine. Therefore, policy authoring can be simplified and does not require any subsequent action from the user, basically making policy authoring a one time effort and users do not have to go back disable the policies once it is past the expiration date. 

This means that: 
# Ranger policy engine needs to be able to recognize the start and end times for policies  and enforce them based on period of validity specified by the user. 
# Active policies should be checked not only based on the resource, user and environment context but also whether the policy is effective.","Duplicated Code, Long Method, , "
"Move Method,Extract Method,Move Attribute,",Optimize Trie constuction and Policy lookup Ranger uses Trie data structure to look up policy resources for efficient access. Trie tree may be optimized to contain fewer nodes and can be made less deep. This will allow faster construction of Trie tree and faster lookup for a resource.,"Duplicated Code, Long Method, , , , "
"Move Class,Rename Method,Move Method,Extract Method,Move Attribute,","Review and update database schema for ranger policies to minimize database queries/updates Currently, ranger policies are fully normalized and stored in a multiple Relational database tables. There is a performance overhead incurred when retrieving a ranger policy, as multiple database accesses are required to fully reconstruct it. This is significant when there are large ranger policies (that is, the number of resources addressed by the policy is large), and/or when there is a large number of ranger policies in an installation. 

This Jira tracks alternate design of database schema, where a policy is stored in a de-normalized way, in its entirely, in one database table (preferably as a JSON string).","Duplicated Code, Long Method, , , , "
"Move Method,Extract Method,","Support for Incremental policy updates to improve performance of ranger-admin and plugins by optimal building of policy-engine Requirements: 
Currently, every change to any policy causes rebuilding of policy-engine from scratch. There are several disadvantages: 
1. Compute time for rebuilding 
2. Large traffic from ranger-admin to each of the plugins 
3. Large demand on JVM memory system resulting in frequent garbage collection and pauses of JVM. 
It will be more optimal to communicate only the changes and apply them to existing policy-engine. 

Design notes: 

Policy changes are logged into a new database table. 
Cache management in ranger-admin is enhanced to use this table to figure out changes using a previously known version number (provided by module requesting updated policies). 
Policy engine supports update operation that accepts policy-deltas and returns a new policy engine with deltas applied. 
Resource Trie structures are copied from older policy-engine selectively, and not rebuilt from scratch. 
Backward compatibility is maintained with older plugins by adding another parameter to REST API for downloading policies. 
Ranger admin as well as component plugins may be configured to optionally use policy deltas for its internal policy-engines. Policy deltas are disabled by default. In ranger-admin, policy-deltas are enabled in the ranger-admin by setting configuration variable 'ranger.admin.supports.policy.deltas' to true. In individual plugins, policy-deltas are enabled by setting configuration variable 'ranger.plugin.<service-type>.policy.rest.supports.policy.deltas' to ""true"". 
Policy delta table is cleared of records older than a week on restart of ranger-admin.","Duplicated Code, Long Method, , , "
"Rename Method,Extract Method,",Implement Import / Export of Policies by Zone Implement Import / Export of Policies by Zone.,"Duplicated Code, Long Method, , "
"Extract Superclass,Extract Method,Move Attribute,","Add custom condition at policy level Add custom conditions at policy level. 
Currently custom condition is at policy item level and having at policy level also will give more flexible and intutive. Also policy evaluation doesn't need to go the policy item level for checking conditions which are applicable at policy level.","Duplicated Code, Long Method, , , Duplicated Code, Large Class, "
"Rename Method,Extract Method,","Create a tag service when a resource service is created and link it to resource service Ranger supports tag-based policies out of the box. However, there are a few configuration steps that need to be performed in order to set up Ranger to perform tag-based authorization. As these steps are often missed, it will be useful to provide a commonly used/structured way of automatically creating tag service and linking it to resource service. 

This may be controlled through few configuration parameters: 

ranger.tagservice.auto.create=<true|false> ==> If tag-service needs to be created when resource-service is created. 

ranger.tagservice.auto.name=<tag-service-name> ==> If value is specified, then it is used to name the tag-service, otherwise the name of tag-service is constructed from the name of the resource-service (by replacing the part after last '_' by string 'tag', and if there is no '_' character in the resource-service name, then tag-service is not created/linked with resource-service). 

ranger.tagservice.auto.link=<true|false> ==> Used only if ranger.tagservice.auto.create is true. Set to true only if resource-service needs to be linked to the tag-service 

 ","Duplicated Code, Long Method, , "
"Rename Class,Inline Method,","Ranger: use Solr API to upload config set (during bootstrapping) Ranger: 
- remove ZK znode check  
- remove ZK config set upload  
- remove ZK acl set 
- use Solr config set API ([https://lucene.apache.org/solr/guide/7_4/config-sets.html]) to upload the config sets (only if it does not exists ... also probably Solr4J should have this call) 

 ",", , "
"Rename Method,Extract Method,",Good coding practices for concurrent policy label creation Good coding practices for concurrent policy label creation.,"Duplicated Code, Long Method, , "
"Rename Method,Extract Method,","Support for Incremental tag updates to improve performance Currently, every change to any tag/service-resource/service-resource->tag mapping causes complete rebuilding of portions of policy-engine that map accessed resource to their tags. There are several disadvantages: 
1. Compute time for rebuilding 
2. Large traffic from ranger-admin to each of the plugins 
3. Large load on JVM memory system because of frequent complete rebuilding of portions of policy-engine. 


It will be more optimal to communicate only the changes to tags and apply them to existing policy-engine.","Duplicated Code, Long Method, , "
"Move Method,Move Attribute,",Good coding practices for storing and retrieving data history in ranger Good coding practices for storing and retrieving data history in ranger,", , , "
"Rename Class,Rename Method,","Add support for TAG based policies Add support for TAG based policies in Ranger. 
The user should be able specifies some resources (like files/folders/tables/columns/ .....) as CONFIDENTIAL, PII ..... and then, able to define access policies for CONFIDENTIAL data (such as ONLY u1,u2,u2 and group1 should be able to access CONFIDENTIAL data)",", "
"Rename Method,Move Method,","Add support for TAG based policies Add support for TAG based policies in Ranger. 
The user should be able specifies some resources (like files/folders/tables/columns/ .....) as CONFIDENTIAL, PII ..... and then, able to define access policies for CONFIDENTIAL data (such as ONLY u1,u2,u2 and group1 should be able to access CONFIDENTIAL data)",", , "
"Rename Method,","Add support for TAG based policies Add support for TAG based policies in Ranger. 
The user should be able specifies some resources (like files/folders/tables/columns/ .....) as CONFIDENTIAL, PII ..... and then, able to define access policies for CONFIDENTIAL data (such as ONLY u1,u2,u2 and group1 should be able to access CONFIDENTIAL data)",", "
"Rename Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Add support for TAG based policies Add support for TAG based policies in Ranger. 
The user should be able specifies some resources (like files/folders/tables/columns/ .....) as CONFIDENTIAL, PII ..... and then, able to define access policies for CONFIDENTIAL data (such as ONLY u1,u2,u2 and group1 should be able to access CONFIDENTIAL data)","Duplicated Code, Long Method, , , , , "
"Rename Class,Rename Method,Move Method,Move Attribute,","Add support for TAG based policies Add support for TAG based policies in Ranger. 
The user should be able specifies some resources (like files/folders/tables/columns/ .....) as CONFIDENTIAL, PII ..... and then, able to define access policies for CONFIDENTIAL data (such as ONLY u1,u2,u2 and group1 should be able to access CONFIDENTIAL data)",", , , "
"Rename Method,Extract Method,Move Attribute,","Add support for TAG based policies Add support for TAG based policies in Ranger. 
The user should be able specifies some resources (like files/folders/tables/columns/ .....) as CONFIDENTIAL, PII ..... and then, able to define access policies for CONFIDENTIAL data (such as ONLY u1,u2,u2 and group1 should be able to access CONFIDENTIAL data)","Duplicated Code, Long Method, , , "
"Move Class,Rename Class,Move And Rename Class,Rename Method,","Add support for aggregating audit logs at source Instead of logging every access, if possible, add a aggregation of audit log entries before sending it to audit destinations (such as user ""steve"", access = ""read"", resource = ""file:/hr/abc.txt"", no-of-times-accessed=17)",", "
"Rename Class,Rename Method,Pull Up Method,Extract Method,","REST, Store: validation of policy/service/service-def Policy/Service/ServiceDef validation should be added during create/update/delete operation. Here are some of the validations to add:
- values are provided for all mandatory fields (names, resources, etc)
- values are valid (for example: based on validationRegEx in the resource def, enum values, no duplicate names for service-def/service/policy-within-a-service)
- ensure that user has permissions to create/update/view policy/service/service-def
- check for conflicting/overlapping policies","Duplicated Code, Long Method, , Duplicated Code, "
"Rename Method,Extract Method,Move Attribute,","Policy evaluation optimization: reorder policies and short-circuit evaluation The policy engine currently evaluates policies in the order received from the cache/REST API. To minimize the policy evaluation time, it will be best to start with policies that are most likely to match for the access request. For example, policies that deal with all-wildcard for resource names, etc.

This JIRA is to implement this optimization. There will be another JIRA to track more sophisticated optimization, like use of caching, usage pattern, etc.","Duplicated Code, Long Method, , , "
"Rename Method,","Policy evaluation optimization: reorder policies and short-circuit evaluation The policy engine currently evaluates policies in the order received from the cache/REST API. To minimize the policy evaluation time, it will be best to start with policies that are most likely to match for the access request. For example, policies that deal with all-wildcard for resource names, etc.

This JIRA is to implement this optimization. There will be another JIRA to track more sophisticated optimization, like use of caching, usage pattern, etc.",", "
"Rename Method,","Remove custom class loader used by ranger admin for resource lookup Remove custom class loader used by ranger admin for resource lookup

Right now ranger admin resource lookup for hdfs, hive and hbase are relying custom classloader org.apache.ranger.plugin.client.HadoopClassLoader. This requires us to define property name to site file (e.g. core-site.xml) mapping in resourcenamemap.properties.

We should try to eliminate the need for the custom class loader and the need to maintain property name to site file mapping to make it easy to onboard new plugins.",", "
"Rename Class,Push Down Method,Extract Method,Inline Method,Push Down Attribute,","Implement reliable streaming audits to configurable destinations Currently for audit to HDFS, Ranger writes to the file and then transfers the entire file to HDFS on regular interval. This adds additional write operation to the local disk.

The proposal is to write a more intelligent audit writer, audits are sent to destination in real time (or batches) and if the destination is down, then write to local file. When the destination is available, then first send the audit logs from the file system and after it is caught up, resume real-time streaming.

This design also need to address use cases where the destination is slower than the audit producer. In which case, if the internal queue reaches a certain threshold, then the audit will be written to local file till the destination till the in-memory queue is drained.

The design should be generic enough to support any type of destination. By default, the implementation for the following destinations should be provided:
1. HDFS
2. Solr
3. Local File
4. Log4J (with any supported appender)

Additional good to have destinations are :
1. RDBMS
2. Kafka
","Duplicated Code, Long Method, , , , , "
"Rename Method,","Implement reliable streaming audits to configurable destinations Currently for audit to HDFS, Ranger writes to the file and then transfers the entire file to HDFS on regular interval. This adds additional write operation to the local disk.

The proposal is to write a more intelligent audit writer, audits are sent to destination in real time (or batches) and if the destination is down, then write to local file. When the destination is available, then first send the audit logs from the file system and after it is caught up, resume real-time streaming.

This design also need to address use cases where the destination is slower than the audit producer. In which case, if the internal queue reaches a certain threshold, then the audit will be written to local file till the destination till the in-memory queue is drained.

The design should be generic enough to support any type of destination. By default, the implementation for the following destinations should be provided:
1. HDFS
2. Solr
3. Local File
4. Log4J (with any supported appender)

Additional good to have destinations are :
1. RDBMS
2. Kafka
",", "
"Pull Up Method,Move Method,","Implement reliable streaming audits to configurable destinations Currently for audit to HDFS, Ranger writes to the file and then transfers the entire file to HDFS on regular interval. This adds additional write operation to the local disk.

The proposal is to write a more intelligent audit writer, audits are sent to destination in real time (or batches) and if the destination is down, then write to local file. When the destination is available, then first send the audit logs from the file system and after it is caught up, resume real-time streaming.

This design also need to address use cases where the destination is slower than the audit producer. In which case, if the internal queue reaches a certain threshold, then the audit will be written to local file till the destination till the in-memory queue is drained.

The design should be generic enough to support any type of destination. By default, the implementation for the following destinations should be provided:
1. HDFS
2. Solr
3. Local File
4. Log4J (with any supported appender)

Additional good to have destinations are :
1. RDBMS
2. Kafka
",", , Duplicated Code, "
"Rename Method,","Provide REST API to change user role Provide REST API to change user role.
",", "
"Rename Method,","Provide a way to clean-up old policy-engine and related resources. When updated service-policies are fetched by a plug-in from ranger-admin, a new policy-engine is created to process authorization requests using the updated service-policies. A clean way to release critical resources held by the old policy-engine instance is needed for optimal use of system resources.",", "
"Move Method,Extract Method,","Ranger policy should support variables like $user It would be good to support variables in resources and users.

E.g.

HDFS Resource = /home/$user 
or
Table Resource = ${user}_*

Users allowed = $user

Where $user will be expanded to the current user. 

I think, resource substitution will be easy. For permission, we can use key word like we use for all users group=""public"". We can use key word like ""USER"" or something like that.
","Duplicated Code, Long Method, , , "
"Move Method,Extract Method,","higher level policy API to hide complexity of policy update/create/delete Ranger has very good fine-grained policy API with which user can define access control rules for any resource. But sometimes it is not human being but third party tools may use Ranger policy API to temporarily block or unblock user. The third party tool just wants to simply tell Ranger that ""please block/unblock this user from accessing resource A"" and the third party tool is not able to analyze the complicated scenarios as follows:
1. The exactly same rule already exists for resource A
2. The current rules for resource A includes the new rule implicitly
3. There is no any rules for resource A

If it's admin to operate the policy, admin can analyze policy semantics and will figure out it's to create a new policy or update an existing policy. 

To better support integration from third party tool, Ranger can provide a higher level API which accepts request like ""block user access to one resource"" and internally figure out what policy to create/update.","Duplicated Code, Long Method, , , "
"Rename Method,Extract Method,","higher level policy API to hide complexity of policy update/create/delete Ranger has very good fine-grained policy API with which user can define access control rules for any resource. But sometimes it is not human being but third party tools may use Ranger policy API to temporarily block or unblock user. The third party tool just wants to simply tell Ranger that ""please block/unblock this user from accessing resource A"" and the third party tool is not able to analyze the complicated scenarios as follows:
1. The exactly same rule already exists for resource A
2. The current rules for resource A includes the new rule implicitly
3. There is no any rules for resource A

If it's admin to operate the policy, admin can analyze policy semantics and will figure out it's to create a new policy or update an existing policy. 

To better support integration from third party tool, Ranger can provide a higher level API which accepts request like ""block user access to one resource"" and internally figure out what policy to create/update.","Duplicated Code, Long Method, , "
"Move And Rename Class,",Create a new project which can serve as a template to write ranger extensions Context Enrichers and Condition Evaluators allow for people to write extension for ranger. It would be helpful to have an separate sub project that can not only serve as a repository of sample code to serve as examples but also provide a maven based project template that someone can use as a base to write/build extensions.,", "
"Rename Method,","Use system supplied mechanism to get users and groups on unix The unix user sync currently reads /etc/passwd /etc/groups . This is often not a reflection of users and groups available on a system especially when nsswitch is configured (eg. sssd, ldap etc).

Secondly in some cases groups will contain user names that are not returned with ""getent passwd"", especially ""external users"" and it is required to add these using the group information.",", "
"Rename Method,Extract Method,","Add Kerberos support for ranger admin and clients Proposing to enable kerberos in ranger admin and its clients (plugins, usersync, KMS and tagsync)

Also see https://issues.apache.org/jira/browse/RANGER-686","Duplicated Code, Long Method, , "
"Rename Method,Extract Method,","Ranger Policy model to support data masking Ability to mask sensitive data based on user, group and other criteria is one of the often asked features. This JIRA is to track update to Ranger policy model and policy engine to support mask features.","Duplicated Code, Long Method, , "
"Rename Method,Extract Method,","Ranger policy model to support row-filtering Ability to apply filters based on user/group/other-criteria to restrict the results returned for queries is one of the often asked features. This JIRA is to track update to Ranger policy model and policy engine to support row-filter feature.
","Duplicated Code, Long Method, , "
"Rename Method,","Support download csv in Reports page as enhancement +*LIst of changes / improvements*+
# Along with download excel spreadsheet feature, add support for download file as CSV format.
# On Reports page for searched policies: downloaded policy file should have multiple policy items on separate row. 
",", "
"Rename Method,","User Authentication using email In additional to existing user authentication using username, an email will be used as the user for authenticating the subject. In the modern social networking sites mostly uses email as the user for authentication.",", "
"Rename Method,","Template mechanism for Enduser UI Provide a mechanism for defining templates (in terms of HTML, CSS and image files) to simply the appearance customization of the Enduser UI.
The the goal of the issue is to provide a way to:
- avoid HTML code duplication, and define reusable components.
- define an HTML template mechanism which aim is to improve customizability of the enduser (enduser is meant to be customized and extended, it is not a finite product, but a proposal from which to start a new implementation).
- exploit as much as possible code re-usability features provided by AngularJS (if possible).
- if needed review actula CSS implementation, in order to better fit the new template mechanism
- do NOT compromise (or change) enduser functionalities at all! Buttons, selects, wizard and other components should preserve their role and function; the core logic should remain the same, though enduser is open to discuss improvements also in that way ;)
[This|https://code.angularjs.org/1.6.3/docs/guide/templates] could be a good starting point to understand how to use AngularJS tools to implement templating.
BTW If you have proposals that does not involve AngularJS features, they are well accepted and can be accepted anyway.
Some of the features described there, like directives, have also been used in the Enduser to define some reusable components (e.g. dynamic-plain-attribute).",", "
"Rename Class,Rename Method,Inline Method,Pull Up Attribute,","Support for BPMN call activity From the [Activiti User Guide|https://www.activiti.org/userguide/#bpmnCallActivity]:

{quote}
BPMN 2.0 makes a distinction between a regular subprocess, often also called embedded subprocess, and the call activity, which looks very similar. From a conceptual point of view, both will call a subprocess when process execution arrives at the activity.

The difference is that the call activity references a process that is external to the process definition, whereas the subprocess is embedded within the original process definition. The main use case for the call activity is to have a reusable process definition that can be called from multiple other process definitions.
{quote}

It is currently possible to create more process definitions (besides the default {{userWorkflow}}) by empowering the REST endpoint

{code}
PUT /workflows/{anyTypeKind}
{code}

The new process(es) defined can then be called from the main {{userWorkflow}} via the {{<callActivity/>}} element(s): the main advantage is that, by doing so, there are no more problems about the process definition versions, as they only apply to the main process (e.g. {{userWorkflow}}).

What is currently lacking is:
# proper management for getting all available process definitions
# proper handling for initial loading of several process definitions from XML files
# proper editing features from Admin Console

as all the items above only consider the possibility that a single process definition is available.",", , Duplicated Code, "
"Rename Class,Rename Method,Move Method,Inline Method,Pull Up Attribute,","Support for BPMN call activity From the [Activiti User Guide|https://www.activiti.org/userguide/#bpmnCallActivity]:

{quote}
BPMN 2.0 makes a distinction between a regular subprocess, often also called embedded subprocess, and the call activity, which looks very similar. From a conceptual point of view, both will call a subprocess when process execution arrives at the activity.

The difference is that the call activity references a process that is external to the process definition, whereas the subprocess is embedded within the original process definition. The main use case for the call activity is to have a reusable process definition that can be called from multiple other process definitions.
{quote}

It is currently possible to create more process definitions (besides the default {{userWorkflow}}) by empowering the REST endpoint

{code}
PUT /workflows/{anyTypeKind}
{code}

The new process(es) defined can then be called from the main {{userWorkflow}} via the {{<callActivity/>}} element(s): the main advantage is that, by doing so, there are no more problems about the process definition versions, as they only apply to the main process (e.g. {{userWorkflow}}).

What is currently lacking is:
# proper management for getting all available process definitions
# proper handling for initial loading of several process definitions from XML files
# proper editing features from Admin Console

as all the items above only consider the possibility that a single process definition is available.",", , , Duplicated Code, "
"Rename Method,","Hide key when creating / editing Security Questions from Admin Console When creating or editing Security Questions from Admin Console, the key is an auto-generated field of no interest.
It is currently reported and not modifiable: it should be better hidden instead.",", "
"Rename Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","JWT-based access to REST services Since the beginning, access to the REST services is protected via Basic Authentication, with credentials sent along each and every request.

As improvement, we can switch to an architecture where there is an explicit REST service for obtaining some sort of token (requiring credentials) and then all other REST services can be accessed by sending along such token instead of credentials.
This will ease future works for enabling SSO via SAML, OAuth 2.0 or other standards.

About the token format, it seems that [JSON Web Tokens|https://jwt.io/] are quite the default choice, especially considering the support that CXF already provides for that.","Duplicated Code, Long Method, , , , , "
"Move Class,Rename Method,Move Method,Extract Method,Pull Up Attribute,Move Attribute,",SAML 2.0 Service Provider feature Provide the ability to perform SSO to Admin Console and Enduser UI via an external SAML 2.0 Identity Provider.,"Duplicated Code, Long Method, , , , Duplicated Code, "
"Rename Class,Pull Up Method,Pull Up Attribute,",SAML 2.0 Service Provider feature Provide the ability to perform SSO to Admin Console and Enduser UI via an external SAML 2.0 Identity Provider.,", Duplicated Code, Duplicated Code, "
"Extract Method,Move Attribute,",SAML 2.0 Service Provider feature Provide the ability to perform SSO to Admin Console and Enduser UI via an external SAML 2.0 Identity Provider.,"Duplicated Code, Long Method, , , "
"Rename Class,Move Class,Move Method,Extract Method,Inline Method,","Replace ActionLinksPanel with TogglePanel Data tables are used across almost all features of Admin Console.
Within data tables, the general UI paradigm is to show (the same) icons per row, with each icon triggering a different action for the entity represented by that row.
The Wicket panel implementing such multi-icon actions is [ActionLinksPanel|https://github.com/apache/syncope/blob/2_0_X/client/console/src/main/java/org/apache/syncope/client/console/wicket/markup/html/form/ActionLinksPanel.java].

While this component has been serving for the purpose, it might be useful to improve the UX by empowering [TogglePanel|https://github.com/apache/syncope/blob/2_0_X/client/console/src/main/java/org/apache/syncope/client/console/panels/TogglePanel.java] - the transparent menu activated when clicking on a node in the Topology.

The idea is to:

# remove all icons shown in each data table row
# when clicking on a given data table row, show a menu based on {{TogglePanel}} providing access to all actions available for the entity represented by that row.","Duplicated Code, Long Method, , , , "
"Rename Class,Push Down Method,","Allow easier extension of REST interface exposed to AngularJS The REST interface exposed by the Wicket component of the Enduser UI is based on Wicket's {{mountResource}} features.
However, all the resources are explicitly mounted in {{SyncopeEnduserApplication#init}}.

Syncope extensions and local deployments might need however, to enrich such REST interface.",", , "
"Move Class,Rename Class,Move And Rename Class,Rename Method,Move Method,Move Attribute,","Replace Activiti-based workflow adapter with Flowable Following the discussion in ML, the idea is to upgrade the current workflow engine based on Activiti 5.X to one featuring Flowable 6.",", , , "
"Rename Class,Move Method,Extract Method,Move Attribute,","Support SAML 2.0 Redirect profile SYNCOPE-1041 introduced the SAML 2.0 SP extension, supporting only the POST binding profile; adding support for the Redirect profile should not be hard, thanks to the underlying support from {{cxf-rt-rs-security-sso-saml}}.","Duplicated Code, Long Method, , , , "
"Rename Method,","More flexible delegated administration model The current implementation of [delegated administration|https://syncope.apache.org/docs/reference-guide.html#delegated-administration] relies on Roles, where each Role associates a set of Entitlements (e.g. administrative actions) to a set of Realms (e.g. containers for Users / Groups / Any Objects).

This requires, however, that the set of Users / Groups / Any Objects to administer is somehow statically defined by containment: ""administrators with role R can manage users under realms /a and /b"" works as long as users to administer are fully contained by the Realms /a and /b; but what if the set of Users that R can administer needs to be dynamically defined, say by the value of a 'department' attribute?",", "
"Rename Class,Pull Up Method,Inline Method,Pull Up Attribute,","More flexible delegated administration model The current implementation of [delegated administration|https://syncope.apache.org/docs/reference-guide.html#delegated-administration] relies on Roles, where each Role associates a set of Entitlements (e.g. administrative actions) to a set of Realms (e.g. containers for Users / Groups / Any Objects).

This requires, however, that the set of Users / Groups / Any Objects to administer is somehow statically defined by containment: ""administrators with role R can manage users under realms /a and /b"" works as long as users to administer are fully contained by the Realms /a and /b; but what if the set of Users that R can administer needs to be dynamically defined, say by the value of a 'department' attribute?",", , Duplicated Code, Duplicated Code, "
"Rename Method,Pull Up Method,Extract Method,Pull Up Attribute,","Extension: Elasticsearch-based search engine As outlined in SYNCOPE-1006, the current search engine is somehow fragile (being based on SQL views) and highly depends on the DBMS used as internal storage - with frequent issues for MySQL / MariaDB.

[As suggested|https://issues.apache.org/jira/browse/SYNCOPE-1006?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15856313#comment-15856313], an idea could be to provide another, optional search engine implementation which requires an Elasticsearch node.","Duplicated Code, Long Method, , Duplicated Code, Duplicated Code, "
"Rename Method,","Extension: Elasticsearch-based search engine As outlined in SYNCOPE-1006, the current search engine is somehow fragile (being based on SQL views) and highly depends on the DBMS used as internal storage - with frequent issues for MySQL / MariaDB.

[As suggested|https://issues.apache.org/jira/browse/SYNCOPE-1006?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15856313#comment-15856313], an idea could be to provide another, optional search engine implementation which requires an Elasticsearch node.",", "
"Rename Method,",Downloaded file for binary attribute better naming Name of the downloaded file should be like: <key>_<schemaName>.<standard extension for that mimetype>,", "
"Move Class,Move Method,Push Down Attribute,Move Attribute,","Option to disable Quartz instances across cluster The only considered aspect, when configuring Syncope for [high-availability|https://syncope.apache.org/docs/reference-guide.html#high-availability], is related to OpenJPA's remote commit provider, at least according to the current documentation.

However, another component is relevant within this regard, e.g. the Quartz scheduler, which is currently simply setup by default for [clustering|http://www.quartz-scheduler.org/documentation/quartz-2.2.x/configuration/ConfigJDBCJobStoreClustering.html].

With such configuration, all cluster nodes are equally selectable for processing jobs; it would be nice, though, to gain some control over this aspect, e.g. to be able to restrict the nodes where jobs can run.
For example, one can have 3 Syncope core nodes configured with OpenJPA remote commit provider, and set 2 of them for processing REST requests, leaving the third dedicated for running jobs (e.g. Pull).",", , , , "
"Rename Method,","Support functions for internal JEXL engine JEXL can register objects or classes used as [function namespaces|https://commons.apache.org/proper/commons-jexl/reference/syntax.html#Functions].

Since JEXL expressions are used everywhere, such feature would enhance the capabilities to adapt to different use cases, as the one intended to be supported by SYNCOPE-1116 for Realms' object link.",", "
"Rename Method,","Third Party JWT SSO integration This task is to support SSO using third party JWT tokens.

It involves two tasks:

a) Create a new interface extending JwsSignatureVerifier to provide a method to resolve a JWT subject into Syncope username (known user).

b) When processing a received token, if the issuer is different from the known issuer (""jwtIssuer"" in security.properties), then instead of retrieving the default jwsSignatureVerifier implementation, the authentication component will enable the ClassPathScanImplementationLookup to dynamically discover an implementation of the interface above.",", "
"Push Down Method,Inline Method,","Update RelationshipTO to also report the ""left"" end of a relationship Currently, the RelationshipTO object only reports the ""right"" end of a relationship. However, as relationships in Syncope are bi-directional, we should also report the ""left"" end of a relationship. This will make searching for AnyTypes a bit easier than it is at present.",", , , "
"Move Class,Move And Rename Class,Rename Method,Inline Method,","Customizable Audit appender The Audit mechanism is based on LOG4J, configured to use the {{JDBCAppender}} to store the audit statements into the {{SyncopeAudit}} table in the internal storage.

Besides this base mechanism, we can introduce the {{AuditAppender}} interface, whose instances can:

# declare which event(s) they will be invoked with
# declare another LOG4J appender to send the statements to (besides the {{JDBCAppender}} as above)
# optionally offer the ability to transform the standard statement to a format more suitable to the target appender",", , "
"Extract Method,Inline Method,","Connector and Resource configuration versioning It often happens that, while playing with Connectors' and Resources' configuration, everything works up until a certain point, then some misconfiguration happens and errors start appearing.

In such situations, it would be handy to have a simple mechanism to revert to a previous (working) situation.","Duplicated Code, Long Method, , , "
"Rename Method,Move Method,Move Attribute,","On-the-fly creation of unmatched users logging via SAML 2.0 As per the current implementation, when logging into the Admin Console via SAML 2.0, an internal user matching the configured attribute of the Authentication Assertion is looked up.
If not found, an error is raised.

Things could be configured, however, to create on-the-fly an internal user with the attributes provided by the Authentication Assertion, and to let them log in.
",", , , "
"Rename Class,Move Class,Rename Method,Extract Method,","Clear out unneeded anonymous authenticated services Following the [discussion|https://wilderness.apache.org/channels/?f=apache-syncope/2017-06-28] we had on IRC with [~coheigea], it seems that a few Entitlements, not available since earlier versions, might be re-introduced, to properly control access to the related REST services:

* GROUP_LIST
* RESOURCE_LIST
* ANYTYPE_LIST
* ANYTYPECLASS_LIST
* SCHEMA_LIST
* SECURITY_QUESTION_LIST
* REALM_LIST

The rationale not to have such Entitlements in Syncope 1.x was that the related information had to be made available during self-registration.

Now that we have the Enduser UI, however, it seems that it is possible to

# introduce dedicated REST endpoint(s) to serve such content for self-registration, with minimal information (for example only group names, no need to provide extra information as attributes, type extensions, etc.)
# restore appropriate access control for the REST endpoints to be accessed for administrative purposes","Duplicated Code, Long Method, , "
"Push Down Method,Push Down Attribute,","Clear out unneeded anonymous authenticated services Following the [discussion|https://wilderness.apache.org/channels/?f=apache-syncope/2017-06-28] we had on IRC with [~coheigea], it seems that a few Entitlements, not available since earlier versions, might be re-introduced, to properly control access to the related REST services:

* GROUP_LIST
* RESOURCE_LIST
* ANYTYPE_LIST
* ANYTYPECLASS_LIST
* SCHEMA_LIST
* SECURITY_QUESTION_LIST
* REALM_LIST

The rationale not to have such Entitlements in Syncope 1.x was that the related information had to be made available during self-registration.

Now that we have the Enduser UI, however, it seems that it is possible to

# introduce dedicated REST endpoint(s) to serve such content for self-registration, with minimal information (for example only group names, no need to provide extra information as attributes, type extensions, etc.)
# restore appropriate access control for the REST endpoints to be accessed for administrative purposes",", , , "
"Rename Class,Extract Interface,Extract Superclass,Rename Method,Extract Method,","Complete mapping for Realm provisioning The current settings for Realm provisioning are minimal, if compared to the mapping that can be provided for Users, Groups or Any Objects; essentially, only External Attribute and Object Link (when applicable) can be specified.

This can lead to limitations, which do not apply to ordinary provisioning.","Duplicated Code, Long Method, , Duplicated Code, Large Class, Large Class, "
"Rename Method,Extract Method,","Further validate SAML responses with CXF's SAMLSSOResponseValidator At the moment, the SAML response is only validated via [SAMLProtocolResponseValidator|https://github.com/apache/cxf/blob/3.1.x-fixes/rt/rs/security/sso/saml/src/main/java/org/apache/cxf/rs/security/saml/sso/SAMLProtocolResponseValidator.java], but CXF also offers [SAMLSSOResponseValidator|https://github.com/apache/cxf/blob/3.1.x-fixes/rt/rs/security/sso/saml/src/main/java/org/apache/cxf/rs/security/saml/sso/SAMLSSOResponseValidator.java], providing further checks.","Duplicated Code, Long Method, , "
"Rename Class,Move And Rename Class,Extract Superclass,Extract Interface,Rename Method,Push Down Method,Move Method,Push Down Attribute,","Realm-based authorization The current authentication / authorization model has some weaknesses, as outlined at [1].

In the same mail thread a refactoring proposal is shown for implementing a system-wide realm-based hierarchical security model.
This will impact nearly every component and layer in the system, so great care should be taken and extensive testing.

Discussion on wiki: https://cwiki.apache.org/confluence/display/SYNCOPE/%5BDISCUSS%5D+Realms

[1] http://syncope-dev.1063484.n5.nabble.com/syncope-dev-Authorization-entitlements-td4830322.html",", , Duplicated Code, Large Class, , , Large Class, "
"Rename Method,Move Method,","Realm-based authorization The current authentication / authorization model has some weaknesses, as outlined at [1].

In the same mail thread a refactoring proposal is shown for implementing a system-wide realm-based hierarchical security model.
This will impact nearly every component and layer in the system, so great care should be taken and extensive testing.

Discussion on wiki: https://cwiki.apache.org/confluence/display/SYNCOPE/%5BDISCUSS%5D+Realms

[1] http://syncope-dev.1063484.n5.nabble.com/syncope-dev-Authorization-entitlements-td4830322.html",", , "
"Rename Class,Rename Method,Pull Up Method,Move Method,Extract Method,Inline Method,","Realm-based authorization The current authentication / authorization model has some weaknesses, as outlined at [1].

In the same mail thread a refactoring proposal is shown for implementing a system-wide realm-based hierarchical security model.
This will impact nearly every component and layer in the system, so great care should be taken and extensive testing.

Discussion on wiki: https://cwiki.apache.org/confluence/display/SYNCOPE/%5BDISCUSS%5D+Realms

[1] http://syncope-dev.1063484.n5.nabble.com/syncope-dev-Authorization-entitlements-td4830322.html","Duplicated Code, Long Method, , , , Duplicated Code, "
"Rename Method,Pull Up Attribute,","Realm-based authorization The current authentication / authorization model has some weaknesses, as outlined at [1].

In the same mail thread a refactoring proposal is shown for implementing a system-wide realm-based hierarchical security model.
This will impact nearly every component and layer in the system, so great care should be taken and extensive testing.

Discussion on wiki: https://cwiki.apache.org/confluence/display/SYNCOPE/%5BDISCUSS%5D+Realms

[1] http://syncope-dev.1063484.n5.nabble.com/syncope-dev-Authorization-entitlements-td4830322.html",", Duplicated Code, "
"Rename Method,Extract Method,","Realm-based authorization The current authentication / authorization model has some weaknesses, as outlined at [1].

In the same mail thread a refactoring proposal is shown for implementing a system-wide realm-based hierarchical security model.
This will impact nearly every component and layer in the system, so great care should be taken and extensive testing.

Discussion on wiki: https://cwiki.apache.org/confluence/display/SYNCOPE/%5BDISCUSS%5D+Realms

[1] http://syncope-dev.1063484.n5.nabble.com/syncope-dev-Authorization-entitlements-td4830322.html","Duplicated Code, Long Method, , "
"Move Method,Extract Method,Move Attribute,","Allow AnyType-based conditions for DynRealms Currently DynRealms allow to specify a single matching condition, which is applied to Users, Groups and AnyObjects.
This is a limitation, because there is no way to express conditions which might be relevant for Users only, for example (as group membership).","Duplicated Code, Long Method, , , , "
"Rename Class,Extract Method,",Allow for easier Pull / Push processes customization The current pull process (driven by {{PullJobDelegate}}) and push process (driven by {{PushJobDelegate}}) do not quite allow to be extended for a given deployment - mainly because of private fields and methods.,"Duplicated Code, Long Method, , "
"Rename Class,Rename Method,Extract Method,",Password change on an external resource only Give option to change password only on one or more connected external resources; currently it's only possible to change password on Syncope core and then propagate accordingly to the connected external resources.,"Duplicated Code, Long Method, , "
"Move Class,Rename Method,Inline Method,","Search funcionality in Schemas I think it would be very useful in environments with several issues to be able to search schemas using it's attributes (key, type,etc)",", , "
"Extract Method,Push Down Attribute,","Binary schema As reported at [1], only String, Enum, Boolean, Long, Double, Date types are supported.

Mail thread about certificates [2].

[1] https://cwiki.apache.org/confluence/display/SYNCOPE/Schema%2C+attributes+and+mapping#Schema%2Cattributesandmapping-Schema
[2] http://syncope-user.1051894.n5.nabble.com/Certificates-in-Syncope-td5706704.html","Duplicated Code, Long Method, , , "
"Rename Class,Extract Interface,Extract Method,Inline Method,","Provide live updates from running tasks and reports It is currently quite hard to figure out what a running task or report is doing. 

While Admin Console provides feedback about the fact the related job is running or not (and log files can be checked to see what's going on), it is impossible to inspect how many users have been pulled so far.","Duplicated Code, Long Method, , , Large Class, "
"Rename Method,Extract Method,Inline Method,","Remediation Errors during pull might arise for various reasons: for example, values are not provided for all mandatory attributes, or values are failing the configured validation. 

Currently, if an entity (User, Group or Any Object) is failing during a Pull Task execution, it is simply reported as an error, and logged as execution result. 

As a new feature, administrators could be given the chance to perform _remediation_ on the failing entities, in a similar fashion they do with approval forms.","Duplicated Code, Long Method, , , "
"Rename Method,","Use Remote Key during Pull to match internal entities Following SYNCOPE-1182, extend the same approach to internal entities matching during Pull, e.g. do not use {{__UID__}} but rather the value for the attribute flagged as {{Remote Key}} in the Mapping.",", "
"Move Method,Extract Method,Move Attribute,",Create a structured wizard to edit SCIM 2.0 configuration ,"Duplicated Code, Long Method, , , , "
"Rename Method,Inline Method,","Manual reconciliation Provide the feature - in Admin Console from either an User / Group / Any Object and from an External Resource (under Topology) - which, given a User / Group / Any Object and an External Resource, allows to force pushing or pulling values for mapped attributes.",", , "
"Rename Class,Rename Method,Move Method,Inline Method,","Manual reconciliation Provide the feature - in Admin Console from either an User / Group / Any Object and from an External Resource (under Topology) - which, given a User / Group / Any Object and an External Resource, allows to force pushing or pulling values for mapped attributes.",", , , "
"Rename Method,Pull Up Method,","Manual reconciliation Provide the feature - in Admin Console from either an User / Group / Any Object and from an External Resource (under Topology) - which, given a User / Group / Any Object and an External Resource, allows to force pushing or pulling values for mapped attributes.",", Duplicated Code, "
"Rename Class,Rename Method,Move Method,","Manual reconciliation Provide the feature - in Admin Console from either an User / Group / Any Object and from an External Resource (under Topology) - which, given a User / Group / Any Object and an External Resource, allows to force pushing or pulling values for mapped attributes.",", , "
"Rename Method,",Resource: ignoreCase match Add a flag to ExternalResource to indicate whether match (during propagation and / or pull) should be performed as case-sensitive (as currently implemented) or not.,", "
"Rename Method,Push Down Method,Move Method,Extract Method,Push Down Attribute,Move Attribute,","Assign membership and role schemas to either all memberships / roles or only some memberships / roles Currently, membership and role schemas are defined for all memberships / roles.
This means that when defining a mandatory role schema, all roles must provide a value for the corresponding attribute. Same applies for memberships.

This mechanism should be extended so that you can choose which schemas are associated to each role / membership, in order to give more flexibility.","Duplicated Code, Long Method, , , , , , "
"Rename Method,Extract Method,Move Attribute,","Don't expose some REST list methods for anonymous Currently, in order to provide some authenticationless features through the console (mainly, self-registration), some REST services don't require any authentication.","Duplicated Code, Long Method, , , "
"Rename Method,","Add pagination for approvals forms If there are many approval tasks, the console takes a long time to load all the forms. 
It's necessary to add pagination in the query. 
 ",", "
"Rename Method,Extract Method,","Add pagination for approvals forms If there are many approval tasks, the console takes a long time to load all the forms. 
It's necessary to add pagination in the query. 
 ","Duplicated Code, Long Method, , "
"Rename Method,Extract Method,","User, role and membership properties for derived schemas Allow user, role and membership properties (like as id and name, for example) to be used in derived schema definition","Duplicated Code, Long Method, , "
"Rename Class,Move And Rename Class,Rename Method,Extract Method,","REST: replace bulk operations with batch requests There are currently quite some bulk REST endpoints; each of these, however, is quite limited as requires to either not provide any particular input besides the set of elements to act on, or to design a dedicated payload for such a purpose. 

The batch approach provided by the [OData 4.0 specification|http://docs.oasis-open.org/odata/odata/v4.0/os/part1-protocol/odata-v4.0-os-part1-protocol.html#_Toc372793748] seems a perfect match to replace the partial approach provided by bulk endpoints.","Duplicated Code, Long Method, , "
"Rename Method,Move Method,","Password reset Provide password reset feature, that can be accessed either trough the console and via REST call.",", , "
"Rename Method,Move Method,Extract Method,","Password reset Provide password reset feature, that can be accessed either trough the console and via REST call.","Duplicated Code, Long Method, , , "
"Rename Method,","Password required for resource subscription Currently, cleartext password is always required when subscribing to a new external resource.
However, in some cases (for example when passwords are stored with some symmetric algorithm) this can be avoided.

For example, it could be:

Case 1: 2-way (a.k.a. symmetric) password cipher algorithm is configured in Syncope

Use decrypted password from SyncopeUser to subscribe new resource.


Case 2: 1-way (a.k.a. hash or asymmetric) password cipher algorithm is configured in Syncope and no clear-text password is available (for example, passed via UserMod or provided by a synchronizing resource)

Provide, on a resource-basis, a mean to configure how new password should be generated:
* constant
* random password generation (compliant with resource password policy, if present - see SYNCOPE-121)
* provide custom Java class


Discussion thread: http://syncope-dev.1063484.n5.nabble.com/new-password-issue-td5589622.html",", "
"Move Class,Rename Class,Move And Rename Class,Rename Method,Move Method,Extract Method,Move Attribute,","User requests With user requests, users can initiate whichever request among the ones defined, for example ""assign me a mobile phone"" or ""give me those groups on AD"", for them or on behalf of others; once initiated, such requests can then follow their own path, which might include one or more approval steps. 
There is also no limitation on the number of concurrent requests that an user can initiate. 

Unfortunately, our current implementation is not able to properly implement the user requests as briefly outlined above; among other things, the impossibility to handle more than an approval process at a time, per user. 

Hence, and a major refactoring is needed: 
# remove approvals features from the current Flowable user workflow adapter 
# define a new UserRequest entity, which includes at least 
## some triggering conditions 
## a Flowable workflow definition, possibly containing approval form(s) 
# adjust REST services, Admin Console and Enduser UI to cope with the new User Request entity","Duplicated Code, Long Method, , , , "
"Move Class,Rename Class,Rename Method,Extract Method,Inline Method,","User requests With user requests, users can initiate whichever request among the ones defined, for example ""assign me a mobile phone"" or ""give me those groups on AD"", for them or on behalf of others; once initiated, such requests can then follow their own path, which might include one or more approval steps. 
There is also no limitation on the number of concurrent requests that an user can initiate. 

Unfortunately, our current implementation is not able to properly implement the user requests as briefly outlined above; among other things, the impossibility to handle more than an approval process at a time, per user. 

Hence, and a major refactoring is needed: 
# remove approvals features from the current Flowable user workflow adapter 
# define a new UserRequest entity, which includes at least 
## some triggering conditions 
## a Flowable workflow definition, possibly containing approval form(s) 
# adjust REST services, Admin Console and Enduser UI to cope with the new User Request entity","Duplicated Code, Long Method, , , "
"Rename Class,Move And Rename Class,Extract Method,","User requests With user requests, users can initiate whichever request among the ones defined, for example ""assign me a mobile phone"" or ""give me those groups on AD"", for them or on behalf of others; once initiated, such requests can then follow their own path, which might include one or more approval steps. 
There is also no limitation on the number of concurrent requests that an user can initiate. 

Unfortunately, our current implementation is not able to properly implement the user requests as briefly outlined above; among other things, the impossibility to handle more than an approval process at a time, per user. 

Hence, and a major refactoring is needed: 
# remove approvals features from the current Flowable user workflow adapter 
# define a new UserRequest entity, which includes at least 
## some triggering conditions 
## a Flowable workflow definition, possibly containing approval form(s) 
# adjust REST services, Admin Console and Enduser UI to cope with the new User Request entity","Duplicated Code, Long Method, , "
"Rename Class,Rename Method,Extract Method,","Support more than one local connector bundles directory and zero or more ConnId's connector servers Currently, a single directory is scanned for searching ConnId's connector bundles.
However, it shouldn't be hard to add support for more directories.

Moreover, it should also be easy to integrate remote ConnId's connector servers (either Java and .NET) via ConnectorInfoManagerFactory.getInstance().getRemoteManager(...) instead of ConnectorInfoManagerFactory.getInstance().getLocalManager(...).","Duplicated Code, Long Method, , "
"Rename Method,Push Down Method,Extract Method,","Reduce usage of Reflection to improve overall performance The source code - especially for core and extensions - is filled up with Reflection-intensive invocation, which are supposed to negatively affect the overall performance; mostly: 
* ReflectionUtils 
* Spring's BeanUtils#copyProperties 
* ReflectionToStringBuilder 
* EqualsBuilder#reflectionEquals 
* HashCodeBuilder#reflectionHashCode","Duplicated Code, Long Method, , , "
"Rename Method,","Reduce usage of Reflection to improve overall performance The source code - especially for core and extensions - is filled up with Reflection-intensive invocation, which are supposed to negatively affect the overall performance; mostly: 
* ReflectionUtils 
* Spring's BeanUtils#copyProperties 
* ReflectionToStringBuilder 
* EqualsBuilder#reflectionEquals 
* HashCodeBuilder#reflectionHashCode",", "
"Rename Method,Extract Method,","Leverage PostgreSQL's jsonb type As [discussed|https://lists.apache.org/thread.html/bcb72efa271c13e86a00b236d05518ebd56ee741624b1160f3fe4ac4@%3Cdev.syncope.apache.org%3E], enhance the JPA layer by empowering [PostgreSQL's jsonb|https://www.postgresql.org/docs/10/datatype-json.html] datatype for user, group and any object attributes.","Duplicated Code, Long Method, , "
"Rename Method,Move Method,Move Attribute,","Leverage PostgreSQL's jsonb type As [discussed|https://lists.apache.org/thread.html/bcb72efa271c13e86a00b236d05518ebd56ee741624b1160f3fe4ac4@%3Cdev.syncope.apache.org%3E], enhance the JPA layer by empowering [PostgreSQL's jsonb|https://www.postgresql.org/docs/10/datatype-json.html] datatype for user, group and any object attributes.",", , , "
"Move And Rename Class,","Leverage PostgreSQL's jsonb type As [discussed|https://lists.apache.org/thread.html/bcb72efa271c13e86a00b236d05518ebd56ee741624b1160f3fe4ac4@%3Cdev.syncope.apache.org%3E], enhance the JPA layer by empowering [PostgreSQL's jsonb|https://www.postgresql.org/docs/10/datatype-json.html] datatype for user, group and any object attributes.",", "
"Rename Method,Extract Method,Inline Method,","Dynamic role and group memberships Introduce the concept of dynamic group and role membership, e.g. an user can be part of a role or a group if matching a given search condition.","Duplicated Code, Long Method, , , "
"Rename Class,Rename Method,","Concurrent propagation Add the option to execute propagation ops concurrently.

Consider that we must continue to propagate towards primary resources sequentially, in respect of the specified priority.
Maybe propagation ops towards non-primary resources can be executed sequentially in respect of the priorities specified and concurrently in case of resources with the same priority.",", "
"Rename Method,",remove user_search_null_attr view Remove user_search_null_attr view because affects performance negatively,", "
"Move Method,Move Attribute,","Perform in-memory match for dynamic conditions Various conditions can be specified - for DynRealms and dynamic Group / Role memberships. 

It is needed in several places to check if a given User, Group or Any Object matches the conditions as above. 

Today such checks are performed via search, which eventually involves database access; this is however not strictly necessary, as the whole check could be performed in-memory.",", , , "
"Rename Method,Pull Up Method,Move Method,Extract Method,Inline Method,Pull Up Attribute,Move Attribute,",Configurable user request approval Make configurable whether UserRequest objects (create / update / delete) need to be approved or not and under which conditions (including membership of certain role(s)).,"Duplicated Code, Long Method, , , , , Duplicated Code, Duplicated Code, "
"Move Class,Extract Interface,Rename Method,","New component: sra (API gateway) At high-level, this [API gateway|https://microservices.io/patterns/apigateway.html] it's an HTTP reverse proxy exposing a +set of public APIs+, where the response for invocation of a public API is the result of a configurable process which involves the invocation of one or more +internal APIs+. 

Capabilities: 
* configurable mapping between public and internal APIs 
* authentication / authorization enforcement 
* throttling 
* monitor / statistics 
* lifecycle management: draft / staging / published / deprecated / ... 

For reference / inspiration: 
* [https://docs.wso2.com/display/AM260/Key+Concepts] 
* [https://istio.io/docs/concepts/what-is-istio/] 
* [https://github.com/getheimdall/heimdall] 

Good candidate for building upon appears to be [Spring Cloud Gateway|https://spring.io/projects/spring-cloud-gateway]",", Large Class, "
"Rename Method,Extract Method,","Add executor information to Task and Report executions It is currently not possible to know which user started a given Task or Report. 

This cannot apply to scheduled executions, naturally, as they will be always run by {{admin}}.","Duplicated Code, Long Method, , "
"Pull Up Method,Pull Up Attribute,","Manage creator, lastmodifier and approvers information about each SyncopeUser bean Add and populate/manage the following SyncopeUser bean attributes:
1. creator (who has created the user)
2. lastModifier (who has performed the last modification to the user profile)
3. approvers (every approver which approved some operation about the user)",", Duplicated Code, Duplicated Code, "
"Rename Class,Move Method,Extract Method,Inline Method,","Rich client library Implement a client library that can be encapsulated in a single JAR, providing client access to all features.","Duplicated Code, Long Method, , , , "
"Rename Method,Inline Method,","Find Anys using FIQL: SQL improvements PR contains improvements when Anys are searched using FIQL queries: 

The resulted SQL query that finds any_to_keys can have a huge list with OR clauses for effective realms. This can be replaced to the IN clause. 

{code:sql} 

SELECT u.any_id,sv.username FROM (SELECT DISTINCT any_id FROM user_search WHERE (realm_id=? AND any_id IN ( SELECT DISTINCT any_id FROM user_search WHERE lastChangeDate<=?))) u,user_search sv WHERE u.any_id=sv.any_id AND u.any_id IN (SELECT any_id FROM user_search WHERE realm_id IN (SELECT id AS realm_id FROM Realm WHERE id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=? OR id=?)) ORDER BY sv.username ASC 

{code} 

When anys are searched by keys a lot of single sql queries are executed. This can be improved using single SQL query with In clause.",", , "
"Rename Class,Rename Method,","Document RESTful services Provide dynamic documentation for RESTful services: the easiest and most effective way seems to be via XSLT processing of WADL information, auto-generated by CXF (see [1] for more information).

Relevant discussion on dev ML: [2].

[1] http://cxf.apache.org/docs/jaxrs-services-description.html#JAXRSServicesDescription-WADLAutoGeneration
[2] http://syncope-dev.1063484.n5.nabble.com/DISCUSS-Document-RESTful-APIs-td5714317.html",", "
"Rename Class,Rename Method,","Support SCIM REST API SCIM (System for Cross-domain Identity Management) is the open API for managing identities.
The specification is now complete and published under IETF.
An overview and detailed specifications can be found at the official website: http://www.simplecloud.info/

The Syncope Core already provides a [full-fledged RESTful interface|https://syncope.apache.org/docs/reference-guide.html#rest], normally available under {{/rest}}; the idea here is to add another RESTful interface available under {{/scim}}, compliant to the SCIM v2 specifications as referred above.

The new REST interface can be developed as an [extension|https://syncope.apache.org/docs/reference-guide.html#extensions], whose features will be:

# expose a fully compliant SCIM v2 RESTful interface
# translate the incoming / outgoing payloads from / to SCIM formats to / from Syncope standard formats
# invoke the underlying [Logic layer|https://syncope.apache.org/docs/reference-guide.html#logic] for actual operation implementation

An additional feature will be needed for mapping the standard Syncope [Schema|https://syncope.apache.org/docs/reference-guide.html#schema] to SCIM attributes.",", "
"Move Class,Move And Rename Class,Move Method,","Support SCIM REST API SCIM (System for Cross-domain Identity Management) is the open API for managing identities.
The specification is now complete and published under IETF.
An overview and detailed specifications can be found at the official website: http://www.simplecloud.info/

The Syncope Core already provides a [full-fledged RESTful interface|https://syncope.apache.org/docs/reference-guide.html#rest], normally available under {{/rest}}; the idea here is to add another RESTful interface available under {{/scim}}, compliant to the SCIM v2 specifications as referred above.

The new REST interface can be developed as an [extension|https://syncope.apache.org/docs/reference-guide.html#extensions], whose features will be:

# expose a fully compliant SCIM v2 RESTful interface
# translate the incoming / outgoing payloads from / to SCIM formats to / from Syncope standard formats
# invoke the underlying [Logic layer|https://syncope.apache.org/docs/reference-guide.html#logic] for actual operation implementation

An additional feature will be needed for mapping the standard Syncope [Schema|https://syncope.apache.org/docs/reference-guide.html#schema] to SCIM attributes.",", , "
"Rename Class,Extract Interface,Rename Method,Push Down Method,Move Method,Inline Method,Push Down Attribute,","Support SCIM REST API SCIM (System for Cross-domain Identity Management) is the open API for managing identities.
The specification is now complete and published under IETF.
An overview and detailed specifications can be found at the official website: http://www.simplecloud.info/

The Syncope Core already provides a [full-fledged RESTful interface|https://syncope.apache.org/docs/reference-guide.html#rest], normally available under {{/rest}}; the idea here is to add another RESTful interface available under {{/scim}}, compliant to the SCIM v2 specifications as referred above.

The new REST interface can be developed as an [extension|https://syncope.apache.org/docs/reference-guide.html#extensions], whose features will be:

# expose a fully compliant SCIM v2 RESTful interface
# translate the incoming / outgoing payloads from / to SCIM formats to / from Syncope standard formats
# invoke the underlying [Logic layer|https://syncope.apache.org/docs/reference-guide.html#logic] for actual operation implementation

An additional feature will be needed for mapping the standard Syncope [Schema|https://syncope.apache.org/docs/reference-guide.html#schema] to SCIM attributes.",", , , , , Large Class, "
"Move Class,Extract Superclass,Pull Up Method,Push Down Method,Move Method,Extract Method,","New admin UI Refactor and rewrite current console into a new, cleaner, admin UI.

It is a important usability improvement to provide sensible and contextual help messages on (mostly) each form field.

[1] http://syncope-dev.1063484.n5.nabble.com/About-admin-console-refactoring-td5710115.html
[2] http://markmail.org/message/wtamknssq42pyjjc","Duplicated Code, Long Method, , , Duplicated Code, Large Class, Duplicated Code, , "
"Move Method,Move Attribute,","New admin UI Refactor and rewrite current console into a new, cleaner, admin UI.

It is a important usability improvement to provide sensible and contextual help messages on (mostly) each form field.

[1] http://syncope-dev.1063484.n5.nabble.com/About-admin-console-refactoring-td5710115.html
[2] http://markmail.org/message/wtamknssq42pyjjc",", , , "
"Move Class,Move Method,Extract Method,Move Attribute,","New admin UI Refactor and rewrite current console into a new, cleaner, admin UI.

It is a important usability improvement to provide sensible and contextual help messages on (mostly) each form field.

[1] http://syncope-dev.1063484.n5.nabble.com/About-admin-console-refactoring-td5710115.html
[2] http://markmail.org/message/wtamknssq42pyjjc","Duplicated Code, Long Method, , , , "
"Rename Class,Move Method,Push Down Attribute,Move Attribute,","New admin UI Refactor and rewrite current console into a new, cleaner, admin UI.

It is a important usability improvement to provide sensible and contextual help messages on (mostly) each form field.

[1] http://syncope-dev.1063484.n5.nabble.com/About-admin-console-refactoring-td5710115.html
[2] http://markmail.org/message/wtamknssq42pyjjc",", , , , "
"Rename Class,Move Method,","New admin UI Refactor and rewrite current console into a new, cleaner, admin UI.

It is a important usability improvement to provide sensible and contextual help messages on (mostly) each form field.

[1] http://syncope-dev.1063484.n5.nabble.com/About-admin-console-refactoring-td5710115.html
[2] http://markmail.org/message/wtamknssq42pyjjc",", , "
"Move Class,Move And Rename Class,Rename Method,Pull Up Method,Move Method,Pull Up Attribute,Push Down Attribute,Move Attribute,","New admin UI Refactor and rewrite current console into a new, cleaner, admin UI.

It is a important usability improvement to provide sensible and contextual help messages on (mostly) each form field.

[1] http://syncope-dev.1063484.n5.nabble.com/About-admin-console-refactoring-td5710115.html
[2] http://markmail.org/message/wtamknssq42pyjjc",", , , , Duplicated Code, Duplicated Code, "
"Move Class,Move Method,Pull Up Attribute,Move Attribute,","New admin UI Refactor and rewrite current console into a new, cleaner, admin UI.

It is a important usability improvement to provide sensible and contextual help messages on (mostly) each form field.

[1] http://syncope-dev.1063484.n5.nabble.com/About-admin-console-refactoring-td5710115.html
[2] http://markmail.org/message/wtamknssq42pyjjc",", , , Duplicated Code, "
"Move Class,Move And Rename Class,Move Attribute,","New admin UI Refactor and rewrite current console into a new, cleaner, admin UI.

It is a important usability improvement to provide sensible and contextual help messages on (mostly) each form field.

[1] http://syncope-dev.1063484.n5.nabble.com/About-admin-console-refactoring-td5710115.html
[2] http://markmail.org/message/wtamknssq42pyjjc",", , "
"Rename Method,Move Method,Extract Method,Move Attribute,","New admin UI Refactor and rewrite current console into a new, cleaner, admin UI.

It is a important usability improvement to provide sensible and contextual help messages on (mostly) each form field.

[1] http://syncope-dev.1063484.n5.nabble.com/About-admin-console-refactoring-td5710115.html
[2] http://markmail.org/message/wtamknssq42pyjjc","Duplicated Code, Long Method, , , , "
"Extract Interface,Extract Superclass,Move Method,","New admin UI Refactor and rewrite current console into a new, cleaner, admin UI.

It is a important usability improvement to provide sensible and contextual help messages on (mostly) each form field.

[1] http://syncope-dev.1063484.n5.nabble.com/About-admin-console-refactoring-td5710115.html
[2] http://markmail.org/message/wtamknssq42pyjjc",", , Duplicated Code, Large Class, Large Class, "
"Rename Class,Move And Rename Class,Rename Method,","New admin UI Refactor and rewrite current console into a new, cleaner, admin UI.

It is a important usability improvement to provide sensible and contextual help messages on (mostly) each form field.

[1] http://syncope-dev.1063484.n5.nabble.com/About-admin-console-refactoring-td5710115.html
[2] http://markmail.org/message/wtamknssq42pyjjc",", "
"Rename Method,","New admin UI Refactor and rewrite current console into a new, cleaner, admin UI.

It is a important usability improvement to provide sensible and contextual help messages on (mostly) each form field.

[1] http://syncope-dev.1063484.n5.nabble.com/About-admin-console-refactoring-td5710115.html
[2] http://markmail.org/message/wtamknssq42pyjjc",", "
"Rename Method,Pull Up Method,Move Method,Inline Method,","New admin UI Refactor and rewrite current console into a new, cleaner, admin UI.

It is a important usability improvement to provide sensible and contextual help messages on (mostly) each form field.

[1] http://syncope-dev.1063484.n5.nabble.com/About-admin-console-refactoring-td5710115.html
[2] http://markmail.org/message/wtamknssq42pyjjc",", , , Duplicated Code, "
"Inline Method,Move Attribute,","New admin UI Refactor and rewrite current console into a new, cleaner, admin UI.

It is a important usability improvement to provide sensible and contextual help messages on (mostly) each form field.

[1] http://syncope-dev.1063484.n5.nabble.com/About-admin-console-refactoring-td5710115.html
[2] http://markmail.org/message/wtamknssq42pyjjc",", , , "
"Move And Rename Class,Extract Method,Inline Method,","New admin UI Refactor and rewrite current console into a new, cleaner, admin UI.

It is a important usability improvement to provide sensible and contextual help messages on (mostly) each form field.

[1] http://syncope-dev.1063484.n5.nabble.com/About-admin-console-refactoring-td5710115.html
[2] http://markmail.org/message/wtamknssq42pyjjc","Duplicated Code, Long Method, , , "
"Move Class,Rename Method,Extract Method,","New admin UI Refactor and rewrite current console into a new, cleaner, admin UI.

It is a important usability improvement to provide sensible and contextual help messages on (mostly) each form field.

[1] http://syncope-dev.1063484.n5.nabble.com/About-admin-console-refactoring-td5710115.html
[2] http://markmail.org/message/wtamknssq42pyjjc","Duplicated Code, Long Method, , "
"Rename Method,Pull Up Method,Move Method,Move Attribute,","New admin UI Refactor and rewrite current console into a new, cleaner, admin UI.

It is a important usability improvement to provide sensible and contextual help messages on (mostly) each form field.

[1] http://syncope-dev.1063484.n5.nabble.com/About-admin-console-refactoring-td5710115.html
[2] http://markmail.org/message/wtamknssq42pyjjc",", , , Duplicated Code, "
"Pull Up Method,Extract Method,Pull Up Attribute,","New admin UI Refactor and rewrite current console into a new, cleaner, admin UI.

It is a important usability improvement to provide sensible and contextual help messages on (mostly) each form field.

[1] http://syncope-dev.1063484.n5.nabble.com/About-admin-console-refactoring-td5710115.html
[2] http://markmail.org/message/wtamknssq42pyjjc","Duplicated Code, Long Method, , Duplicated Code, Duplicated Code, "
"Rename Class,Move Class,Rename Method,Move Method,Move Attribute,","New admin UI Refactor and rewrite current console into a new, cleaner, admin UI.

It is a important usability improvement to provide sensible and contextual help messages on (mostly) each form field.

[1] http://syncope-dev.1063484.n5.nabble.com/About-admin-console-refactoring-td5710115.html
[2] http://markmail.org/message/wtamknssq42pyjjc",", , , "
"Rename Class,Extract Interface,Rename Method,Extract Method,Inline Method,","New admin UI Refactor and rewrite current console into a new, cleaner, admin UI.

It is a important usability improvement to provide sensible and contextual help messages on (mostly) each form field.

[1] http://syncope-dev.1063484.n5.nabble.com/About-admin-console-refactoring-td5710115.html
[2] http://markmail.org/message/wtamknssq42pyjjc","Duplicated Code, Long Method, , , Large Class, "
"Rename Class,Rename Method,","New admin UI Refactor and rewrite current console into a new, cleaner, admin UI.

It is a important usability improvement to provide sensible and contextual help messages on (mostly) each form field.

[1] http://syncope-dev.1063484.n5.nabble.com/About-admin-console-refactoring-td5710115.html
[2] http://markmail.org/message/wtamknssq42pyjjc",", "
"Rename Class,Rename Method,Extract Method,","New admin UI Refactor and rewrite current console into a new, cleaner, admin UI.

It is a important usability improvement to provide sensible and contextual help messages on (mostly) each form field.

[1] http://syncope-dev.1063484.n5.nabble.com/About-admin-console-refactoring-td5710115.html
[2] http://markmail.org/message/wtamknssq42pyjjc","Duplicated Code, Long Method, , "
"Move And Rename Class,Extract Superclass,Pull Up Method,Move Method,Move Attribute,","New admin UI Refactor and rewrite current console into a new, cleaner, admin UI.

It is a important usability improvement to provide sensible and contextual help messages on (mostly) each form field.

[1] http://syncope-dev.1063484.n5.nabble.com/About-admin-console-refactoring-td5710115.html
[2] http://markmail.org/message/wtamknssq42pyjjc",", , , Duplicated Code, Large Class, Duplicated Code, "
"Rename Class,Rename Method,",CLI admin tool It should not be hard to make a simple command-line tool to get and set SyncopeConf items (and more).,", "
"Rename Method,",CLI admin tool It should not be hard to make a simple command-line tool to get and set SyncopeConf items (and more).,", "
"Rename Method,Extract Method,",CLI admin tool It should not be hard to make a simple command-line tool to get and set SyncopeConf items (and more).,"Duplicated Code, Long Method, , "
"Rename Class,Rename Method,Move Method,",CLI admin tool It should not be hard to make a simple command-line tool to get and set SyncopeConf items (and more).,", , "
"Move And Rename Class,Move Method,Extract Method,Move Attribute,",CLI admin tool It should not be hard to make a simple command-line tool to get and set SyncopeConf items (and more).,"Duplicated Code, Long Method, , , , "
"Extract Method,Move Attribute,",CLI admin tool It should not be hard to make a simple command-line tool to get and set SyncopeConf items (and more).,"Duplicated Code, Long Method, , , "
"Rename Class,Move Class,Pull Up Method,Move Method,Extract Method,Move Attribute,",CLI admin tool It should not be hard to make a simple command-line tool to get and set SyncopeConf items (and more).,"Duplicated Code, Long Method, , , , Duplicated Code, "
"Rename Method,Move Method,Extract Method,",CLI admin tool It should not be hard to make a simple command-line tool to get and set SyncopeConf items (and more).,"Duplicated Code, Long Method, , , "
"Move Class,Move Method,Move Attribute,",CLI admin tool It should not be hard to make a simple command-line tool to get and set SyncopeConf items (and more).,", , , "
"Move Class,Move Method,Inline Method,Move Attribute,",CLI admin tool It should not be hard to make a simple command-line tool to get and set SyncopeConf items (and more).,", , , , "
"Rename Method,Extract Method,Move Attribute,",CLI admin tool It should not be hard to make a simple command-line tool to get and set SyncopeConf items (and more).,"Duplicated Code, Long Method, , , "
"Move Class,Rename Method,Move Method,Move Attribute,",CLI admin tool It should not be hard to make a simple command-line tool to get and set SyncopeConf items (and more).,", , , "
"Rename Method,Inline Method,",Passthrough authentication Provide the possibility to authenticate users on external resources.,", , "
"Rename Method,","Check for mandatory condition on Virtual / Derived attributes ""Mandatory condition"" is a JEXL expression evaluating to boolean that can be specified during schema mapping [1].

When 'enforce mandatory condition"" is checked as well, Syncope will consider any attribute part of the schema mapping (with mandatory condition evaluated to true) as mandatory, even though the corresponding attribute schema is not defined as mandatory.

Currently, this feature is working for plain attribute schemas, not derived or virtual.

[1] https://cwiki.apache.org/confluence/display/SYNCOPE/Schema,+attributes+and+mapping#Schema%2Cattributesandmapping-SchemaMapping",", "
"Rename Class,Extract Superclass,Rename Method,Pull Up Method,Pull Up Attribute,","Role owner Add the notion of (optional and inheritable) role owner, a SyncopeUser or a SyncopeRole with rights to manage a role.

Role owner will be automatically entitled to make modifications on the owned role and on all descendants with inheritOwner = true",", Duplicated Code, Large Class, Duplicated Code, Duplicated Code, "
"Rename Method,","Using Standard JAX-RS API in Syncope (Introducing Apache CXF WS Stack) Current REST Interfaces are based on Spring Webservice framework. 
Goal of this task is to replace Spring with CXF and to relay on JAX-B and JAX-RS annotations rather then Spring annotations.",", "
"Rename Method,Extract Method,Inline Method,","Using Standard JAX-RS API in Syncope (Introducing Apache CXF WS Stack) Current REST Interfaces are based on Spring Webservice framework. 
Goal of this task is to replace Spring with CXF and to relay on JAX-B and JAX-RS annotations rather then Spring annotations.","Duplicated Code, Long Method, , , "
"Move Method,Extract Method,",Remove code replication managing resource schema mappings Remove code replication managing resource schema mappings: use SchemaMappingUtil class to wrap common actions.,"Duplicated Code, Long Method, , , "
"Rename Class,Move Method,Extract Method,Move Attribute,","Move persistence and persistence impl into separate modules The core module currently contains many parts of syncope. This makes it bigger and more complex than necessary.

A possible modularization is to move the internal model (org.apache.syncope.core.persistence*) and the persistence impl (org.apache.syncope.core.persistence.impl) out of core and into separate modules.

One big advantage would be that the jpa code enhancements would then run in the model module only. Currently we run into some problems in the cxf migration when running the rest itests in core that may be caused by eclipse overwriting the enhanced classes with plain classes. If the model (peristence) classes are in a separate module we could leave it out of eclipse and so this would be no issue anymore.

Another advantage would be that the persistence tests could run in the persistence impl module so when working on the core they would not have to run each time. 
","Duplicated Code, Long Method, , , , "
"Move Class,Extract Interface,","Move persistence and persistence impl into separate modules The core module currently contains many parts of syncope. This makes it bigger and more complex than necessary.

A possible modularization is to move the internal model (org.apache.syncope.core.persistence*) and the persistence impl (org.apache.syncope.core.persistence.impl) out of core and into separate modules.

One big advantage would be that the jpa code enhancements would then run in the model module only. Currently we run into some problems in the cxf migration when running the rest itests in core that may be caused by eclipse overwriting the enhanced classes with plain classes. If the model (peristence) classes are in a separate module we could leave it out of eclipse and so this would be no issue anymore.

Another advantage would be that the persistence tests could run in the persistence impl module so when working on the core they would not have to run each time. 
",", Large Class, "
"Move Class,Move Method,Move Attribute,","Move persistence and persistence impl into separate modules The core module currently contains many parts of syncope. This makes it bigger and more complex than necessary.

A possible modularization is to move the internal model (org.apache.syncope.core.persistence*) and the persistence impl (org.apache.syncope.core.persistence.impl) out of core and into separate modules.

One big advantage would be that the jpa code enhancements would then run in the model module only. Currently we run into some problems in the cxf migration when running the rest itests in core that may be caused by eclipse overwriting the enhanced classes with plain classes. If the model (peristence) classes are in a separate module we could leave it out of eclipse and so this would be no issue anymore.

Another advantage would be that the persistence tests could run in the persistence impl module so when working on the core they would not have to run each time. 
",", , , "
"Move Class,Extract Interface,","Resolve dependency cycles between persistence and the rest of syncope core When analysing if we could move the persistence and persistence impl into separate modules I found that there are a lot of dependency cycles in the syncope core module. I have added a structure 101 diagram of the cycles to the issue so you can take a look.

Especially the cycles between persistence and the rest of core are important as they prevent us from moving these packages out of core.

I have already done some experimentations how to solve the cycles and am pretty sure I can fix that.
",", Large Class, "
"Rename Method,Extract Method,","Implement RoleOwnerSchema for role propagation and synchronization SYNCOPE-225 introduced the concept of role owner, than could be either a user or another role (not both at the same time).

Test content provides an example of how role owner can be propagated by empowering a derived attribute (ownerDN): this approach is working only for propagation and makes the AccountLink expression duplicated.

A more complete approach is to define a new type of internal mapping, RoleOwnerSchema.

During role propagation (in MappingUtil.getIntValues()):
* if userOwner != null and the propagating resource has UMapping defined
* if roleOwner != null (the propagating resource has RMapping because of the ongoing propagation)
the AccountLink (or AccountId if no AccountLink is defined) is generated and given as value for the external attribute mapped to RoleOwnerSchema

During role synchronization (in ConnObjectUtil.getAttributableTOFromConnObject()), if a value is present in the ConnectorObject for the role being synchronized, this value must be used for searching the same connector for either ObjectClass.ACCOUNT and ObjectClass.GROUP; if a unique match is found, the matching ConnectorObject can be used to find the corresponding Syncope entity (user or role); now userOwner or roleOwner of the role being synchronized can be set.

Especially in case of roleOwner, precedence issues must be taken into account: it might happen, in fact, that the owned role is being synchronized before the owner role synchronization takes place.","Duplicated Code, Long Method, , "
"Rename Method,Extract Method,","Create transitional Service interfaces and switch tests and console to use them As preparation of the change to use CXF instead of Spring MVC REST controllers this issue is to introduce transitional service interfaces (like as UserService).

The UserService interface should later be used in the core to provide the UserController and on the console to access the service remotely.

To make the transition easier the idea is to already introduce the interface upfront and change all tests and the console to use it. Before the switch the implementation of the interface will simply use the restTemplate under the covers.

This to be applied similarly to all Spring MVC REST controllers.
","Duplicated Code, Long Method, , "
"Rename Class,Rename Method,Extract Method,","Create transitional Service interfaces and switch tests and console to use them As preparation of the change to use CXF instead of Spring MVC REST controllers this issue is to introduce transitional service interfaces (like as UserService).

The UserService interface should later be used in the core to provide the UserController and on the console to access the service remotely.

To make the transition easier the idea is to already introduce the interface upfront and change all tests and the console to use it. Before the switch the implementation of the interface will simply use the restTemplate under the covers.

This to be applied similarly to all Spring MVC REST controllers.
","Duplicated Code, Long Method, , "
"Rename Method,","Create transitional Service interfaces and switch tests and console to use them As preparation of the change to use CXF instead of Spring MVC REST controllers this issue is to introduce transitional service interfaces (like as UserService).

The UserService interface should later be used in the core to provide the UserController and on the console to access the service remotely.

To make the transition easier the idea is to already introduce the interface upfront and change all tests and the console to use it. Before the switch the implementation of the interface will simply use the restTemplate under the covers.

This to be applied similarly to all Spring MVC REST controllers.
",", "
"Move Class,Extract Method,","Create transitional Service interfaces and switch tests and console to use them As preparation of the change to use CXF instead of Spring MVC REST controllers this issue is to introduce transitional service interfaces (like as UserService).

The UserService interface should later be used in the core to provide the UserController and on the console to access the service remotely.

To make the transition easier the idea is to already introduce the interface upfront and change all tests and the console to use it. Before the switch the implementation of the interface will simply use the restTemplate under the covers.

This to be applied similarly to all Spring MVC REST controllers.
","Duplicated Code, Long Method, , "
"Extract Superclass,Pull Up Method,Extract Method,Pull Up Attribute,","Create transitional Service interfaces and switch tests and console to use them As preparation of the change to use CXF instead of Spring MVC REST controllers this issue is to introduce transitional service interfaces (like as UserService).

The UserService interface should later be used in the core to provide the UserController and on the console to access the service remotely.

To make the transition easier the idea is to already introduce the interface upfront and change all tests and the console to use it. Before the switch the implementation of the interface will simply use the restTemplate under the covers.

This to be applied similarly to all Spring MVC REST controllers.
","Duplicated Code, Long Method, , Duplicated Code, Large Class, Duplicated Code, Duplicated Code, "
"Rename Method,","Enable Rest IntegrationTests to run more than once (per build) Currently many Rest IntegrationTests can run only once. If you try to rerun some Tests they will fail, due to the fact, that a resource with the same same already exists, or that a resource was deleted previously and is not available any longer. This works fine for ""mvn clean verify"" since all test run exactly once. But for development phase this is inconvenient, because while testing new features or other refactorings, one would like to run tests several times (especially if they fail), without the need to rebuild/package/deploy the whole core module.

Tasks of this JIRA ticket is to use random identifier for resources (user, role, ...) to avoid collisions, when running tests multiple times. In some cases it will also be preferable to use a try { } final { } statement to cleanup previously created resources.",", "
"Rename Class,Rename Method,","Encrypted schema 1. Main purpose: store some arbitrary string values encrypted in the database; this can be enforced by law, for example.

2. When defining an encrypted schema, you must provide the cypher algorithm to be used and a passphrase.
Such passphrase will be stored by Syncope as encrypted with an internal key (more or less like we are already doing with user passwords).

3. When creating an attribute with such schema, the value(s) will be automatically encrypted by Syncope using the provided algorithm and passphrase.

4. When reading an attribute with such schema (e.g. contained in an AttributeTO), the value(s) will be sent encrypted.
Only who knows the algorithm and the passphrase will be able to decrypt.
-Moreover, you can think to make the admin console able to show such attribute value(s) as encrypted by default and to decrypt them on demand after asking for algorithm and passphase.-

-5. When propagating / synchronizing attribute with such schema, GuardedString will be used, not String.-

6. When changing algorithm or passpshase of an existing schema, new values will be encrypted with these, old values will remain as they are. 
Naturally, one can provide an update procedure.

[1] http://markmail.org/message/rg7ryeknkrzae4xj",", "
"Move Method,Extract Method,","Connector instance timeout Provide execution timeout for ConnectorFacadeProxy methods.
Timeout must be specified:
1. for 1_0_X by using a global configuration parameter (the same timeout for each connector instance)
2. for 1_1_X by using a connector instance configuration parameter (different timeout for different instance)","Duplicated Code, Long Method, , , "
"Rename Method,Push Down Method,Extract Method,Inline Method,Push Down Attribute,","Typed SyncopeConf Currently SyncopeConf instances are <String, String> pairs: this has of course various drawbacks (no check for mandatory values, enums not available, ...)

It should be relatively easy to create a set of SSchema / SAttr / SAttrValue / SAttrUniqueValue (similar to what currently available for user / membership / role) to replace the current SyncopeConf.","Duplicated Code, Long Method, , , , , "
"Rename Class,Rename Method,Extract Method,Inline Method,","Typed SyncopeConf Currently SyncopeConf instances are <String, String> pairs: this has of course various drawbacks (no check for mandatory values, enums not available, ...)

It should be relatively easy to create a set of SSchema / SAttr / SAttrValue / SAttrUniqueValue (similar to what currently available for user / membership / role) to replace the current SyncopeConf.","Duplicated Code, Long Method, , , "
"Rename Method,","Show information (version, license, ...) Currently there is only a static label with core and console versions.
Add a new Link to open a ModalPage with version and other project information (eg. link to license)",", "
"Rename Method,Extract Method,","Mapping to SyncopeClientCompositeException on client side Actually almost all exceptions with status BAD_REQUEST and NOT_FOUND are mapped to SyncopeClientCompositeErrorException on the client side.
It is absolutely OK for composite exceptions containing number of sub-exceptions (like validation and propagation), however for some single exceptions it makes more sense to map not to SyncopeClientCompositeErrorException, but directly to corresponded exception type.
Candidates are:

Deadlock
ExistingResource
DataIntegrityViolation
GenericPersistence
UnauthorizedRole

Proposed mapping makes exception processing more easy and effective.

https://cwiki.apache.org/confluence/display/SYNCOPE/Remote+Exceptions","Duplicated Code, Long Method, , "
"Rename Class,Rename Method,Move Method,Inline Method,",Provide feature for reloading all connectors It is a feature particularly useful during project deployment that will allow to refresh all connector bundles and configuration.,", , , "
"Rename Method,Extract Method,","Provide access to user / role data on external resources From StatusPanel, show read-only data read via ResourceController#getObject, for each associated resource.","Duplicated Code, Long Method, , "
"Rename Method,","Provide access to user / role data on external resources From StatusPanel, show read-only data read via ResourceController#getObject, for each associated resource.",", "
"Rename Method,","Disable mapping tab when the underlying connector does not support correspondent ObjectClass If a given connector supports ObjectClass.ACCOUNT, enable the ""User mapping"" tab, otherwise keep it disabled.
If a given connector supports ObjectClass.GROUP, enable the ""Role mapping"" tab, otherwise keep it disabled.",", "
"Rename Method,Extract Method,","Full reconciliation from syncope to resource Implement a full reconciliation from syncope towards a specific resource.

Unmatching (user found on syncope but not on the resource):
* ignore;
* unlink the resource (keep user on syncope and remove resource link)
* create (create user on resource - if create capability is given)
* delete (remove user on syncope) 

Matching (users found on syncope and on the resource):
* ignore
* update
* unlink (perform deprovisioning - if delete capability is given)
* delete (delete on syncope and perform deprovisioning - if delete capability is given)

","Duplicated Code, Long Method, , "
"Push Down Method,Extract Method,Pull Up Attribute,","Full reconciliation from syncope to resource Implement a full reconciliation from syncope towards a specific resource.

Unmatching (user found on syncope but not on the resource):
* ignore;
* unlink the resource (keep user on syncope and remove resource link)
* create (create user on resource - if create capability is given)
* delete (remove user on syncope) 

Matching (users found on syncope and on the resource):
* ignore
* update
* unlink (perform deprovisioning - if delete capability is given)
* delete (delete on syncope and perform deprovisioning - if delete capability is given)

","Duplicated Code, Long Method, , , Duplicated Code, "
"Rename Method,","Full reconciliation from syncope to resource Implement a full reconciliation from syncope towards a specific resource.

Unmatching (user found on syncope but not on the resource):
* ignore;
* unlink the resource (keep user on syncope and remove resource link)
* create (create user on resource - if create capability is given)
* delete (remove user on syncope) 

Matching (users found on syncope and on the resource):
* ignore
* update
* unlink (perform deprovisioning - if delete capability is given)
* delete (delete on syncope and perform deprovisioning - if delete capability is given)

",", "
"Rename Class,Extract Superclass,Extract Method,Inline Method,","Resource unlink Give the possibility to unlink resource without performing de-provision.
1. take care of roles as well
2. feature must be independent on connector capability","Duplicated Code, Long Method, , Duplicated Code, Large Class, , "
"Rename Class,Move Method,Move Attribute,","Replace logback with log4j 2 As discussed in dev ML [1]

[1] http://syncope-dev.1063484.n5.nabble.com/DISCUSS-Replace-logback-with-log4j-2-td5714012.html",", , , "
"Rename Class,Move Attribute,","Invoke bean validation via JPA entity listener Currently, bean validation is triggered via custom AOP interceptor [1] on save() method of DAO classes.

However, this could be changed to happen in a JPA entity listener[2], resulting in more widespread and robust control; moreover, some dependency (namely AspectJ) could be cut.

[1] https://svn.apache.org/repos/asf/syncope/trunk/core/src/main/java/org/apache/syncope/core/persistence/validation/entity/EntityValidationInterceptor.java
[2] http://openjpa.apache.org/builds/2.2.2/apache-openjpa/docs/jpa_overview_pc_callbacks.html#jpa_overview_entity_listeners_using",", , "
"Rename Method,",Add claim for user requests and trace user request history into SyncopeUser bean Add claim for user requests and trace user request history into SyncopeUser bean,", "
"Rename Class,Rename Method,","Provide user / role pre-processing mechanism Before storing/propagating/synchronizing a specific user or role a pre-processor could be called in order to allow any kind of manipulation before any actual operation takes place.

The idea is to provide an interface to be implemented into the overlay in order to perform custom pre-processing operations.
The custom implementation class name could be defined into the global configuration.

This new feature would give the ""handle"" to provide several attribute value manipulation like specific value translations, or something else.",", "
"Move Class,Rename Class,Move And Rename Class,Extract Superclass,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Further REST refactoring Complete, for what is possible, the adherence to RESTful best practices started with CXF migration in 1.1.0.
In particular, fix all warnings reported in [1] and take actions discussed in [2].

[1] https://cwiki.apache.org/confluence/display/SYNCOPE/REST+API+upgrade
[2] http://markmail.org/message/i3mtvq2vkseukbq2","Duplicated Code, Long Method, , , , Duplicated Code, Large Class, , "
"Rename Method,","Provide resource link, associate and provision independent features Complete AbstractResourceAssociator [1], created by SYNCOPE-393, with 'positive' methods, e.g. 

* link()
* associate()
* provision()

[1] https://svn.apache.org/repos/asf/syncope/trunk/core/src/main/java/org/apache/syncope/core/rest/controller/AbstractResourceAssociator.java",", "
"Rename Method,Move Method,","Support ETag-based conditional requests for users and roles SYNCOPE-15 introduced 'lastChangeDate' and 'creationDate' fields on some entities (namely SyncopeUser, SyncopeRole and ExternalResource); such information is then transferred to matching TO (e.g. UserTO, RoleTO and ResourceTO).

This means that the respective services (UserService and RoleService) could be enhanced in the following way:

1. add ETag response header to any method - directly or indirectly via Response.Entity - generated from creationDate's or lastChangeDate's value

2. implement some ""conditional POST/PUT"" logic for update: an effective update request will be effectively accepted only if provided ETag matches the latest modification currently available",", , "
"Rename Method,Extract Method,Pull Up Attribute,","Support returning no content after create / update Some REST methods (mostly in UserService and RoleService) are now returning the created / updated / deleted resource in the Response body as entity.

It could be nice to provide a 'Prefer' / 'Preference-Applied' (similar respectively to [1] and [2]) HTTP header mechanism allowing caller to declare that he is not interested in getting the created / updated / deleted resource back, but only in operation result status.

[1] http://msdn.microsoft.com/en-us/library/hh537533.aspx
[2] http://msdn.microsoft.com/en-us/library/hh554623.aspx","Duplicated Code, Long Method, , Duplicated Code, "
"Rename Method,","Replace role action labels with icons As available throughout the admin console, replace the links for accessing role management features ('Add child', 'Edit', 'Drop') with icons.",", "
"Rename Method,Extract Method,","Embed Activiti modeler for graphical workflow editing Activiti, the default user workflow engine, is shipping in its latest versions the open source version of the KIS BPM process solution.
Basically, a web editor that can be used to author BPMN 2.0 compliant processes graphically. More information at: http://www.activiti.org/components.html

The idea is to embed such component in the admin console, next to the current XML-based editor.","Duplicated Code, Long Method, , "
"Rename Method,Extract Method,","Add ability to search for Roles via the REST API It is currently not possible to search for roles via the REST API. You can, however, search for a list of users that are members of a particular role. But you may want the ability to search for a role by attributes, etc.

Don't forget to refactor console's search panel accordingly.","Duplicated Code, Long Method, , "
"Rename Class,Move And Rename Class,Extract Superclass,Rename Method,Pull Up Method,Move Method,Pull Up Attribute,Move Attribute,","Add ability to search for Roles via the REST API It is currently not possible to search for roles via the REST API. You can, however, search for a list of users that are members of a particular role. But you may want the ability to search for a role by attributes, etc.

Don't forget to refactor console's search panel accordingly.",", , , Duplicated Code, Large Class, Duplicated Code, Duplicated Code, "
"Pull Up Method,Move Method,Extract Method,Pull Up Attribute,","Use cached virtual attribute values with offline resources Virtual attribute cache mustn't be expired in case of offline resources.
Cached values have to be returned in case of resource unavailability.

See http://syncope-user.1051894.n5.nabble.com/Virtual-attributes-with-off-line-resources-td5707443.html for discussion thread.","Duplicated Code, Long Method, , , Duplicated Code, Duplicated Code, "
"Rename Method,",Add information to what components refer to a certain policy Add a third tab to policy modal windows showing which other components are referring to the policy being edited.,", "
"Rename Class,Extract Superclass,Extract Interface,Rename Method,Push Down Method,Extract Method,Push Down Attribute,","Implement correlation rule management for push task 1. Extend the connector infrastructure in order to perform a search based on one or more attributes in 'AND' condition. 
2. give the possibility to specify a correlation rule for push tasks like already done for sync tasks","Duplicated Code, Long Method, , Duplicated Code, Large Class, , , Large Class, "
"Rename Class,Extract Method,","Allow list of PropagationActions for Resource, SyncActions for SyncTask and PushActions for PushTask Currently {{Resource}} can define a single [PropagationActions|https://cwiki.apache.org/confluence/display/SYNCOPE/PropagationActionsClass] class, {{SyncTask}} can define a single [SyncActions|https://cwiki.apache.org/confluence/display/SYNCOPE/SyncActionsClass] and {{PushTask}} can define a single [PushActions|https://cwiki.apache.org/confluence/display/SYNCOPE/PushActionsClass] class.

This is somewhat limiting: a list of actions class, to be invoked in the specified order, looks more powerful.","Duplicated Code, Long Method, , "
"Extract Method,Move Attribute,","Remove MD5 as a supported password cipher algorithm 
MD5 is currently used as the default password cipher algorithm. We should remove the ability to use MD5 and switch to using a more secure alternative.","Duplicated Code, Long Method, , , "
"Rename Class,Extract Method,",Empower ETag from console The admin console is not (yet) empowering the ETag feature introduced by SYNCOPE-429.,"Duplicated Code, Long Method, , "
"Extract Method,Move Attribute,","Use JSON for serialized POJOs in the internal storage Currently different kinds of POJOs are stored serialized in some JPA entity's field (connector configuration, attributes for propagation tasks, user / role templates for sync tasks, ...).

Till now such objects are (de)serialized in XML via {{XStream}} - we can remove such dependency and also improve performances by using Jackson and JSON.","Duplicated Code, Long Method, , , "
"Extract Method,Inline Method,","Improving the management of the xml and properties files inside the installer Currently the installer doesn't read the xml and properties files inside the core and console modules, but are embedded statically into java classes causing the following errors in syncope (SYNCOPE-615).","Duplicated Code, Long Method, , , "
"Rename Class,Rename Method,Inline Method,","Code re-organization Heading to 2.0.0, a proposal is [being discussed|http://markmail.org/message/xfxn2xc6iolcmysp] for code reorganization in order to obtain more modularity with our growing codebase.",", , "
"Move Class,Move Method,Move Attribute,","Code re-organization Heading to 2.0.0, a proposal is [being discussed|http://markmail.org/message/xfxn2xc6iolcmysp] for code reorganization in order to obtain more modularity with our growing codebase.",", , , "
"Move Class,Rename Class,Move And Rename Class,Extract Interface,Rename Method,Move Method,Move Attribute,","Code re-organization Heading to 2.0.0, a proposal is [being discussed|http://markmail.org/message/xfxn2xc6iolcmysp] for code reorganization in order to obtain more modularity with our growing codebase.",", , , Large Class, "
"Move Class,Move And Rename Class,Extract Interface,Move Method,","Code re-organization Heading to 2.0.0, a proposal is [being discussed|http://markmail.org/message/xfxn2xc6iolcmysp] for code reorganization in order to obtain more modularity with our growing codebase.",", , Large Class, "
"Move Method,Move Attribute,","Code re-organization Heading to 2.0.0, a proposal is [being discussed|http://markmail.org/message/xfxn2xc6iolcmysp] for code reorganization in order to obtain more modularity with our growing codebase.",", , , "
"Rename Method,","Code re-organization Heading to 2.0.0, a proposal is [being discussed|http://markmail.org/message/xfxn2xc6iolcmysp] for code reorganization in order to obtain more modularity with our growing codebase.",", "
"Move Class,Rename Class,Move And Rename Class,","Code re-organization Heading to 2.0.0, a proposal is [being discussed|http://markmail.org/message/xfxn2xc6iolcmysp] for code reorganization in order to obtain more modularity with our growing codebase.",", "
"Rename Method,Extract Method,","Code re-organization Heading to 2.0.0, a proposal is [being discussed|http://markmail.org/message/xfxn2xc6iolcmysp] for code reorganization in order to obtain more modularity with our growing codebase.","Duplicated Code, Long Method, , "
"Rename Class,Extract Superclass,Rename Method,Move Method,Move Attribute,","Code re-organization Heading to 2.0.0, a proposal is [being discussed|http://markmail.org/message/xfxn2xc6iolcmysp] for code reorganization in order to obtain more modularity with our growing codebase.",", , , Duplicated Code, Large Class, "
"Rename Class,Extract Interface,Rename Method,","Code re-organization Heading to 2.0.0, a proposal is [being discussed|http://markmail.org/message/xfxn2xc6iolcmysp] for code reorganization in order to obtain more modularity with our growing codebase.",", Large Class, "
"Move Class,Move And Rename Class,","Code re-organization Heading to 2.0.0, a proposal is [being discussed|http://markmail.org/message/xfxn2xc6iolcmysp] for code reorganization in order to obtain more modularity with our growing codebase.",", "
"Move Class,Rename Method,Move Method,Move Attribute,","Code re-organization Heading to 2.0.0, a proposal is [being discussed|http://markmail.org/message/xfxn2xc6iolcmysp] for code reorganization in order to obtain more modularity with our growing codebase.",", , , "
"Push Down Method,Push Down Attribute,","Code re-organization Heading to 2.0.0, a proposal is [being discussed|http://markmail.org/message/xfxn2xc6iolcmysp] for code reorganization in order to obtain more modularity with our growing codebase.",", , , "
"Move Class,Move And Rename Class,Extract Interface,",Improve VirAttrCache management Currently VirAttrCache is a final class; turning it into an interface (with default implementation) would open the possibility to provide implementation for proper cache providers (say EhCache).,", Large Class, "
"Rename Method,","Provisioning manager integration Last year with the help of Syncope team I proposed an early reformulation of provisioning phase. Provisioning refers to methods used to provide users and roles functionality: the main goal of my proposal was to re-organize this phase, allowing custom provisioning behaviours through a definition of a provisioning manager (see here [1] more details).

The current strategy adopted in Syncope makes difficult the definition of custom provisioning behaviour. The solution enclosed with this issue aim to decompose the provisioning phase. Previously the user/role controller deals directly with provisioning, while now with this proposal both the controller delegates this task to provisioning manager. 

A first task was to move all provisioning functionality into the provisioning manager. If you now inspect the user/role controller code, you will not find anymore workflow and propagation dependencies.

During the development, we thought to make the provisioning pluggable, in order to allow the choice of provisioning engine. The default provisioning engine can be choosed editing the provisioning.properties file. For this reason we hardcoded previous strategies as default provisioning manager, in order to keep the standard Syncope behaviour. As we proposed initially, we wanted also to experiment a provisionig manager based on a Apache Camel. Camel is a powerful integration framework implementing enterprise integration pattern. Our current solution embeds the provisioning logics into camel routes.

Moreover the current solution extends also Syncope Console: we added new a new functionality - related to Camel case - that allows to read and edit routes definitions. You can find this new service under the Console->Configuration->Routes section. In this case, routes are expressed through Spring DSL. 

To finish, I thought to create a github pull request [2],in order to give to Syncope Team members to examine this work and possibly integrate it.

[1] http://syncope-dev.1063484.n5.nabble.com/Proposal-An-Apache-Camel-Integratation-Proposal-td5714531.html
[2] https://github.com/apache/syncope/pull/2",", "
"Move Class,Rename Method,","Domains The purpose of this new feature is to provide the possibility of defining separated ""containers"" for all entities currently managed by Syncope in order to allow the execution in multitenant environments.

Discussion on wiki: https://cwiki.apache.org/confluence/display/SYNCOPE/%5BDISCUSS%5D+Domains",", "
"Rename Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Domains The purpose of this new feature is to provide the possibility of defining separated ""containers"" for all entities currently managed by Syncope in order to allow the execution in multitenant environments.

Discussion on wiki: https://cwiki.apache.org/confluence/display/SYNCOPE/%5BDISCUSS%5D+Domains","Duplicated Code, Long Method, , , , , "
"Rename Class,Move Class,Rename Method,Push Down Method,Move Method,Extract Method,Push Down Attribute,Move Attribute,","Domains The purpose of this new feature is to provide the possibility of defining separated ""containers"" for all entities currently managed by Syncope in order to allow the execution in multitenant environments.

Discussion on wiki: https://cwiki.apache.org/confluence/display/SYNCOPE/%5BDISCUSS%5D+Domains","Duplicated Code, Long Method, , , , , , "
"Move Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Domains The purpose of this new feature is to provide the possibility of defining separated ""containers"" for all entities currently managed by Syncope in order to allow the execution in multitenant environments.

Discussion on wiki: https://cwiki.apache.org/confluence/display/SYNCOPE/%5BDISCUSS%5D+Domains","Duplicated Code, Long Method, , , , , "
"Rename Method,","Domains The purpose of this new feature is to provide the possibility of defining separated ""containers"" for all entities currently managed by Syncope in order to allow the execution in multitenant environments.

Discussion on wiki: https://cwiki.apache.org/confluence/display/SYNCOPE/%5BDISCUSS%5D+Domains",", "
"Move Class,Extract Interface,Move Method,Extract Method,Move Attribute,","Domains The purpose of this new feature is to provide the possibility of defining separated ""containers"" for all entities currently managed by Syncope in order to allow the execution in multitenant environments.

Discussion on wiki: https://cwiki.apache.org/confluence/display/SYNCOPE/%5BDISCUSS%5D+Domains","Duplicated Code, Long Method, , , , Large Class, "
"Move Class,Move Method,","Extend control over asynchronous job execution Asynchronous job execution, generally delegated to Quartz, can currently be checked only at completion, and only by indirectly checking the status on related JPA entities (TaskExec, ReportExec, ...).

A REST endpoint should be provided for reporting on jobs execution and providing the ability to control (e.g. stop / pause / resume / ...) .",", , "
"Rename Class,Rename Method,","Remove overloaded methods from REST services Various REST services offer overloaded {{list()}} or {{search()}} methods, e.g. methods that only differ for the parameters in the signature.
This triggers the need of CXF's custom {{QueryResourceInfoComparator}} in order to be able to select the appropriate method based on HTTP parameters.

This is not in-line with JAX-RS best practice, specifically because parameters should not affect method selection.",", "
"Rename Method,","Option to ignore users / roles during synchronization or push Currently all users or roles provided by an external resource during synchronization - or sent to an external resource during push - are considered for synchronization or push (with matching / unmatching rules).

It is a powerful addition to provide a mean to safely ignore user or roles - a proper place for implementing such custom logic is {{SyncActions}} / {{PushActions}}.",", "
"Rename Method,","Any objects Introduce the concept of Any object, e.g. to extend the provisioning engine to support general-purpose definable entities, besides current users and groups (see this page about realms to understand why former roles were renamed to groups).

With this feature onboard, Syncope will be suitable for managing printers, services, or any other ""thing"" (as in Internet-Of-Things).

Details at https://cwiki.apache.org/confluence/display/SYNCOPE/%5BDISCUSS%5D+Any+objects",", "
"Extract Superclass,Rename Method,Move Method,","Any objects Introduce the concept of Any object, e.g. to extend the provisioning engine to support general-purpose definable entities, besides current users and groups (see this page about realms to understand why former roles were renamed to groups).

With this feature onboard, Syncope will be suitable for managing printers, services, or any other ""thing"" (as in Internet-Of-Things).

Details at https://cwiki.apache.org/confluence/display/SYNCOPE/%5BDISCUSS%5D+Any+objects",", , Duplicated Code, Large Class, "
"Rename Method,Extract Method,","Any objects Introduce the concept of Any object, e.g. to extend the provisioning engine to support general-purpose definable entities, besides current users and groups (see this page about realms to understand why former roles were renamed to groups).

With this feature onboard, Syncope will be suitable for managing printers, services, or any other ""thing"" (as in Internet-Of-Things).

Details at https://cwiki.apache.org/confluence/display/SYNCOPE/%5BDISCUSS%5D+Any+objects","Duplicated Code, Long Method, , "
"Rename Class,Rename Method,","Any objects Introduce the concept of Any object, e.g. to extend the provisioning engine to support general-purpose definable entities, besides current users and groups (see this page about realms to understand why former roles were renamed to groups).

With this feature onboard, Syncope will be suitable for managing printers, services, or any other ""thing"" (as in Internet-Of-Things).

Details at https://cwiki.apache.org/confluence/display/SYNCOPE/%5BDISCUSS%5D+Any+objects",", "
"Rename Class,Rename Method,Pull Up Method,Extract Method,","Any objects Introduce the concept of Any object, e.g. to extend the provisioning engine to support general-purpose definable entities, besides current users and groups (see this page about realms to understand why former roles were renamed to groups).

With this feature onboard, Syncope will be suitable for managing printers, services, or any other ""thing"" (as in Internet-Of-Things).

Details at https://cwiki.apache.org/confluence/display/SYNCOPE/%5BDISCUSS%5D+Any+objects","Duplicated Code, Long Method, , Duplicated Code, "
"Move Class,Rename Method,","Any objects Introduce the concept of Any object, e.g. to extend the provisioning engine to support general-purpose definable entities, besides current users and groups (see this page about realms to understand why former roles were renamed to groups).

With this feature onboard, Syncope will be suitable for managing printers, services, or any other ""thing"" (as in Internet-Of-Things).

Details at https://cwiki.apache.org/confluence/display/SYNCOPE/%5BDISCUSS%5D+Any+objects",", "
"Rename Class,Rename Method,Push Down Method,Move Method,Extract Method,Inline Method,Push Down Attribute,Move Attribute,","Any objects Introduce the concept of Any object, e.g. to extend the provisioning engine to support general-purpose definable entities, besides current users and groups (see this page about realms to understand why former roles were renamed to groups).

With this feature onboard, Syncope will be suitable for managing printers, services, or any other ""thing"" (as in Internet-Of-Things).

Details at https://cwiki.apache.org/confluence/display/SYNCOPE/%5BDISCUSS%5D+Any+objects","Duplicated Code, Long Method, , , , , , , "
"Rename Method,","Deferred tasks The current {{SchedTask}} / {{SyncTask}} / {{PushTask}} are immediately set for execution - with or without cron expression.
This aspect can be enhanced by adding an instant in the future when the given task will be scheduled from.",", "
"Rename Class,Move And Rename Class,Rename Method,Move Method,Inline Method,Move Attribute,",Custom Account / Password policy specifications Account and password policies are now fixed in their specifications: it is worth re-considering this aspect in order to allow deployment-based extensions and customization.,", , , , "
"Rename Class,Move And Rename Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","PATCH and PUT update for users, groups and any objects Currently {{AnyService}} (and its derivative, providing REST services for users, groups and any objects) defines the update method as follows:

{code}
@POST
@Path(""{key}"")
@Produces({ MediaType.APPLICATION_XML, MediaType.APPLICATION_JSON })
@Consumes({ MediaType.APPLICATION_XML, MediaType.APPLICATION_JSON })
Response update(@NotNull MOD anyMod);
{code}

where {{MOD extends AnyMod}}.

The idea is to move to a definition like as follows:

{code}
@PATCH
@Path(""{key}"")
@Produces({ MediaType.APPLICATION_XML, MediaType.APPLICATION_JSON })
@Consumes({ MediaType.APPLICATION_XML, MediaType.APPLICATION_JSON })
Response update(@NotNull P anyPatch);
{code}

e.g. to more REST-compliant patch-based update.
This has the additional benefit of simplifying the interaction with clients (JavaScript, in particular) not based on {{syncope-client}} Java library.

It could also be useful to add a second update method as follows:

{code}
@PUT
@Path(""{key}"")
@Produces({ MediaType.APPLICATION_XML, MediaType.APPLICATION_JSON })
@Consumes({ MediaType.APPLICATION_XML, MediaType.APPLICATION_JSON })
Response update(@NotNull TO anyTO);
{code}

where {{TO extends AnyTO}}.

This latter would allow to build simpler create / update interactions for clients based on {{syncope-client}} Java library.","Duplicated Code, Long Method, , , , , "
"Rename Class,Extract Superclass,Rename Method,Push Down Method,Move Method,Extract Method,Inline Method,Push Down Attribute,","Allow to restrict task list Provide mean to filter the task list returned by {{TaskService}}: by external resource (for Push, Sync and Propagation tasks) or by external resource and user / group / any object (for Propagation tasks only).

Currently {{TaskService#list}} returns all the matching tasks of the given type.","Duplicated Code, Long Method, , , Duplicated Code, Large Class, , , , "
"Move Method,Extract Method,","Pluggable transformation for resource mapping items Currently, when internal attributes are transferred to external resources according to the related mapping, the actual values are simply copied; so, if one wants to transform such values during propagation, the only option available is a general-purpose [PropagationActions|https://cwiki.apache.org/confluence/display/SYNCOPE/PropagationActionsClass] class - or, similarily during synchronization, a general-purpose [SyncActions|https://cwiki.apache.org/confluence/display/SYNCOPE/SyncActionsClass] class.

Such approach, which is working anyway, might be overkill: an additional extension point can be defined to plug own value transformer(s), to optionally associate to mapping item(s).","Duplicated Code, Long Method, , , "
"Rename Class,Rename Method,","Documentation artifacts As [discussed|http://markmail.org/message/dpleneuzrfcsmq2r] in mailing list, setup the {{asciidoctor-maven-plugin}} in order to generate, alongside with the build, the project documentation in HTML and PDF.

The generated documentation will then be part of release artifacts and always up-to-date with current release.

Preliminary results available at http://syncope.apache.org/2.0.0-SNAPSHOT/docs/",", "
"Rename Class,Rename Method,Extract Method,","Virtual attributes management refactoring Change the way how virtual attributes are internally managed by introducing the concept of linking mapping and removing the need to explicitly assign virtual attributes to user, groups or any objects.

Discussion on wiki: https://cwiki.apache.org/confluence/display/SYNCOPE/%5BDISCUSS%5D+Virtual+attributes+management+refactoring","Duplicated Code, Long Method, , "
"Rename Class,Rename Method,","Add the possibility to override the capabilities of the connector Currently, in the resource configuration you can override only the connector instance properties. It would be useful to extend this behaviour to the connector capabilities.",", "
"Rename Method,","Add the possibility to override the capabilities of the connector Currently, in the resource configuration you can override only the connector instance properties. It would be useful to extend this behaviour to the connector capabilities.",", "
"Rename Method,Move Method,","Derived attributes management refactoring Similarly to SYNCOPE-709 with virtual, change the way how derived attributes are handled; in particular

# derived attributes are not assigned any more to users (groups, any objects) but, as long as an user (group, any object) is assigned the AnyTypeClass of which the derived schema is part, it is also assigned the related derived attribute
# derived attributes can only be mapped for propagation, not synchronization",", , "
"Rename Class,Rename Method,Extract Method,Move Attribute,","Fine-grained entitlements for any objects Current entitlements for any objects are managed as for users and groups; as a result, if the {{ANY_OBJECT_CREATE}} entitlement is owned, one can create instances of all any types defined.

We need to change this to a more fine-grained management so that entitlements are defined for each any type defined: no more {{ANY_OBJECT_CREATE}} but rather {{PRINTER_CREATE}}, {{FOLDER_CREATE}} and so on.","Duplicated Code, Long Method, , , "
"Rename Method,","Filtered reconciliation for synchronization As [discussed in mailing list|http://markmail.org/message/ck64v6jy4nseau2s], a nice improvement for synchronization would be to allow definition of filtered reconciliation, allowing to synchronize only certain entities from an external resource.

At the moment synchronization is allowed as:
# full reconciliation (relying on ConnId's {{search()}}, without any filter)
# active synchronization (relying on ConnId's {{sync()}})

By leveraging ConnId's {{search()}} with appropriate filter, filtered reconciliation can be obtained.",", "
"Rename Method,","Statistics Simple and inexpensive numbers (memory usage, users count, resources count, ...) can be provided by default.
Additional metrics, when enabled, can be collected on purpose.

The idea is to let the admin console display such numbers in the dashboard, as graphs.

Usage of JMX may be worth.",
"Rename Class,Move And Rename Class,Rename Method,","Statistics Simple and inexpensive numbers (memory usage, users count, resources count, ...) can be provided by default.
Additional metrics, when enabled, can be collected on purpose.

The idea is to let the admin console display such numbers in the dashboard, as graphs.

Usage of JMX may be worth.",
"Rename Method,","Allow dynamic reloading of report stylesheets As with SYNCOPE-760 for mail templates, manage report's and reportlet's XSLT / XSL-FO content as entities, do not use the filesystem.",
"Move Class,Rename Class,Move And Rename Class,Extract Interface,Rename Method,Move Method,Move Attribute,","Rename Sync to Pull Rename all Sync* things (throughout the whole system) to Pull, for a couple of reasons:

# the word ""synchronization"" evokes some kind of bidirectional flow, while instead we intend pulling users, groups and any objects from external resources by relying on either {{search()}} or {{sync()}} ConnId operations
# we are already using ""push"" for the opposite operation

Moreover, it would be good, once changed all references, to include a ""synchronization use case"" in the documentation from SYNCOPE-700, which involves creating a pull task with assign matching request so that, for example, users pulled from LDAP will also be automatically assigned the LDAP resource.",", , , Large Class, "
"Rename Class,Extract Interface,Rename Method,Extract Method,",Allow user / group / any object admin form customization In Syncope 1.2 there used to be the possibility to determine which fields to show (and in which order) by configuring from admin console.,"Duplicated Code, Long Method, , Large Class, "
"Rename Method,Extract Method,","Validate ""standalone"" resource provisioning A lot of the resources configured in the ""standalone"" distribution appear to be missing internal attributes in the provisioning mappings. For example, in ""resource-ldap"" no internal attribute is specified for the mapping to ""postal"". So if you click ""Next"" on the mapping page you get an error.","Duplicated Code, Long Method, , "
"Rename Class,Move Method,","When editing realms, select account and password policies from combo box As done when editing an external resource (under ""Security""), allow selecting account and password policies from combo box, when editing realms.",", , "
"Extract Method,Pull Up Attribute,","Allow to optionally specify the MappingItemTransformer class, for each mapping item It is now possible, for each row of the mapping, for a given objectclass on a given resource, to specify an optional {{MappingItemTransformer}} which will take care of transforming attribute values during propagation and / or pull.

While this is possible via REST, the admin console does not allow it yet.","Duplicated Code, Long Method, , Duplicated Code, "
"Rename Method,","Replace Long autogenerated keys with UUIDs As [discussed in mailing list|http://markmail.org/message/fhdrwerdwdm3opdx], switch the JPA entities currently set for using Long autogenerated keys to UUIDs.",
"Rename Class,Extract Interface,Rename Method,Pull Up Method,Inline Method,","Replace Long autogenerated keys with UUIDs As [discussed in mailing list|http://markmail.org/message/fhdrwerdwdm3opdx], switch the JPA entities currently set for using Long autogenerated keys to UUIDs.",", , Duplicated Code, Large Class, "
"Move Method,Inline Method,","Push/Pull task ""names"" not marked as mandatory in the console When creating a push/pull task in the console, there is no ""*"" beside the ""name"" indicating it is mandatory. You can click ""Next"" to go to the next screen, but you only see an error on ""Finish"":

InvalidSchedTask [Standard: {javax.validation.constraints.NotNull.message}]

I think the Destination Realm + Pull Mode should also be marked mandatory for the pull task.",", , , "
"Extract Interface,Pull Up Method,",Allow to specify any templates and logic actions from realm Add the possibility to specify any templates and select logic actions from realm details.,", Duplicated Code, Large Class, "
"Rename Method,","Associate notification tasks to related notifications Currently the {{NotificationManager}} generates {{NotificationTask}} instances for all defined {{Notification}} instance, under the given conditions.

Once generated, however, there is no link any more between {{Notification}} and {{NotificationTask}}: such connection can be useful to trace from task to notification, to see when a given {{Notification}} became effective, and so on.",
"Rename Method,Move Attribute,","Allow to configure groups' type extensions Add the option, for a given group, to specify which additional classes are assigned to users or any objects with membership of such group (this is referenced as type extension).",", , "
"Rename Method,","Use gzip compression by default Since SYNCOPE-705 it is possible to optionally configure the client library to transparently support the GZIP content-encoding.

Enable this by default for all client applications (console, enduser, cli).",
"Rename Method,Extract Method,","Synchronization token management enhancement in case of errors The overall logic of the synchronization (1.2) / incremental pull (2.0) process can be summarized as:

# invoke underlying connector's {{getLatestSyncToken}}
# invoke underlying connector's {{sync}}
# store the value for {{getLatestSyncToken}} for subsequent invocation

As a consequence, when one synchronizing item ({{SyncDelta}}) generates an error, the whole process might be interrupted without saving the updated sync token, and next invocation will start again from the beginning, without considering the successful items passed before the error occurred.

The process should be changed as follows, instead:

# invoke underlying connector's {{getLatestSyncToken}}
# invoke underlying connector's {{sync}}: for each {{SyncDelta}}, temporary store the related {{syncToken}} field;
# store the last successful item's {{syncToken}} value - or {{getLatestSyncToken}} if no error occurred - for subsequent invocation
","Duplicated Code, Long Method, , "
"Rename Method,Inline Method,","Synchronization token management enhancement in case of errors The overall logic of the synchronization (1.2) / incremental pull (2.0) process can be summarized as:

# invoke underlying connector's {{getLatestSyncToken}}
# invoke underlying connector's {{sync}}
# store the value for {{getLatestSyncToken}} for subsequent invocation

As a consequence, when one synchronizing item ({{SyncDelta}}) generates an error, the whole process might be interrupted without saving the updated sync token, and next invocation will start again from the beginning, without considering the successful items passed before the error occurred.

The process should be changed as follows, instead:

# invoke underlying connector's {{getLatestSyncToken}}
# invoke underlying connector's {{sync}}: for each {{SyncDelta}}, temporary store the related {{syncToken}} field;
# store the last successful item's {{syncToken}} value - or {{getLatestSyncToken}} if no error occurred - for subsequent invocation
",", , "
"Move And Rename Class,Rename Method,","JEXL-based transformation for mapping items Mapping item transformers can be currently specified; it would be handy to have the chance to provide JEXL-based expressions to apply to values before propagating and after pulling.

Also, mapping item transformers should be provided with more context about the ongoing operation (at the moment only the value list is passed).",
"Extract Interface,Rename Method,Pull Up Method,Push Down Method,Extract Method,Push Down Attribute,","Membership and type extension improvements The current state of group membership / type extension does not allow to implement a scenario like as the one in the attached picture, e.g.:

Let's have user U assigned to groups G1, G2 and G3, all including, in their type extensions, the attribute A.
User U will have three different values for A - A1 for G1, A2 for G2 and A3 for G3.","Duplicated Code, Long Method, , , Duplicated Code, , Large Class, "
"Move Class,Extract Method,","Membership and type extension improvements The current state of group membership / type extension does not allow to implement a scenario like as the one in the attached picture, e.g.:

Let's have user U assigned to groups G1, G2 and G3, all including, in their type extensions, the attribute A.
User U will have three different values for A - A1 for G1, A2 for G2 and A3 for G3.","Duplicated Code, Long Method, , "
"Rename Class,Move Method,Extract Method,Move Attribute,","Membership and type extension improvements The current state of group membership / type extension does not allow to implement a scenario like as the one in the attached picture, e.g.:

Let's have user U assigned to groups G1, G2 and G3, all including, in their type extensions, the attribute A.
User U will have three different values for A - A1 for G1, A2 for G2 and A3 for G3.","Duplicated Code, Long Method, , , , "
"Move Method,Move Attribute,","Remove list() methods from User, Group and AnyObject REST APIs {{UserService}}, {{GroupService}} and {{AnyObjectService}} provides both {{list()}} and {{search()}} endpoints, where the former looks now quite as duplicated of the latter.

As improvement we can remove {{list()}} and adapt the remaining code to work with {{search()}}, with very small changes.",", , , "
"Rename Class,Rename Method,Move Method,Extract Method,Move Attribute,","Realm provisioning When realms were [introduced|https://cwiki.apache.org/confluence/display/SYNCOPE/%5BDISCUSS%5D+Realms], the main purpose was to set a simpler and neat approach to the internal security model.

There are, though, at least some popular ConnId connectors (LDAP, Active Directory, GoogleApps, ...) bearing the concept of ""organizational unit"", as user / group / other container.

Extending realms capabilities to match such concept will further improve the Syncope capabilities to handle complex scenarios.","Duplicated Code, Long Method, , , , "
"Rename Method,Extract Method,","Realm provisioning When realms were [introduced|https://cwiki.apache.org/confluence/display/SYNCOPE/%5BDISCUSS%5D+Realms], the main purpose was to set a simpler and neat approach to the internal security model.

There are, though, at least some popular ConnId connectors (LDAP, Active Directory, GoogleApps, ...) bearing the concept of ""organizational unit"", as user / group / other container.

Extending realms capabilities to match such concept will further improve the Syncope capabilities to handle complex scenarios.","Duplicated Code, Long Method, , "
"Rename Method,","Realm provisioning When realms were [introduced|https://cwiki.apache.org/confluence/display/SYNCOPE/%5BDISCUSS%5D+Realms], the main purpose was to set a simpler and neat approach to the internal security model.

There are, though, at least some popular ConnId connectors (LDAP, Active Directory, GoogleApps, ...) bearing the concept of ""organizational unit"", as user / group / other container.

Extending realms capabilities to match such concept will further improve the Syncope capabilities to handle complex scenarios.",
"Rename Method,","Identity Recertification Identify Re-certification is required for many national and international standards like SOX, GxP, etc.

The idea is to implement one scheduled task that filter users basing on some attributes (example: last recertification date, role,...) and move then on one state ""to be certified"" and assign one task to some group that has the responsibility to recertified the user, or delete it from the system.

Some report should report evidence about when the users have been recertified and who was the certifier.

This feature would be also the starting point to create account, role and groups re-certifications.",
"Rename Method,","Introduce new Camel propagation component This task is to introduce a new Camel propagation component, instead of having Camel Processor instances handle each individual use-case. It'll make the routes a bit more compact and easier to read.",
"Move And Rename Class,Move Method,","Introduce new Camel propagation component This task is to introduce a new Camel propagation component, instead of having Camel Processor instances handle each individual use-case. It'll make the routes a bit more compact and easier to read.",", "
"Move And Rename Class,",Allow domain selection from Swagger UI Currently it is not possible to select one of defined domains (hence {{Master}} is assumed) when performing any of the REST calls via Swagger UI.,
"Rename Method,","Use Java 8 language features As Java 8 is now the minimum requirement for 2.1, all the codebase should be scanned for making profit of Java 8 language features - which might eventually lead to remove {{commons-lang3}} / {{commons-collections4}} / ...",
"Rename Method,Inline Method,","Optionally provide schema information with attribute values When searching for users, groups or any objects via REST, there is currently the option to specify the {{details}} flag:
* when {{false}} only (user)name, plain and derived attributes are returned
* when {{true}} also virtual attributes, roles, relationships and memberships are included

Moreover, [attributes|https://github.com/apache/syncope/blob/2_0_X/common/lib/src/main/java/org/apache/syncope/common/lib/to/AttrTO.java#L86] are returned alongside with the sole ""meta"" information about the related schema being defined as read-only.

The {{details}} flag above can be used to additionally indicate, when true, that the full information about the attributes' schema is to be returned, together with value(s).",", "
"Rename Class,Move And Rename Class,Rename Method,Extract Method,","Allow for scripted customizations The core can be customized in [several ways|http://syncope.apache.org/docs/reference-guide.html#customization-core]; all customizations require to be written as Java classes - which is generally good but requires redeploy to be made effective, unless some class reloading mechanism is in place (as JRebel).

By leveraging Groovy, we could overcome such limitation and allow to write customizations which can be immediately available for execution at runtime.

Once implemented in the core, such feature will require editing capabilities to be added to console and IDE plugins.","Duplicated Code, Long Method, , "
"Rename Method,","Allow for scripted customizations The core can be customized in [several ways|http://syncope.apache.org/docs/reference-guide.html#customization-core]; all customizations require to be written as Java classes - which is generally good but requires redeploy to be made effective, unless some class reloading mechanism is in place (as JRebel).

By leveraging Groovy, we could overcome such limitation and allow to write customizations which can be immediately available for execution at runtime.

Once implemented in the core, such feature will require editing capabilities to be added to console and IDE plugins.",
"Move Method,Extract Method,","Allow for scripted customizations The core can be customized in [several ways|http://syncope.apache.org/docs/reference-guide.html#customization-core]; all customizations require to be written as Java classes - which is generally good but requires redeploy to be made effective, unless some class reloading mechanism is in place (as JRebel).

By leveraging Groovy, we could overcome such limitation and allow to write customizations which can be immediately available for execution at runtime.

Once implemented in the core, such feature will require editing capabilities to be added to console and IDE plugins.","Duplicated Code, Long Method, , , "
"Move Method,Inline Method,Move Attribute,","Allow for scripted customizations The core can be customized in [several ways|http://syncope.apache.org/docs/reference-guide.html#customization-core]; all customizations require to be written as Java classes - which is generally good but requires redeploy to be made effective, unless some class reloading mechanism is in place (as JRebel).

By leveraging Groovy, we could overcome such limitation and allow to write customizations which can be immediately available for execution at runtime.

Once implemented in the core, such feature will require editing capabilities to be added to console and IDE plugins.",", , , "
"Rename Class,Rename Method,Move Method,Extract Method,Inline Method,Move Attribute,","Allow for scripted customizations The core can be customized in [several ways|http://syncope.apache.org/docs/reference-guide.html#customization-core]; all customizations require to be written as Java classes - which is generally good but requires redeploy to be made effective, unless some class reloading mechanism is in place (as JRebel).

By leveraging Groovy, we could overcome such limitation and allow to write customizations which can be immediately available for execution at runtime.

Once implemented in the core, such feature will require editing capabilities to be added to console and IDE plugins.","Duplicated Code, Long Method, , , , , "